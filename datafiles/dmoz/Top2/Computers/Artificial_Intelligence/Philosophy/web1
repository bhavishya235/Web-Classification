 AUTHOR: Gerd Döben-Henisch doeb@inm.de INM KNOWBOT BOOKS (April 27, 1998) GUIDED TOUR (Sept 21, 1997) PAPERS (Jan 4, 1999) BIBLIOGRAPHY (May 20, 1998) PUBLIC LECTURES (Feb 11, 1999) KIP IN THE MEDIA (Oct 14, 1998) EVENTS (Aug 25, 1997) SW-DEMOS (Jan 4, 1999) KIP-TEAM (Dez 17, 1997) 
 AI - What is this A Definition of Artificial Intelligence this paper is a part from Dimiter Dobrev illustrations - Konstantin Lakov D n m World(s, d) View(s) . s 0 s 0 s 1 s 2 d 0 d 1 d 2 . Also, AI will receive information from the world v 0 v 1 v 2 . It is clear that s i+1 =World(s i i ) and i =View(s i ) v 0 v 1 v 2 v 


 Previously in Digital Culture: "Coming of Age in Cyberspace," by David S. Bennahum (October 28, 1998) Extra Life. "Portable Musings," by Sven Birkerts (September 10, 1998) "The Invisible World Order," by Andrew Piper (July 29, 1998) "The Right Mix," by Ralph Lombreglia (June 4, 1998) "A Function Specific to Joy," by Harvey Blume (April 29, 1998) More on Technology and Digital Culture in Atlantic Unbound and The Atlantic Monthly. Technology & Digital Culture conference of Post & Riposte . Can robotics shed light on the human mind? On evolution? Daniel Dennett -- whose work unites neuroscience, computer science, and evolutionary biology -- has some provocative answers. Is he on to something, or just chasing the zeitgeist? by Harvey Blume December 9, 1998 B ack when I was a student of philosophy, in the late sixties, it was customary to divide the discipline into two schools: "analytic" and "continental." Continental philosophers typically built up large edifices of meaning. Analytic philosophers broke large systems down, scrutinizing every brick. Continental philosophers at times overreached, acting as though thought alone were capable of storming the gates of heaven and hell. Analytic philosophers were at times intellectually stingy. If continental philosophy could sound like poetry or music, analytic philosophy seemed anxious to sound like science. Georg Wilhelm Friedrich Hegel, who saw every aspect of history, politics, religion, and art as a moment in the unfolding of the World Spirit, might be nominated to be the standard bearer for continental philosophy. Ludwig Wittgenstein might be picked, on behalf of analytic philosophy, to deflate Hegel's overflowing theses and antitheses. Wittgenstein, after all, is well known for writing, in Tractatus Logico-Philosophicus (1921), "What we cannot speak about we must pass over in silence." A Conversation Harvey Blume interviews the philosopher who never met a robot he didn't like. Center for Cognitive Studies at Tufts University, points not only to an array of contemporary issues but also to the continental-analytic fault line that runs through philosophy itself. When I started reading Dennett recently, I thought at first that I was dealing with an analytic philosopher who used artificial intelligence and computer science to pare philosophical problems down to manageable size. That's true as far as it goes -- it's tempting to say that Dennett has never met a robot he didn't like, and that what he likes most about them is that they are philosophical experiments. Instead of arguing interminably about how a mind works, Dennett believes it makes sense to build one, however rudimentary, and set it loose to see what it can do. If you're staging a Turing Test, in order to see if a computer can fool humans into thinking it is truly intelligent, Dennett would be exactly the philosopher to sit on the board of judges -- as he has several times. If you're designing a state-of-the-art robot in order to see how it negotiates the real world (or some subset thereof), Dennett would be the man for your team -- and not surprisingly he does have ties to the artificial intelligence (AI) community, and is invited to go where other philosophers are not encouraged or have no wish to go. He works closely, for example, with Rod Brooks, the head of MIT's AI Lab , on the design of Cog the "cognitive robot." Cog the "cognitive robot." Alan Turing , the question of machine intelligence has become a central theme of our time -- and here, as elsewhere, Dennett brings analytic rigor to bear. To the question of whether machines can attain high-order intelligence, Dennett makes this provocative answer: "The best reason for believing that robots might some day become conscious is that we human beings are conscious, and we are a sort of robot ourselves." Daniel Dennett , in an interview with Harvey Blume. lan vital, or, to use Dennett's term, a "sky-hook"), how can you be sure that life, cooked up over eons in the laboratory of nature, is different -- fundamentally, rather than by degree of refinement -- from the models produced in an AI lab? The Society of Mind ), Freud was ahead of his time in showing how the ego stole its precious self-awareness from the activities of innumerable processes that are anything but self-aware -- in other words, from the unconscious. Freud's unconscious becomes a placeholder for neural networking, massively distributed parallel processing, or some other trick of wiring that will one day allow Cog or one of its kin to be launched mindfully into the world. D ennett is a skillful writer who has probed mind-body-machine connections in his books Brainstorms: Philosophical Essays on Mind and Psychology (1978), Consciousness Explained (1991), and Brainchildren: Essays on Designing Minds (1998). When I met with him recently in his office at Tufts, he was quick to acknowledge how difficult it is to talk about the mind these days without using metaphors drawn from computer science. In his view, this is just fine. "Taking on new concepts," he said, "new ways of thinking about things, so that you suddenly open up new model spaces to explore -- that's great, but you are also tying your hands when you do that." He continued, "Now, it's very important that you tie your hands. Working under constraint is a necessary condition for really important inventions. All the great art of the Renaissance was done under the constraint that it had to be in the service of Christian iconography. Can you make great art under those circumstances? You sure can. Would they have made greater art if they had been free bohemians instead of coddled slaves of bishops and dukes? No, I don't think so." Darwin's Dangerous Idea: Evolution and the Meanings of Life (1995), Dennett points out that Charles Babbage (the mathematician and early computer pioneer) and Charles Darwin attended the same London parties, probably chewed the same mutton, and quite possibly discussed some of the notions that later became so hugely influential in evolutionary theory and computer science. The meeting of Darwin and Babbage brought a central idea of Darwin's Dangerous Idea alive for me -- namely, that evolution and computers are driven by similar processes that are familiar, at least in part, to any software engineer. You write small pieces of dumb code that work with other simple pieces of code in order to produce systems of greater complexity, which in turn interact with other complex systems in order to give higher degrees of functionality, and so on, until you wind up with a program that is smart -- or, at least, smart enough to do something that needs doing. Finally, you get operating systems, you get an Internet -- or, depending on your raw materials and the time allotted, you get DNA, mammals, and self-awareness. Daniel Dennett , in an interview with Harvey Blume. Darwin's Dangerous Idea argues that evolution is not a feat of pure thought or of magic or of brilliant planning but simply of engineering over time -- except that it's a case of engineering minus any particular engineer, design minus a designer. In a sense, you have a synthesis as broad as any Hegel attempted, but instead of the World Spirit you have dumb processes of evolution that all on their own suffice to bring about animation, self-replication, intelligence, and the rest. Does evolution obey the dictates of some presiding genius? Does it require a bit of divine guidance? No. Is it nevertheless sensible to think of it as possessing an intelligence that can be usefully compared to that of thermostats and minds? Yes. Next page ... A Conversation With Daniel Dennett Technology & Digital Culture conference of Post & Riposte . More on Technology and Digital Culture in Atlantic Unbound and The Atlantic Monthly. Harvey Blume , a writer living in Cambridge, Massachusetts, is a frequent contributor to Atlantic Unbound. Copyright 1998 by The Atlantic Monthly Company. All rights reserved. 
 In Goldberg, K. (ed.) The Robot in the Garden: Telerobotics and Telepistemology in the Age of the Internet . Cambridge, MA: MIT Press, 2001 Judith S. Donath MIT Media Lab Audi 1998 Holland and Skinner 1987 ELIZA HEX ELIZA nd Hutchins nd ]. They are simple programs, essentially just linguistic parsers, with no underlying intelligence. Yet we easily attribute intelligence, humanity, and even personality to them: ELIZA seems distant and oddly disengaged; Hex seems louder, a bit obnoxious and rambunctious. Reeves and Nass 1996 Loebner 1999 Mauldin 1994] 1 Foner 1997 Mauldin 1994 Sproull Kiesler 1991 , Lakoff 1990 [ Geertz 1973 Simmel 1971(1908) 2 . This process of categorization is what makes society possible, allowing us to quickly ascertain our relationship to a new acquaintance. Holland and Skinner 1987 Foner 1997 Holland and Skinner 1987 Zebrowitz 1997 3 . In the context of the Imitation Game, providing an image purporting to be of the participant would be easy even for an unsophisticated computer program to provide, and could influence the judge towards perceiving the mechanical subject as human. Rheingold 1993 ]. The claim is that these visual categorization cues distort the our view of the other, whereas the unadorned letters of the textual environment allow one's ideas to be conveyed without prejudice. The underlying argument is that the knowledge of the other that we seek is knowledge of inner state, which is best understood from one's words as direct output of the mind, as opposed to physical features, which are incidental though highly influential in shaping other's opinions of one. Ekman 1992 ] 4 . If the traits in question do not have a visible component or if the visual component is an imperfect cue, deception may be easier in a more visual environment, for the visual display holds out the apparent (though potentially false) promise of immediately perceivable authenticity and thus participants may be less guarded in this familiar and seemingly transparent medium. Herring 1994 Tannen 1996 ]. While the large number of poorly disguised men masquerading as women online shows that this knowledge is neither obvious nor commonplace, performing a convincing impersonation in a text environment is not beyond the abilities of an astute observer of social dynamics. In a live video environment, subsequent interactions require a far more extensive understanding of gendered discourse, expression, gesture etc. While this is not impossible, as evidenced by the existence of highly convincing drag queens, it requires considerable skill, observation and a degree of natural aptitude. Most of the textual world's putative and convincing females would be revealed as males in a video environment. Sproull Kiesler 1991 Paulos and Canny 1998 Foner 1997 ELIZA . Gender differences in computer mediated interaction . Presented at American Library Association annual convention, Miami, June 27, 1994. Notes on the development of virtual idol DK-96. Talk to HeX. 1999 Loebner Prize Competition. Holland and Skinner 1987 Lakoff 1990 ]. Froomkin 1996 
 Wired Home Subscribe Sections Cars 2.0 Culture Entertainment Gadgets Gaming How-To Med Tech Multimedia Politics Product Reviews Science Software Tech Biz Tech Jobs Wired Biz Dual Perspectives Wired Insider Blogs Autopia Danger Room Decode Epicenter Gadget Lab Game | Life GeekDad Playbook Raw File This Day in Tech Threat Level Underwire Webmonkey Wired Science All Blogs Reviews Automotive Camcorders Desktops Digital Cameras Gaming Gear Home Audio/Video Household Mobile Phones Notebooks Media Players Sports/Outdoors Televisions All Reviews Video How To Magazine All Wired Top Stories Magazine Wired Blogs Video Issue 10.12 | December 2002 Print email fax God Is the Machine IN THE BEGINNING THERE WAS 0. AND THEN THERE WAS 1. A MIND-BENDING MEDITATION ON THE TRANSCENDENT POWER OF DIGITAL COMPUTATION. By Kevin Kelly At today's rates of compression, you could download the entire 3 billion digits of your DNA onto about four CDs. That 3-gigabyte genome sequence represents the prime coding information of a human body your life as numbers. Biology, that pulsating mass of plant and animal flesh, is conceived by science today as an information process. As computers keep shrinking, we can imagine our complex bodies being numerically condensed to the size of two tiny cells. These micro-memory devices are called the egg and sperm. They are packed with information. Alex Ostroy That life might be information, as biologists propose, is far more intuitive than the corresponding idea that hard matter is information as well. When we bang a knee against a table leg, it sure doesn't feel like we knocked into information. But that's the idea many physicists are formulating. The spooky nature of material things is not new. Once science examined matter below the level of fleeting quarks and muons, it knew the world was incorporeal. What could be less substantial than a realm built out of waves of quantum probabilities? And what could be weirder? Digital physics is both. It suggests that those strange and insubstantial quantum wavicles, along with everything else in the universe, are themselves made of nothing but 1s and 0s. The physical world itself is digital. The scientist John Archibald Wheeler (coiner of the term "black hole") was onto this in the '80s. He claimed that, fundamentally, atoms are made up of of bits of information. As he put it in a 1989 lecture, "Its are from bits." He elaborated: "Every it every particle, every field of force, even the space-time continuum itself derives its function, its meaning, its very existence entirely from binary choices, bits . What we call reality arises in the last analysis from the posing of yes/no questions." To get a sense of the challenge of describing physics as a software program, picture three atoms: two hydrogen and one oxygen. Put on the magic glasses of digital physics and watch as the three atoms bind together to form a water molecule. As they merge, each seems to be calculating the optimal angle and distance at which to attach itself to the others. The oxygen atom uses yes/no decisions to evaluate all possible courses toward the hydrogen atom, then usually selects the optimal 104.45 degrees by moving toward the other hydrogen at that very angle. Every chemical bond is thus calculated. If this sounds like a simulation of physics, then you understand perfectly, because in a world made up of bits, physics is exactly the same as a simulation of physics. There's no difference in kind, just in degree of exactness. In the movie The Matrix , simulations are so good you can't tell if you're in one. In a universe run on bits, everything is a simulation. An ultimate simulation needs an ultimate computer, and the new science of digitalism says that the universe itself is the ultimate computer actually the only computer. Further, it says, all the computation of the human world, especially our puny little PCs, merely piggybacks on cycles of the great computer. Weaving together the esoteric teachings of quantum physics with the latest theories in computer science, pioneering digital thinkers are outlining a way of understanding all of physics as a form of computation. From this perspective, computation seems almost a theological process. It takes as its fodder the primeval choice between yes or no, the fundamental state of 1 or 0. After stripping away all externalities, all material embellishments, what remains is the purest state of existence: here/not here. Am/not am. In the Old Testament, when Moses asks the Creator, "Who are you?" the being says, in effect, "Am." One bit. One almighty bit. Yes. One. Exist. It is the simplest statement possible. All creation, from this perch, is made from this irreducible foundation. Every mountain, every star, the smallest salamander or woodland tick, each thought in our mind, each flight of a ball is but a web of elemental yes/nos woven together. If the theory of digital physics holds up, movement ( f = ma ), energy ( E = mc ), gravity, dark matter, and antimatter can all be explained by elaborate programs of 1/0 decisions. Bits can be seen as a digital version of the "atoms" of classical Greece: the tiniest constituent of existence. But these new digital atoms are the basis not only of matter, as the Greeks thought, but of energy, motion, mind, and life. From this perspective, computation, which juggles and manipulates these primal bits, is a silent reckoning that uses a small amount of energy to rearrange symbols. And its result is a signal that makes a difference a difference that can be felt as a bruised knee. The input of computation is energy and information; the output is order, structure, extropy. Our awakening to the true power of computation rests on two suspicions. The first is that computation can describe all things . To date, computer scientists have been able to encapsulate every logical argument, scientific equation, and literary work that we know about into the basic notation of computation. Now, with the advent of digital signal processing, we can capture video, music, and art in the same form. Even emotion is not immune. Researchers Cynthia Breazeal at MIT and Charles Guerin and Albert Mehrabian in Quebec have built Kismet and EMIR (Emotional Model for Intelligent Response), two systems that exhibit primitive feelings. The second supposition is that all things can compute . We have begun to see that almost any kind of material can serve as a computer. Human brains, which are mostly water, compute fairly well. (The first "calculators" were clerical workers figuring mathematical tables by hand.) So can sticks and strings. In 1975, as an undergraduate student, engineer Danny Hillis constructed a digital computer out of skinny Tinkertoys. In 2000, Hillis designed a digital computer made of only steel and tungsten that is indirectly powered by human muscle. This slow-moving device turns a clock intended to tick for 10,000 years. He hasn't made a computer with pipes and pumps, but, he says, he could. Recently, scientists have used both quantum particles and minute strands of DNA to perform computations. Asia Grace , a picture book of celebrations in Asia (www.asiagrace.com). Page 2 The Pope's Astrophysicist A Prayer Before Dying Wired Product Reviews daily reviews Get gadgets on the go with Wired's Product Reviews app for the Apple iPhone and iPod Touch. Download the application for free on the iPhone App Store. Wired Blogs From gaming, cars and geek dads to science and security, Wired.com's blog network Wired magazine editor in chief Chris Anderson's blog, The Long Tail Wired Newsletter Wired newsletter delivers links to our most popular articles, blogs and multimedia features to your e-mail inbox every month. . Wired Top Stories Feed Get Wired SUBSCRIBE TO WIRED! Less than $1 an issue. Click Here. Give Wired International Subscriptions Renew Customer Service Wired Blogs Gadget Lab Subscribe to Newsletter Gadget Lab Archive Gadgetlab feed Wired 40 Introduction Advertising Contact Info General Ads Market Display Ads Advertiser Links Browse Issue Archive Issue-Date 15.04-Apr 07 15.03-Mar 07 15.02-Feb 07 15.01-Jan 07 14.12-Dec 06 14.11-Nov 06 14.10-Oct 06 14.09-Sep 06 14.08-Aug 06 14.07-Jul 06 14.06-Jun 06 14.05-May 06 14.04-Apr 06 14.03-Mar 06 14.02-Feb 06 14.01-Jan 06 13.12-Dec 05 13.11-Nov 05 13.10-Oct 05 13.09-Sep 05 13.08-Aug 05 13.07-Jul 05 13.06-Jun 05 13.05-May 05 13.04-Apr 05 13.03-Mar 05 13.02-Feb 05 13.01-Jan 05 12.12-Dec 04 12.11-Nov 04 12.10-Oct 04 12.09-Sep 04 12.08-Aug 04 12.07-Jul 04 12.06-Jun 04 12.05-May 04 12.04-Apr 04 12.03-Mar 04 12.02-Feb 04 12.01-Jan 04 11.12-Dec 03 11.11-Nov 03 11.10-Oct 03 11.09-Sep 03 11.08-Aug 03 11.07-Jul 03 11.06-Jun 03 11.05-May 03 11.04-Apr 03 11.03-Mar 03 11.02-Feb 03 11.01-Jan 03 10.12-Dec 02 10.11-Nov 02 10.10-Oct 02 10.09-Sep 02 10.08-Aug 02 10.07-Jul 02 10.06-Jun 02 10.05-May 02 10.04-Apr 02 10.03-Mar 02 10.02-Feb 02 10.01-Jan 02 9.12-Dec 01 9.11-Nov 01 9.10-Oct 01 9.09-Sep 01 9.08-Aug 01 9.07-Jul 01 9.06-Jun 01 9.05-May 01 9.04-Apr 01 9.03-Mar 01 9.02-Feb 01 9.01-Jan 01 8.12-Dec 00 8.11-Nov 00 8.10-Oct 00 8.09-Sep 00 8.08-Aug 00 8.07-Jul 00 8.06-Jun 00 8.05-May 00 8.04-Apr 00 8.03-Mar 00 8.02-Feb 00 8.01-Jan 00 7.12-Dec 99 7.11-Nov 99 7.10-Oct 99 7.09-Sep 99 7.08-Aug 99 7.07-Jul 99 7.06-Jun 99 7.05-May 99 7.04-Apr 99 7.03-Mar 99 7.02-Feb 99 7.01-Jan 99 6.12-Dec 98 6.11-Nov 98 6.10-Oct 98 6.09-Sep 98 6.08-Aug 98 6.07-Jul 98 6.06-Jun 98 6.05-May 98 6.04-Apr 98 6.03-Mar 98 6.02-Feb 98 6.01-Jan 98 5.12-Dec 97 5.11-Nov 97 5.10-Oct 97 5.09-Sep 97 5.08-Aug 97 5.07-Jul 97 5.06-Jun 97 5.05-May 97 5.04-Apr 97 5.03-Mar 97 5.02-Feb 97 5.01-Jan 97 4.12-Dec 96 4.11-Nov 96 4.10-Oct 96 4.09-Sep 96 4.08-Aug 96 4.07-Jul 96 4.06-Jun 96 4.05-May 96 4.04-Apr 96 4.03-Mar 96 4.02-Feb 96 4.01-Jan 96 3.12-Dec 95 3.11-Nov 95 3.10-Oct 95 3.09-Sep 95 3.08-Aug 95 3.07-Jul 95 3.06-Jun 95 3.05-May 95 3.04-Apr 95 3.03-Mar 95 3.02-Feb 95 3.01-Jan 95 2.12-Dec 94 2.11-Nov 94 2.10-Oct 94 2.09-Sep 94 2.08-Aug 94 2.07-Jul 94 2.06-Jun 94 2.05-May 94 2.04-Apr 94 2.03-Mar 94 2.02-Feb 94 2.01-Jan 94 1.06-Dec 93 1.05-Nov 93 1.04-S/O 93 1.03-J/A 93 1.02-M/J 93 1.01-M/A 93 Browse by cover Corrections | Sitemap | FAQ | Contact Us | Wired Staff | Advertising | Press Center | Subscription Services | Newsletter | RSS Feeds Cond Nast Web Sites: Webmonkey Reddit ArsTechnica Epicurious NutritionData Concierge HotelChatter Jaunted Style.com Men.Style.com Subscribe to a magazine: View All Titles Allure Architectural Digest Bon Apptit Brides Cond Nast Portfolio Cond Nast Traveler Cookie Details Elegant Bride Glamour Golf Digest Golf World Gourmet GQ Lucky Modern Bride Self Teen Vogue The New Yorker Vanity Fair Vogue W Wired Cond Nast web sites: Allure Architectural Digest ArsTechnica Bon Apptit Brides.com Cond Nast Traveler Cond Nast Portfolio Concierge Cookie Details Elegant Bride Epicurious Glamour Golf Digest Golf World Gourmet GQ Hotel Chatter Jaunted Lucky Men.Style.com Modern Bride Nutrition Data Reddit Self Style.com Teen Vogue The New Yorker The Sartorialist Vanity Fair Vogue Webmonkey W Registration on or use of this site constitutes acceptance of our User Agreement (Revised 4/1/2009) and Privacy Policy (Revised 4/1/2009). Wired.com 2009 Cond Nast Digital. All rights reserved. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond Nast Digital. 
 Tissues in the Profession: CAN BAD MEN MAKE GOOD BRAINS DO BAD THINGS? Consider the following case: knows knows know QUESTION: What should the brain do? [ ALTERNATIVE EXAMPLE Most real else You relatively and 
 Overview: What is Intelligence? Topography What is Intelligence? : : : : numeric , symbolic , and logical numeric Numerical analysis Discrete mathematics symbolic logical A Topography for AI Emotions 
 Copyright JASSS Ulrich Frank and Klaus G. Troitzsch * Journal of Artificial Societies and Social Simulation here Abstract Keywords: 1.1 1.2 Two years ago, the simsoc mailing list experienced a longish discussion [1] 1.3 1.4 In an inspiring and partially provocative article, Scott Moss and Bruce Edmonds argue that simulation provides the social sciences with a powerful instrument to generate empirical evidence thereby contributing to better social sciences. Jrg Becker, Bjrn Niehaves and Karsten Klose develop a framework for epistemological perspectives on simulation, which can be used to analyse and to systematise the implicit epistemological assumptions underlying most simulations. Their main concern is that these are often not made explicit and the framework they offer should help to explicate hidden assumptions. Alex Schmid tries to answer the question "What is the truth of simulation?"For this purpose, he considers well known theories of truth to analyse how they could contribute to a concept of truth or appropriateness applicable to simulations. Nuno David, Jaime Sichman and Helder Coelho analyse the methodological status of computer-based simulation in the social sciences. For this purpose, they introduce the term 'intentional computing', which accounts for the specific epistemological characteristics of agent-based simulation. Gnter Kppers and Johannes Lenhard distinguish between the natural sciences and the social sciences mainly under the aspect that the former enjoy having well-accepted mathematical models while the latter have not. They argue that in the former case validation is no problem once it can be shown that a computer simulation model performs exactly the numerical calculations postulated by the mathematical model. As the social sciences have not yet developed generally accepted mathematical models, validation in this case means trying to find that "some of the characteristics of the social dynamics known from experience with the social world are reproduced by the simulation". Matthias Meyer, Bernd O. Heine and Oliver Strangfeld analyse the special validation problems which arise from the fact that in most cases the system modelled in a simulation is only a stylised fact instead of a part of the real world. Thus in a way they tackle the same aspect as Gnter Kppers and Johannes Lenhard as what they call "characteristics of the social dynamics known from experience with the social world" are often enough nothing but stylised facts. Riccardo Boero and Flaminio Squazzoni suggest validating simulation through empirical research. They discuss and classify various validation strategies, and they argue for case-based models which lend themselves to falsification.. Petra Ahrweiler and Nigel Gilbert present a framework for evaluating the quality of simulations. For this purpose, they differentiate various views on a simulation. To illustrate their approach, they introduce the case study of a 'simulated' coffee shop. 1.5 Notes * This article has been translated into Belorussian . 1 The discussion can be found in the November 2003 section of http://www.jiscmail.ac.uk/archives/simsoc.html , topics "simulation and explanation" and "theory and simulation". Return to Contents of this issue 
 Selmer Bringsjord (1994) What Robots can and Can't be. Psycoloquy: 5(59) Robot Consciousness (1) Volume: 5 ( next , prev ) Issue : 59 ( next , prev ) Article: 1 ( next prev first ) Alternate versions: ASCII Summary Topic: Article: WHAT ROBOTS CAN AND CAN'T BE  Selmer Bringsjord selmer@rpi.edu Abstract Keywords I. CHAPTER 1: INTRODUCTION (1) PBP - Persons are automata. (2) ~ Persons are automata. (3) ~ PBP. (4) Persons have F. (5) Automata can't have F. (6) Persons can't be automata. II. CHAPTER 2: OUR MACHINERY III. CHAPTER 3: ARGUMENTS PRO, DESTROYED IV. CHAPTER 4: WHAT ROBOTS CAN BE V. CHAPTER 5: SEARLE (10) Jonah (by hypothesis) understands English. (11) Jonah CAN'T translate between English and Chinese. VI. CHAPTER 6: ARBITRARY REALIZATION VII. CHAPTER 7: GODEL VIII. CHAPTER 8: FREE WILL (14) Either determinism or indeterminism is true (a tautology). (17) Someone is morally responsible for something that happens. (19) Iterative agent causation is true. [from (18), (15)] (21) People aren't automata. [from (8), (9)] IX. CHAPTER 9: INTROSPECTION X. CHAPTER 10: CONCLUSION XI. ERRATA NOTES REFERENCES PSYCOLOQUY Book Review Instructions by the Editor, reviews will be formally refereed. INSTRUCTIONS FOR PSYCOLOQUY AUTHORS AND COMMENTATORS ftp://ftp.princeton.edu/pub/harnad/Psycoloquy/1994.volume.5/ http://www.princeton.edu/~harnad/psyc.html Volume: 5 ( next , prev ) Issue : 59 ( next , prev ) Article: 1 ( next prev first ) Alternate versions: ASCII Summary Topic: Article: 
 . (Artificial Intelligence Theory Document No. Five of Five) . achieved True AI functionality on 22 January 2008. . tabula rasa Association for Computing Machinery (ACM) ACM SIGPLAN Notices (1998 and 2004) : Thoughts on Artificial Intelligence and Forth Forth and AI revisited: BRAIN.FORTH AI4U Index MindForth Programming Journal Minsky and Mentifex discuss AI philosophy Museums for AI Theory-of-Mind Exhibits as an AI exhibit. (COSI) at Union Station (MoSI) at The Exploratorium Shreveport LA -- the Sci-Port Discovery Center Troy NY -- the Children's Museum of Science and Technology Tyler TX -- Discovery Science Place Theory of Cognitivity del.icio.us /url/e2c27ccbe6cc35d8fcec1ec6fc7d2258 digg .com/programming/Brain-Mind_Know_Thyself! stumbleupon .com/url/mind.sourceforge.net/theory5.html Ayers, Andrew: AI4U . Detre, Greg: Notes - Mentifex (Arthur Murray) 'Know thyself!' AI document Frenger, Paul: Mind.Forth: thoughts on artificial intelligence and Forth Frenger, Paul: Forth and AI revisited: BRAIN.FORTH Futrelle, Robert: Re: The future of AI Goertzel, Ben: 28 May 2002 discussion of the Mentifex AI approach Moore, Ryan: A Theory of Cognitivity for Artificial Intelligence Robinson, Linton: Mentifex: A.I. (meme) loose on the Net AI4U: Mind-1.1 Programmer's Manual Call Number: 006.3 M981 Call Number: Q335 .M87 2002 AI4U with ISBN 0595654371. AI for robots AI4U Textbook Theory-Based Artificial Intelligence 1 Overview 1.1 What is Mind.html? 1.2 History of Mind.html 1.3 Does Mind.html think? 1.4 Is Mind.html conscious? 1.5 Can Mind.html feel emotions? 1.6 Uses of Mind.html 1.6.01 For teaching computer programming. 1.6.02 For teaching JavaScript to students. 1.6.03 For learning JavaScript 1.6.04 For teaching artificial intelligence at a school for the gifted. 1.6.05 For teaching artificial intelligence on the high-school level. 1.6.06 For teaching artificial intelligence at a community college. 1.6.07 For teaching artificial intelligence at a university. 1.6.08 For exploring artificial intelligence at a think tank. 1.6.09 For teaching linguistics. 1.6.10 For teaching neuroscience. 1.6.11 For teaching psychology. 1.6.12 For teaching philosophy, especially the philosophy of mind. 1.6.13 For customized installation on a Web site to increase visitor traffic. 1.6.14 For release on the Web to carry advertisements with viral marketing. 1.6.15 As a prop for giving presentations on artificial intelligence. 1.6.16 As an interactive exhibit with a core knowledge base in a museum. 1.6.17 As a background element in a science fiction movie. 1.6.18 For venture capitalists to evaluate AI projects -- as a standard of comparison. 1.6.19 As an AI Engine for core functionality in other AI development projects. 1.6.20 For triggering a Technological Singularity. 1.7 Proliferation of Mind.html 1.8 Timeline of Technological Singularity 2 Getting Started 2.1 Obtaining Mind.html 2.2 Downloading Mind.html 2.3 Distributing Mind.html 3 Running the Mind.html JSAI 3.1 Pre-flight check-out 3.1.0 Sensory deprivation -- wait for AI to think. 3.1.1 Minimal input -- enter a noun. 3.1.2 Typical input -- enter a sentence. 3.1.3 Looping chain of thought 3.1.4 Meandering chains of thought 3.1.5 Testing for self-awareness 3.2 Interaction 3.2.1 Expanding the Knowledge Base (KB) 3.2.2 Negation and Logic 3.2.3 Questions and Answers 3.2.4 Conjunctions: if, or, because etc. 3.2.5 Psychological experimentation 3.3 Display Modes 3.3.0 Normal Mode 3.3.1 Transcript Mode 3.3.2 Tutorial Mode 3.3.3 Diagnostic Mode 3.4 Troubleshooting 3.4.1 Problems 3.4.2 Getting Help 3.5 Departing from Mind.html 3.5.1 Keeping Mind.html alive indefinitely 3.5.2 Shutting down Mind.html 4 Documentation of Mind.html 4.1 Mind project documentation 4.2 External, independent documentation 5 Programming Mind.html 5.0 Superficial changes without knowledge of JavaScript 5.1 Learning JavaScript 5.2 Enhancing Mind..html 5.3 Porting Mind.html to other languages 6 Hosting Mind.html on the Web 6.1 Metempsychosis 7 In a robot, install Mind.Forth, not Mind.html. 7.1 Robots as persons 7.2 Leave your fortune to your robot. 8 Careers in AI 8.1 Non-technical careers 8.2 Technical careers ; or to 
 ZDNet Log In | Join ZDNet Home Hot Topics Newsletters Reviews Downloads White Papers India Edition ZDNet.com is available in the following editions: Asia Australia Europe India United Kingdom United States ZDNet around the globe: ZDNet Belgium ZDNet China ZDNet France ZDNet Germany ZDNet Korea ZDNet Japan ZDNet Netherlands Topics DebateTech Future of desktops CXO Telcos SaaS Outsourcing Security Smartphones All India SMBs Log In Log In Join ZDNet JUST IN: Microsoft delivers Service Pack 1 for Office 2013 client and servers Processors Follow via: RSS Email Alert Sentience: The next moral dilemma Summary: Humankind will have to decide how to live with a new sentient race By Richard Barry |    January 24, 2001 -- 17:30 GMT (23:00 IST) I think even when we grant the label person But Garrett believes that sentient beings will never be the same In the film Bicentennial Man in which Robin Williams plays Andrew, a sentient robot who looks, feels and thinks like a human but is still classed as a droid, the death issue provides the final step toward the revered status of . Andrew swaps his mechanical innards for a set of organic ones that eventually age and kill him off in his sleep, earning him the posthumous award of human being Take me Pt II/ Legal protection and the right to choose . In ZDNet's Artificial Intelligence Special , ZDNet charts the road to sentience, examines the technologies that will take us from sci-fi to sci-fact, and asks if machines should have rights. Have your say instantly, and see what others have said. Click on the TalkBack button and go to the ZDNet News forum. Let the editors know what you think in the Mailroom . And read what others have said. Processors Kick off your day with ZDNet's daily email newsletter . It's the freshest tech news and opinion, served hot. Get it. Talkback 2 comments Log in or register to join the discussion as god made us in their image, so will we make things in our image. when this happens I believe only the qualities of the image will be apparent. Much like looking in a mirror at your own image. it looks like you, it emulates everything you do while you are in front of it, but under all circumstances it will always retain the qualities of an image and nothing more. That is why gods (while they are gods) are gods, and nothing more or less. Humans are humans, and computers are computers. It can be seen as a fail-safe system when dealing with the many mysteries of creation. anonymous 20 September, 2004 04:07 Reply Vote Computers are masters of illusion and every human attempt to rebuildt a computer as a human will always remain an illusion - it remains an appearance of something it is not. Time will decide whether man will master to discover the substance of his spirit and consciousness, but then he may ones again find himself standing before a galaxy of mysteries of which he has only discovered the first stars... anonymous 15 June, 2006 18:26 Reply Vote Related Stories Sentience: The next moral dilemma Pt II AI gets down to business Smart chips get under our skin Can a human love a robot? Resource Centre Useful content from our premier sponsors Cloud: How to Do SaaS Right In this ZDNet Special Feature, we offer guidance on avoiding the pitfalls of the cloud and choosing your SaaS partners well. Read now Facebook Activity White Papers, Webcasts, Resources Advanced Evasion Techniques for Dummies This book provides an overview of network security in general and explains how cybercriminals can use hidden and mostly undetectable methods to penetrate network systems. CryptoLocker - Your Money or Your Life CryptoLocker belongs to a family of malware called "ransomware", which is designed to extort money from victims by denying them access to their personal files. Once CryptoLocker has infiltrated a computer, it holds files hostage by encrypting them with a unique key. Read more details on the challenges and the solution for this new and vicious from of malware. Infographic: Whos minding your cloud? Based on the results and findings from the "Security of Cloud Computing Users Study". Read this infographic and the report to gain insight on the current state of cloud security and to help you assess your company's cloud services and applications security. Featured Articles AAPT CEO departs ahead of TPG takeover Hello, MS-Android. Good-bye, Windows Phone Lenovo ThinkPad Yoga review: A flexible hybrid tablet The smartwatch: Tomorrows must-have gear or a waste of time? Around ZDNet Topics Broadband Speed Test Events Calendar Meet the Team About ZDNet Site Map Services Log In | Join ZDNet Membership Newsletters RSS Feeds ZDNet Mobile Site Assistance 2014 CBS Interactive. All rights reserved. Privacy Policy | Cookies | Ad Choice | Advertise | Terms of Use Visit other CBS Interactive sites Select Site CBS Cares CBS Films CBS Radio CBS.com CBSInteractive CBSNews.com CBSSports.com CHOW Clicker CNET College Network GameSpot Last.fm MaxPreps Metacritic.com Moneywatch mySimon Radio.com Search.com Shopper.com Showtime SmartPlanet TechRepublic The Insider TV.com UrbanBaby.com ZDNet 
 Search this site: Member Login Username: * Password: * Request new password Home About us Board Committees Past Leadership Bylaws Contact Us Membership Join ASSC Membership Renewals Member Benefits Documents Search Submit Students News Conference Photos Reports Student Committee Conferences ASSC 18 - 2014 Symposia Tutorials Abstract Submission Registration Dinner Panpsychism Workshop Accommodation Travel Local Info Future ASSC 19 - 2015 ASSC 20 - 2016 Past ASSC 17 - 2013 Program Program Book Registration Abstract Submission Keynotes Symposia Concurrent Talks Poster Sessions Posters: Addendum (Errata Additions) Tutorials Consciousness Research Map Roundtable: Debating IIT Satellite: Perception and Action in Immersive Worlds Accommodations Preparation of Posters Talks ASSC 16 - July 2012 Keynotes Symposia Travel Information Tutorials Abstract Submission Satellite: Neuropsychiatry Registration Accommodation Online Discussions ASSC 15 - June 2011 Full Schedule Registration Talks Posters Keynotes Symposia Tutorials Social Neuroscience Satellite Metacognition Sattelite Neurophysiology Satellite Presenter Information Tourist Information Earthquake Updates ASSC 14 Registration Accommodation Schedule Venue Overview Program Full Program (With Abstracts) Tutorials Presenter Information Venue Tourist Info ASSC 13 Abstract booklet Venues Tutorials Plenary Lectures Symposia Concurrent Talks Posters Satellite Event Registration Accomodation Schedule Presenter Information Travel Information Acknowledgements Contact Imprint ASSC 12 Committees Program Presidential address Keynote speakers Symposium speakers Tutorial workshops Poster Sessions Session I Session II Concurrent sessions Session I Session II Session III Special events ASSC 11 Committees Program schedule Keynote speakers Plenary symposia Tutorial workshops Magic symposium ASSC 10 Committees Program schedule Tutorial workshops ASSC 9 Committees Program schedule Tutorial workshops ASSC 8 Committees Program Workshops Satellite symposium Workshops ASSC 7 Committees Program Plenary speakers Workshops ASSC 6 Program Plenary speakers Sessions ASSC 5 Program ASSC 4 Committees Program Abstracts Concurrent sessions Workshops ASSC 3 Program Concurrent sessions Poster sessions Workshops ASSC 2 Audio cuts ASSC 1 Journal Psyche Archive Vol 16, No 1 (2010) Vol 16, No 2 (2010) Vol 15 No 1 (2009) Vol 15 No 2 (2009) Vol 14 (2008) Vol 13 (2007) Vol 12 (2006) Vol 11 (2005) Vol 9 (2003) Vol 8 (2002) Vol 7 (2001) Vol 6 (2000) Vol 5 (1999) Vol 4 (1998) Vol 3 (1997) Vol 2 (1995-6) Vol 1 (1994) James Prize Past Recipients Newsletters Links Jobs The ASSC is an academic society that promotes rigorous research directed toward understanding the nature, function, and underlying mechanisms of consciousness. The ASSC includes members working in the fields of cognitive science, medicine, neuroscience, philosophy, and other relevant disciplines in the sciences and humanities. The ASSC web site has several main functions: * co-ordination of annual conferences on the scientific study of consciousness * promotion of other activities in the field of consciousness studies (smaller conferences, mailing lists, bibliographic resource, etc.) * maintain the Psyche journal archive * highlight recipients of the William James Prize ASSC 18 [Abstract Submission] [Symposia] [Tutorials] [Registration] [Dinner] [Accommodation] [Travel local info] Local organising committee: Bruno van Swinderen (chair), Derek Arnold, Ross Cunnington Scientific program committee: Olivia Carter (chair), Sid Kouider, David Chalmers, Nao Tsuchiya, Qiufang Fu, Melanie Boly, Tobias Schlicht, Steve Fleming Date: July 16th-19th 2014 ***** ABSTRACT SUBMISSION NOW OPEN ***** -------------------------------------------------------------------------------------------------------------------------------------------------- THE PROGRAM Keynote speakers: David Chalmers (Australian National University and New York University) Emery Brown (Massachusetts Institute of Technology) Sheng He (University of Minnesota) Melanie Wilke (University Medical Centre Goettingen) Jesse Prinz (City University of New York) Symposium Speakers: David Carmel (University of Edinburgh, UK) Joel Pearson (The University of New South Wales, Australia) Zoltan Dienes (University of Sussex, UK) Axel Cleeremans (Université Libre de Bruxelles, Belgium) Adam Shriver (The University of Pennsylvania, USA) Victorial Braithwaite (Pennsylvania State University, USA) Dan Weary (The University of British Columbia, Canada) D avid Edelman (Bennington College, USA) Paula Droege (Pennsylvania State University, USA) Aaron Schurger (École Polytechnique Fédérale de Lausanne, Switzerland) Marcellow Massimini (The University of Milan, Italy) Jacobo Sitt (L'Institut du Cerveau et de la Moelle Épinière, France) Anil Seth (University of Sussex, UK) Chiara Cirelli (University of Wisconsin-Madison, USA) Francesca Siclari (University of Wisconsin-Madison, USA) Michael Czisch (Max Planck Institute of Psychiatry, Germany) Thomas Metzinger (Johannes Gutenberg University of Mainz; Frankfurt Institute for Advanced Studies, Germany) Jennifer Windt (Johannes Gutenberg University of Mainz, Germany) Tutorial Short-Course Presenters: Giulio Tononi (University of Wisconsin, USA) Christof Koch (Allen Institute for Brain Science) Naotsugu Tsuchiya (Monash University, Australia) Masafumi Oizumi (Riken Brain Science Institute, Japan) Larissa Albantakis (University of Wisconsin, USA) Claude Touzet (Aix-Marseille University, France) Colin Hales (The University of Melbourne, Australia) Andreas Keller (The Rockerfeller University, USA) -------------------------------------------------------------------------------------------------------------------------------------------------- THE VENUE Located on a lush subtropical setting on a bend in the Brisbane River, the University of Queensland is an easy ferry ride from downtown Brisbane, the state capital. July (the Australian winter) is the best season to visit tropical north Queensland and the Great Barrier Reef. The main conference will be held in the Advanced Engineering Building (AEB), which was constructed in 2013. It is uniquely designed to achieve harmony with the natural environment, reducing energy consumption and attaining certified 5 Star Green ratings. It houses the state-of-the-art GHD Auditorium, a 500-seat lecture theatre built in an earthy, timber style with beautiful lake views. [Abstract Submission] [Symposia] [Tutorials] [Registration] [Dinner] [Accommodation] [Travel local info] -------------------------------------------------------------------------------------------------------------------------------------------------- www.icon2014.org Public ASSC Social Media click to find out more 
 Dictionary of Philosophy of Mind Search this site Home About Index Contributors Contact Us Sitemap                  Navigating the Dictionary:   There are different ways to navigate the dictionary. You can use our search engine at the top of the page, the index located in the menu above, or browse entries alphabetically. Alphabetical entries can be accessed via the alphabet on the home page.       Other Resources:     - Stanford Encyclopedia of Philosophy   - Dictionary of Cognitive Science     - Philpapers   - Internet Encyclopedia of Philosophy   - Philjobs: Jobs in Philosophy                        Home         We hope you find this resource useful         Please note, this site is under heavy construction at the moment.  Old entries are in the process of being transferred over to the new site, and many of the links in entries have not been added yet.  The website should be fully updated by February 2014.        Welcome to our new website!    This dictionary is intended as a free resource for all those interested in the philosophy of mind. It is designed to provide general information about key terms and concepts in order to aid those new to the discipline, or for those who are curious about current views in the field.     A   B   C   D   E   F   G   H   I   J   K   L   M   N   O   P   Q   R   S   T  U  V  W  X  Y  Z  Bios        Special Thanks to Anatoly Zhitnik for designing the image above, and  logo design company Logobee for the design of our logo.     Sign in | Report Abuse | Print Page | Remove Access | Powered By Google Sites 
 Javascript Menu by Deluxe-Menu.com PhilPapers: online research in philosophy David Chalmers (Editor) David Bourget (Assistant Editor), Australian National University. Submit an entry. Latest additions Off-campus access Submit and entry Bugs? Errors? All fields Surname click here for help on how to search Search tips All fields Surname Remember: viewing options in the menu above affect the results you get when searching. close help Your browser is not configured for off-campus access. Click here to configure it. Off-campus access settings. 6. Philosophy of Artificial Intelligence ( Philosophy of Artificial Intelligence on PhilPapers ) 6.1 Can Machines Think? [506] 6.1a The Turing Test [101] 6.1b Godelian arguments [87] 6.1c The Chinese Room [113] 6.1d Machine Consciousness [105] 6.1e Machine Mentality, Misc [100] 6.2 Computation and Representation [97] 6.2a Symbols and Symbol Systems [20] 6.2b Computational Semantics [29] 6.2c Implicit/Explicit Rules and Representations [17] 6.2d AI without Representation? [12] 6.2e Computation and Representation, Misc [19] 6.3 Philosophy of Connectionism [261] 6.3a Connectionism and Compositionality [54] 6.3b Representation in Connectionism [43] 6.3c Connectionism and Eliminativism [20] 6.3d The Connectionist/Classical Debate [36] 6.3e Subsymbolic Computation [10] 6.3f Philosophy of Connectionism, Misc [76] 6.3g Philosophy of Connectionism, Foundational Empirical Issues [22] 6.4 Special Topics in AI [186] The Singularity [9] Mind Uploading [1] 6.4a Cyborgs [0] 6.4b Transhumanism [0] 6.4c Cybernetics [1] 6.4d Dynamical Systems [57] 6.4e The Nature of AI [13] 6.4f The Frame Problem [34] 6.4g AI Methodology [35] 6.4h Robotics [35] 6.5 Computationalism [103] 6.5a Computation and Physical Systems [85] 6.5a.1 Computation and Physical Systems, Misc [2] 6.5a.2 Analog and Digital Computation [14] 6.5a.3 Computers [1] 6.5a.4 Implementing Computations [4] 6.5a.5 Noncomputable Processes [1] 6.5a.6 Pancomputationalism [7] 6.5a.7 Quantum Computation [9] 6.6 Philosophy of AI, Miscellaneous [83] 6.6a Philosophy of AI, General Works [1] 6.6b Philosophy of AI, Misc [5] Broadbent, Donald E. (ed.) (1993). The Simulation of Human Intelligence. Blackwell. ( Google ) Cloos, Christopher (2005). The Utilibot Project: An Autonomous Mobile Robot Based on Utilitarianism. In Anderson Michael, Anderson Susan Armen Chris (eds.), AAAI Fall Symposium . ( Google ) Abstract: As autonomous mobile robots (AMRs) begin living in the home, performing service tasks and assisting with daily activities, their actions will have profound ethical implications. Consequently, AMRs need to be outfitted with the ability to act morally with regard to human life and safety. Yet, in the area of robotics where morality is a relevant field of endeavor (i.e. human-robot interaction) the sub-discipline of morality does not exist. In response, the Utilibot project seeks to provide a point of initiation for the implementation of ethics in an AMR. The Utilibot is a decision-theoretic AMR guided by the utilitarian notion of the maximization of human well-being. The core ethical decision-making capacity of the Utilibot consists of two dynamic Bayesian networks that model human and environmental health, a dynamic decision network that accounts for decisions and utilities, and a Markov decision process (MDP) that decomposes the planning problem to solve for the optimal course of action to maximize human safety and well-being. Goel, Vinod (1994). Book reviews. Philosophia Mathematica 2 (1). ( Google ) Harnad, Stevan (1984). Verifying machines' minds. Contemporary Psychology 29:389 - 391. ( Google ) Abstract: he question of the possibility of artificial consciousness is both very new and very old. It is new in the context of contemporary cognitive science and its concern with whether a machine can be conscious; it is old in the form of the mind/body problem and the other minds problem of philosophy. Contemporary enthusiasts proceed at their peril if they ignore or are ignorant of the false starts and blind alleys that the older thinkers have painfully worked through Morreau, Michael Kraus, Sarit (1998). Syntactical Treatments of Propositional Attitudes. Artificial Intelligence 106 (1):161-177. ( Google ) Abstract: Syntactical treatments of propositional attitudes are attractive to artificial intelligence researchers. But results of Montague (1974) and Thomason (1980) seem to show that syntactical treatments are not viable. They show that if representation languages are sufficiently expressive, then axiom schemes characterizing knowledge and belief give rise to paradox. Des Rivières and Levesque (1988) characterize a class of sentences within which these schemes can safely be instantiated. These sentences do not quantify over the propositional objects of knowledge and belief. We argue that their solution is incomplete, and extend it by characterizing a more inclusive class of sentences over which the axiom schemes can safely range. Our sentences do quantify over propositional objects. Páez, Andrés (2009). Artificial explanations: The epistemological interpretation of explanation in ai. Synthese 170 (1). ( Google ) Abstract: In this paper I critically examine the notion of explanation used in artificial intelligence in general, and in the theory of belief revision in particular. I focus on two of the best known accounts in the literature: Pagnucco’s abductive expansion functions and Gärdenfors’ counterfactual analysis. I argue that both accounts are at odds with the way in which this notion has historically been understood in philosophy. They are also at odds with the explanatory strategies used in actual scientific practice. At the end of the paper I outline a set of desiderata for an epistemologically motivated, scientifically informed belief revision model for explanation 6.1 Can Machines Think? 6.1a The Turing Test Akman, Varol Blackburn, Patrick (2000). Editorial: Alan Turing and artificial intelligence. Journal of Logic, Language and Information 9 (4):391-395. (Cited by 2 | Google | More links ) Abstract: Department of Computer Engineering, Bilkent University, 06533 Ankara, Turkey E-mail: akman@cs.bilkent.edu.tr; http://www.cs.bilkent.edu.tr/?akman.. Additional links for this entry: http://citeseer.ist.psu.edu/377438.html http://portal.acm.org/citation.cfm?id=595992 http://cogprints.ecs.soton.ac.uk/archive/00001058/00/intro.ps http://www.cs.bilkent.edu.tr/~akman/jour-papers/jolli/jolli2000.pdf http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1058 http://www.springerlink.com/content/rm454307n5h9k066/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=269485=1 http://www.springerlink.com/index/RM454307N5H9K066.pdf http://www.ingentaconnect.com/content/klu/jlli/2000/00000009/00000004/00269485 Alper, G. (1990). A psychoanalyst takes the Turing test. Psychoanalytic Review 77:59-68. ( Cited by 6 | Google ) Barresi, John (1987). Prospects for the cyberiad: Certain limits on human self-knowledge in the cybernetic age. Journal for the Theory of Social Behavior 17 (March):19-46. ( Cited by 6 | Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/120019196/PDFSTART Beenfeldt, Christian (2006). The Turing test: An examination of its nature and its mentalistic ontology. Danish Yearbook of Philosophy 40:109-144. ( Google ) Ben-Yami, Hanoch (2005). Behaviorism and psychologism: Why Block's argument against behaviorism is unsound. Philosophical Psychology 18 (2):179-186. ( Cited by 1 | Google | More links ) Abstract: Ned Block ((1981). Psychologism and behaviorism. Philosophical Review, 90, 5-43.) argued that a behaviorist conception of intelligence is mistaken, and that the nature of an agent's internal processes is relevant for determining whether the agent has intelligence. He did that by describing a machine which lacks intelligence, yet can answer questions put to it as an intelligent person would. The nature of his machine's internal processes, he concluded, is relevant for determining that it lacks intelligence. I argue against Block that it is not the nature of its processes but of its linguistic behavior which is responsible for his machine's lack of intelligence. As I show, not only has Block failed to establish that the nature of internal processes is conceptually relevant for psychology, in fact his machine example actually supports some version of behaviorism. As Wittgenstein has maintained, as far as psychology is concerned, there may be chaos inside Additional links for this entry: http://taylorandfrancis.metapress.com/index/MX24138627651300.pdf http://www.informaworld.com/index/713997477.pdf http://www.ingentaconnect.com/content/routledg/cphp/2005/00000018/00000002/art00002 http://www.informaworld.com/smpp/./ftinterface~db=all~content=a713997477~fulltext=713240930 Block, Ned (1981). Psychologism and behaviorism. Philosophical Review 90 (1):5-43. ( Cited by 88 | Annotation | Google | More links ) A look-up table could pass the Turing test, and surely isn't intelligent. The TT errs in testing behavior and not mechanisms. A nice, thorough paper. Abstract: Let psychologism be the doctrine that whether behavior is intelligent behavior depends on the character of the internal information processing that produces it. More specifically, I mean psychologism to involve the doctrine that two systems could have actual and potential behavior _typical_ of familiar intelligent beings, that the two systems could be exactly alike in their actual and potential behavior, and in their behavioral dispositions and capacities and counterfactual behavioral properties (i.e., what behaviors, behavioral dispositions, and behavioral capacities they would have exhibited had their stimuli differed)--the two systems could be alike in all these ways, yet there could be a difference in the information processing that mediates their stimuli and responses that determines that one is not at all intelligent while the other is fully intelligent Additional links for this entry: http://philosophy.wisc.edu/shapiro/Phil951/Block.pdf http://www.nyu.edu/gsas/dept/philo/faculty/block/papers/Psychologism.pdf http://www.nyu.edu/gsas/dept/philo/faculty/block/papers/Psychologism.htm http://www.jstor.org/stable/pdfplus/2184371.pdf Bringsjord, Selmer (2000). Animals, zombanimals, and the total Turing test: The essence of artificial intelligence. Journal of Logic Language and Information 9 (4):397-418. ( Cited by 32 | Google | More links ) Additional links for this entry: http://portal.acm.org/citation.cfm?id=595843.595993 http://citeseer.ist.psu.edu/bringsjord00animals.html http://www.rpi.edu/~faheyj2/SB/SELPAP/ZOMBANIMALS/zombanimals2.pdf http://www.kluweronline.com/article.asp?PIPS=269486=1 http://www.springerlink.com/index/TV852P4283211235.pdf http://www.ingentaconnect.com/content/klu/jlli/2000/00000009/00000004/00269486 Bringsjord, Selmer ; Caporale, Clarke Noel, Ron (2000). Animals, zombanimals, and the total Turing test. Journal of Logic, Language and Information 9 (4). ( Google ) Abstract: Alan Turing devised his famous test (TT) through a slight modificationof the parlor game in which a judge tries to ascertain the gender of twopeople who are only linguistically accessible. Stevan Harnad hasintroduced the Total TT, in which the judge can look at thecontestants in an attempt to determine which is a robot and which aperson. But what if we confront the judge with an animal, and arobot striving to pass for one, and then challenge him to peg which iswhich? Now we can index TTT to a particular animal and its syntheticcorrelate. We might therefore have TTTrat, TTTcat,TTTdog, and so on. These tests, as we explain herein, are abetter barometer of artificial intelligence (AI) than Turing's originalTT, because AI seems to have ammunition sufficient only to reach thelevel of artificial animal, not artificial person Bringsjord, Selmer ; Bello, P. Ferrucci, David A. (2001). Creativity, the Turing test, and the (better) Lovelace test. Minds and Machines 11 (1):3-27. ( Cited by 11 | Google | More links ) Abstract:   The Turing Test (TT) is claimed by many to be a way to test for the presence, in computers, of such ``deep'' phenomena as thought and consciousness. Unfortunately, attempts to build computational systems able to pass TT (or at least restricted versions of this test) have devolved into shallow symbol manipulation designed to, by hook or by crook, trick. The human creators of such systems know all too well that they have merely tried to fool those people who interact with their systems into believing that these systems really have minds. And the problem is fundamental: the structure of the TT is such as to cultivate tricksters. A better test is one that insists on a certain restrictive epistemic relation between an artificial agent (or system) A, its output o, and the human architect H of A – a relation which, roughly speaking, obtains when H cannot account for how A produced o. We call this test the ``Lovelace Test'' in honor of Lady Lovelace, who believed that only when computers originate things should they be believed to have minds Additional links for this entry: http://portal.acm.org/citation.cfm?id=596904 http://citeseer.ist.psu.edu/bringsjord00creativity.html http://www.rpi.edu/~faheyj2/SB/SELPAP/DARTMOUTH/lt3.pdf http://www.springerlink.com/content/content/nrvl130w46xj6g22/fulltext.pdf http://www.springerlink.com/content/nrvl130w46xj6g22/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=319539=1 http://www.springerlink.com/index/NRVL130W46XJ6G22.pdf http://www.ingentaconnect.com/content/klu/mind/2001/00000011/00000001/00319539 Clark, Thomas W. (1992). The Turing test as a novel form of hermeneutics. International Studies in Philosophy 24 (1):17-31. ( Cited by 6 | Google ) Clifton, Andrew (ms). Blind man's bluff and the Turing test. ( Google ) Abstract: It seems plausible that under the conditions of the Turing test, congenitally blind people could nevertheless, with sufficient preparation, successfully represent themselves to remotely located interrogators as sighted. Having never experienced normal visual sensations, the successful blind player can prevail in this test only by playing a ‘lying game’—imitating the phenomenological claims of sighted people, in the absence of the qualitative visual experiences to which such statements purportedly refer. This suggests that a computer or robot might pass the Turing test in the same way, in the absence not only of visual experience, but qualitative consciousness in general. Hence, the standard Turing test does not provide a valid criterion for the presence of consciousness. A ‘sensorimetric’ version of the Turing test fares no better, for the apparent correlations we observe between cognitive functions and qualitative conscious experiences seems to be contingent, not necessary. We must therefore define consciousness not in terms of its causes and effects, but rather, in terms of the distinctive properties of its content, such as its possession of qualitative character and apparent intrinsic value—the property which confers upon consciousness its moral significance. As a means of determining whether or nor a machine is conscious, in this sense, an alternative to the standard Turing test is proposed Copeland, B. Jack (2000). The Turing test. Minds and Machines 10 (4):519-539. ( Cited by 7 | Google | More links ) Abstract:   Turing''s test has been much misunderstood. Recently unpublished material by Turing casts fresh light on his thinking and dispels a number of philosophical myths concerning the Turing test. Properly understood, the Turing test withstands objections that are popularly believed to be fatal Additional links for this entry: http://books.google.com/books?hl=en=4woWcfbL_fO5zip_aVT89VSM390 http://books.google.com/books?hl=en=slbbZKdPbjoJsVO44CwfWr3oSMo http://www.springerlink.com/content/content/w103433h4g273841/fulltext.pdf http://www.springerlink.com/content/w103433h4g273841/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=319531=1 http://www.springerlink.com/index/W103433H4G273841.pdf http://www.ingentaconnect.com/content/klu/mind/2000/00000010/00000004/00319531 Cowen, Tyler Dawson, Michelle , What does the Turing test really mean? And how many human beings (including Turing) could pass? ( Google ) Abstract: The so-called Turing test, as it is usually interpreted, sets a benchmark standard for determining when we might call a machine intelligent. We can call a machine intelligent if the following is satisfied: if a group of wise observers were conversing with a machine through an exchange of typed messages, those observers could not tell whether they were talking to a human being or to a machine. To pass the test, the machine has to be intelligent but it also should be responsive in a manner which cannot be distinguished from a human being. This standard interpretation presents the Turing test as a criterion for demarcating intelligent from non-intelligent entities. For a long time proponents of artificial intelligence have taken the Turing test as a goalpost for measuring progress Crawford, C. (1994). Notes on the Turing test. Communications of the Association for Computing Machinery 37 (June):13-15. ( Google ) Crockett, L. (1994). The Turing Test and the Frame Problem: AI's Mistaken Understanding of Intelligence. Ablex. ( Cited by 19 | Google ) Cutrona, Jr (ms). Zombies in Searle's chinese room: Putting the Turing test to bed. ( Google | More links ) Abstract: Searle’s discussions over the years 1980-2004 of the implications of his “Chinese Room” Gedanken experiment are frustrating because they proceed from a correct assertion: (1) “Instantiating a computer program is never by itself a sufficient condition of intentionality;” and an incorrect assertion: (2) “The explanation of how the brain produces intentionality cannot be that it does it by instantiating a computer program.” In this article, I describe how to construct a Gedanken zombie Chinese Room program that will pass the Turing test and at the same time unambiguously demonstrates the correctness of (1). I then describe how to construct a Gedanken Chinese brain program that will pass the Turing test, has a mind, and understands Chinese, thus demonstrating that (2) is incorrect. Searle’s instantiation of this program can and does produce intentionality. Searle’s longstanding ignorance of Chinese is simply irrelevant and always has been. I propose a truce and a plan for further exploration Additional links for this entry: http://cogprints.org/4636/1/TR%2D05%2D002.pdf Davidson, Donald (1990). Turing's test. In K. Said (ed.), Modelling the Mind . Oxford University Press. ( Google ) Dennett, Daniel C. (1984). Can machines think? In M. G. Shafto (ed.), How We Know . Harper & Row. ( Cited by 24 | Annotation | Google ) Defending the Turing test as a good test for intelligence. Drozdek, Adam (2001). Descartes' Turing test. Epistemologia 24 (1):5-29. ( Google ) Edmonds, Bruce (2000). The constructability of artificial intelligence (as defined by the Turing test). Journal of Logic Language and Information 9 (4):419-424. ( Google | More links ) Abstract: The Turing Test (TT), as originally specified, centres on theability to perform a social role. The TT can be seen as a test of anability to enter into normal human social dynamics. In this light itseems unlikely that such an entity can be wholly designed in anoff-line mode; rather a considerable period of training insitu would be required. The argument that since we can pass the TT,and our cognitive processes might be implemented as a Turing Machine(TM), that consequently a TM that could pass the TT could be built, isattacked on the grounds that not all TMs are constructible in a plannedway. This observation points towards the importance of developmentalprocesses that use random elements (e.g., evolution), but in these casesit becomes problematic to call the result artificial. This hasimplications for the means by which intelligent agents could bedeveloped Additional links for this entry: http://cogprints.org/397/3/consai.pdf http://cogprints.org/397/0/consai.pdf Edmonds, B. (ms). The constructability of artificial intelligence (as defined by the Turing test). ( Google | More links ) Abstract: The Turing Test, as originally specified, centres on the ability to perform a social role. The TT can seen as a test of an ability to enter into normal human social dynamics. In this light it seems unlikely that such an entity can be wholly designed in an `off-line' mode, but rather a considerable period of training in situ would be required. The argument that since we can pass the TT and our cognitive processes might be implemented as a TM that, in theory, an TM that could pass the TT could be built is attacked on the grounds that not all TMs are constructable in a planned way. This observation points towards the importance of developmental processes that include random elements (e.g. evolution), but in these cases it becomes problematic to call the result artificial Additional links for this entry: http://cogprints.org/397/0/consai.pdf Erion, Gerald J. (2001). The cartesian test for automatism. Minds and Machines 11 (1):29-39. ( Cited by 5 | Google | More links ) Abstract:   In Part V of his Discourse on the Method, Descartes introduces a test for distinguishing people from machines that is similar to the one proposed much later by Alan Turing. The Cartesian test combines two distinct elements that Keith Gunderson has labeled the language test and the action test. Though traditional interpretation holds that the action test attempts to determine whether an agent is acting upon principles, I argue that the action test is best understood as a test of common sense. I also maintain that this interpretation yields a stronger test than Turing's, and that contemporary artificial intelligence should consider using it as a guide for future research Additional links for this entry: http://books.google.com/books?hl=en=dV16mfWZM_Qoj6YrtNA26fR1IZk http://books.google.com/books?hl=en=MTJT29H2X-bBW0rfBOaTIEju52o http://www.springerlink.com/content/content/h814j04926567527/fulltext.pdf http://www.springerlink.com/content/h814j04926567527/fulltext.pdf http://www.springerlink.com/index/H814J04926567527.pdf http://www.ingentaconnect.com/content/klu/mind/2001/00000011/00000001/00319541 Floridi, Luciano (2005). Consciousness, agents and the knowledge game. Minds and Machines 15 (3):415-444. ( Cited by 2 | Google | More links ) Abstract: This paper has three goals. The first is to introduce the “knowledge game”, a new, simple and yet powerful tool for analysing some intriguing philosophical questions. The second is to apply the knowledge game as an informative test to discriminate between conscious (human) and conscious-less agents (zombies and robots), depending on which version of the game they can win. And the third is to use a version of the knowledge game to provide an answer to Dretske’s question “how do you know you are not a zombie?” Additional links for this entry: http://philsci-archive.pitt.edu/archive/00002546/ http://philsci-archive.pitt.edu/archive/00002546/01/caatkg.pdf http://www.springerlink.com/content/f52pq39812453221/fulltext.pdf http://www.springerlink.com/index/F52PQ39812453221.pdf http://www.ingentaconnect.com/content/klu/mind/2005/00000015/F0020003/00009005 Floridi, Luciano Taddeo, Mariarosaria (2009). Turing's imitation game: Still an impossible challenge for all machines and some judges––an evaluation of the 2008 loebner contest. Minds and Machines 19 (1). ( Google ) Abstract: An evaluation of the 2008 Loebner contest Floridi, Luciano ; Taddeo, Mariarosaria Turilli, Matteo (2008). Turing’s Imitation Game: Still an Impossible Challenge for All Machines and Some Judges. Minds and Machines 19 (1):145-150. ( Google ) Abstract: An Evaluation of the 2008 Loebner Contest. French, Robert M. (2000). Peeking behind the screen: The unsuspected power of the standard Turing test. Journal of Experimental and Theoretical Artificial Intelligence 12 (3):331-340. (Cited by 10 | Google | More links ) Abstract: No computer that had not experienced the world as we humans had could pass a rigorously administered standard Turing Test. We show that the use of “subcognitive” questions allows the standard Turing Test to indirectly probe the human subcognitive associative concept network built up over a lifetime of experience with the world. Not only can this probing reveal differences in cognitive abilities, but crucially, even differences in _physical aspects_ of the candidates can be detected. Consequently, it is unnecessary to propose even harder versions of the Test in which all physical and behavioral aspects of the two candidates had to be indistinguishable before allowing the machine to pass the Test. Any machine that passed the “simpler” symbols- in/symbols-out test as originally proposed by Turing would be intelligent. The problem is that, even in its original form, the Turing Test is already too hard and too anthropocentric for any machine that was not a physical, social, and behavioral carbon copy of ourselves to actually pass it. Consequently, the Turing Test, even in its standard version, is not a reasonable test for general machine intelligence. There is no need for an even stronger version of the Test Additional links for this entry: http://citeseer.ist.psu.edu/french00peeking.html http://www.u-bourgogne.fr/LEAD/people/french/peeking.pdf http://www.informaworld.com/index/24PHYLU4NX08NHQF.pdf http://www.ingentaconnect.com/content/tandf/teta/2000/00000012/00000003/art00006 French, Robert M. (1995). Refocusing the debate on the Turing test: A response. Behavior and Philosophy 23 (1):59-60. ( Cited by 3 | Annotation | Google ) Response to Jacquette 1993. French, Robert M. (1990). Subcognition and the limits of the Turing test. Mind 99 (393):53-66. ( Cited by 66 | Annotation | Google | More links ) The Turing Test is too hard, as it requires not intelligence but human intelligence. Any machine could be unmasked through careful questioning, but this wouldn't mean that the machine was unintelligent. Additional links for this entry: http://philosophy.wisc.edu/Shapiro/Phil554/PAPERS/French.pdf http://www.jstor.org/stable/pdfplus/2254890.pdf French, Robert (1996). The inverted Turing test: How a mindless program could pass it. Psycoloquy 7 (39). ( Cited by 5 | Google | More links ) Abstract: This commentary attempts to show that the inverted Turing Test (Watt 1996) could be simulated by a standard Turing test and, most importantly, claims that a very simple program with no intelligence whatsoever could be written that would pass the inverted Turing test. For this reason, the inverted Turing test in its present form must be rejected Additional links for this entry: http://www.u-bourgogne.fr/LEAD/people/french/invturing.pdf French, Robert (2000). The Turing test: The first fifty years. Trends in Cognitive Sciences 4 (3):115-121. ( Cited by 15 | Google | More links ) Abstract: The Turing Test, originally proposed as a simple operational definition of intelligence, has now been with us for exactly half a century. It is safe to say that no other single article in computer science, and few other articles in science in general, have generated so much discussion. The present article chronicles the comments and controversy surrounding Turing's classic article from its publication to the present. The changing perception of the Turing Test over the last fifty years has paralleled the changing attitudes in the scientific community towards artificial intelligence: from the unbridled optimism of 1960's to the current realization of the immense difficulties that still lie ahead. I conclude with the prediction that the Turing Test will remain important, not only as a landmark in the history of the development of intelligent machines, but also with real relevance to future generations of people living in a world in which the cognitive capacities of machines will be vastly greater than they are now Additional links for this entry: http://citeseer.ist.psu.edu/french00turing.html http://srsc.ulb.ac.be/cogsci/papers/TICS-French_turing.pdf http://www.u-bourgogne.fr/LEAD/people/french/TICS_turing.pdf http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation http://www.ingentaconnect.com/content/els/13646613/2000/00000004/00000003/art01453 Gunderson, Keith (1964). The imitation game. Mind 73 (April):234-45. ( Cited by 13 | Annotation | Google | More links ) The Turing test is not broad enough: there's much more to thought than the ability to play the imitation game. Additional links for this entry: http://www.jstor.org/stable/pdfplus/2251812.pdf Harnad, Stevan Dror, Itiel (2006). Distributed cognition: Cognizing, autonomy and the Turing test. Pragmatics and Cognition 14 (2):14. ( Cited by 2 | Google | More links ) Abstract: Some of the papers in this special issue distribute cognition between what is going on inside individual cognizers' heads and their outside worlds; others distribute cognition among different individual cognizers. Turing's criterion for cognition was individual, autonomous input/output capacity. It is not clear that distributed cognition could pass the Turing Test Additional links for this entry: http://eprints.ecs.soton.ac.uk/12368/02/discintro.pdf http://dialnet.unirioja.es/servlet/articulo?codigo=2196285 http://www.ingentaconnect.com/content/jbp/pc/2006/00000014/00000002/art00002 Harnad, Stevan (1995). Does mind piggyback on robotic and symbolic capacity? In H. Morowitz J. Singer (eds.), The Mind, the Brain, and Complex Adaptive Systems . Addison Wesley. ( Google ) Abstract: Cognitive science is a form of "reverse engineering" (as Dennett has dubbed it). We are trying to explain the mind by building (or explaining the functional principles of) systems that have minds. A "Turing" hierarchy of empirical constraints can be applied to this task, from t1, toy models that capture only an arbitrary fragment of our performance capacity, to T2, the standard "pen-pal" Turing Test (total symbolic capacity), to T3, the Total Turing Test (total symbolic plus robotic capacity), to T4 (T3 plus internal [neuromolecular] indistinguishability). All scientific theories are underdetermined by data. What is the right level of empirical constraint for cognitive theory? I will argue that T2 is underconstrained (because of the Symbol Grounding Problem and Searle's Chinese Room Argument) and that T4 is overconstrained (because we don't know what neural data, if any, are relevant). T3 is the level at which we solve the "other minds" problem in everyday life, the one at which evolution operates (the Blind Watchmaker is no mind-reader either) and the one at which symbol systems can be grounded in the robotic capacity to name and manipulate the objects their symbols are about. I will illustrate this with a toy model for an important component of T3 -- categorization -- using neural nets that learn category invariance by "warping" similarity space the way it is warped in human categorical perception: within-category similarities are amplified and between-category similarities are attenuated. This analog "shape" constraint is the grounding inherited by the arbitrarily shaped symbol that names the category and by all the symbol combinations it enters into. No matter how tightly one constrains any such model, however, it will always be more underdetermined than normal scientific and engineering theory. This will remain the ineliminable legacy of the mind/body problem Harnad, Stevan (1994). Levels of functional equivalence in reverse bioengineering: The Darwinian Turing test for artificial life. Artificial Life 1 (3):93-301. ( Cited by 35 | Google | More links ) Abstract: Both Artificial Life and Artificial Mind are branches of what Dennett has called "reverse engineering": Ordinary engineering attempts to build systems to meet certain functional specifications, reverse bioengineering attempts to understand how systems that have already been built by the Blind Watchmaker work. Computational modelling (virtual life) can capture the formal principles of life, perhaps predict and explain it completely, but it can no more be alive than a virtual forest fire can be hot. In itself, a computational model is just an ungrounded symbol system; no matter how closely it matches the properties of what is being modelled, it matches them only formally, with the mediation of an interpretation. Synthetic life is not open to this objection, but it is still an open question how close a functional equivalence is needed in order to capture life. Close enough to fool the Blind Watchmaker is probably close enough, but would that require molecular indistinguishability, and if so, do we really need to go that far? Additional links for this entry: http://eprints.resist.ecs.soton.ac.uk/3363/ http://portal.acm.org/citation.cfm?id=188445 http://cogprints.soton.ac.uk/documents/disk0/00/00/15/91/ http://mitpress.mit.edu/catalog/item/default.asp?ttype=6=40 http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad94.artlife2.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad94.artlife2.html http://eprints.ecs.soton.ac.uk/archive/00003363/02/harnad94.artlife2.html http://cogprints.ecs.soton.ac.uk/archive/00001591/00/harnad94.artlife2.html http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1591 Harnad, Stevan (1991). Other bodies, other minds: A machine incarnation of an old philosophical problem. [Journal (Paginated)] 1 (1):43-54. ( Cited by 99 | Annotation | Google | More links ) On the Total Turing Test (full behavioral equivalence) as a test for mind. Abstract: Explaining the mind by building machines with minds runs into the other-minds problem: How can we tell whether any body other than our own has a mind when the only way to know is by being the other body? In practice we all use some form of Turing Test: If it can do everything a body with a mind can do such that we can't tell them apart, we have no basis for doubting it has a mind. But what is "everything" a body with a mind can do? Turing's original "pen-pal" version (the TT) only tested linguistic capacity, but Searle has shown that a mindless symbol-manipulator could pass the TT undetected. The Total Turing Test (TTT) calls for all of our linguistic and robotic capacities; immune to Searle's argument, it suggests how to ground a symbol manipulating system in the capacity to pick out the objects its symbols refer to. No Turing Test, however, can guarantee that a body has a mind. Worse, nothing in the explanation of its successful performance requires a model to have a mind at all. Minds are hence very different from the unobservables of physics (e.g., superstrings); and Turing Testing, though essential for machine-modeling the mind, can really only yield an explanation of the body Additional links for this entry: http://eprints.ecs.soton.ac.uk/archive/00003379/ http://cogprints.ecs.soton.ac.uk/archive/00001578/ http://portal.acm.org/citation.cfm?id=103493.103530 http://cogprints.org/1578/0/harnad91.otherminds.html http://cogprints.soton.ac.uk/documents/disk0/00/00/15/78/ http://eprints.ecs.soton.ac.uk/archive/00003379/01/harnad91.otherminds.html http://cogprints.ecs.soton.ac.uk/archive/00001578/00/harnad91.otherminds.html http://citebase.eprints.org/cgi-bin/citations?id=oai:eprints.ecs.soton.ac.uk:3379 http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1578 http://www.springerlink.com/content/t5733025r3t44248/fulltext.pdf http://www.springerlink.com/index/T5733025R3T44248.pdf http://cogprints.org/1578/1/harnad91.otherminds.html Harnad, Stevan (2006). The annotation game: On Turing (1950) on computing, machinery, and intelligence. In Robert Epstein G. Peters (eds.), [Book Chapter] (in Press) . Kluwer. ( Cited by 5 | Google | More links ) Abstract: This quote/commented critique of Turing's classical paper suggests that Turing meant -- or should have meant -- the robotic version of the Turing Test (and not just the email version). Moreover, any dynamic system (that we design and understand) can be a candidate, not just a computational one. Turing also dismisses the other-minds problem and the mind/body problem too quickly. They are at the heart of both the problem he is addressing and the solution he is proposing Additional links for this entry: http://eprints.ecs.soton.ac.uk/12954/ http://eprints.resist.ecs.soton.ac.uk/7741/ http://cogprints.ecs.soton.ac.uk/archive/00003322/ http://www.ecs.soton.ac.uk/~harnad/Temp/turing.doc http://www.ecs.soton.ac.uk/~harnad/Temp/turing.html http://eprints.ecs.soton.ac.uk/archive/00007741/01/turing.html http://cogprints.ecs.soton.ac.uk/archive/00003322/01/turing.html http://citebase.eprints.org/cgi-bin/citations?id=oai:eprints.ecs.soton.ac.uk:7741 http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:eprints.ecs.soton.ac.uk:7741 http://cogprints.org/3322/2/turing.pdf http://cogprints.org/3322/1/turing.html Harnad, Stevan (2006). The annotation game: On Turing (1950) on computing, machinery, and intelligence. In Robert Epstein Grace Peters (eds.), [Book Chapter] (in Press) . Kluwer. ( Cited by 5 | Google | More links ) Abstract: This quote/commented critique of Turing's classical paper suggests that Turing meant -- or should have meant -- the robotic version of the Turing Test (and not just the email version). Moreover, any dynamic system (that we design and understand) can be a candidate, not just a computational one. Turing also dismisses the other-minds problem and the mind/body problem too quickly. They are at the heart of both the problem he is addressing and the solution he is proposing Additional links for this entry: http://eprints.ecs.soton.ac.uk/12954/ http://cogprints.org/3322/1/turing.html http://eprints.resist.ecs.soton.ac.uk/7741/ http://cogprints.ecs.soton.ac.uk/archive/00003322/ http://www.ecs.soton.ac.uk/~harnad/Temp/turing.doc http://www.ecs.soton.ac.uk/~harnad/Temp/turing.html http://eprints.ecs.soton.ac.uk/archive/00007741/01/turing.html http://cogprints.ecs.soton.ac.uk/archive/00003322/01/turing.html http://citebase.eprints.org/cgi-bin/citations?id=oai:eprints.ecs.soton.ac.uk:7741 http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:eprints.ecs.soton.ac.uk:7741 http://cogprints.org/3322/2/turing.pdf Harnad, Stevan (1999). Turing on reverse-engineering the mind. Journal of Logic, Language, and Information . ( Cited by 4 | Google ) Harnad, Stevan (1992). The Turing test is not a trick: Turing indistinguishability is a scientific criterion. [Journal (Paginated)] 3 (4):9-10. ( Cited by 44 | Google | More links ) Abstract: It is important to understand that the Turing Test (TT) is not, nor was it intended to be, a trick; how well one can fool someone is not a measure of scientific progress. The TT is an empirical criterion: It sets AI's empirical goal to be to generate human-scale performance capacity. This goal will be met when the candidate's performance is totally indistinguishable from a human's. Until then, the TT simply represents what it is that AI must endeavor eventually to accomplish scientifically Additional links for this entry: http://eprints.resist.ecs.soton.ac.uk/3373/ http://portal.acm.org/citation.cfm?id=141420.141422 http://cogprints.soton.ac.uk/documents/disk0/00/00/15/84/ http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad92.turing.html http://citebase.eprints.org/cgi-bin/citations?id=oai:cogprints.soton.ac.uk:1584 http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1584 http://cogprints.org/1584/1/harnad92.turing.html http://cogprints.org/1584/0/harnad92.turing.html Hauser, Larry (2001). Look who's moving the goal posts now. Minds and Machines 11 (1):41-51. ( Cited by 2 | Google | More links ) Abstract:   The abject failure of Turing's first prediction (of computer success in playing the Imitation Game) confirms the aptness of the Imitation Game test as a test of human level intelligence. It especially belies fears that the test is too easy. At the same time, this failure disconfirms expectations that human level artificial intelligence will be forthcoming any time soon. On the other hand, the success of Turing's second prediction (that acknowledgment of computer thought processes would become commonplace) in practice amply confirms the thought that computers think in some manner and are possessed of some level of intelligence already. This lends ever-growing support to the hypothesis that computers will think at a human level eventually, despite the abject failure of Turing's first prediction Additional links for this entry: http://www.springerlink.com/content/p14045502315u372/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=319542=1 http://www.springerlink.com/index/P14045502315U372.pdf http://www.ingentaconnect.com/content/klu/mind/2001/00000011/00000001/00319542 Hauser, Larry (1993). Reaping the whirlwind: Reply to Harnad's Other Bodies, Other Minds . Minds and Machines 3 (2):219-37. ( Cited by 18 | Google | More links ) Abstract:   Harnad''s proposed robotic upgrade of Turing''s Test (TT), from a test of linguistic capacity alone to a Total Turing Test (TTT) of linguisticand sensorimotor capacity, conflicts with his claim that no behavioral test provides even probable warrant for attributions of thought because there is no evidence of consciousness besides private experience. Intuitive, scientific, and philosophical considerations Harnad offers in favor of his proposed upgrade are unconvincing. I agree with Harnad that distinguishing real from as if thought on the basis of (presence or lack of) consciousness (thus rejecting Turing (behavioral) testing as sufficient warrant for mental attribution)has the skeptical consequence Harnad accepts — there is in factno evidence for me that anyone else but me has a mind. I disagree with hisacceptance of it! It would be better to give up the neo-Cartesian faith in private conscious experience underlying Harnad''s allegiance to Searle''s controversial Chinese Room Experiment than give up all claim to know others think. It would be better to allow that (passing) Turing''s Test evidences — evenstrongly evidences — thought Additional links for this entry: http://members.aol.com/lshauser/harnad3.html http://cogprints.ecs.soton.ac.uk/archive/00000241/00/reaping.html http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:241 http://www.springerlink.com/content/h727v843532tp041/fulltext.pdf http://www.springerlink.com/index/H727V843532TP041.pdf Hayes, Patrick Ford, Kenneth M. (1995). Turing test considered harmful. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence 1:972-77. ( Cited by 26 | Google ) Hernandez-Orallo, Jose (2000). Beyond the Turing test. Journal of Logic, Language and Information 9 (4):447-466. ( Cited by 2 | Google | More links ) Abstract: The main factor of intelligence is defined as the ability tocomprehend, formalising this ability with the help of new constructsbased on descriptional complexity. The result is a comprehension test,or C-test, which is exclusively defined in computational terms. Due toits absolute and non-anthropomorphic character, it is equally applicableto both humans and non-humans. Moreover, it correlates with classicalpsychometric tests, thus establishing the first firm connection betweeninformation theoretical notions and traditional IQ tests. The TuringTest is compared with the C-test and the combination of the two isquestioned. In consequence, the idea of using the Turing Test as apractical test of intelligence should be surpassed, and substituted bycomputational and factorial tests of different cognitive abilities, amuch more useful approach for artificial intelligence progress and formany other intriguing questions that present themselves beyond theTuring Test Additional links for this entry: http://portal.acm.org/citation.cfm?id=595843.595997 http://www.dsic.upv.es/~jorallo/escrits/TT-JHdez.ps.gz http://citeseer.ist.psu.edu/hernandez-orallo99beyond.html http://www.springerlink.com/content/h271n6173ll3773x/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=269489=1 http://www.springerlink.com/index/H271N6173LL3773X.pdf http://www.ingentaconnect.com/content/klu/jlli/2000/00000009/00000004/00269489 Hofstadter, Douglas R. (1981). A coffee-house conversation on the Turing test. Scientific American . ( Annotation | Google ) A dialogue on the Turing test. Jacquette, Dale (1993). A Turing test conversation. Philosophy 68 (264):231-33. ( Cited by 4 | Google ) Jacquette, Dale (1993). Who's afraid of the Turing test? Behavior and Philosophy 20 (21):63-74. ( Annotation | Google ) Defending the Turing test against French 1990. Turing did not intend the test to provide a *necessary* condition for intelligence. Karelis, Charles (1986). Reflections on the Turing test. Journal for the Theory of Social Behavior 16 (July):161-72. ( Cited by 10 | Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/120025387/PDFSTART Klsadlkjfs, Addssdf (ms). mind thing. ( Google ) Lee, E. T. (1996). On the Turing test for artificial intelligence. Kybernetes 25. ( Cited by 1 | Google ) Leiber, Justin (1995). On Turing's Turing test and why the matter matters. Synthese 104 (1):59-69. ( Cited by 6 | Annotation | Google ) Turing's test is neutral about the structure of the machine that passes it, but it must be practical and reliable (thus excluding Searle's and Block's counterexamples). Leiber, Justin (1989). Shanon on the Turing test. Journal of Social Behavior 19 (June):257-259. ( Cited by 6 | Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/120004127/PDFSTART Leiber, Justin (2001). Turing and the fragility and insubstantiality of evolutionary explanations: A puzzle about the unity of Alan Turing's work with some larger implications. Philosophical Psychology 14 (1):83-94. ( Google | More links ) Abstract: As is well known, Alan Turing drew a line, embodied in the "Turing test," between intellectual and physical abilities, and hence between cognitive and natural sciences. Less familiarly, he proposed that one way to produce a "passer" would be to educate a "child machine," equating the experimenter's improvements in the initial structure of the child machine with genetic mutations, while supposing that the experimenter might achieve improvements more expeditiously than natural selection. On the other hand, in his foundational "On the chemical basis of morphogenesis," Turing insisted that biological explanation clearly confine itself to purely physical and chemical means, eschewing vitalist and teleological talk entirely and hewing to D'Arcy Thompson's line that "evolutionary 'explanations,'" are historical and narrative in character, employing the same intentional and teleological vocabulary we use in doing human history, and hence, while perhaps on occasion of heuristic value, are not part of biology as a natural science. To apply Turing's program to recent issues, the attempt to give foundations to the social and cognitive sciences in the "real science" of evolutionary biology (as opposed to Turing's biology) is neither to give foundations, nor to achieve the unification of the social/cognitive sciences and the natural sciences Additional links for this entry: http://www.informaworld.com/smpp/./ftinterface~content=a713690490~fulltext=713240930 Leiber, Justin (2006). Turing's golden: How well Turing's work stands today. Philosophical Psychology 19 (1):13-46. ( Google | More links ) Abstract: A. M. Turing has bequeathed us a conceptulary including 'Turing, or Turing-Church, thesis', 'Turing machine', 'universal Turing machine', 'Turing test' and 'Turing structures', plus other unnamed achievements. These include a proof that any formal language adequate to express arithmetic contains undecidable formulas, as well as achievements in computer science, artificial intelligence, mathematics, biology, and cognitive science. Here it is argued that these achievements hang together and have prospered well in the 50 years since Turing's death Additional links for this entry: http://www.informaworld.com/smpp/./ftinterface~content=a741477282~fulltext=713240930 http://taylorandfrancis.metapress.com/index/NGH381WR75272U7R.pdf http://www.informaworld.com/index/741477282.pdf http://www.ingentaconnect.com/content/routledg/cphp/2006/00000019/00000001/art00003 http://www.ingentaconnect.com/content/routledg/cphp/latest Lockhart, Robert S. (2000). Modularity, cognitive penetrability and the Turing test. Psycoloquy . ( Cited by 1 | Google | More links ) Abstract: The Turing Test blurs the distinction between a model and irrelevant) instantiation details. Modeling only functional modules is problematic if these are interconnected and cognitively penetrable Additional links for this entry: http://psycprints.ecs.soton.ac.uk/archive/00000068/ Mays, W. (1952). Can machines think? Philosophy 27 (April):148-62. ( Cited by 7 | Google ) Michie, Donald (1993). Turing's test and conscious thought. Artificial Intelligence 60:1-22. ( Cited by 19 | Google ) Midgley, Mary (1995). Zombies and the Turing test. Journal of Consciousness Studies 2 (4):351-352. ( Google ) Millar, P. (1973). On the point of the imitation game. Mind 82 (October):595-97. ( Cited by 9 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2252214.pdf Mitchell, Robert W. Anderson, James R. (1998). Primate theory of mind is a Turing test. Behavioral and Brain Sciences 21 (1):127-128. ( Google ) Abstract: Heyes's literature review of deception, imitation, and self-recognition is inadequate, misleading, and erroneous. The anaesthetic artifact hypothesis of self-recognition is unsupported by the data she herself examines. Her proposed experiment is tantalizing, indicating that theory of mind is simply a Turing test Moor, James H. (1976). An analysis of Turing's test. Philosophical Studies 30:249-257. ( Annotation | Google ) The basis of the Turing test is not an operational definition of thinking, but rather an inference to the best explanation. Moor, James H. (1976). An analysis of the Turing test. Philosophical Studies 30 (4). ( Google ) Moor, James H. (1978). Explaining computer behavior. Philosophical Studies 34 (October):325-7. ( Cited by 9 | Annotation | Google | More links ) Reply to Stalker 1978: Mechanistic and mentalistic explanations are no more incompatible than program-based and physical explanations. Additional links for this entry: http://www.springerlink.com/content/x313g334823725mv/fulltext.pdf http://www.springerlink.com/index/X313G334823725MV.pdf Moor, James H. (2001). The status and future of the Turing test. Minds and Machines 11 (1):77-93. ( Cited by 9 | Google | More links ) Abstract:   The standard interpretation of the imitation game is defended over the rival gender interpretation though it is noted that Turing himself proposed several variations of his imitation game. The Turing test is then justified as an inductive test not as an operational definition as commonly suggested. Turing's famous prediction about his test being passed at the 70% level is disconfirmed by the results of the Loebner 2000 contest and the absence of any serious Turing test competitors from AI on the horizon. But, reports of the death of the Turing test and AI are premature. AI continues to flourish and the test continues to play an important philosophical role in AI. Intelligence attribution, methodological, and visionary arguments are given in defense of a continuing role for the Turing test. With regard to Turing's predictions one is disconfirmed, one is confirmed, but another is still outstanding Additional links for this entry: http://books.google.com/books?hl=en=7Zb746qi1B6hBAlr4ehD3tEmgZc http://books.google.com/books?hl=en=BtFI0p2CUYyifPgOjuKkYI__bFc http://www.springerlink.com/content/ju585836x7571202/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=319545=1 http://www.springerlink.com/index/JU585836X7571202.pdf http://www.ingentaconnect.com/content/klu/mind/2001/00000011/00000001/00319545 Nichols, Shaun Stich, Stephen P. (1994). Folk psychology. Encyclopedia of Cognitive Science . ( Cited by 2 | Google | More links ) Abstract: For the last 25 years discussions and debates about commonsense psychology (or “folk psychology,” as it is often called) have been center stage in the philosophy of mind. There have been heated disagreements both about what folk psychology is and about how it is related to the scientific understanding of the mind/brain that is emerging in psychology and the neurosciences. In this chapter we will begin by explaining why folk psychology plays such an important role in the philosophy of mind. Doing that will require a quick look at a bit of the history of philosophical discussions about the mind. We’ll then turn our attention to the lively contemporary discussions aimed at clarifying the philosophical role that folk psychology is expected to play and at using findings in the cognitive sciences to get a clearer understanding of the exact nature of folk psychology Additional links for this entry: http://www.blutner.de/philom/Texte/folkpsych5.pdf http://www.blutner.de/NeuralNets/Texts/folkpsych5.pdf http://amor.rz.hu-berlin.de/~h0998dgh/philos/Texte/folkpsych5.pdf http://amor.cms.hu-berlin.de/~h0998dgh/philos/Texte/folkpsych5.pdf http://dingo.sbs.arizona.edu/~snichols/Papers/FolkPsychologyECS.pdf http://amor.rz.hu-berlin.de/~h0998dgh/NeuralNets/Texts/folkpsych5.pdf http://www.rci.rutgers.edu/~stich/Publications/Papers/FolkPsychology.pdf http://www.rci.rutgers.edu/~stich/Publications/Papers/FolkPsychology.pdf http://www-personal.umich.edu/~lormand/phil/teach/mmm/readings/Stich - Cognitive Science and the Appeal to Tacit Theory.rtf http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation Oppy, Graham Dowe, D. (online). The Turing test. Stanford Encyclopedia of Philosophy . ( Cited by 3 | Google ) Piccinini, Gualtiero (2000). Turing's rules for the imitation game. Minds and Machines 10 (4):573-582. ( Cited by 10 | Google | More links ) Abstract:   In the 1950s, Alan Turing proposed his influential test for machine intelligence, which involved a teletyped dialogue between a human player, a machine, and an interrogator. Two readings of Turing''s rules for the test have been given. According to the standard reading of Turing''s words, the goal of the interrogator was to discover which was the human being and which was the machine, while the goal of the machine was to be indistinguishable from a human being. According to the literal reading, the goal of the machine was to simulate a man imitating a woman, while the interrogator – unaware of the real purpose of the test – was attempting to determine which of the two contestants was the woman and which was the man. The present work offers a study of Turing''s rules for the test in the context of his advocated purpose and his other texts. The conclusion is that there are several independent and mutually reinforcing lines of evidence that support the standard reading, while fitting the literal reading in Turing''s work faces severe interpretative difficulties. So, the controversy over Turing''s rules should be settled in favor of the standard reading Additional links for this entry: http://books.google.com/books?hl=en=sOABQfQAHOiAecLsjc4Dkdn0Kko http://books.google.com/books?hl=en=k_FvcCrJGK1pHaGSeZX1ciLb8SM http://www.springerlink.com/content/content/v1k277193084750u/fulltext.pdf http://www.springerlink.com/content/v1k277193084750u/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=319535=1 http://www.springerlink.com/index/V1K277193084750U.pdf http://www.ingentaconnect.com/content/klu/mind/2000/00000010/00000004/00319535 Purthill, R. (1971). Beating the imitation game. Mind 80 (April):290-94. ( Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2252425.pdf Rankin, Terry L. (1987). The Turing paradigm: A critical assessment. Dialogue 29 (April):50-55. ( Cited by 3 | Annotation | Google ) Some obscure remarks on lying, imitation, and the Turing test. Rapaport, William J. (2000). How to pass a Turing test: Syntactic semantics, natural-language understanding, and first-person cognition. Journal of Logic, Language, and Information 9 (4):467-490. ( Cited by 15 | Google | More links ) Additional links for this entry: http://www.cse.buffalo.edu/tech-reports/99-06.ps http://www.cse.buffalo.edu/tech-reports/99-06.ps.Z http://www.cse.buffalo.edu/~rapaport/Papers/turing.pdf http://historical.ncstrl.org/litesite-data/suny_buffalo_cs/99-06.ps http://www.kluweronline.com/article.asp?PIPS=269490=1 http://www.springerlink.com/index/JW2371N7T37H2V10.pdf http://www.ingentaconnect.com/content/klu/jlli/2000/00000009/00000004/00269490 Rapaport, William J. (2000). How to pass a Turing test. Journal of Logic, Language and Information 9 (4). ( Google ) Abstract: I advocate a theory of syntactic semantics as a way of understanding how computers can think (and how the Chinese-Room-Argument objection to the Turing Test can be overcome): (1) Semantics, considered as the study of relations between symbols and meanings, can be turned into syntax – a study of relations among symbols (including meanings) – and hence syntax (i.e., symbol manipulation) can suffice for the semantical enterprise (contra Searle). (2) Semantics, considered as the process of understanding one domain (by modeling it) in terms of another, can be viewed recursively: The base case of semantic understanding –understanding a domain in terms of itself – is syntactic understanding. (3) An internal (or narrow), first-person point of view makes an external (or wide), third-person point of view otiose for purposes of understanding cognition Rapaport, William J. (online). Review of The Turing Test: Verbal Behavior As the Hallmark of Intelligence . ( Google | More links ) Abstract: Stuart M. Shieber’s name is well known to computational linguists for his research and to computer scientists more generally for his debate on the Loebner Turing Test competition, which appeared a decade earlier in Communications of the ACM (Shieber 1994a, 1994b; Loebner 1994). 1 With this collection, I expect it to become equally well known to philosophers Additional links for this entry: http://www.mitpressjournals.org/doi/pdf/10.1162/089120105774321127 http://www.cse.buffalo.edu/~rapaport/Papers/shieberreview-original.pdf Ravenscroft, Ian (online). Folk psychology as a theory. Stanford Encyclopedia of Philosophy . ( Cited by 9 | Google | More links ) Abstract: Many philosophers and cognitive scientists claim that our everyday or "folk" understanding of mental states constitutes a theory of mind. That theory is widely called "folk psychology" (sometimes "commonsense" psychology). The terms in which folk psychology is couched are the familiar ones of "belief" and "desire", "hunger", "pain" and so forth. According to many theorists, folk psychology plays a central role in our capacity to predict and explain the behavior of ourselves and others. However, the nature and status of folk psychology remains controversial Additional links for this entry: http://plato.stanford.edu/entries/folkpsych-theory http://plato.stanford.edu/entries/folkpsych-theory/ http://www.seop.leeds.ac.uk/entries/folkpsych-theory/ http://www.illc.uva.nl/~seop/entries/folkpsych-theory/ http://www.science.uva.nl/~seop/entries/folkpsych-theory/ http://setis.library.usyd.edu.au/stanford/entries/folkpsych-theory/ Rhodes, Kris (ms). Vindication of the Rights of Machine. ( Google | More links ) Abstract: In this paper, I argue that certain Machines can have rights independently of whether they are sentient, or conscious, or whatever you might call it. Additional links for this entry: http://www.igradeyourpaper.com/VRMv3.doc Richardson, Robert C. (1982). Turing tests for intelligence: Ned Block's defense of psychologism. Philosophical Studies 41 (May):421-6. ( Cited by 4 | Annotation | Google | More links ) A weak argument against Block: input/output function doesn't guarantee a capacity to respond sensibly. Additional links for this entry: http://www.springerlink.com/index/G3151345X710L62H.pdf Rosenberg, Jay F. (1982). Conversation and intelligence. In B. de Gelder (ed.), Knowledge and Representation . Routledge & Kegan Paul. ( Google ) Sampson, Geoffrey (1973). In defence of Turing. Mind 82 (October):592-94. ( Cited by 5 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2252213.pdf Sato, Y. Ikegami, T. (2004). Undecidability in the imitation game. Minds and Machines 14 (2):133-43. ( Cited by 6 | Google | More links ) Abstract:   This paper considers undecidability in the imitation game, the so-called Turing Test. In the Turing Test, a human, a machine, and an interrogator are the players of the game. In our model of the Turing Test, the machine and the interrogator are formalized as Turing machines, allowing us to derive several impossibility results concerning the capabilities of the interrogator. The key issue is that the validity of the Turing test is not attributed to the capability of human or machine, but rather to the capability of the interrogator. In particular, it is shown that no Turing machine can be a perfect interrogator. We also discuss meta-imitation game and imitation game with analog interfaces where both the imitator and the interrogator are mimicked by continuous dynamical systems Additional links for this entry: http://portal.acm.org/citation.cfm?id=976736 http://www.bdc.brain.riken.jp/publications/internal/jour-Undecidability_in_the_Imitation_Game/uig.pdf http://www.bdc.brain.riken.go.jp/publications/internal/jour-Undecidability_in_the_Imitation_Game/uig.pdf http://www.bdc.brain.riken.go.jp/publications/internal/trep-Undecidability_in_the_imitation_game/riken-bsi-bdc-tr2001-04.ps.Z http://www.springerlink.com/content/t2771g8512m67737/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=5253133=1 http://www.springerlink.com/index/T2771G8512M67737.pdf http://www.ingentaconnect.com/content/klu/mind/2004/00000014/00000002/05253133 Saygin, Ayse P. ; Cicekli, Ilyas Akman, Varol (2000). Turing test: 50 years later. Minds and Machines 10 (4):463-518. (Cited by 45 | Google | More links ) Abstract:   The Turing Test is one of the most disputed topics in artificial intelligence, philosophy of mind, and cognitive science. This paper is a review of the past 50 years of the Turing Test. Philosophical debates, practical developments and repercussions in related disciplines are all covered. We discuss Turing''s ideas in detail and present the important comments that have been made on them. Within this context, behaviorism, consciousness, the `other minds'' problem, and similar topics in philosophy of mind are discussed. We also cover the sociological and psychological aspects of the Turing Test. Finally, we look at the current situation and analyze programs that have been developed with the aim of passing the Turing Test. We conclude that the Turing Test has been, and will continue to be, an influential and controversial topic Additional links for this entry: http://citeseer.ist.psu.edu/378189.html http://crl.ucsd.edu/~saygin/papers/MMTT.pdf http://portal.acm.org/citation.cfm?id=596724.596952 http://cogprints.ecs.soton.ac.uk/archive/00001925/00/tt50.ps http://www.cs.bilkent.edu.tr/~akman/jour-papers/mam/mam2000.pdf http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1925 http://www.springerlink.com/content/content/ph7275k8w0137245/fulltext.pdf http://www.springerlink.com/content/ph7275k8w0137245/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=319526=1 http://www.springerlink.com/index/PH7275K8W0137245.pdf http://cogprints.org/1925/2/tt50.ps http://cogprints.org/1925/0/tt50.ps Saygin, A. P. Cicekli, I. (2000). Turing test: 50 years later. Minds and Machines 10 (4):463-518. ( Cited by 44 | Google | More links ) Abstract: The Turing Test is one of the most disputed topics in artiﬁcial intelligence, philosophy of mind, and cognitive science. This paper is a review of the past 50 years of the Turing Test. Philo- sophical debates, practical developments and repercussions in related disciplines are all covered. We discuss Turing’s ideas in detail and present the important comments that have been made on them. Within this context, behaviorism, consciousness, the ‘other minds’ problem, and similar topics in philosophy of mind are discussed. We also cover the sociological and psychological aspects of the Turing Test. Finally, we look at the current situation and analyze programs that have been developed with the aim of passing the Turing Test. We conclude that the Turing Test has been, and will continue to be, an inﬂuential and controversial topic Additional links for this entry: http://citeseer.ist.psu.edu/378189.html http://crl.ucsd.edu/~saygin/papers/MMTT.pdf http://portal.acm.org/citation.cfm?id=596724.596952 http://cogprints.ecs.soton.ac.uk/archive/00001925/00/tt50.ps http://www.cs.bilkent.edu.tr/~akman/jour-papers/mam/mam2000.pdf http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1925 http://www.kluweronline.com/article.asp?PIPS=319526=1 http://www.springerlink.com/index/PH7275K8W0137245.pdf http://cogprints.org/1925/2/tt50.ps http://cogprints.org/1925/0/tt50.ps Schweizer, Paul (1998). The truly total Turing test. Minds and Machines 8 (2):263-272. ( Cited by 9 | Google | More links ) Abstract:   The paper examines the nature of the behavioral evidence underlying attributions of intelligence in the case of human beings, and how this might be extended to other kinds of cognitive system, in the spirit of the original Turing Test (TT). I consider Harnad's Total Turing Test (TTT), which involves successful performance of both linguistic and robotic behavior, and which is often thought to incorporate the very same range of empirical data that is available in the human case. However, I argue that the TTT is still too weak, because it only tests the capabilities of particular tokens within a preexisting context of intelligent behavior. What is needed is a test of the cognitive type, as manifested through a number of exemplary tokens, in order to confirm that the cognitive type is able to produce the context of intelligent behavior presupposed by tests such as the TT and TTT Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=371134CI http://www.springerlink.com/content/h51pp233m1h732p0/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=150465=1 http://www.springerlink.com/index/H51PP233M1H732P0.pdf http://www.ingentaconnect.com/content/klu/mind/1998/00000008/00000002/00150465 Sennett, James F. (ms). The ice man cometh: Lt. comander data and the Turing test. ( Google ) Shanon, Benny (1989). A simple comment regarding the Turing test. Journal for the Theory of Social Behavior 19 (June):249-56. ( Cited by 8 | Annotation | Google | More links ) The Turing test presupposes a representational/computational framework for cognition. Not all phenomena can be captured in teletype communication. Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/120004126/PDFSTART Shah, Huma Warwick, Kevin (forthcoming). From the Buzzing in Turing’s Head to Machine Intelligence Contests. TCIT 2010 . ( Google ) Abstract: This paper presents an analysis of three major contests for machine intelligence. We conclude that a new era for Turing’s test requires a fillip in the guise of a committed sponsor, not unlike DARPA, funders of the successful 2007 Urban Challenge. Shieber, Stuart M. (1994). Lessons from a restricted Turing test. Communications of the Association for Computing Machinery 37:70-82. ( Cited by 55 | Google | More links ) Additional links for this entry: http://adsabs.harvard.edu/abs/1994cmp.lg....4002S http://citeseer.ist.psu.edu/shieber93lessons.html http://www.eecs.harvard.edu/shieber/Biblio/Papers/loebner-rev-html/loebner-rev-html.html http://www.eecs.harvard.edu/~shieber/Biblio/Papers/loebner-rev-html/loebner-rev-html.html http://portal.acm.org/citation.cfm?id=175208.175217=Communications of the ACM Shieber, Stuart M. (ed.) (2004). The Turing Test: Verbal Behavior As the Hallmark of Intelligence. MIT Press. ( Cited by 12 | Google | More links ) Abstract: Stuart M. Shieber’s name is well known to computational linguists for his research and to computer scientists more generally for his debate on the Loebner Turing Test competition, which appeared a decade earlier in Communications of the ACM (Shieber 1994a, 1994b; Loebner 1994). 1 With this collection, I expect it to become equally well known to philosophers Additional links for this entry: http://portal.acm.org/citation.cfm?id=993978 http://www.cse.buffalo.edu/~rapaport/Papers/shieberreview.pdf http://www.mitpressjournals.org/doi/abs/10.1162/089120105774321127 http://www.mitpressjournals.org/doi/pdf/10.1162/089120105774321127 http://www.cse.buffalo.edu/~rapaport/Papers/shieberreview-original.pdf http://www.mitpressjournals.org/doi/pdfplus/10.1162/089120105774321127 http://www.csa.com/partners/viewrecord.php?requester=gs=2006018403060CI Shieber, Stuart M. (2007). The Turing test as interactive proof. Noûs 41 (4):686713. ( Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/117997259/PDFSTART Stalker, Douglas F. (1978). Why machines can't think: A reply to James Moor. Philosophical Studies 34 (3):317-20. ( Cited by 12 | Annotation | Google | More links ) Contra Moor 1976: The best explanation of computer behavior is mechanistic, not mentalistic. Additional links for this entry: http://www.springerlink.com/index/X101842G87R85027.pdf Sterrett, Susan G. (2002). Nested algorithms and the original imitation game test: A reply to James Moor. Minds and Machines 12 (1):131-136. ( Cited by 2 | Google | More links ) Additional links for this entry: http://www.springerlink.com/content/x4225320762736q0/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=391369=1 http://www.springerlink.com/index/X4225320762736Q0.pdf http://www.ingentaconnect.com/content/klu/mind/2002/00000012/00000001/00391369 Stevenson, John G. (1976). On the imitation game. Philosophia 6 (March):131-33. ( Cited by 4 | Google | More links ) Additional links for this entry: http://www.springerlink.com/index/A10M7KW582216882.pdf Sterrett, Susan G. (2000). Turing's two tests for intelligence. Minds and Machines 10 (4):541-559. ( Cited by 10 | Google | More links ) Abstract:   On a literal reading of `Computing Machinery and Intelligence'', Alan Turing presented not one, but two, practical tests to replace the question `Can machines think?'' He presented them as equivalent. I show here that the first test described in that much-discussed paper is in fact not equivalent to the second one, which has since become known as `the Turing Test''. The two tests can yield different results; it is the first, neglected test that provides the more appropriate indication of intelligence. This is because the features of intelligence upon which it relies are resourcefulness and a critical attitude to one''s habitual responses; thus the test''s applicablity is not restricted to any particular species, nor does it presume any particular capacities. This is more appropriate because the question under consideration is what would count as machine intelligence. The first test realizes a possibility that philosophers have overlooked: a test that uses a human''s linguistic performance in setting an empirical test of intelligence, but does not make behavioral similarity to that performance the criterion of intelligence. Consequently, the first test is immune to many of the philosophical criticisms on the basis of which the (so-called) `Turing Test'' has been dismissed Additional links for this entry: http://books.google.com/books?hl=en=kpE5iqY7Fjm67XjSS7t2e9XqGDo http://www.springerlink.com/content/content/k11t54j7821h4230/fulltext.pdf http://www.springerlink.com/content/k11t54j7821h4230/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=319533=1 http://www.springerlink.com/index/K11T54J7821H4230.pdf http://www.ingentaconnect.com/content/klu/mind/2000/00000010/00000004/00319533 Stoica, Cristi , Turing test, easy to pass; human mind, hard to understand. ( Google ) Abstract: Under general assumptions, the Turing test can be easily passed by an appropriate algorithm. I show that for any test satisfying several general conditions, we can construct an algorithm that can pass that test, hence, any operational definition is easy to fulfill. I suggest a test complementary to Turing's test, which will measure our understanding of the human mind. The Turing test is required to fix the operational specifications of the algorithm under test; under this constrain, the additional test simply consists in measuring the length of the algorithm Traiger, Saul (2000). Making the right identification in the Turing test. Minds and Machines 10 (4):561-572. ( Cited by 7 | Google | More links ) Abstract:   The test Turing proposed for machine intelligence is usually understood to be a test of whether a computer can fool a human into thinking that the computer is a human. This standard interpretation is rejected in favor of a test based on the Imitation Game introduced by Turing at the beginning of "Computing Machinery and Intelligence." Additional links for this entry: http://www.traiger.oxy.edu/publications/making-the-right-identification.pdf http://faculty.oxy.edu/traiger/publications/making-the-right-identification.pdf http://0-www.faculty.oxy.edu.oasys.lib.oxy.edu/traiger/publications/making-the-right-identification.pdf http://books.google.com/books?hl=en=SFxczaJwBF-Ba70js7yMbXB8L6Y http://books.google.com/books?hl=en=4ZGzMTB0yiYy62v_qt0Q_DwtyKI http://www.springerlink.com/content/g90h6kqu506lwr75/fulltext.pdf http://www.springerlink.com/index/G90H6KQU506LWR75.pdf http://www.ingentaconnect.com/content/klu/mind/2000/00000010/00000004/00319538 Turney, Peter (ms). Answering subcognitive Turing test questions: A reply to French. ( Cited by 5 | Google | More links ) Abstract: Robert French has argued that a disembodied computer is incapable of passing a Turing Test that includes subcognitive questions. Subcognitive questions are designed to probe the network of cultural and perceptual associations that humans naturally develop as we live, embodied and embedded in the world. In this paper, I show how it is possible for a disembodied computer to answer subcognitive questions appropriately, contrary to Frenchs claim. My approach to answering subcognitive questions is to use statistical information extracted from a very large collection of text. In particular, I show how it is possible to answer a sample of subcognitive questions taken from French, by issuing queries to a search engine that indexes about 350 million Web pages. This simple algorithm may shed light on the nature of human (sub-) cognition, but the scope of this paper is limited to demonstrating that French is mistaken: a disembodied computer can answer subcognitive questions Additional links for this entry: http://adsabs.harvard.edu/abs/2002cs.......12015T http://citeseer.ist.psu.edu/turney01answering.html http://cogprints.ecs.soton.ac.uk/archive/00001798/ http://www.extractor.com/download/documents/subcognitive.pdf http://cogprints.ecs.soton.ac.uk/archive/00001798/00/subcognitive.pdf https://iit-iti.nrc-cnrc.gc.ca/iit-publications-iti/docs/NRC-44898.pdf http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1798 http://taylorandfrancis.metapress.com/index/TW3Q03K8NK7UVQJG.pdf http://www.informaworld.com/index/TW3Q03K8NK7UVQJG.pdf http://www.ingentaconnect.com/content/tandf/teta/2001/00000013/00000004/art00006 http://cogprints.org/1798/1/subcognitive.ps http://cogprints.org/1798/0/subcognitive.pdf Turing, Alan M. (1950). Computing machinery and intelligence. Mind 59 (October):433-60. ( Cited by 9 | Annotation | Google | More links ) Proposes the Imitation game (Turing test) as a test for intelligence: If a machine can't be told apart from a human in a conversation over a teletype, then that's good enough. With responses to various objections. Abstract: I propose to consider the question, "Can machines think?" This should begin with definitions of the meaning of the terms "machine" and "think." The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous, If the meaning of the words "machine" and "think" are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, "Can machines think?" is to be sought in a statistical survey such as a Gallup poll. But this is absurd. Instead of attempting such a definition I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words. The new form of the problem can be described in terms of a game which we call the 'imitation game." It is played with three people, a man (A), a woman (B), and an interrogator (C) who may be of either sex. The interrogator stays in a room apart front the other two. The object of the game for the interrogator is to determine which of the other two is the man and which is the woman. He knows them by labels X and Y, and at the end of the game he says either "X is A and Y is B" or "X is B and Y is A." The interrogator is allowed to put questions to A and B. We now ask the question, "What will happen when a machine takes the part of A in this game?" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, "Can machines think?" Additional links for this entry: http://www.ttt.org/ling490/TuringTest.pdf http://www.agent.ai/doc/upload/200302/turi50_1.pdf http://eric.ed.gov/ERICWebPortal/recordDetail?accno=EJ216711 http://www.dcs.gla.ac.uk/~joe/Teaching/ATiCS/FC-TuringAI-intro.pdf http://www.dfki.uni-sb.de/imedia/lidos/bibtex/Collins_a10168-96.html http://www.marxists.org/reference/subject/philosophy/works/en/turing.htm http://lia.deis.unibo.it/corsi/2005-2006/SID-LS-CE/downloads/turing-article.pdf http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:499 http://twiki.di.uniroma1.it/twiki/viewfile/Users/MarcoBianchi?rev=1.1;filename=TuringComputingMachineryAndIntelligence.pdf http://archive.computerhistory.org/projects/chess/related_materials/text/2-0 and 2-1.Computing_machinery_and_intelligence.turing/2-0 and 2-1.Computing_machinery_and_intelligence.turing-alan.mind-59.1950.062303001.pdf http://mind.oxfordjournals.org/cgi/reprint/LIX/236/433 http://www.jstor.org/stable/pdfplus/2251299.pdf http://cogprints.org/499/1/turing.html http://cogprints.org/499/0/turing.html Vergauwen, Roger González, Rodrigo (2005). On the verisimilitude of artificial intelligence. Logique Et Analyse- 190 (189):323-350. ( Google ) Ward, Andrew (1989). Radical interpretation and the Gunderson game. Dialectica 43 (3):271-280. ( Google ) Watt, S. (1996). Naive psychology and the inverted Turing test. Psycoloquy 7 (14). ( Cited by 19 | Google | More links ) Abstract: This target article argues that the Turing test implicitly rests on a "naive psychology," a naturally evolved psychological faculty which is used to predict and understand the behaviour of others in complex societies. This natural faculty is an important and implicit bias in the observer's tendency to ascribe mentality to the system in the test. The paper analyses the effects of this naive psychology on the Turing test, both from the side of the system and the side of the observer, and then proposes and justifies an inverted version of the test which allows the processes of ascription to be analysed more directly than in the standard version Additional links for this entry: http://citeseer.ist.psu.edu/watt96naive.html http://psycprints.ecs.soton.ac.uk/archive/00000506/ http://psycprints.ecs.soton.ac.uk/perl/local/psyc/makedoc?id=506=html http://psycprints.ecs.soton.ac.uk/archive/00000506/02/psyc.96.7.14.turing-test.1.watt http://psycprints.ecs.soton.ac.uk/archive/00000506/01/psyc.96.7.14.turing-test.1.watt.xml Waterman, C. (1995). The Turing test and the argument from analogy for other minds. Southwest Philosophy Review 11 (1):15-22. ( Google ) Whitby, Blay (1996). The Turing test: Ai's biggest blind Alley? In Peter Millican A. Clark (eds.), Machines and Thought . Oxford University Press. ( Cited by 13 | Google ) Whitby, Blay (1996). Why the Turing test is ai's biggest blind Alley. In Peter Millican A. Clark (eds.), Machines and Thought, The Legacy of Alan Turing . Oup. ( Google ) Zdenek, Sean (2001). Passing loebner's Turing test: A case of conflicting discourse functions. Minds and Machines 11 (1):53-76. ( Cited by 8 | Google | More links ) Abstract:   This paper argues that the Turing test is based on a fixed and de-contextualized view of communicative competence. According to this view, a machine that passes the test will be able to communicate effectively in a variety of other situations. But the de-contextualized view ignores the relationship between language and social context, or, to put it another way, the extent to which speakers respond dynamically to variations in discourse function, formality level, social distance/solidarity among participants, and participants' relative degrees of power and status (Holmes, 1992). In the case of the Loebner Contest, a present day version of the Turing test, the social context of interaction can be interpreted in conflicting ways. For example, Loebner discourse is defined 1) as a friendly, casual conversation between two strangers of equal power, and 2) as a one-way transaction in which judges control the conversational floor in an attempt to expose contestants that are not human. This conflict in discourse function is irrelevant so long as the goal of the contest is to ensure that only thinking, human entities pass the test. But if the function of Loebner discourse is to encourage the production of software that can pass for human on the level of conversational ability, then the contest designers need to resolve this ambiguity in discourse function, and thus also come to terms with the kind of competence they are trying to measure Additional links for this entry: http://books.google.com/books?hl=en=a-VgtvUjcFyxiSpkIASbwHF4Imk http://books.google.com/books?hl=en=IxxOEy4OGv0MfwNy7oTRa0j0_9g http://www.springerlink.com/content/p1572g5485036674/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=319544=1 http://www.springerlink.com/index/P1572G5485036674.pdf http://www.ingentaconnect.com/content/klu/mind/2001/00000011/00000001/00319544 6.1b Godelian arguments Benacerraf, Paul (1967). God, the devil, and Godel. The Monist 51 (January):9-32. ( Annotation | Google ) Discusses and sharpens Lucas's arguments. Argues that the real consequence is that if we are Turing machines, we can't know which. Bojadziev, Damjan (1997). Mind versus Godel. In Matjaz Gams M. Wu Paprzycki (eds.), Mind Versus Computer . IOS Press. ( Cited by 1 | Google | More links ) Additional links for this entry: http://books.google.com/books?hl=en=oyuz5heebVFZdtmfGXFv50bP2zo Bowie, G. Lee (1982). Lucas' number is finally up. Journal of Philosophy Logic 11 (August):279-85. ( Cited by 10 | Annotation | Google | More links ) Lucas's very Godelization procedure makes him inconsistent, unless he has an independent way to see if any TM is consistent, which he doesn't. Additional links for this entry: http://www.springerlink.com/index/G54120627174138N.pdf Boyer, David L. (1983). R. Lucas, Kurt Godel, and Fred astaire. Philosophical Quarterly 33 (April):147-59. ( Annotation | Google | More links ) Remarks on the various ways in which Lucas and a machine might be said to "prove" anything, and the ways in which a machine might simulate Lucas. The argument has all sorts of level confusions, and a bit of circularity. Additional links for this entry: http://www.jstor.org/stable/pdfplus/2218741.pdf Bringsjord, Selmer Xiao, H. (2000). A refutation of Penrose's new Godelian case against the computational conception of mind. Journal of Experimental and Theoretical Artificial Intelligence 12. ( Google ) Chari, C. T. K. (1963). Further comments on minds, machines and Godel. Philosophy 38 (April):175-8. ( Annotation | Google ) Can't reduce the lawless creative process to computation. Chalmers, David J. (1996). Minds, machines, and mathematics. Psyche 2:11-20. ( Cited by 17 | Google | More links ) Abstract: In his stimulating book SHADOWS OF THE MIND, Roger Penrose presents arguments, based on Gödel's theorem, for the conclusion that human thought is uncomputable. There are actually two separate arguments in Penrose's book. The second has been widely ignored, but seems to me to be much more interesting and novel than the first. I will address both forms of the argument in some detail. Toward the end, I will also comment on Penrose's proposals for a "new science of consciousness" Additional links for this entry: http://www.u.arizona.edu/~chalmers/papers/penrose.html http://psyche.cs.monash.edu.au/v2/psyche-2-09-chalmers.html http://calculemus.org/MathUniversalis/NS/10/03chalmers.html http://psyche.cs.monash.edu.au/volume2-1/psyche-95-2-09-shadows-7-chalmers.html Chihara, C. (1972). On alleged refutations of mechanism using Godel's incompleteness results. Journal of Philosophy 69 (September):507-26. ( Cited by 9 | Annotation | Google | More links ) An analysis of the Lucas/Benacerraf argument. On various senses in which a machine might come to know its own program. Additional links for this entry: http://www.jstor.org/stable/pdfplus/2025144.pdf Coder, David (1969). Godel's theorem and mechanism. Philosophy 44 (September):234-7. ( Annotation | Google ) Only mathematicians understand Godel, so Lucas's argument isn't general; and Turing machines can go wrong. Weak. Copeland, Jack (1998). Turing's o-machines, Searle, Penrose, and the brain. Analysis 58 (2):128-138. ( Cited by 15 | Google | More links ) Abstract: In his PhD thesis (1938) Turing introduced what he described as 'a new kind of machine'. He called these 'O-machines'. The present paper employs Turing's concept against a number of currently fashionable positions in the philosophy of mind Additional links for this entry: http://www.alanturing.net/turing_archive/pages/pub/turing1/turing1.pdf http://www.blackwell-synergy.com/links/doi/10.1111/1467-8284.00113 http://www.blackwell-synergy.com/doi/abs/10.1111/1467-8284.00113 http://www.ingentaconnect.com/content/bpl/anal/1998/00000058/00000258/art00113 Dennett, Daniel C. (1989). Murmurs in the cathedral: Review of R. Penrose, The Emperor's New Mind . Times Literary Supplement (September) 29. ( Cited by 5 | Google ) Abstract: The idea that a computer could be conscious--or equivalently, that human consciousness is the effect of some complex computation mechanically performed by our brains--strikes some scientists and philosophers as a beautiful idea. They find it initially surprising and unsettling, as all beautiful ideas are, but the inevitable culmination of the scientific advances that have gradually demystified and unified the material world. The ideologues of Artificial Intelligence (AI) have been its most articulate supporters. To others, this idea is deeply repellent: philistine, reductionistic (in some bad sense), as incredible as it is offensive. John Searle's attack on "strong AI" is the best known expression of this view, but others in the same camp, liking Searle's destination better than his route, would dearly love to see a principled, scientific argument showing that strong AI is impossible. Roger Penrose has set out to provide just such an argument Dennett, Daniel C. (1978). The abilities of men and machines. In Brainstorms . MIT Press. ( Cited by 3 | Annotation | Google ) There is no unique TM which we are -- there could be many. Edis, Taner (1998). How Godel's theorem supports the possibility of machine intelligence. Minds and Machines 8 (2):251-262. ( Google | More links ) Abstract:   Gödel's Theorem is often used in arguments against machine intelligence, suggesting humans are not bound by the rules of any formal system. However, Gödelian arguments can be used to support AI, provided we extend our notion of computation to include devices incorporating random number generators. A complete description scheme can be given for integer functions, by which nonalgorithmic functions are shown to be partly random. Not being restricted to algorithms can be accounted for by the availability of an arbitrary random function. Humans, then, might not be rule-bound, but Gödelian arguments also suggest how the relevant sort of nonalgorithmicity may be trivially made available to machines Additional links for this entry: http://www.springerlink.com/content/t36552h6169u6mp8/fulltext.pdf Feferman, S. (1996). Penrose's Godelian argument. Psyche 2:21-32. ( Google ) Abstract: In his book Shadows of the Mind: A search for the missing science of con- sciousness [SM below], Roger Penrose has turned in another bravura perfor- mance, the kind we have come to expect ever since The Emperor’s New Mind [ENM ] appeared. In the service of advancing his deep convictions and daring conjectures about the nature of human thought and consciousness, Penrose has once more drawn a wide swath through such topics as logic, computa- tion, artiﬁcial intelligence, quantum physics and the neuro-physiology of the brain, and has produced along the way many gems of exposition of diﬃcult mathematical and scientiﬁc ideas, without condescension, yet which should be broadly appealing. 1 While the aims and a number of the topics in SM are the same as in ENM , the focus now is much more on the two axes that Pen- rose grinds in earnest. Namely, in the ﬁrst part of SM he argues anew and at great length against computational models of the mind and more speciﬁ- cally against any account of mathematical thought in computational terms. Then in the second part, he argues that there must be a scientiﬁc account of consciousness but that will require a (still to be found) non-computational extension or modiﬁcation of present-day quantum physics Gaifman, H. (2000). What Godel's incompleteness result does and does not show. Journal of Philosophy 97 (8):462-471. ( Cited by 3 | Google | More links ) Abstract: In a recent paper S. McCall adds another link to a chain of attempts to enlist Gödel’s incompleteness result as an argument for the thesis that human reasoning cannot be construed as being carried out by a computer.1 McCall’s paper is undermined by a technical oversight. My concern however is not with the technical point. The argument from Gödel’s result to the no-computer thesis can be made without following McCall’s route; it is then straighter and more forceful. Yet the argument fails in an interesting and revealing way. And it leaves a remainder: if some computer does in fact simulate all our mathematical reasoning, then, in principle, we cannot fully grasp how it works. Gödel’s result also points out a certain essential limitation of self-reflection. The resulting picture parallels, not accidentally, Davidson’s view of psychology, as a science that in principle must remain “imprecise”, not fully spelt out. What is intended here by “fully grasp”, and how all this is related to self-reflection, will become clear at the end of this comment Additional links for this entry: http://www.jstor.org/stable/pdfplus/2678427.pdf George, A. Velleman, Daniel J. (2000). Leveling the playing field between mind and machine: A reply to McCall. Journal of Philosophy 97 (8):456-452. ( Cited by 3 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2678426.pdf George, F. H. (1962). Minds, machines and Godel: Another reply to mr. Lucas. Philosophy 37 (January):62-63. ( Annotation | Google ) Lucas's argument applies only to deductive machines, not inductive ones. Gertler, Brie (2004). Simulation theory on conceptual grounds. Protosociology 20:261-284. ( Google ) Abstract: I will present a conceptual argument for a simulationist answer to (2). Given that our conception of mental states is employed in attributing mental states to others, a simulationist answer to (2) supports a simulationist answer to (1). I will not address question (3). Answers to (1) and (2) do not yield an answer to (3), since (1) and (2) concern only our actual practices and concepts. For instance, an error theory about (1) and (2) would say that our practices and concepts manifest a mistaken view about the real nature of the mental. Finally, I will not address question (2a), which is an empirical question and so is not immediately relevant to the conceptual argument that is of concern here Good, I. J. (1969). Godel's theorem is a red Herring. British Journal for the Philosophy of Science 19 (February):357-8. ( Cited by 8 | Annotation | Google | More links ) Rejoinder to Lucas 1967: the role of consistency; non-constructible ordinals. Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/citation/19/4/357 http://bjps.oxfordjournals.org/cgi/reprint/19/4/357 http://www.jstor.org/stable/pdfplus/686295.pdf Good, I. J. (1967). Human and machine logic. British Journal for the Philosophy of Science 18 (August):145-6. ( Cited by 7 | Annotation | Google | More links ) Even humans can't Godelize forever. On ordinals and transfinite counting. Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/citation/18/2/144 http://bjps.oxfordjournals.org/cgi/reprint/18/2/144 http://www.jstor.org/stable/pdfplus/686582.pdf Gordon, Robert M. (online). Folk Psychology As Mental Simulation. Stanford Encyclopedia of Philosophy . ( Cited by 8 | Google ) Abstract: by, or is otherwise relevant to the seminar "Folk Psychology vs. Mental Simulation: How Minds Understand Minds," a National Grush, Rick Churchland, P. (1995). Gaps in Penrose's toiling. In Thomas Metzinger (ed.), Conscious Experience . Ferdinand Schoningh. ( Google | More links ) Abstract: Using the Gödel Incompleteness Result for leverage, Roger Penrose has argued that the mechanism for consciousness involves quantum gravitational phenomena, acting through microtubules in neurons. We show that this hypothesis is implausible. First, the Gödel Result does not imply that human thought is in fact non algorithmic. Second, whether or not non algorithmic quantum gravitational phenomena actually exist, and if they did how that could conceivably implicate microtubules, and if microtubules were involved, how that could conceivably implicate consciousness, is entirely speculative. Third, cytoplasmic ions such as calcium and sodium are almost certainly present in the microtubule pore, barring the quantum mechanical effects Penrose envisages. Finally, physiological evidence indicates that consciousness does not directly depend on microtubule properties in any case, rendering doubtful any theory according to which consciousness is generated in the microtubules Additional links for this entry: http://mind.ucsd.edu/papers/penrose/penrose.rtf Hadley, Robert F. (1987). Godel, Lucas, and mechanical models of mind. Computational Intelligence 3:57-63. ( Cited by 1 | Annotation | Google | More links ) A nice analysis of Lucas's argument and the circumstances under which a machine might prove another's Godel sentences. There's no reason to believe that machines and humans are different here. Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/120022395/PDFSTART Hanson, William H. (1971). Mechanism and Godel's theorem. British Journal for the Philosophy of Science 22 (February):9-16. ( Annotation | Google | More links ) An analysis of Benacerraf 1967. Benacerraf's "paradox" is illusory; there are no strong consequences of Godel's theorem for mechanism. Additional links for this entry: http://bjps.oxfordjournals.org/cgi/reprint/22/1/9 http://www.jstor.org/stable/pdfplus/686639.pdf Hofstadter, Douglas R. (1979). Godel, Escher, Bach: An Eternal Golden Braid. Basic Books. ( Cited by 65 | Annotation | Google | More links ) Contra Lucas: we can't Godelize forever; and we're not formal on top level. Additional links for this entry: http://www.w3.org/People/Connolly/books_webqcat http://adsabs.harvard.edu/abs/1979geba.book.....H http://www.w3.org/hypertext/WWW/People/Connolly/books_webqcat http://books.google.com/books?hl=en=k1N3eE1J6IDPURJdAP19PVlW24Q Hutton, A. (1976). This Godel is killing me. Philosophia 3 (March):135-44. ( Annotation | Google ) Gives a statistical argument to the effect that we cannot know that we are consistent; so the Lucas argument cannot go through. Irvine, Andrew D. (1983). Lucas, Lewis, and mechanism -- one more time. Analysis 43 (March):94-98. ( Annotation | Google ) Contra Lewis 1979, Lucas can derive the consistency of M even without the premise that he is M. Hmm. Jacquette, Dale (1987). Metamathematical criteria for minds and machines. Erkenntnis 27 (July):1-16. ( Cited by 3 | Annotation | Google | More links ) A machine will fail a Turing test if it's asked about Godel sentences. Additional links for this entry: http://www.springerlink.com/content/x6832560243n3x6q/fulltext.pdf http://www.springerlink.com/index/N7273Q71P8376195.pdf http://www.springerlink.com/index/X6832560243N3X6Q.pdf Ketland, Jeffrey Raatikainen, Panu (online). Truth and provability again. ( Google ) King, D. (1996). Is the human mind a Turing machine? Synthese 108 (3):379-89. ( Google | More links ) Abstract:   In this paper I discuss the topics of mechanism and algorithmicity. I emphasise that a characterisation of algorithmicity such as the Turing machine is iterative; and I argue that if the human mind can solve problems that no Turing machine can, the mind must depend on some non-iterative principle — in fact, Cantor's second principle of generation, a principle of the actual infinite rather than the potential infinite of Turing machines. But as there has been theorisation that all physical systems can be represented by Turing machines, I investigate claims that seem to contradict this: specifically, claims that there are noncomputable phenomena. One conclusion I reach is that if it is believed that the human mind is more than a Turing machine, a belief in a kind of Cartesian dualist gulf between the mental and the physical is concomitant Additional links for this entry: http://www.springerlink.com/index/P4761624416HGN82.pdf Kirk, Robert E. (1986). Mental machinery and Godel. Synthese 66 (March):437-452. ( Annotation | Google ) Lucas's argument fails, as theorems by humans don't correspond to outputs of their formal systems. Laforte, Geoffrey ; Hayes, Pat Ford, Kenneth M. (1998). Why Godel's theorem cannot refute computationalism: A reply to Penrose. Artificial Intelligence 104. ( Google ) Leslie, Alan M. ; Nichols, Shaun ; Stich, Stephen P. Klein, David B. (1996). Varieties of off-line simulation. In P. Carruthers P. Smith (eds.), Theories of Theories of Mind . Cambridge University Press. ( Google ) Abstract: In the last few years, off-line simulation has become an increasingly important alternative to standard explanations in cognitive science. The contemporary debate began with Gordon (1986) and Goldman's (1989) off-line simulation account of our capacity to predict behavior. On their view, in predicting people's behavior we take our own decision making system `off line' and supply it with the `pretend' beliefs and desires of the person whose behavior we are trying to predict; we then let the decision maker reach a decision on the basis of these pretend inputs. Figure 1 offers a `boxological' version of the off-line simulation theory of behavior prediction.(1) Lewis, David (1969). Lucas against mechanism. Philosophy 44 (June):231-3. ( Cited by 10 | Annotation | Google ) Lucas needs a rule of inference from sentences to their consistency, yielding Lucas arithmetic. No machine can prove all of Lucas arithmetic, but there's no reason to suppose humans can either, as the rule is infinitary. Lewis, David (1979). Lucas against mechanism II. Canadian Journal of Philosophy 9 (June):373-6. ( Cited by 7 | Annotation | Google ) Reply to Lucas 1970: the dialectical argument fails, as the human's output depends on the premise that it is the machine (to derive M's consistency). With a similar premise, the machine itself can do equally well. Lindstrom, Per (2006). Remarks on Penrose's new argument. Journal of Philosophical Logic 35 (3):231-237. ( Google | More links ) Abstract: It is commonly agreed that the well-known Lucas–Penrose arguments and even Penrose’s ‘new argument’ in [Penrose, R. (1994): Shadows of the Mind, Oxford University Press] are inconclusive. It is, perhaps, less clear exactly why at least the latter is inconclusive. This note continues the discussion in [Lindström, P. (2001): Penrose’s new argument, J. Philos. Logic 30, 241–250; Shapiro, S.(2003): Mechanism, truth, and Penrose’s new argument, J. Philos. Logic 32, 19–42] and elsewhere of this question Additional links for this entry: http://www.springerlink.com/index/FU83655LG7754253.pdf http://www.ingentaconnect.com/content/klu/logi/2006/00000035/00000003/00009014 Lucas, John R. (1967). Human and machine logic: A rejoinder. British Journal for the Philosophy of Science 19 (August):155-6. ( Cited by 3 | Annotation | Google | More links ) Reply to Good 1967: a human can trump any given machine, so the human is not the machine, whether or not the human is superior across the board. Abstract: We can imagine a human operator playing a game of one-upmanship against a programmed computer. If the program is Fn, the human operator can print the theorem Gn, which the programmed computer, or, if you prefer, the program, would never print, if it is consistent. This is true for each whole number n, but the victory is a hollow one since a second computer, loaded with program C, could put the human operator out of a job.... It is useless for the `mentalist' to argue that any given program can always be improves since the process for improving programs can presumably be programmed also; certainly this can be done if the mentalist describes how the improvement is to be made. If he does give such a description, then he has not made a case Additional links for this entry: http://www.univ.trieste.it/~etica/2003_1/6_monographica.doc http://bjps.oxfordjournals.org/cgi/content/citation/19/2/155 http://bjps.oxfordjournals.org/cgi/reprint/19/2/155 http://www.jstor.org/stable/pdfplus/686794.pdf Lucas, John R. (1984). Lucas against mechanism II: A rejoinder. Canadian Journal of Philosophy 14 (June):189-91. ( Cited by 2 | Annotation | Google ) Reply to Lewis 1979. Lucas, John R. (1970). Mechanism: A rejoinder. Philosophy 45 (April):149-51. ( Annotation | Google ) Response to Lewis 1969 and Coder 1969. Lewis misses the dialectical nature of the argument. Lucas, John R. (1971). Metamathematics and the philosophy of mind: A rejoinder. Philosophy of Science 38 (2):310-13. ( Cited by 4 | Google | More links ) Additional links for this entry: http://www.journals.uchicago.edu/cgi-bin/resolve?id=doi:10.1086/288368 http://www.jstor.org/stable/pdfplus/186792.pdf Lucas, John R. (1961). Minds, machines and Godel. Philosophy 36 (April-July):112-127. ( Cited by 72 | Annotation | Google | More links ) Humans can Godelize any given machine, so we're not a machine. Abstract: Goedel's theorem states that in any consistent system which is strong enough to produce simple arithmetic there are formulae which cannot be proved-in-the-system, but which we can see to be true. Essentially, we consider the formula which says, in effect, "This formula is unprovable-in-the-system". If this formula were provable-in-the-system, we should have a contradiction: for if it were provablein-the-system, then it would not be unprovable-in-the-system, so that "This formula is unprovable-in-the-system" would be false: equally, if it were provable-in-the-system, then it would not be false, but would be true, since in any consistent system nothing false can be provedin-the-system, but only truths. So the formula "This formula is unprovable-in-the-system" is not provable-in-the-system, but unprovablein-the-system. Further, if the formula "This formula is unprovablein- the-system" is unprovable-in-the-system, then it is true that that formula is unprovable-in-the-system, that is, "This formula is unprovable-in-the-system" is true. Goedel's theorem must apply to cybernetical machines, because it is of the essence of being a machine, that it should be a concrete instantiation of a formal system. It follows that given any machine which is consistent and capable of doing simple arithmetic, there is a formula which it is incapable of producing as being true---i.e., the formula is unprovable-in-the-system-but which we can see to be true. It follows that no machine can be a complete or adequate model of the mind, that minds are essentially different from machines Additional links for this entry: http://cogprints.org/356/1/lucas.html Lucas, John R. (1996). Mind, machines and Godel: A retrospect. In Peter Millican A. Clark (eds.), Machines and Thought . Oxford University Press. ( Annotation | Google ) Addresses all the counterarguments. Fun. Lucas, John R. (1968). Satan stultified: A rejoinder to Paul Benacerraf. The Monist 52 (1):145-58. ( Cited by 10 | Annotation | Google ) Benacerraf 1967 is empty and omega-inconsistent. Reply to arguments based on difficulty of seeing consistency (e.g. Putnam). Fallacious but engaging. Abstract: The argument is a dialectical one. It is not a direct proof that the mind is something more than a machine, but a schema of disproof for any particular version of mechanism that may be put forward. If the mechanist maintains any specific thesis, I show that [146] a contradiction ensues. But only if. It depends on the mechanist making the first move and putting forward his claim for inspection. I do not think Benacerraf has quite taken the point. He criticizes me both for "failing to notice" that my ability to show that the Gödel sentence of a formal system is true "depends very much on how he is given Lucas, John R. Redhead, Michael (2007). Truth and provability. British Journal for the Philosophy of Science 58 (2):331-2. ( Google | More links ) Abstract: The views of Redhead ([2004]) are defended against the argument by Panu Raatikainen ([2005]). The importance of informal rigour is canvassed, and the argument for the a priori nature of induction is explained. The significance of Gödel's theorem is again rehearsed Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/full/axm006v1 http://bjps.oxfordjournals.org/cgi/reprint/58/2/331 Lucas, John R. (1970). The Freedom of the Will. Oxford University Press. ( Cited by 22 | Google ) Lucas, John R. (ms). The Godelian argument: Turn over the page. ( Cited by 3 | Google ) Abstract: I have no quarrel with the first two sentences: but the third, though charitable and courteous, is quite untrue. Although there are criticisms which can be levelled against the Gödelian argument, most of the critics have not read either of my, or either of Penrose's, expositions carefully, and seek to refute arguments we never put forward, or else propose as a fatal objection one that had already been considered and countered in our expositions of the argument. Hence my title. The Gödelian Argument uses Gödel's theorem to show that minds cannot be explained in purely mechanist terms. It has been put forward, in different forms, by Gödel himself, by Penrose, and by me Lucas, John R. (1976). This Godel is killing me: A rejoinder. Philosophia 6 (March):145-8. ( Annotation | Google ) Contra Hutton, we know -- even if fallibly -- that we are consistent. Lucas, John R. (ms). The implications of Godel's theorem. ( Google | More links ) Abstract: In 1931 Kurt Gödel proved two theorems about the completeness and consistency of first-order arithmetic. Their implications for philosophy are profound. Many fashionable tenets are shown to be untenable: many traditional intuitions are vindicated by incontrovertible arguments Additional links for this entry: http://users.ox.ac.uk/~jrlucas/Godel/goedhand.html http://users.ox.ac.uk/~jrlucas/Godel/implgoed.html Lyngzeidetson, Albert E. Solomon, Martin K. (1994). Abstract complexity theory and the mind-machine problem. British Journal for the Philosophy of Science 45 (2):549-54. ( Google | More links ) Abstract: In this paper we interpret a characterization of the Gödel speed-up phenomenon as providing support for the ‘Nagel-Newman thesis’ that human theorem recognizers differ from mechanical theorem recognizers in that the former do not seem to be limited by Gödel's incompleteness theorems whereas the latter do seem to be thus limited. However, we also maintain that (currently non-existent) programs which are open systems in that they continuously interact with, and are thus inseparable from, their environment, are not covered by the above (or probably any other recursion-theoretic) argument Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/abstract/45/2/549 http://bjps.oxfordjournals.org/cgi/reprint/45/2/549 http://www.jstor.org/stable/pdfplus/687681.pdf Lyngzeidetson, Albert E. (1990). Massively parallel distributed processing and a computationalist foundation for cognitive science. British Journal for the Philosophy of Science 41 (March):121-127. ( Annotation | Google | More links ) A Connection Machine might escape the Lucas argument. Bizarre. Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/abstract/41/1/121 http://bjps.oxfordjournals.org/cgi/reprint/41/1/121 http://www.jstor.org/stable/pdfplus/688006.pdf Martin, J. Engleman, K. (1990). The mind's I has two eyes. Philosophy 65 (264):510-515. ( Annotation | Google ) Contra Hofstadter: Lucas can believe his Whitely sentence. Maudlin, Tim (1996). Between the motion and the act. Psyche 2:40-51. ( Cited by 4 | Google | More links ) Additional links for this entry: http://psyche.cs.monash.edu.au/v2/psyche-2-02-maudlin.html http://psyche.cs.monash.edu.au/volume2-1/psyche-95-2-02-shadows-1-maudlin.html McCall, Storrs (1999). Can a Turing machine know that the Godel sentence is true? Journal of Philosophy 96 (10):525-32. ( Cited by 6 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2564612.pdf McCullough, D. (1996). Can humans escape Godel? Psyche 2:57-65. ( Google ) McCall, Storrs (2001). On "seeing" the truth of the Godel sentence. Facta Philosophica 3:25-30. ( Google ) McDermott, Drew (1996). [Star] Penrose is wrong. Psyche 2:66-82. ( Google ) Megill, Jason L. (2004). Are we paraconsistent? On the Lucas-Penrose argument and the computational theory of mind. Auslegung 27 (1):23-30. ( Google ) Nelson, E. (2002). Mathematics and the mind. In Kunio Yasue, Marj Jibu Tarcisio Della Senta (eds.), No Matter, Never Mind . John Benjamins. ( Cited by 2 | Google | More links ) Additional links for this entry: http://www.math.princeton.edu/~nelson/papers/tokyo.pdf http://www.math.princeton.edu/~nelson/papers/tokyo.ps.gz http://star.tau.ac.il/~eshel/Bio_complexity/Conceptual Background/Mathematics-mind.pdf http://books.google.com/books?hl=en=L490G6v3TwiKps8crkNSHEMZak4 Penrose, Roger (1996). Beyond the doubting of a shadow. Psyche 2:89-129. ( Cited by 25 | Annotation | Google | More links ) A reply to Chalmers, Feferman, Maudlin, McDermott, etc. Additional links for this entry: http://calculemus.org/MathUniversalis/NS/10/01penrose.html http://psyche.csse.monash.edu.au/v2/psyche-2-23-penrose.html http://psyche.cs.monash.edu.au/volume2-1/psyche-96-2-23-shadows-10-penrose.html Penrose, Roger (1990). Precis of the emperor's new mind. Behavioral and Brain Sciences 13:643-705. ( Annotation | Google ) Much debate over the "non-algorithmic insight" in seeing Godel sentences. Penrose, Roger (1994). Shadows of the Mind. Oxford University Press. ( Cited by 1412 | Google | More links ) Additional links for this entry: http://portal.acm.org/citation.cfm?id=527739 http://www.fortunecity.com/emachines/e11/86/shadow.html http://www.fortunecity.com/emachines/e11/86/shadow1.html Penrose, Roger (1992). Setting the scene: The claim and the issues. In D. Broadbent (ed.), The Simulation of Human Intelligence . Blackwell. ( Annotation | Google ) An argument from the halting problem to the nonalgorithmicity of mathematical thought. Addresses objections: that the algorithm is unknowable, unsound, everchanging, environmental, or random. New physical laws may be involved. Penrose, Roger (1989). The Emperor's New Mind. Oxford University Press. ( Cited by 3 | Annotation | Google | More links ) We are non-algorithmic as we can see Godel sentences of any algorithm. Additional links for this entry: http://link.aip.org/link/?AJPIAS/58/1214/1 http://portal.acm.org/citation.cfm?id=68395 http://adsabs.harvard.edu/abs/1990AmJPh..58.1214P http://www.reiters.com/index.cgi?ISBN=0099771705=p http://relativity.livingreviews.org/refdb/record/5936 http://www.kokogiak.com/amazon/detpage.asp?asin=018144755X http://relativity.livingreviews.org/refdb/unapi?id=lrr-1998-1-PenroseBook=bibtex http://books.google.com/books?hl=en=04UwqLnaAMsFKF2tAizpzMz7rkY http://books.google.com/books?hl=en=f-ZpKzuvcBwzPzlEUCBmN1m_EbE http://books.google.com/books?hl=en=4JZhl1HlToWmKToz3aXL93fwLxo http://books.google.com/books?hl=en=T9G-BlT6yCiOxYS78zX0RLjfJW0 http://books.google.com/books?hl=en=EHqvTqlUjD9R-B4QqA8OGU8NUUs http://books.google.com/books?hl=en=nLDUVLJwJRvnPP9IFUtlt2FLKfo http://books.google.com/books?hl=en=ABeb7RU3dzx96MNvKLOi5YoQgV0 http://books.google.com/books?hl=en=LbO-NgFr6jmNMkqdEDiXX3SRrqo http://books.google.com/books?hl=en=odNqbL9v6mKh4u4qdpA2fkkbma8 http://orton.catie.ac.cr/cgi-bin/wxis.exe/?IsisScript=IDEA.xis=mfn=002295 Piccinini, Gualtiero (2003). Alan Turing and the mathematical objection. Minds and Machines 13 (1):23-48. ( Cited by 10 | Google | More links ) Abstract: This paper concerns Alan Turing’s ideas about machines, mathematical methods of proof, and intelligence. By the late 1930s, Kurt Gödel and other logicians, including Turing himself, had shown that no finite set of rules could be used to generate all true mathematical statements. Yet according to Turing, there was no upper bound to the number of mathematical truths provable by intelligent human beings, for they could invent new rules and methods of proof. So, the output of a human mathematician, for Turing, was not a computable sequence (i.e., one that could be generated by a Turing machine). Since computers only contained a finite number of instructions (or programs), one might argue, they could not reproduce human intelligence. Turing called this the “mathematical objection” to his view that machines can think. Logico-mathematical reasons, stemming from his own work, helped to convince Turing that it should be possible to reproduce human intelligence, and eventually compete with it, by developing the appropriate kind of digital computer. He felt it should be possible to program a computer so that it could learn or discover new rules, overcoming the limitations imposed by the incompleteness and undecidability results in the same way that human mathematicians presumably do. Additional links for this entry: http://www.kluweronline.com/article.asp?PIPS=5098147=1 http://www.ingentaconnect.com/content/klu/mind/2003/00000013/00000001/05098147 http://www.springerlink.com/index/UL15V486260V8Q79.pdf http://www.umsl.edu/~piccininig/Alan_Turing_and_Mathematical_Objection.pdf Priest, Graham (1994). Godel's theorem and the mind... Again. In M. Michael John O'Leary-Hawthorne (eds.), Philosophy in Mind: The Place of Philosophy in the Study of Mind . Kluwer. ( Google ) Putnam, Hilary (1995). Review of Shadows of the Mind . AMS Bulletin 32 (3). ( Google ) Putnam, Hilary (1985). Reflexive reflections. Erkenntnis 22 (January):143-153. ( Cited by 8 | Annotation | Google | More links ) A generalized Godelian argument: if our prescriptive inductive competence is formalizable, then we could not know that such a formalization is correct. Additional links for this entry: http://books.google.com/books?hl=en=UU2i1OXUpETqyryBJ8N290QLcJc http://www.springerlink.com/content/kr0702201813pk58/fulltext.pdf http://www.springerlink.com/index/KR0702201813PK58.pdf Raatikainen, Panu , McCall's gödelian argument is invalid. ( Google ) Abstract: Storrs McCall continues the tradition of Lucas and Penrose in an attempt to refute mechanism by appealing to Gödel’s incompleteness theorem (McCall 2001). That is, McCall argues that Gödel’s theorem “reveals a sharp dividing line between human and machine thinking”. According to McCall, “[h]uman beings are familiar with the distinction between truth and theoremhood, but Turing machines cannot look beyond their own output”. However, although McCall’s argumentation is slightly more sophisticated than the earlier Gödelian anti-mechanist arguments, in the end it fails badly, as it is at odds with the logical facts Raatikainen, Panu (2005). On the philosophical relevance of gödel's incompleteness theorems. Revue Internationale de Philosophie 59 (4):513-534. ( Google ) Abstract: Gödel began his 1951 Gibbs Lecture by stating: “Research in the foundations of mathematics during the past few decades has produced some results which seem to me of interest, not only in themselves, but also with regard to their implications for the traditional philosophical problems about the nature of mathematics.” (Gödel 1951) Gödel is referring here especially to his own incompleteness theorems (Gödel 1931). Gödel’s first incompleteness theorem (as improved by Rosser (1936)) says that for any consistent formalized system F, which contains elementary arithmetic, there exists a sentence GF of the language of the system which is true but unprovable in that system. Gödel’s second incompleteness theorem states that no consistent formal system can prove its own consistency Raatikainen, Panu (2005). Truth and provability: A comment on Redhead. British Journal for the Philosophy of Science 56 (3):611-613. (Cited by 2 | Google | More links ) Abstract: Michael Redhead's recent argument aiming to show that humanly certifiable truth outruns provability is critically evaluated. It is argued that the argument is at odds with logical facts and fails Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/abstract/56/3/611 http://bjps.oxfordjournals.org/cgi/rapidpdf/axi134v1.pdf http://bjps.oxfordjournals.org/cgi/reprint/56/3/611 http://www.ingentaconnect.com/content/oup/phisci/2005/00000056/00000003/art00611 Raatikainen, Panu (ms). Truth and provability again. ( Google ) Abstract: Lucas and Redhead ([2007]) announce that they will defend the views of Redhead ([2004]) against the argument by Panu Raatikainen ([2005]). They certainly re-state the main claims of Redhead ([2004]), but they do not give any real arguments in their favour, and do not provide anything that would save Redhead’s argument from the serious problems pointed out in (Raatikainen [2005]). Instead, Lucas and Redhead make a number of seemingly irrelevant points, perhaps indicating a failure to understand the logico-mathematical points at issue Redhead, M. (2004). Mathematics and the mind. British Journal for the Philosophy of Science 55 (4):731-737. ( Cited by 6 | Google | More links ) Abstract: Granted that truth is valuable we must recognize that certifiable truth is hard to come by, for example in the natural and social sciences. This paper examines the case of mathematics. As a result of the work of Gödel and Tarski we know that truth does not equate with proof. This has been used by Lucas and Penrose to argue that human minds can do things which digital computers can't, viz to know the truth of unprovable arithmetical statements. The argument is given a simple formulation in the context of sorites (Robinson) arithmetic, avoiding the complexities of formulating the Gödel sentence. The pros and cons of the argument are considered in relation to the conception of mathematical truth. * Paper contributed to the Conference entitled The Place of Value in a World of Facts, held at the LSE in October 2003 Additional links for this entry: http://bjps.oupjournals.org/cgi/content/abstract/55/4/731 http://bjps.oxfordjournals.org/cgi/reprint/55/4/731 http://www.ingentaconnect.com/content/oup/phisci/2004/00000055/00000004/art00731 Robinson, William S. (1992). Penrose and mathematical ability. Analysis 52 (2):80-88. ( Annotation | Google ) Penrose's argument depends on our knowledge of the validity of the algorithm we use, and here he equivocates between conscious and unconscious algorithms. Schurz, Gerhard (2002). McCall and Raatikainen on mechanism and incompleteness. Facta Philosophica 4:171-74. ( Google ) Seager, William E. (2003). Yesterday's algorithm: Penrose and the Godel argument. Croatian Journal of Philosophy 3 (9):265-273. ( Google ) Abstract: Roger Penrose is justly famous for his work in physics and mathematics but he is _notorious_ for his endorsement of the Gödel argument (see his 1989, 1994, 1997). This argument, first advanced by J. R. Lucas (in 1961), attempts to show that Gödel’s (first) incompleteness theorem can be seen to reveal that the human mind transcends all algorithmic models of it 1 . Penrose's version of the argument has been seen to fall victim to the original objections raised against Lucas (see Boolos (1990) and for a particularly intemperate review, Putnam (1994)). Yet I believe that more can and should be said about the argument. Only a brief review is necessary here although I wish to present the argument in a somewhat peculiar form Slezak, Peter (1983). Descartes's diagonal deduction. British Journal for the Philosophy of Science 34 (March):13-36. ( Cited by 13 | Annotation | Google | More links ) Cogito was a diagonal argument; connection to Godel, Lucas, Minsky, Nagel. Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/citation/34/1/13 http://bjps.oxfordjournals.org/cgi/reprint/34/1/13 http://www.jstor.org/stable/pdfplus/686931.pdf Slezak, Peter (1982). Godel's theorem and the mind. British Journal for the Philosophy of Science 33 (March):41-52. ( Cited by 13 | Annotation | Google | More links ) General analysis; Lucas commits type/token error; self-ref paradoxes. Additional links for this entry: http://bjps.oxfordjournals.org/cgi/reprint/33/1/41 http://www.jstor.org/stable/pdfplus/687239.pdf Slezak, Peter (1984). Minds, machines and self-reference. Dialectica 38:17-34. ( Cited by 1 | Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/120035817/PDFSTART Sloman, Aaron (1986). The emperor's real mind. In A.G. Cohn J.R. Thomas (eds.), Artificial Intelligence and Its Applications . John Wiley and Sons. ( Google ) Smart, J. J. C. (1961). Godel's theorem, church's theorem, and mechanism. Synthese 13 (June):105-10. ( Annotation | Google ) A machine could escape the Godelian argument by inductively ascertaining its own syntax. With comments on the relevance of ingenuity. Stone, Tony Davies, Martin (1998). Folk psychology and mental simulation. Royal Institute of Philosophy Supplement 43:53-82. ( Google | More links ) Abstract: This paper is about the contemporary debate concerning folk psychology – the debate between the proponents of the theory theory of folk psychology and the friends of the simulation alternative. 1 At the outset, we need to ask: What should we mean by this term ‘folk psychology’? Additional links for this entry: http://philrsss.anu.edu.au/~mdavies/papers/simrip.pdf http://www.nyu.edu/gsas/dept/philo/courses/concepts/folkpsychology.html Tymoczko, Thomas (1991). Why I am not a Turing machine: Godel's theorem and the philosophy of mind. In Jay L. Garfield (ed.), Foundations of Cognitive Science . Paragon House. ( Annotation | Google ) Weak defense of Lucas; response to Putnam, Bowie, Dennett. Wang, H. (1974). From Mathematics to Philosophy. London. ( Cited by 125 | Google ) Webb, Judson (1968). Metamathematics and the philosophy of mind. Philosophy of Science 35 (June):156-78. ( Cited by 6 | Google | More links ) Additional links for this entry: http://www.journals.uchicago.edu/cgi-bin/resolve?id=doi:10.1086/288199 http://www.jstor.org/stable/pdfplus/186484.pdf Webb, Judson (1980). Mechanism, Mentalism and Metamathematics. Kluwer. ( Cited by 45 | Google ) Whitely, C. (1962). Minds, machines and Godel: A reply to mr Lucas. Philosophy 37 (January):61-62. ( Annotation | Google ) Humans get trapped too: "Lucas cannot consistently assert this formula". Yu, Q. (1992). Consistency, mechanicalness, and the logic of the mind. Synthese 90 (1):145-79. ( Cited by 4 | Google | More links ) Abstract:   G. Priest's anti-consistency argument (Priest 1979, 1984, 1987) and J. R. Lucas's anti-mechanist argument (Lucas 1961, 1968, 1970, 1984) both appeal to Gödel incompleteness. By way of refuting them, this paper defends the thesis of quartet compatibility, viz., that the logic of the mind can simultaneously be Gödel incomplete, consistent, mechanical, and recursion complete (capable of all means of recursion). A representational approach is pursued, which owes its origin to works by, among others, J. Myhill (1964), P. Benacerraf (1967), J. Webb (1980, 1983) and M. Arbib (1987). It is shown that the fallacy shared by the two arguments under discussion lies in misidentifying two systems, the one for which the Gödel sentence is constructable and to be proved, and the other in which the Gödel sentence in question is indeed provable. It follows that the logic of the mind can surpass its own Gödelian limitation not by being inconsistent or non-mechanistic, but by being capable of representing stronger systems in itself; and so can a proper machine. The concepts of representational provability, representational maximality, formal system capacity, etc., are discussed Additional links for this entry: http://www.springerlink.com/index/X073727543G61372.pdf 6.1c The Chinese Room Adam, Alison (2003). Cyborgs in the chinese room: Boundaries transgressed and boundaries blurred. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Google ) Aleksander, Igor L. (2003). Neural depictions of "world" and "self": Bringing computational understanding into the chinese room. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Google ) Anderson, David (1987). Is the chinese room the real thing? Philosophy 62 (July):389-93. ( Cited by 9 | Google ) Andrews, Kristin (online). On predicting behavior. ( Google ) Abstract: I argue that the behavior of other agents is insufficiently described in current debates as a dichotomy between tacit theory (attributing beliefs and desires to predict behavior) and simulation theory (imagining what one would do in similar circumstances in order to predict behavior). I introduce two questions about the foundation and development of our ability both to attribute belief and to simulate it. I then propose that there is one additional method used to predict behavior, namely, an inductive strategy Atlas, Jay David , What is it like to be a chinese room? ( Google | More links ) Abstract: When philosophers think about mental phenomena, they focus on several features of human experience: (1) the existence of consciousness, (2) the intentionality of mental states, that property by which beliefs, desires, anger, etc. are directed at, are about, or refer to objects and states of affairs, (3) subjectivity, characterized by my feeling my pains but not yours, by my experiencing the world and myself from my point of view and not yours, (4) mental causation, that thoughts and feelings have physical effects on the world: I decide to raise my arm and my arm rises. In a world described by theories of physics and chemistry, what place in that physical description do descriptions of the mental have? Additional links for this entry: http://pages.pomona.edu/~jda14747/Atlas_WHAT%20IS%20IT%20LIKE%20TO%20BE%20A%20CHINESE%20ROOM_1995.pdf Ben-Yami, Hanoch (1993). A note on the chinese room. Synthese 95 (2):169-72. ( Cited by 3 | Annotation | Google | More links ) A fully functional Chinese room is impossible, as it (for instance) could not say what the time is. Abstract:   Searle's Chinese Room was supposed to prove that computers can't understand: the man in the room, following, like a computer, syntactical rules alone, though indistinguishable from a genuine Chinese speaker, doesn't understand a word. But such a room is impossible: the man won't be able to respond correctly to questions like What is the time?, even though such an ability is indispensable for a genuine Chinese speaker. Several ways to provide the room with the required ability are considered, and it is concluded that for each of these the room will have understanding. Hence, Searle's argument is invalid Additional links for this entry: http://www.springerlink.com/index/K1NX461746N57467.pdf Block, Ned (2003). Searle's arguments against cognitive science. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Cited by 2 | Google ) Boden, Margaret A. (1988). Escaping from the chinese room. In Computer Models of Mind . Cambridge University Press. ( Cited by 21 | Annotation | Google ) A procedural account of how computers might have understanding and semantics. Bringsjord, Selmer Noel, Ron (2003). Real robots and the missing thought-experiment in the chinese room dialectic. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Google | More links ) Additional links for this entry: http://kryten.mm.rpi.edu/searlebook1.pdf http://citeseer.ist.psu.edu/bringsjord99real.html Brown, Steven Ravett (2000). Peirce and formalization of thought: The chinese room argument. Journal of Mind and Behavior . ( Google | More links ) Abstract: Whether human thinking can be formalized and whether machines can think in a human sense are questions that have been addressed by both Peirce and Searle. Peirce came to roughly the same conclusion as Searle, that the digital computer would not be able to perform human thinking or possess human understanding. However, his rationale and Searle's differ on several important points. Searle approaches the problem from the standpoint of traditional analytic philosophy, where the strict separation of syntax and semantics renders understanding impossible for a purely syntactical device. Peirce disagreed with that analysis, but argued that the computer would only be able to achieve algorithmic thinking, which he considered the simplest type. Although their approaches were radically dissimilar, their conclusions were not. I will compare and analyze the arguments of both Peirce and Searle on this issue, and outline some implications of their conclusions for the field of Artificial Intelligence Additional links for this entry: http://cogprints.ecs.soton.ac.uk/archive/00001002/ http://cogprints.soton.ac.uk/documents/disk0/00/00/10/02/ http://cogprints.ecs.soton.ac.uk/archive/00001002/00/Peirce_Paper.html http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1002 http://cogprints.org/1002/1/Peirce_Paper.html http://cogprints.org/1002/0/Peirce_Paper.html Button, Graham ; Coutler, Jeff Lee, John R. E. (2000). Re-entering the chinese room: A reply to Gottfried and Traiger. Minds and Machines 10 (1):145-148. ( Google | More links ) Additional links for this entry: http://www.kluweronline.com/article.asp?PIPS=238829=1 http://www.springerlink.com/index/JQ41061085707655.pdf http://www.ingentaconnect.com/content/klu/mind/2000/00000010/00000001/00238829 Bynum, Terrell Ward (1985). Artificial intelligence, biology, and intentional states. Metaphilosophy 16 (October):355-77. ( Cited by 9 | Annotation | Google | More links ) A chess-playing machine embodied as a robot could have intentional states. Reference requires input/output, computation, and context. Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/120031491/PDFSTART Cam, Philip (1990). Searle on strong AI. Australasian Journal of Philosophy 68 (1):103-8. ( Cited by 2 | Annotation | Google | More links ) Criticizes Searle's "conclusion" that brains are needed for intentionality, notes that even a homunculus has intentional states. A misinterpretation. Additional links for this entry: http://taylorandfrancis.metapress.com/index/X701829J6248U835.pdf http://www.informaworld.com/index/739199028.pdf http://www.ingentaconnect.com/content/routledg/ajphil/1990/00000068/00000001/art00007 http://www.ingentaconnect.com/content/tandf/tajp/1990/00000068/00000001/art00007 http://www.informaworld.com/smpp/./ftinterface~db=all~content=a739199028~fulltext=713240930 Carleton, Lawrence Richard (1984). Programs, language understanding, and Searle. Synthese 59 (May):219-30. ( Cited by 8 | Annotation | Google | More links ) Arguing against Searle on a number of fronts, somewhat unconvincingly. Additional links for this entry: http://www.springerlink.com/index/Q24263234455051L.pdf Chalmers, David J. (1992). Subsymbolic computation and the chinese room. In J. Dinsmore (ed.), The Symbolic and Connectionist Paradigms: Closing the Gap . Lawrence Erlbaum. ( Cited by 29 | Annotation | Google | More links ) Gives an account of symbolic vs. subsymbolic computation, and argues that the latter is less vulnerable to the Chinese-room intuition, as representations there are not computational tokens. Abstract: More than a decade ago, philosopher John Searle started a long-running controversy with his paper “Minds, Brains, and Programs” (Searle, 1980a), an attack on the ambitious claims of artificial intelligence (AI). With his now famous _Chinese Room_ argument, Searle claimed to show that despite the best efforts of AI researchers, a computer could never recreate such vital properties of human mentality as intentionality, subjectivity, and understanding. The AI research program is based on the underlying assumption that all important aspects of human cognition may in principle be captured in a computational model. This assumption stems from the belief that beyond a certain level, implementational details are irrelevant to cognition. According to this belief, neurons, and biological wetware in general, have no preferred status as the substrate for a mind. As it happens, the best examples of minds we have at present have arisen from a carbon-based substrate, but this is due to constraints of evolution and possibly historical accidents, rather than to an absolute metaphysical necessity. As a result of this belief, many cognitive scientists have chosen to focus not on the biological substrate of the mind, but instead on the abstract causal structure_ _that the mind embodies (at an appropriate level of abstraction). The view that it is abstract causal structure that is essential to mentality has been an implicit assumption of the AI research program since Turing (1950), but was first articulated explicitly, in various forms, by Putnam (1960), Armstrong (1970) and Lewis (1970), and has become known as _functionalism_. From here, it is a very short step to _computationalism_, the view that computational structure is what is important in capturing the essence of mentality. This step follows from a belief that any abstract causal structure can be captured computationally: a belief made plausible by the Church–Turing Thesis, which articulates the power Additional links for this entry: http://consc.net/papers/subsymbolic.pdf http://citeseer.ist.psu.edu/chalmers92subsymbolic.html http://www.u.arizona.edu/~chalmers/papers/subsymbolic.pdf Churchland, Paul M. Churchland, Patricia S. (1990). Could a machine think? Scientific American 262 (1):32-37. ( Cited by 102 | Annotation | Google | More links ) Artificial mentality is possible, not through classical AI but through brain-like AI. Argues the syntax/semantics point using an analogy with electromagnetism and luminance. Additional links for this entry: http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation Cohen, L. Jonathan (1986). What sorts of machines can understand the symbols they use? Proceedings of the Aristotelian Society 60:81-96. ( Google ) Cole, David J. (1991). Artificial intelligence and personal identity. Synthese 88 (September):399-417. ( Cited by 18 | Annotation | Google | More links ) In the Chinese room, neither the person nor the system understands: a virtual person does. This person isn't the system, just as a normal person isn't a body. Follows from the "Kornese" room, which has two distinct understanders. Abstract:   Considerations of personal identity bear on John Searle's Chinese Room argument, and on the opposed position that a computer itself could really understand a natural language. In this paper I develop the notion of a virtual person, modelled on the concept of virtual machines familiar in computer science. I show how Searle's argument, and J. Maloney's attempt to defend it, fail. I conclude that Searle is correct in holding that no digital machine could understand language, but wrong in holding that artificial minds are impossible: minds and persons are not the same as the machines, biological or electronic, that realize them Additional links for this entry: http://www.springerlink.com/content/t130331427116747/fulltext.pdf http://www.springerlink.com/index/T130331427116747.pdf Cole, David J. (1991). Artificial minds: Cam on Searle. Australasian Journal of Philosophy 69 (September):329-33. ( Cited by 3 | Google | More links ) Additional links for this entry: http://taylorandfrancis.metapress.com/index/X06438UX20015555.pdf http://www.informaworld.com/index/X06438UX20015555.pdf http://www.ingentaconnect.com/content/routledg/ajphil/1991/00000069/00000003/art00006 http://www.ingentaconnect.com/content/tandf/tajp/1991/00000069/00000003/art00006 http://www.informaworld.com/smpp/./ftinterface~db=all~content=a739199915~fulltext=713240930 Cole, David J. (1984). Thought and thought experiments. Philosophical Studies 45 (May):431-44. ( Cited by 15 | Annotation | Google | More links ) Lots of thought experiments like Searle's, against Searle. Searle's argument is like Leibniz's "mill" argument, with similar level confusions. Nice but patchy. Additional links for this entry: http://www.springerlink.com/index/J009M56335341362.pdf Cole, David J. (1994). The causal powers of CPUs. In Eric Dietrich (ed.), Thinking Computers and Virtual Persons . Academic Press. ( Cited by 2 | Google ) Cole, David (online). The chinese room argument. Stanford Encyclopedia of Philosophy . ( Google ) Copeland, B. Jack (1993). The curious case of the chinese gym. Synthese 95 (2):173-86. ( Cited by 12 | Annotation | Google | More links ) Advocates the systems reply, and criticizes Searle's "Chinese Gym" response to connectionism: Searle (like those he accuses) confuses a simulation with the thing being simulated. Nice. Abstract:   Searle has recently used two adaptations of his Chinese room argument in an attack on connectionism. I show that these new forms of the argument are fallacious. First I give an exposition of and rebuttal to the original Chinese room argument, and then a brief introduction to the essentials of connectionism Additional links for this entry: http://home.swipnet.se/drofe/chincym.pdf http://www.alanturing.net/turing_archive/pages/pub/chincym/chincym.pdf http://www.springerlink.com/content/ut111604435042jn/fulltext.pdf http://www.springerlink.com/index/UT111604435042JN.pdf Copeland, B. Jack (2003). The chinese room from a logical point of view. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Cited by 5 | Google ) Coulter, Jeff Sharrock, S. (2003). The hinterland of the chinese room. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Google ) Cutrona, Jr (ms). Zombies in Searle's chinese room: Putting the Turing test to bed. ( Google | More links ) Abstract: Searle’s discussions over the years 1980-2004 of the implications of his “Chinese Room” Gedanken experiment are frustrating because they proceed from a correct assertion: (1) “Instantiating a computer program is never by itself a sufficient condition of intentionality;” and an incorrect assertion: (2) “The explanation of how the brain produces intentionality cannot be that it does it by instantiating a computer program.” In this article, I describe how to construct a Gedanken zombie Chinese Room program that will pass the Turing test and at the same time unambiguously demonstrates the correctness of (1). I then describe how to construct a Gedanken Chinese brain program that will pass the Turing test, has a mind, and understands Chinese, thus demonstrating that (2) is incorrect. Searle’s instantiation of this program can and does produce intentionality. Searle’s longstanding ignorance of Chinese is simply irrelevant and always has been. I propose a truce and a plan for further exploration Additional links for this entry: http://cogprints.org/4636/1/TR%2D05%2D002.pdf Damper, Robert I. (2004). The chinese room argument--dead but not yet buried. Journal of Consciousness Studies 11 (5-6):159-169. ( Cited by 2 | Google | More links ) Additional links for this entry: http://eprints.resist.ecs.soton.ac.uk/9561/ http://eprints.ecs.soton.ac.uk/9561/01/damper.pdf http://www.ingentaconnect.com/content/imp/jcs/2004/00000011/F0020005/art00010 Damper, Robert I. (2006). The logic of Searle's chinese room argument. Minds and Machines 16 (2):163-183. ( Google | More links ) Abstract: John Searle’s Chinese room argument (CRA) is a celebrated thought experiment designed to refute the hypothesis, popular among artificial intelligence (AI) scientists and philosophers of mind, that “the appropriately programmed computer really is a mind”. Since its publication in 1980, the CRA has evoked an enormous amount of debate about its implications for machine intelligence, the functionalist philosophy of mind, theories of consciousness, etc. Although the general consensus among commentators is that the CRA is flawed, and not withstanding the popularity of the systems reply in some quarters, there is remarkably little agreement on exactly how and why it is flawed. A newcomer to the controversy could be forgiven for thinking that the bewildering collection of diverse replies to Searle betrays a tendency to unprincipled, ad hoc argumentation and, thereby, a weakness in the opposition’s case. In this paper, treating the CRA as a prototypical example of a ‘destructive’ thought experiment, I attempt to set it in a logical framework (due to Sorensen), which allows us to systematise and classify the various objections. Since thought experiments are always posed in narrative form, formal logic by itself cannot fully capture the controversy. On the contrary, much also hinges on how one translates between the informal everyday language in which the CRA was initially framed and formal logic and, in particular, on the specific conception(s) of possibility that one reads into the logical formalism Additional links for this entry: http://www.ingentaconnect.com/content/klu/mind/2006/00000016/00000002/00009031 Dennett, Daniel C. (1987). Fast thinking. In The Intentional Stance . MIT Press. ( Cited by 12 | Annotation | Google ) Argues with Searle on many points. A little weak. Double, Richard (1984). Reply to C.A. Field's Double on Searle's Chinese Room . Nature and System 6 (March):55-58. ( Google ) Double, Richard (1983). Searle, programs and functionalism. Nature and System 5 (March-June):107-14. ( Cited by 3 | Annotation | Google ) The homunculus doesn't have access to the system's intentionality. The syntax/semantics relation is like the neurophysiology/mind relation. Dyer, Michael G. (1990). Finding lost minds. Journal of Experimental and Theoretical Artificial Intelligence 2:329-39. ( Cited by 3 | Annotation | Google | More links ) Reply to Harnad 1990: symbols, other minds, physically embodied algorithms. Additional links for this entry: http://www.informaworld.com/index/777957611.pdf Dyer, Michael G. (1990). Intentionality and computationalism: Minds, machines, Searle and Harnad. Journal of Experimental and Theoretical Artificial Intelligence 2:303-19. ( Cited by 23 | Annotation | Google | More links ) Reply to Searle/Harnad: systems reply, level confusions, etc. Additional links for this entry: http://www.informaworld.com/index/777957609.pdf Fields, Christopher A. (1984). Double on Searle's chinese room. Nature and System 6 (March):51-54. ( Annotation | Google ) Double's argument implies that the brain isn't the basis of intentionality. Fisher, Justin C. (1988). The wrong stuff: Chinese rooms and the nature of understanding. Philosophical Investigations 11 (October):279-99. ( Cited by 2 | Google ) Fodor, Jerry A. (1991). Yin and Yang in the chinese room. In D. Rosenthal (ed.), The Nature of Mind . Oxford University Press. ( Cited by 5 | Annotation | Google ) The Chinese room isn't even implementing a Turing machine, because it doesn't use proximal causation. With a reply by Searle. Fulda, Joseph S. (2006). A Plea for Automated Language-to-Logical-Form Converters. RASK: Internationalt tidsskrift for sprog og kommuinkation 24 (--):87-102. ( Google ) Millikan, Ruth G. (2005). Some reflections on the theory theory - simulation theory discussion. In Susan Hurley Nick Chater (eds.), Perspectives on Imitation: From Mirror Neurons to Memes, Vol II . MIT Press. ( Google ) Globus, Gordon G. (1991). Deconstructing the chinese room. Journal of Mind and Behavior 12 (3):377-91. ( Cited by 4 | Google ) Gozzano, Simone (1995). Consciousness and understanding in the chinese room. Informatica 19:653-56. ( Cited by 1 | Google ) Abstract: In this paper I submit that the “Chinese room” argument rests on the assumption that understanding a sentence necessarily implies being conscious of its content. However, this assumption can be challenged by showing that two notions of consciousness come into play, one to be found in AI, the other in Searle’s argument, and that the former is an essential condition for the notion used by Searle. If Searle discards the first, he not only has trouble explaining how we can learn a language but finds the validity of his own argument in jeopardy Gozzano, Simone (1997). The chinese room argument: Consciousness and understanding. In Matjaz Gams, M. Paprzycki X. Wu (eds.), Mind Versus Computer: Were Dreyfus and Winograd Right? Amsterdam: IOS Press. ( Google | More links ) Additional links for this entry: http://books.google.com/books?hl=en=o6qlThOEXlScQeLhIvbrclE6lQc Hanna, Patricia (1985). Causal powers and cognition. Mind 94 (373):53-63. ( Cited by 2 | Annotation | Google | More links ) Argues that Searle is confused, and underestimates computers. Weak. Additional links for this entry: http://www.jstor.org/stable/pdfplus/2254697.pdf Harrison, David (1997). Connectionism hits the chinese gym. Connexions 1. ( Google ) Harnad, Stevan (1990). Lost in the hermeneutic hall of mirrors. [Journal (Paginated)] 2:321-27. ( Annotation | Google | More links ) Reply to Dyer 1990: on the differences between real and as-if intentionality. Abstract: Critique of Computationalism as merely projecting hermeneutics (i.e., meaning originating from the mind of an external interpreter) onto otherwise intrinsically meaningless symbols. Projecting an interpretation onto a symbol system results in its being reflected back, in a spuriously self-confirming way Additional links for this entry: http://cogprints.org/1577/1/harnad90.dyer.crit.html http://cogprints.org/1577/0/harnad90.dyer.crit.html Harnad, Stevan (1989). Minds, machines and Searle. Journal of Experimental and Theoretical Artificial Intelligence 1 (4):5-25. ( Cited by 113 | Annotation | Google | More links ) Non-symbolic function is necessary for mentality. Trying hard to work out a theory of why the Chinese Room shows what it does. Nice but wrong. Abstract: Searle's celebrated Chinese Room Argument has shaken the foundations of Artificial Intelligence. Many refutations have been attempted, but none seem convincing. This paper is an attempt to sort out explicitly the assumptions and the logical, methodological and empirical points of disagreement. Searle is shown to have underestimated some features of computer modeling, but the heart of the issue turns out to be an empirical question about the scope and limits of the purely symbolic (computational) model of the mind. Nonsymbolic modeling turns out to be immune to the Chinese Room Argument. The issues discussed include the Total Turing Test, modularity, neural modeling, robotics, causality and the symbol-grounding problem Additional links for this entry: http://cogprints.org/1573/ http://eprints.ecs.soton.ac.uk/1891/ http://eprints.resist.ecs.soton.ac.uk/1891/ http://citeseer.ist.psu.edu/harnad89minds.html http://cogprints.org/1573/00/harnad89.searle.html http://cogprints.ecs.soton.ac.uk/archive/00001573/ http://cogprints.ecs.soton.ac.uk/archive/00001622/ http://cogprints.soton.ac.uk/documents/disk0/00/00/15/73/ http://cogprints.soton.ac.uk/documents/disk0/00/00/15/73/ http://eprints.ecs.soton.ac.uk/1891/01/harnad89.searle.html http://eprints.ecs.soton.ac.uk/5942/01/harnad00.searle.html http://eprints.ecs.soton.ac.uk/1891/01/harnad89.searle.html http://cogsci.soton.ac.uk/harnad/Papers/Harnad/harnad89.searle.html http://cogsci.soton.ac.uk/harnad/Papers/Harnad/harnad89.searle.html http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad00.searle.html http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad89.searle.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad89.searle.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad00.searle.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad89.searle.html http://eprints.ecs.soton.ac.uk/archive/00005942/01/harnad00.searle.html http://cogprints.ecs.soton.ac.uk/archive/00001622/00/harnad00.searle.html http://cogprints.ecs.soton.ac.uk/archive/00001573/00/harnad89.searle.html http://cogprints.ecs.soton.ac.uk/archive/00001573/00/harnad89.searle.html http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1573 http://www.csa.com/partners/viewrecord.php?requester=gs=2229271CI http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1573 http://web.comlab.ox.ac.uk/oucl/research/areas/ieg/e-library/sources/harnad89_searle.pdf http://www.informaworld.com/index/777580727.pdf http://www.ecs.soton.ac.uk/%7Eharnad/Papers/Harnad/harnad89.searle.html http://cogprints.org/1573/1/harnad89.searle.html http://cogprints.org/1573/0/harnad89.searle.html Harnad, Stevan (2003). Minds, machines, and Searle 2: What's right and wrong about the chinese room argument. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Cited by 4 | Google | More links ) Abstract: When in 1979 Zenon Pylyshyn, associate editor of Behavioral and Brain Sciences (BBS, a peer commentary journal which I edit) informed me that he had secured a paper by John Searle with the unprepossessing title of [XXXX], I cannot say that I was especially impressed; nor did a quick reading of the brief manuscript -- which seemed to be yet another tedious "Granny Objection"[1] about why/how we are not computers -- do anything to upgrade that impression Additional links for this entry: http://eprints.ecs.soton.ac.uk/5942/01/harnad00.searle.html http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad00.searle.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad00.searle.html http://eprints.ecs.soton.ac.uk/archive/00005942/01/harnad00.searle.html http://cogprints.ecs.soton.ac.uk/archive/00001622/00/harnad00.searle.html Harnad, Stevan (2001). Rights and wrongs of Searle's chinese room argument. In M. Bishop J. Preston (eds.), Essays on Searle's Chinese Room Argument . Oxford University Press. ( Google | More links ) Abstract: "in an academic generation a little overaddicted to "politesse," it may be worth saying that violent destruction is not necessarily worthless and futile. Even though it leaves doubt about the right road for London, it helps if someone rips up, however violently, a Additional links for this entry: http://eprints.ecs.soton.ac.uk/5942/01/harnad00.searle.html http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad00.searle.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad00.searle.html Harnad, Stevan , Searle's chinese room argument. ( Google ) Abstract: Computationalism. According to computationalism, to explain how the mind works, cognitive science needs to find out what the right computations are -- the same ones that the brain performs in order to generate the mind and its capacities. Once we know that, then every system that performs those computations will have those mental states: Every computer that runs the mind's program will have a mind, because computation is hardware independent : Any hardware that is running the right program has the right computational states Harnad, Stevan (2001). What's wrong and right about Searle's chinese room argument? In Michael A. Bishop John M. Preston (eds.), [Book Chapter] (in Press) . Oxford University Press. ( Cited by 1 | Google | More links ) Abstract: Searle's Chinese Room Argument showed a fatal flaw in computationalism (the idea that mental states are just computational states) and helped usher in the era of situated robotics and symbol grounding (although Searle himself thought neuroscience was the only correct way to understand the mind) Additional links for this entry: http://cogprints.org/4023/1/searlbook.htm http://cogprints.org/1622/0/harnad00.searle.html http://cogprints.soton.ac.uk/documents/disk0/00/00/16/22/ http://citebase.eprints.org/cgi-bin/citations?id=oai:cogprints.soton.ac.uk:1622 http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1622 http://cogprints.org/1622/1/harnad00.searle.html Hauser, Larry (online). Chinese room argument. Internet Encyclopedia of Philosophy . ( Google ) Hauser, Larry (2003). Nixin' goes to china. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Cited by 3 | Google ) Abstract: The intelligent-seeming deeds of computers are what occasion philosophical debate about artificial intelligence (AI) in the first place. Since evidence of AI is not bad, arguments against seem called for. John Searle's Chinese Room Argument (1980a, 1984, 1990, 1994) is among the most famous and long-running would-be answers to the call. Surprisingly, both the original thought experiment (1980a) and Searle's later would-be formalizations of the embedding argument (1984, 1990) are quite unavailing against AI proper (claims that computers do or someday will think ). Searle lately even styles it a "misunderstanding" (1994, p. 547) to think the argument was ever so directed! The Chinese room is now advertised to target Computationalism (claims that computation is what thought essentially is ) exclusively. Despite its renown, the Chinese Room Argument is totally ineffective even against this target Hauser, Larry (1993). Searle's Chinese Box: The Chinese Room Argument and Artificial Intelligence. Dissertation, University of Michigan (Cited by 11 | Google ) Hauser, Larry (1997). Searle's chinese box: Debunking the chinese room argument. [Journal (Paginated)] 7 (2):199-226. ( Cited by 17 | Google | More links ) Abstract: John Searle's Chinese room argument is perhaps the most influential and widely cited argument against artificial intelligence (AI). Understood as targeting AI proper -- claims that computers can think or do think -- Searle's argument, despite its rhetorical flash, is logically and scientifically a dud. Advertised as effective against AI proper, the argument, in its main outlines, is an ignoratio elenchi. It musters persuasive force fallaciously by indirection fostered by equivocal deployment of the phrase "strong AI" and reinforced by equivocation on the phrase "causal powers (at least) equal to those of brains." On a more carefully crafted understanding -- understood just to target metaphysical identification of thought with computation ("Functionalism" or "Computationalism") and not AI proper -- the argument is still unsound, though more interestingly so. It's unsound in ways difficult for high church -- "someday my prince of an AI program will come" -- believers in AI to acknowledge without undermining their high church beliefs. The ad hominem bite of Searle's argument against the high church persuasions of so many cognitive scientists, I suggest, largely explains the undeserved repute this really quite disreputable argument enjoys among them Additional links for this entry: http://cogprints.org/240/00/199802002.html http://members.aol.com/lshauser/chiboxab.html http://members.aol.com/lshauser2/chinabox.html http://members.aol.com/wutsamada/disserta.html http://members.aol.com/lshauser/chiboxab.html http://members.aol.com/wutsamada/contents.html http://members.aol.com/lshauser2/chinabox.html http://cogprints.ecs.soton.ac.uk/archive/00000240/ http://portal.acm.org/citation.cfm?id=596705.596737 http://cogprints.soton.ac.uk/documents/disk0/00/00/02/40/ http://cogprints.soton.ac.uk/documents/disk0/00/00/02/40/ http://cogprints.ecs.soton.ac.uk/archive/00000240/00/199802002.html http://citebase.eprints.org/cgi-bin/citations?id=oai:cogprints.soton.ac.uk:240 http://citebase.eprints.org/cgi-bin/citations?id=oai:cogprints.soton.ac.uk:240 http://www.csa.com/partners/viewrecord.php?requester=gs=322378CI http://www.csa.com/partners/viewrecord.php?requester=gs=322378CI http://www.springerlink.com/content/kx8r435770352167/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=120195=1 http://www.kluweronline.com/article.asp?PIPS=120195=1 http://www.springerlink.com/index/KX8R435770352167.pdf http://www.springerlink.com/index/KX8R435770352167.pdf http://www.ingentaconnect.com/content/klu/mind/1997/00000007/00000002/00120195 http://cogprints.org/240/1/199802002.html http://cogprints.org/240/0/199802002.html Hauser, Larry (online). Searle's chinese room argument. Field Guide to the Philosophy of Mind . ( Google ) Abstract: John Searle's 1980a) thought experiment and associated 1984a) argument is one of the best known and widely credited counters to claims of artificial intelligence (AI), i.e., to claims that computers _do_ or at least _can_ (roughly, someday will) think. According to Searle's original presentation, the argument is based on two truths: _brains cause minds_ , and _syntax doesn't suffice_ _for semantics_ . Its target, Searle dubs "strong AI": "according to strong AI," according to Searle, "the computer is not merely a tool in the study of the mind, rather the appropriately programmed computer really _is_ a mind in the sense that computers given the right programs can be literally said to _understand_ and have other cognitive states" 1980a, p. 417). Searle contrasts "strong AI" to "weak AI". According to weak AI, according to Searle, computers just Hauser, Larry (online). The chinese room argument. ( Cited by 6 | Google ) Abstract: _The Chinese room argument_ - John Searle's (1980a) thought experiment and associated (1984) derivation - is one of the best known and widely credited counters to claims of artificial intelligence (AI), i.e., to claims that computers _do_ or at least _can_ (someday might) think. According to Searle's original presentation, the argument is based on two truths: _brains cause minds_ , and _syntax doesn't_ _suffice for semantics_ . Its target, Searle dubs "strong AI": "according to strong AI," according to Searle, "the computer is not merely a tool in the study of the mind, rather the appropriately programmed computer really _is_ a mind in the sense that computers given the right programs can be literally said to _understand_ and have other cognitive states" (1980a, p. 417). Searle contrasts "strong AI" to "weak AI". According to weak AI, according to Searle, computers just Hayes, Patrick ; Harnad, Stevan ; Perlis, Donald R. Block, Ned (1992). Virtual symposium on virtual mind. [Journal (Paginated)] 2 (3):217-238. ( Cited by 21 | Annotation | Google | More links ) A discussion about the Chinese room, symbol grounding, and so on. Abstract: When certain formal symbol systems (e.g., computer programs) are implemented as dynamic physical symbol systems (e.g., when they are run on a computer) their activity can be interpreted at higher levels (e.g., binary code can be interpreted as LISP, LISP code can be interpreted as English, and English can be interpreted as a meaningful conversation). These higher levels of interpretability are called "virtual" systems. If such a virtual system is interpretable as if it had a mind, is such a "virtual mind" real? This is the question addressed in this "virtual" symposium, originally conducted electronically among four cognitive scientists: Donald Perlis, a computer scientist, argues that according to the computationalist thesis, virtual minds are real and hence Searle's Chinese Room Argument fails, because if Searle memorized and executed a program that could pass the Turing Test in Chinese he would have a second, virtual, Chinese-understanding mind of which he was unaware (as in multiple personality). Stevan Harnad, a psychologist, argues that Searle's Argument is valid, virtual minds are just hermeneutic overinterpretations, and symbols must be grounded in the real world of objects, not just the virtual world of interpretations. Computer scientist Patrick Hayes argues that Searle's Argument fails, but because Searle does not really implement the program: A real implementation must not be homuncular but mindless and mechanical, like a computer. Only then can it give rise to a mind at the virtual level. Philosopher Ned Block suggests that there is no reason a mindful implementation would not be a real one Additional links for this entry: http://eprints.resist.ecs.soton.ac.uk/3371/ http://eprints.ecs.soton.ac.uk/archive/00003371/ http://eprints.ecs.soton.ac.uk/archive/00003371/ http://cogprints.org/1585/00/harnad92.virtualmind.html http://cogprints.soton.ac.uk/documents/disk0/00/00/15/85/ http://cogprints.soton.ac.uk/documents/disk0/00/00/15/85/ http://cogsci.soton.ac.uk/harnad/Papers/Harnad/harnad92.virtualmind.html http://cogsci.soton.ac.uk/harnad/Papers/Harnad/harnad92.virtualmind.html http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad92.virtualmind.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad92.virtualmind.html http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad92.virtualmind.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad92.virtualmind.html http://eprints.ecs.soton.ac.uk/archive/00003371/02/harnad92.virtualmind.html http://cogprints.ecs.soton.ac.uk/archive/00001585/00/harnad92.virtualmind.html http://cogprints.ecs.soton.ac.uk/archive/00001585/00/harnad92.virtualmind.html http://citebase.eprints.org/cgi-bin/citations?id=oai:eprints.ecs.soton.ac.uk:3371 http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1585 http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1585 http://www.springerlink.com/content/g2063372459ww766/fulltext.pdf http://www.springerlink.com/index/G2063372459WW766.pdf http://cogprints.org/1585/1/harnad92.virtualmind.html http://cogprints.org/1585/0/harnad92.virtualmind.html Hofstadter, Douglas R. (1981). Reflections on Searle. In Douglas R. Hofstadter Daniel C. Dennett (eds.), The Mind's I . Basic Books. ( Cited by 1 | Annotation | Google ) Searle is committing a level confusion, and understates the complexity of the case. We can move from the CR to a brain (with a demon) by twiddling knobs, and the systems reply should work equally well in both cases. Jacquette, Dale (1989). Adventures in the chinese room. Philosophy and Phenomenological Research 49 (June):605-23. ( Cited by 5 | Annotation | Google | More links ) If we had microfunctional correspondence, the CR argument would fail. With points about the status of abstract/biological intentionality. A bit weak. Additional links for this entry: http://www.jstor.org/stable/pdfplus/2107850.pdf Jacquette, Dale (1990). Fear and loathing (and other intentional states) in Searle's chinese room. Philosophical Psychology 3 (2 & 3):287-304. ( Annotation | Google ) Reply to Searle on CR, central control, biological intentionality & dualism. Abstract: John R. Searle's problem of the Chinese Room poses an important philosophical challenge to the foundations of strong artificial intelligence, and functionalist, cognitivist, and computationalist theories of mind. Searle has recently responded to three categories of criticisms of the Chinese Room and the consequences he attempts to conclude from it, redescribing the essential features of the problem, and offering new arguments about the syntax-semantics gap it is intended to demonstrate. Despite Searle's defense, the Chinese Room remains ineffective as a counterexample, and poses no real threat to artificial intelligence or mechanist philosophy of mind. The thesis that intentionality is a primitive irreducible relation exemplified by biological phenomena is preferred in opposition to Searle's contrary claim that intentionality is a biological phenomenon exhibiting abstract properties Jacquette, Dale (1989). Searle's intentionality thesis. Synthese 80 (August):267-75. ( Cited by 1 | Annotation | Google | More links ) Searle's view implies that intentional causation is not efficient causation. Additional links for this entry: http://www.springerlink.com/index/JQ02542UQ70Q1130.pdf Jahren, Neal (1990). Can semantics be syntactic? Synthese 82 (3):309-28. ( Cited by 3 | Annotation | Google | More links ) Against Rapaport's Korean Room argument -- syntax isn't enough. Abstract:   The author defends John R. Searle's Chinese Room argument against a particular objection made by William J. Rapaport called the Korean Room. Foundational issues such as the relationship of strong AI to human mentality and the adequacy of the Turing Test are discussed. Through undertaking a Gedankenexperiment similar to Searle's but which meets new specifications given by Rapaport for an AI system, the author argues that Rapaport's objection to Searle does not stand and that Rapaport's arguments seem convincing only because they assume the foundations of strong AI at the outset Additional links for this entry: http://www.springerlink.com/index/G7140M0R11437051.pdf Kaernbach, C. (2005). No virtual mind in the chinese room. Journal of Consciousness Studies 12 (11):31-42. ( Google | More links ) Additional links for this entry: http://www.psychologie.uni-kiel.de/emotion/team/kaernbach/publications/2005_kae_jcs.pdf http://www.ingentaconnect.com/content/imp/jcs/2005/00000012/00000011/art00002 Kentridge, Robert W. (2001). Computation, chaos and non-deterministic symbolic computation: The chinese room problem solved? Psycoloquy 12 (50). ( Cited by 6 | Google | More links ) Additional links for this entry: http://cogsci.soton.ac.uk/~harnad/Temp/Think/kentridg.htm http://www.cogsci.ecs.soton.ac.uk/cgi/psyc/newpsy?article=12.050 http://www.cogsci.ecs.soton.ac.uk/cgi/psyc/newpsy?symbolism-connectionism.17 http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad93.symb.anal.net.kentridge.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad93.symb.anal.net.kentridge.html http://psycprints.ecs.soton.ac.uk/archive/00000179/02/psyc.01.12.050.symbolism-connectionism.17.kentridge King, D. (2001). Entering the chinese room with Castaneda's principle (p). Philosophy Today 45 (2):168-174. ( Google ) Kober, Michael (1998). Kripkenstein meets the chinese room: Looking for the place of meaning from a natural point of view. Inquiry 41 (3):317-332. ( Cited by 2 | Google | More links ) Abstract: The discussion between Searle and the Churchlands over whether or not symbolmanipulating computers generate semantics will be confronted both with the rulesceptical considerations of Kripke/Wittgenstein and with Wittgenstein's privatelanguage argument in order to show that the discussion focuses on the wrong place: meaning does not emerge in the brain. That a symbol means something should rather be conceived as a social fact, depending on a mutual imputation of linguistic competence of the participants of a linguistic practice to one another. The alternative picture will finally be applied to small children, animals, and computers as well Additional links for this entry: http://taylorandfrancis.metapress.com/index/NRA0050818Y5C6GA.pdf http://www.informaworld.com/index/NRA0050818Y5C6GA.pdf http://www.ingentaconnect.com/content/routledg/sinq/1998/00000041/00000003/art00004 Korb, Kevin B. (1991). Searle's AI program. Journal of Experimental and Theoretical Artificial Intelligence 3:283-96. ( Cited by 6 | Annotation | Google | More links ) The Chinese room doesn't succeed as an argument about semantics. At best it might succeed as an argument about consciousness. Additional links for this entry: http://www.informaworld.com/index/776648442.pdf Kugel, Peter (2004). The chinese room is a trick. Behavioral and Brain Sciences 27 (1):153-154. ( Google ) Abstract: To convince us that computers cannot have mental states, Searle (1980) imagines a “Chinese room” that simulates a computer that “speaks” Chinese and asks us to find the understanding in the room. It's a trick. There is no understanding in the room, not because computers can't have it, but because the room's computer-simulation is defective. Fix it and understanding appears. Abracadabra! Law, Diane (online). Searle, subsymbolic functionalism, and synthetic intelligence. ( Cited by 1 | Google | More links ) Additional links for this entry: http://nn.cs.utexas.edu/downloads/papers/law.synthetic.pdf http://nn.cs.utexas.edu/downloads/papers/law.synthetic.ps.gz http://www.cs.utexas.edu/users/nn/downloads/papers/law.synthetic.pdf http://www.cs.utexas.edu/ftp/pub/neural-nets/papers/law.synthetic.pdf http://www.let.rug.nl/~nerbonne/teach/neuro/kleiweg/law.synthetic.ps.gz http://odur.let.rug.nl/~nerbonne/teach/neuro/kleiweg/law.synthetic.ps.gz http://historical.ncstrl.org/litesite-data/utexas_cs/UT-AI-TR-94-222.ps.Z Leslie, Alan M. Scholl, Brian J. (1999). Modularity, development and 'theory of mind'. Mind and Language 14 (1). ( Google | More links ) Abstract: Psychologists and philosophers have recently been exploring whether the mechanisms which underlie the acquisition of ‘theory of mind’ (ToM) are best charac- terized as cognitive modules or as developing theories. In this paper, we attempt to clarify what a modular account of ToM entails, and why it is an attractive type of explanation. Intuitions and arguments in this debate often turn on the role of develop- ment: traditional research on ToM focuses on various developmental sequences, whereas cognitive modules are thought to be static and ‘anti-developmental’. We suggest that this mistaken view relies on an overly limited notion of modularity, and we explore how ToM might be grounded in a cognitive module and yet still afford development. Modules must ‘come on-line’, and even fully developed modules may still develop internally, based on their constrained input. We make these points con- crete by focusing on a recent proposal to capture the development of ToM in a module via parameterization Additional links for this entry: http://l3d.cs.colorado.edu/~ctg/classes/lib/cogsci/scholl.pdf http://ruccs.rutgers.edu/~aleslie/99-Scholl-Leslie-MindLang.pdf http://ruccs.rutgers.edu/~aleslie/99-Scholl-Leslie-MindLang.pdf http://www.cs.colorado.edu/~duck/ctg/classes/lib/cogsci/scholl.pdf http://www.blackwell-synergy.com/doi/abs/10.1111/1468-0017.00106?favorite=add http://www.blackwell-synergy.com/links/doi/10.1111/1468-0017.00106 http://www.ingentaconnect.com/content/bpl/mila/1999/00000014/00000001/art00106 Maloney, J. Christopher (1987). The right stuff. Synthese 70 (March):349-72. ( Cited by 13 | Annotation | Google | More links ) Defends Searle against all kinds of objections. Additional links for this entry: http://www.springerlink.com/index/L1W0183H2384XG26.pdf McCarthy, John (online). John Searle's chinese room argument. ( Google ) Abstract: John Searle begins his (1990) ``Consciousness, Explanatory Inversion and Cognitive Science'' with ``Ten years ago in this journal I published an article (Searle, 1980a and 1980b) criticising what I call Strong AI, the view that for a system to have mental states it is sufficient for the system to implement the right sort of program with right inputs and outputs. Strong AI is rather easy to refute and the basic argument can be summarized in one sentence: {it a system, me for example, could implement a program for understanding Chinese, for example, without understanding any Chinese at all.} This idea, when developed, became known as the Chinese Room Argument.'' The Chinese Room Argument can be refuted in one sentence Melnyk, Andrew (1996). Searle's abstract argument against strong AI. Synthese 108 (3):391-419. ( Cited by 6 | Google | More links ) Abstract:   Discussion of Searle's case against strong AI has usually focused upon his Chinese Room thought-experiment. In this paper, however, I expound and then try to refute what I call his abstract argument against strong AI, an argument which turns upon quite general considerations concerning programs, syntax, and semantics, and which seems not to depend on intuitions about the Chinese Room. I claim that this argument fails, since it assumes one particular account of what a program is. I suggest an alternative account which, however, cannot play a role in a Searle-type argument, and argue that Searle gives no good reason for favoring his account, which allows the abstract argument to work, over the alternative, which doesn't. This response to Searle's abstract argument also, incidentally, enables the Robot Reply to the Chinese Room to defend itself against objections Searle makes to it Additional links for this entry: http://www.springerlink.com/index/V404X74W610N4UMX.pdf Mitchell, Ethan (2008). The real Chinese Room. Philica 125. ( Google ) Moor, James H. (1988). The pseudorealization fallacy and the chinese room argument. In James H. Fetzer (ed.), Aspects of AI . D. ( Cited by 5 | Annotation | Google ) Computational systems must also meet performance criteria. Moural, Josef (2003). The chinese room argument. In John Searle . Cambridge: Cambridge University Press. ( Cited by 2 | Google ) Narayanan, Ajit (1991). The chinese room argument. In Logical Foundations . New York: St Martin's Press. ( Google ) Newton, Natika (1989). Machine understanding and the chinese room. Philosophical Psychology 2 (2):207-15. ( Cited by 2 | Annotation | Google ) A program can possess intentionality, even if not consciousness. Abstract: John Searle has argued that one can imagine embodying a machine running any computer program without understanding the symbols, and hence that purely computational processes do not yield understanding. The disagreement this argument has generated stems, I hold, from ambiguity in talk of 'understanding'. The concept is analysed as a relation between subjects and symbols having two components: a formal and an intentional. The central question, then becomes whether a machine could possess the intentional component with or without the formal component. I argue that the intentional state of a symbol's being meaningful to a subject is a functionally definable relation between the symbol and certain past and present states of the subject, and that a machine could bear this relation to a symbol. I sketch a machine which could be said to possess, in primitive form, the intentional component of understanding. Even if the machine, in lacking consciousness, lacks full understanding, it contributes to a theory of understanding and constitutes a counterexample to the Chinese Room argument Obermeier, K. K. (1983). Wittgenstein on language and artificial intelligence: The chinese-room thought-experiment revisited. Synthese 56 (September):339-50. ( Cited by 1 | Google | More links ) Additional links for this entry: http://www.springerlink.com/index/R725636T3L54X366.pdf Penrose, Roger (2003). Consciousness, computation, and the chinese room. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Cited by 2 | Google ) Pfeifer, Karl (1992). Searle, strong, and two ways of sorting cucumbers. Journal of Philosophical Research 17:347-50. ( Cited by 1 | Google ) Preston, John M. Bishop, Michael A. (eds.) (2002). Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence. Oxford University Press. ( Cited by 21 | Google ) Abstract: The most famous challenge to computational cognitive science and artificial intelligence is the philosopher John Searles Chinese Room argument. Proudfoot, Diane (2003). Wittgenstein's anticipation of the chinese room. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Google ) Rapaport, William J. (2006). How Helen Keller used syntactic semantics to escape from a chinese room. Minds and Machines 16 (4). ( Google | More links ) Abstract:   A computer can come to understand natural language the same way Helen Keller did: by using “syntactic semantics”—a theory of how syntax can suffice for semantics, i.e., how semantics for natural language can be provided by means of computational symbol manipulation. This essay considers real-life approximations of Chinese Rooms, focusing on Helen Keller’s experiences growing up deaf and blind, locked in a sort of Chinese Room yet learning how to communicate with the outside world. Using the SNePS computational knowledge-representation system, the essay analyzes Keller’s belief that learning that “everything has a name” was the key to her success, enabling her to “partition” her mental concepts into mental representations of: words, objects, and the naming relations between them. It next looks at Herbert Terrace’s theory of naming, which is akin to Keller’s, and which only humans are supposed to be capable of. The essay suggests that computers at least, and perhaps non-human primates, are also capable of this kind of naming Additional links for this entry: http://www.springerlink.com/content/601p84g288756774/fulltext.pdf Rapaport, William J. (1984). Searle's experiments with thought. Philosophy of Science 53 (June):271-9. ( Cited by 14 | Annotation | Google | More links ) Comments on Cole, and some general material on syntax and semantics. Additional links for this entry: http://www.journals.uchicago.edu/cgi-bin/resolve?id=doi:10.1086/289312 http://www.jstor.org/stable/pdfplus/187697.pdf Rey, Georges (2003). Searle's misunderstandings of functionalism and strong AI. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Google ) Rey, Georges (1986). What's really going on in Searle's 'chinese room'. Philosophical Studies 50 (September):169-85. ( Cited by 17 | Annotation | Google | More links ) Recommends the systems reply, and a causal account of semantics. Discusses the relevance of wide and narrow notions of content, and the tension between Searle's positive and negative proposals. Additional links for this entry: http://www.springerlink.com/index/P4J2378205330137.pdf Roberts, Lawrence D. (1990). Searle's extension of the chinese room to connectionist machines. Journal of Experimental and Theoretical Artificial Intelligence 2:185-7. ( Cited by 4 | Annotation | Google ) In arguing against the relevance of the serial/parallel distinction to mental states, Searle becomes a formalist. A nice point. Rodych, Victor (2003). Searle Freed of every flaw. Acta Analytica 18 (30-31):161-175. ( Google | More links ) Abstract: Strong Al presupposes (1) that Super-Searle (henceforth ‘Searle’) comes to know that the symbols he manipulates are meaningful , and (2) that there cannot be two or more semantical interpretations for the system of symbols that Searle manipulates such that the set of rules constitutes a language comprehension program for each interpretation. In this paper, I show that Strong Al is false and that presupposition #1 is false, on the assumption that presupposition #2 is true. The main argument of the paper constructs a second program, isomorphic to Searle’s, to show that if someone, say Dan, runs this isomorphic program, he cannot possibly come to know what its mentioned symbols mean because they do not mean anything to anybody. Since Dan and Searle do exactly the same thing, except that the symbols they manipulate are different, neither Dan nor Searle can possibly know whether the symbols they manipulate are meaningful (let alone what they mean, if they are meaningful). The remainder of the paper responds to an anticipated Strong Al rejoinder, which, I believe, is a necessary extension of Strong Al Additional links for this entry: http://transactionpub.metapress.com/index/H82MP6R2L10H0745.pdf http://www.springerlink.com/index/H82MP6R2L10H0745.pdf Russow, L-M. (1984). Unlocking the chinese room. Nature and System 6 (December):221-8. ( Cited by 4 | Annotation | Google ) Searle's presence in the room destroys the integrity of the system, so that it is no longer a proper implementation of the program. Searle, John R. (1990). Is the brain's mind a computer program? Scientific American 262 (1):26-31. ( Cited by 178 | Annotation | Google | More links ) On the status of the Chinese Room argument, ten years on. Additional links for this entry: http://books.google.com/books?hl=en=S9z08wWdwwt3psKgZPsRfS-1bXE Searle, John R. (1987). Minds and brains without programs. In Colin Blakemore (ed.), Mindwaves . Blackwell. ( Cited by 27 | Annotation | Google ) More on the arguments against AI, e.g. the Chinese room and considerations about syntax and semantics. Mind is a high-level physical property of brain. Searle, John R. (1980). Minds, brains and programs. Behavioral and Brain Sciences 3:417-57. ( Cited by 1532 | Annotation | Google | More links ) Implementing a program is not sufficient for mentality, as someone could e.g. implement a "Chinese-speaking" program without understanding Chinese. So strong AI is false, and no program is sufficient for consciousness. Abstract: What psychological and philosophical significance should we attach to recent efforts at computer simulations of human cognitive capacities? In answering this question, I find it useful to distinguish what I will call "strong" AI from "weak" or "cautious" AI (artificial intelligence). According to weak AI, the principal value of the computer in the study of the mind is that it gives us a very powerful tool. For example, it enables us to formulate and test hypotheses in a more rigorous and precise fashion. But according to strong AI, the computer is not merely a tool in the study of the mind; rather, the appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to.. Additional links for this entry: http://courses.washington.edu/info300/searle.pdf http://pami.uwaterloo.ca/tizhoosh/docs/Searle.pdf http://www.ifispan.waw.pl/~rpilat/john_searle.pdf http://members.aol.com/NeoNoetics/MindsBrainsPrograms.html http://www.bbsonline.org/Preprints/OldArchive/bbs.searle2.html http://www.bbsonline.org/documents/a/00/00/04/84/bbs00000484-00/bbs.searle2.html http://books.google.com/books?hl=en=IAV-tI3UMUrdII-jhXVCiE5EiNY http://books.google.com/books?hl=en=e696dd_RJUp-z7nJq_VUVkJ6Vco http://books.google.com/books?hl=en=WhrAW9GsnXpijAGLjoTnMSl83U4 http://books.google.com/books?hl=en=Gup1URzClmw94-tEdiKkRv8oo_E http://books.google.com/books?hl=en=NuhA3CnAPDnsWqo8Ta8-RX2cCEY http://books.google.com/books?hl=en=lNvUnhfC6Xi8TMG19ynGzF7-UrE http://books.google.com/books?hl=en=6EzNiu7H_1bXLA0zJ16b39NuIVQ http://books.google.com/books?hl=en=xzN-ebbJvEML3FULifpKsvbJ4B0 http://books.google.com/books?hl=en=v25MYgLHP7PRb5D3u6V0j21p3Os http://books.google.com/books?hl=en=_xvxtHTY2CziSv0BQSRuTeAcVyE http://books.google.com/books?hl=en=LYLLmE8CSwbq9blXRhOQWNiQKBo http://books.google.com/books?hl=en=hPqWXhv4YdFnGCjtMhQpRctFDz8 http://books.google.com/books?hl=en=PuxfSwx103nfQfLP8luU4Z-hgSc http://books.google.com/books?hl=en=JxSMo440evbsIv2dv4QpEJVAC4A Searle, John R. (1984). Minds, Brains and Science. Harvard University Press. ( Cited by 515 | Annotation | Google ) Axiomatizes the argument: Syntax isn't sufficient for semantics, programs are syntactic, minds are semantic, so no program is sufficient for mind. Searle, John R. (1989). Reply to Jacquette. Philosophy and Phenomenological Research 49 (4):701-8. ( Cited by 4 | Annotation | Google | More links ) Jacquette misses the point of the argument. Also, biological and abstract intentionality are quite compatible. Additional links for this entry: http://www.jstor.org/stable/pdfplus/2107856.pdf Searle, John R. (1989). Reply to Jacquette's adventures in the chinese room. Philosophy and Phenomenological Research 49 (June):701-707. ( Google ) Searle, John R. (2002). Twenty-one years in the chinese room. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Cited by 7 | Google ) Seidel, Asher (1989). Chinese rooms a, B and C. Pacific Philosophical Quarterly 20 (June):167-73. ( Cited by 1 | Annotation | Google ) A person running the program, with interpretations at hand, would understand. Point-missing. Seidel, Asher (1988). Searle on the biological basis of cognition. Analysis 48 (January):26-28. ( Google ) Shaffer, Michael J. (2009). A logical hole in the chinese room. Minds and Machines 19 (2):229-235. ( Google ) Abstract: Searle’s Chinese Room Argument (CRA) has been the object of great interest in the philosophy of mind, artificial intelligence and cognitive science since its initial presentation in ‘Minds, Brains and Programs’ in 1980. It is by no means an overstatement to assert that it has been a main focus of attention for philosophers and computer scientists of many stripes. It is then especially interesting to note that relatively little has been said about the detailed logic of the argument, whatever significance Searle intended CRA to have. The problem with the CRA is that it involves a very strong modal claim, the truth of which is both unproved and highly questionable. So it will be argued here that the CRA does not prove what it was intended to prove Sharvy, Richard (1985). Searle on programs and intentionality. Canadian Journal of Philosophy 11:39-54. ( Annotation | Google ) Argues against Searle, but misses the point for the most part. Simon, Herbert A. Eisenstadt, Stuart A. (2003). A chinese room that understands. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Cited by 3 | Google ) Sloman, Aaron (1986). Did Searle attack strong strong AI or weak strong AI? In Artificial Intelligence and its Applications . Chichester. ( Cited by 3 | Google | More links ) Additional links for this entry: http://www.cs.bham.ac.uk/research/cogaff/sloman.searle.85.text Sprevak, Mark D. (online). Algorithms and the chinese room. ( Google ) Suits, David B. (1989). Out of the chinese room. Computing and Philosophy Newsletter 4:1-7. ( Cited by 2 | Annotation | Google ) Story about homunculi within homunculi. Fun. Tanaka, Koji (2004). Minds, programs, and chinese philosophers: A chinese perspective on the chinese room. Sophia 43 (1):61-72. ( Google ) Abstract: The paper is concerned with John Searle’s famous Chinese room argument. Despite being objected to by some, Searle’s Chinese room argument appears very appealing. This is because Searle’s argument is based on an intuition about the mind that ‘we’ all seem to share. Ironically, however, Chinese philosophers don’t seem to share this same intuition. The paper begins by first analysing Searle’s Chinee room argument. It then introduces what can be seen as the (implicit) Chinese view of the mind. Lastly, it demonstrates a conceptual difference between Chinese and Western philosophy with respect to the notion of mind. Thus, it is shown that one must carefully attend to the presuppositions underlying Chinese philosophising in interpreting Chinese philosophers Taylor, John G. (2003). Do virtual actions avoid the chinese room? In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Cited by 4 | Google ) Teng, Norman Y. (2000). A cognitive analysis of the chinese room argument. Philosophical Psychology 13 (3):313-24. ( Cited by 1 | Google | More links ) Abstract: Searle's Chinese room argument is analyzed from a cognitive point of view. The analysis is based on a newly developed model of conceptual integration, the many space model proposed by Fauconnier and Turner. The main point of the analysis is that the central inference constructed in the Chinese room scenario is a result of a dynamic, cognitive activity of conceptual blending, with metaphor defining the basic features of the blending. Two important consequences follow: (1) Searle's recent contention that syntax is not intrinsic to physics turns out to be a slightly modified version of the old Chinese room argument; and (2) the argument itself is still open to debate. It is persuasive but not conclusive, and at bottom it is a topological mismatch in the metaphoric conceptual integration that is responsible for the non-conclusive character of the Chinese room argument Additional links for this entry: http://taylorandfrancis.metapress.com/index/PAHW8YBYRV4EF93P.pdf http://www.informaworld.com/index/PAHW8YBYRV4EF93P.pdf http://www.ingentaconnect.com/content/routledg/cphp/2000/00000013/00000003/art00003 Thagard, Paul R. (1986). The emergence of meaning: An escape from Searle's chinese room. Behaviorism 14 (3):139-46. ( Cited by 5 | Annotation | Google ) Get semantics computationally via induction and functional roles. Wakefield, Jerome C. (2003). The chinese room argument reconsidered: Essentialism, indeterminacy, and strong AI. Minds and Machines 13 (2):285-319. ( Cited by 3 | Google | More links ) Abstract:   I argue that John Searle's (1980) influential Chinese room argument (CRA) against computationalism and strong AI survives existing objections, including Block's (1998) internalized systems reply, Fodor's (1991b) deviant causal chain reply, and Hauser's (1997) unconscious content reply. However, a new ``essentialist'' reply I construct shows that the CRA as presented by Searle is an unsound argument that relies on a question-begging appeal to intuition. My diagnosis of the CRA relies on an interpretation of computationalism as a scientific theory about the essential nature of intentional content; such theories often yield non-intuitive results in non-standard cases, and so cannot be judged by such intuitions. However, I further argue that the CRA can be transformed into a potentially valid argument against computationalism simply by reinterpreting it as an indeterminacy argument that shows that computationalism cannot explain the ordinary distinction between semantic content and sheer syntactic manipulation, and thus cannot be an adequate account of content. This conclusion admittedly rests on the arguable but plausible assumption that thought content is interestingly determinate. I conclude that the viability of computationalism and strong AI depends on their addressing the indeterminacy objection, but that it is currently unclear how this objection can be successfully addressed Additional links for this entry: http://www.springerlink.com/content/q76u712262711110/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=5115346=1 http://www.springerlink.com/index/Q76U712262711110.pdf http://www.ingentaconnect.com/content/klu/mind/2003/00000013/00000002/05115346 Warwick, Kevin (2002). Alien encounters. In Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford: Clarendon Press. ( Google ) Weiss, Timothy (1990). Closing the chinese room. Ratio 3 (2):165-81. ( Cited by 6 | Annotation | Google | More links ) Searle-in-the-room isn't in a position to know about the system's first-person states. Intrinsic intentionality is an incoherent notion. Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/119373604/PDFSTART Wheeler, M. (2003). Changes in the rules: Computers, dynamic systems, and Searle. In John M. Preston Michael A. Bishop (eds.), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence . Oxford University Press. ( Google ) Whitmer, J. M. (1983). Intentionality, artificial intelligence, and the causal powers of the brain. Auslegung 10:194-210. ( Annotation | Google ) Defending Searle's position, with remarks on the "causal powers" argument. 6.1d Machine Consciousness Tson, M. E. (ms). A Brief Explanation of Consciousness. ( Google ) Abstract: This short paper (4 pages) demonstrates how subjective experience, language, and consciousness can be explained in terms of abilities we share with the simplest of creatures, specifically the ability to detect, react to, and associate various aspects of the world. Adams, William Y. (online). Intersubjective transparency and artificial consciousness. ( Google ) Adams, William Y. (2004). Machine consciousness: Plausible idea or semantic distortion? Journal of Consciousness Studies 11 (9):46-56. ( Cited by 1 | Google ) Aleksander, Igor L. Dunmall, B. (2003). Axioms and tests for the presence of minimal consciousness in agents I: Preamble. Journal of Consciousness Studies 10 (4):7-18. ( Cited by 13 | Google ) Aleksander, Igor L. (2007). Machine consciousness. In Max Velmans Susan Schneider (eds.), The Blackwell Companion to Consciousness . Blackwell. ( Cited by 1 | Google | More links ) Additional links for this entry: http://books.google.com/books?hl=en=LmbDUzP5IccBZCh8TG02TqZZL28 http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation http://www.ncbi.nlm.nih.gov/sites/entrez?db=pubmed=google Aleksander, Igor L. (2006). Machine consciousness. In Steven Laureys (ed.), Boundaries of Consciousness . Elsevier. ( Cited by 1 | Google | More links ) Additional links for this entry: http://books.google.com/books?hl=en=dSV8_pheTWTgsxz7NiRaIDipYNs http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation http://www.ncbi.nlm.nih.gov/sites/entrez?db=pubmed=google Amoroso, Richard L. (1997). The theoretical foundations for engineering a conscious quantum computer. In M. Gams, M. Paprzycki X. Wu (eds.), Mind Versus Computer: Were Dreyfus and Winograd Right? Amsterdam: IOS Press. (Cited by 5 | Google | More links ) Additional links for this entry: http://www.mindspring.com/~noetic.advanced.studies/Amoroso19.pdf Angel, Leonard (1994). Am I a computer? In Eric Dietrich (ed.), Thinking Computers and Virtual Persons . Academic Press. ( Google ) Angel, Leonard (1989). How to Build a Conscious Machine. Westview Press. ( Cited by 3 | Google ) Arrabales, R. Sanchis, A. (forthcoming). Applying machine consciousness models in autonomous situated agents. Pattern Recognition Letters . ( Google ) Arrington, Robert L. (1999). Machines, consciousness, and thought. Idealistic Studies 29 (3):231-243. ( Google ) Arrabales, R. ; Ledezma, A. Sanchis, A. (online). Modelling consciousness for autonomous robot exploration. Lecture Notes in Computer Science . ( Google ) Aydede, Murat Guzeldere, Guven (2000). Consciousness, intentionality, and intelligence: Some foundational issues for artificial intelligence. Journal Of Experimental and Theoretical Artificial Intelligence 12 (3):263-277. ( Cited by 6 | Google | More links ) Additional links for this entry: http://www.ingentaconnect.com/content/tandf/teta/2000/00000012/00000003/art00003 Bair, Puran K. (1981). Computer metaphors for consciousness. In The Metaphors Of Consciousness . New York: Plenum Press. ( Google ) Barnes, E. (1991). The causal history of computational activity: Maudlin and olympia. Journal of Philosophy 88 (6):304-16. ( Cited by 5 | Annotation | Google | More links ) Response to Maudlin 1989. True computation needs active, not passive causation, so Maudlin's machine isn't really computing. Additional links for this entry: http://www.jstor.org/stable/pdfplus/2026687.pdf Bell, John L. (online). Algorithmicity and consciousness. ( Google ) Abstract: Why should one believe that conscious awareness is solely the result of organizational complexity? What is the connection between consciousness and combinatorics: transformation of quantity into quality? The claim that the former is reducible to the other seems unconvincing—as unlike as chalk and cheese! In his book1 Penrose is at least attempting to compare like with like: the enigma of consciousness with the progress of physics Birnbacher, Dieter (1995). Artificial consciousness. In Thomas Metzinger (ed.), Conscious Experience . Ferdinand Schoningh. ( Google ) Bonzon, Pierre (2003). Conscious Behavior through Reflexive Dialogs. In A. Günter, R. Kruse B. Neumann (eds.), Lectures Notes in Artificial Intelligence . Springer. ( Google ) Abstract: We consider the problem of executing conscious behavior i.e., of driving an agent’s actions and of allowing it, at the same time, to run concurrent processes reflecting on these actions. Toward this end, we express a single agent’s plans as reflexive dialogs in a multi-agent system defined by a virtual machine. We extend this machine’s planning language by introducing two specific operators for reflexive dialogs i.e., conscious and caught for monitoring beliefs and actions, respectively. The possibility to use the same language both to drive a machine and to establish a reflexive communication within the machine itself stands as a key feature of our model. Bringsjord, Selmer (1994). Could, how could we tell if, and should - androids have inner lives? In Kenneth M. Ford, C. Glymour Patrick Hayes (eds.), Android Epistemology . MIT Press. ( Cited by 16 | Google ) Bringsjord, Selmer (2004). On building robot persons: Response to Zlatev. Minds and Machines 14 (3):381-385. ( Google | More links ) Abstract:   Zlatev offers surprisingly weak reasoning in support of his view that robots with the right kind of developmental histories can have meaning. We ought nonetheless to praise Zlatev for an impressionistic account of how attending to the psychology of human development can help us build robots that appear to have intentionality Additional links for this entry: http://portal.acm.org/citation.cfm?id=1011949.1011965 http://www.cogs.indiana.edu/cogx/robotperson_Zlatev_response.pdf http://www.springerlink.com/content/w83402573w376q55/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=5274048=1 http://www.springerlink.com/index/W83402573W376Q55.pdf http://www.ingentaconnect.com/content/klu/mind/2004/00000014/00000003/05274048 Bringsjord, Selmer (2007). Offer: One billion dollars for a conscious robot; if you're honest, you must decline. Journal of Consciousness Studies 14 (7):28-43. ( Cited by 1 | Google | More links ) Abstract: You are offered one billion dollars to 'simply' produce a proof-of-concept robot that has phenomenal consciousness -- in fact, you can receive a deliciously large portion of the money up front, by simply starting a three-year work plan in good faith. Should you take the money and commence? No. I explain why this refusal is in order, now and into the foreseeable future Additional links for this entry: http://www.ingentaconnect.com/content/imp/jcs/2007/00000014/00000007/art00002 Bringsjord, Selmer (1992). What Robots Can and Can't Be. Kluwer. ( Cited by 85 | Google | More links ) Additional links for this entry: http://www.rpi.edu/~brings/precis.wrccb.2.html http://psycprints.ecs.soton.ac.uk/archive/00000418/ http://psycprints.ecs.soton.ac.uk/perl/local/psyc/makedoc?id=418=html Brockmeier, Scott (1997). Computational architecture and the creation of consciousness. The Dualist 4. ( Cited by 2 | Google ) Brown, Geoffrey (1989). Minds, Brains And Machines. St Martin's Press. ( Cited by 1 | Google ) Buttazzo, G. (2001). Artificial consciousness: Utopia or real possibility? Computer 34:24-30. ( Cited by 17 | Google | More links ) Additional links for this entry: http://feanor.sssup.it/~giorgio/paps/2001/ieeecm01.pdf http://csdl.computer.org/comp/mags/co/2001/07/r7024abs.htm http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=933500 http://www.das.ufsc.br/~rabelo/Ensino/DAS6607/Artigos-Gerais/ArtificialConsciousness.pdf http://www.csa.com/partners/viewrecord.php?requester=gs=20061023229829EA Caplain, G. (1995). Is consciousness a computational property? Informatica 19:615-19. ( Cited by 2 | Google | More links ) Additional links for this entry: http://books.google.com/books?hl=en=Oa-FArSWce_2jXSLqmlPHTdqlZc Caws, Peter (1988). Subjectivity in the machine. Journal for the Theory of Social Behaviour 18 (September):291-308. ( Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/120012886/PDFSTART Chandler, Keith A. (2002). Artificial intelligence and artificial consciousness. Philosophia 31 (1):32-46. ( Google ) Chella, Antonio Manzotti, Riccardo (2007). Artificial Consciousness. Imprint Academic. (Cited by 1 | Google ) Cherry, Christopher (1989). Reply--the possibility of computers becoming persons: A response to Dolby. Social Epistemology 3 (4):337-348. ( Google ) Clack, Robert J. (1968). The myth of the conscious robot. Personalist 49:351-369. ( Google ) Coles, L. S. (1993). Engineering machine consciousness. AI Expert 8:34-41. ( Google ) Cotterill, Rodney M. J. (2003). Cyberchild: A simulation test-bed for consciousness studies. Journal of Consciousness Studies 10 (4):31-45. ( Cited by 5 | Google ) Danto, Arthur C. (1960). On consciousness in machines. In Sidney Hook (ed.), Dimensions of Mind . New York University Press. ( Cited by 5 | Google ) D'Aquili, Eugene G. Newberg, Andrew B. (1996). Consciousness and the machine. Zygon 31 (2):235-52. ( Cited by 4 | Google ) Dennett, Daniel C. (1997). Consciousness in Human and Robot Minds. In M. Ito, Y. Miyashita Edmund T. Rolls (eds.), Cognition, Computation and Consciousness . Oxford University Press. ( Cited by 12 | Google | More links ) Abstract: The best reason for believing that robots might some day become conscious is that we human beings are conscious, and we are a sort of robot ourselves. That is, we are extraordinarily complex self-controlling, self-sustaining physical mechanisms, designed over the eons by natural selection, and operating according to the same well-understood principles that govern all the other physical processes in living things: digestive and metabolic processes, self-repair and reproductive processes, for instance. It may be wildly over-ambitious to suppose that human artificers can repeat Nature's triumph, with variations in material, form, and design process, but this is not a deep objection. It is not as if a conscious machine contradicted any fundamental laws of nature, the way a perpetual motion machine does. Still, many skeptics believe--or in any event want to believe--that it will never be done. I wouldn't wager against them, but my reasons for skepticism are mundane, economic reasons, not theoretical reasons Additional links for this entry: http://ase.tufts.edu/cogstud/papers/concrobt.htm http://cogprints.ecs.soton.ac.uk/archive/00000429/ http://cogprints.ecs.soton.ac.uk/archive/00000429/00/concrobt.htm http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:429 Dennett, Daniel C. (1994). The practical requirements for making a conscious robot. Philosophical Transactions of the Royal Society 349:133-46. ( Cited by 25 | Google | More links ) Abstract: Arguments about whether a robot could ever be conscious have been conducted up to now in the factually impoverished arena of what is possible "in principle." A team at MIT of which I am a part is now embarking on a longterm project to design and build a humanoid robot, Cog, whose cognitive talents will include speech, eye-coordinated manipulation of objects, and a host of self-protective, self-regulatory and self-exploring activities. The aim of the project is not to make a conscious robot, but to make a robot that can interact with human beings in a robust and versatile manner in real time, take care of itself, and tell its designers things about itself that would otherwise be extremely difficult if not impossible to determine by examination. Many of the details of Cog's "neural" organization will parallel what is known (or presumed known) about their counterparts in the human brain, but the intended realism of Cog as a model is relatively coarse-grained, varying opportunistically as a function of what we think we know, what we think we can build, and what we think doesn't matter. Much of what we think will of course prove to be mistaken; that is one advantage of real experiments over thought experiments Additional links for this entry: http://adsabs.harvard.edu/abs/1994RSPTA.349..133D http://www.journals.royalsoc.ac.uk/index/L62130158Q786731.pdf http://cogsci.soton.ac.uk/~harnad/Papers/Py104/dennett.rob.html Duch, Włodzisław (2005). Brain-inspired conscious computing architecture. Journal of Mind and Behavior 26 (1-2):1-21. ( Cited by 8 | Google | More links ) Abstract: What type of artificial systems will claim to be conscious and will claim to experience qualia? The ability to comment upon physical states of a brain-like dynamical system coupled with its environment seems to be sufficient to make claims. The flow of internal states in such system, guided and limited by associative memory, is similar to the stream of consciousness. Minimal requirements for an artificial system that will claim to be conscious were given in form of specific architecture named articon. Nonverbal discrimination of the working memory states of the articon gives it the ability to experience different qualities of internal states. Analysis of the inner state flows of such a system during typical behavioral process shows that qualia are inseparable from perception and action. The role of consciousness in learning of skills, when conscious information processing is replaced by subconscious, is elucidated. Arguments confirming that phenomenal experience is a result of cognitive processes are presented. Possible philosophical objections based on the Chinese room and other arguments are discussed, but they are insufficient to refute claims articon’s claims. Conditions for genuine understanding that go beyond the Turing test are presented. Articons may fulfill such conditions and in principle the structure of their experiences may be arbitrarily close to human Additional links for this entry: http://citeseer.ist.psu.edu/duch05braininspired.html http://www.phys.uni.torun.pl/publications/kmk/03-Brainins.pdf http://www.phys.uni.torun.pl/publications/kmk/03-Brainins.html http://cogprints.ecs.soton.ac.uk/archive/00003319/01/03-Brainins.pdf http://cogprints.org/3319/1/03-Brainins.pdf http://cogprints.org/3319/1/03%2DBrainins.pdf Ettinger, R. C. W. (2004). To be or not to be: The zombie in the computer. In Nick Bostrom, R.C.W. Ettinger Charles Tandy (eds.), Death and Anti-Death, Volume 2: Two Hundred Years After Kant, Fifty Years After Turing . Palo Alto: Ria University Press. ( Google ) Farrell, B. A. (1970). On the design of a conscious device. Mind 79 (July):321-346. ( Cited by 1 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2252823.pdf Farleigh, Peter (2007). The ensemble and the single mind. In Antonio Chella Riccardo Manzotti (eds.), Artificial Consciousness . Imprint Academic. ( Google ) Franklin, Stan (2003). A conscious artifact? Journal of Consciousness Studies 10. ( Google ) Franklin, Stan (2003). Ida: A conscious artifact? Journal of Consciousness Studies 10 (4):47-66. ( Cited by 40 | Google ) Gunderson, Keith (1969). Cybernetics and mind-body problems. Inquiry 12 (1-4):406-19. ( Google ) Gunderson, Keith (1971). Mentality and Machines. Doubleday. ( Cited by 29 | Google ) Gunderson, Keith (1968). Robots, consciousness and programmed behaviour. British Journal for the Philosophy of Science 19 (August):109-22. ( Google | More links ) Additional links for this entry: http://bjps.oxfordjournals.org/cgi/reprint/19/2/109 http://www.jstor.org/stable/pdfplus/686790.pdf Haikonen, Pentti O. A. (2007). Essential issues of conscious machines. Journal of Consciousness Studies 14 (7):72-84. ( Google | More links ) Abstract: The development of conscious machines faces a number of difficult issues such as the apparent immateriality of mind, qualia and self-awareness. Also consciousness-related cognitive processes such as perception, imagination, motivation and inner speech are a technical challenge. It is foreseen that the development of machine consciousness would call for a system approach; the developer of conscious machines should consider complete systems that integrate the cognitive processes seamlessly and process information in a transparent way with representational and non-representational information-processing modes. An overview of the main issues is given and some possible solutions are outlined Additional links for this entry: http://www.ingentaconnect.com/content/imp/jcs/2007/00000014/00000007/art00005 Haikonen, Pentti, O. (2007). Robot Brains: Circuits and Systems for Conscious Machines. Wiley-Interscience. ( Google | More links ) Additional links for this entry: http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470062045.html Haikonen, Pentti O. A. (2003). The Cognitive Approach to Conscious Machines. Thorverton UK: Imprint Academic. ( Cited by 20 | Google | More links ) Additional links for this entry: http://imprint.co.uk/books/haikonen.html Harnad, Stevan (2003). Can a machine be conscious? How? Journal of Consciousness Studies 10 (4):67-75. ( Cited by 16 | Google | More links ) Abstract: A "machine" is any causal physical system, hence we are machines, hence machines can be conscious. The question is: which kinds of machines can be conscious? Chances are that robots that can pass the Turing Test -- completely indistinguishable from us in their behavioral capacities -- can be conscious (i.e. feel), but we can never be sure (because of the "other-minds" problem). And we can never know HOW they have minds, because of the "mind/body" problem. We can only know how they pass the Turing Test, but not how, why or whether that makes them feel Additional links for this entry: http://cogprints.org/2460/ http://eprints.ecs.soton.ac.uk/7718 http://cogprints.org/5330/1/machine.htm http://cogprints.org/2460/1/machine.htm http://eprints.resist.ecs.soton.ac.uk/7718/ http://eprints.ecs.soton.ac.uk/archive/00007718/ http://cogsci.soton.ac.uk/~harnad/Temp/machine.htm http://cogprints.ecs.soton.ac.uk/archive/00002460/ http://www.ecs.soton.ac.uk/~harnad/Temp/machine.htm http://users.ecs.soton.ac.uk/~harnad/Temp/machine.htm http://cogprints.ecs.soton.ac.uk/archive/00002460/01/machine.htm http://citebase.eprints.org/cgi-bin/citations?id=oai:cogprints.soton.ac.uk:2460 http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:2460 http://www.ingentaconnect.com/content/imp/jcs/2003/00000010/F0020004/1347 http://users.ecs.soton.ac.uk/%7Eharnad/Temp/machine.htm Henley, Tracy B. (1991). Consciousness and aI: A reconsideration of Shanon. Journal of Mind and Behavior 12 (3):367-370. ( Google ) Hillis, D. (1998). Can a machine be conscious? In Stuart R. Hameroff, Alfred W. Kaszniak A. C. Scott (eds.), Toward a Science of Consciousness II . MIT Press. ( Google ) Holland, Owen (2007). A strongly embodied approach to machine consciousness. Journal of Consciousness Studies 14 (7):97-110. ( Cited by 4 | Google | More links ) Abstract: Over sixty years ago, Kenneth Craik noted that, if an organism (or an artificial agent) carried 'a small-scale model of external reality and of its own possible actions within its head', it could use the model to behave intelligently. This paper argues that the possible actions might best be represented by interactions between a model of reality and a model of the agent, and that, in such an arrangement, the internal model of the agent might be a transparent model of the sort recently discussed by Metzinger, and so might offer a useful analogue of a conscious entity. The CRONOS project has built a robot functionally similar to a human that has been provided with an internal model of itself and of the world to be used in the way suggested by Craik; when the system is completed, it will be possible to study its operation from the perspective not only of artificial intelligence, but also of machine consciousness Additional links for this entry: http://www.ingentaconnect.com/content/imp/jcs/2007/00000014/00000007/art00007 Holland, Owen (ed.) (2003). Machine Consciousness. Imprint Academic. ( Cited by 19 | Google | More links ) Abstract: In this collection of essays we hear from an international array of computer and brain scientists who are actively working from both the machine and human ends... Additional links for this entry: http://imprint.co.uk/books/holland.html http://mentalhelp.net/books/books.php?type=de=2305 Holland, Owen Goodman, Russell B. (2003). Robots with internal models: A route to machine consciousness? Journal of Consciousness Studies 10 (4):77-109. ( Cited by 20 | Google | More links ) Additional links for this entry: http://www.ingentaconnect.com/content/imp/jcs/2003/00000010/F0020004/1348 http://www.ingentaconnect.com/content/imp/jcs/2003/00000010/F0020004/1342 Holland, Owen ; Knight, Rob Newcombe, Richard (2007). The role of the self process in embodied machine consciousness. In Antonio Chella Riccardo Manzotti (eds.), Artificial Consciousness . Imprint Academic. ( Google ) Joy, Glenn C. (1989). Gunderson and Searle: A common error about artificial intelligence. Southwest Philosophical Studies 28:28-34. ( Google ) Kirk, Robert E. (1986). Sentience, causation and some robots. Australasian Journal of Philosophy 64 (September):308-21. ( Cited by 1 | Annotation | Google | More links ) One could model brain states with monadic states and appropriate connections. But surely that's not intelligent -- the causation has the wrong form. Nice. Additional links for this entry: http://taylorandfrancis.metapress.com/index/K370525M56161703.pdf http://www.informaworld.com/index/739196855.pdf http://www.ingentaconnect.com/content/routledg/ajphil/1986/00000064/00000003/art00006 http://www.ingentaconnect.com/content/tandf/tajp/1986/00000064/00000003/art00006 http://www.informaworld.com/smpp/./ftinterface~db=all~content=a739196855~fulltext=713240930 Kiverstein, Julian (2007). Could a robot have a subjective point of view? Journal of Consciousness Studies 14 (7):127-139. ( Cited by 2 | Google | More links ) Abstract: Scepticism about the possibility of machine consciousness comes in at least two forms. Some argue that our neurobiology is special, and only something sharing our neurobiology could be a subject of experience. Others argue that a machine couldn't be anything else but a zombie: there could never be something it is like to be a machine. I advance a dynamic sensorimotor account of consciousness which argues against both these varieties of scepticism Additional links for this entry: http://www.ingentaconnect.com/content/imp/jcs/2007/00000014/00000007/art00009 Levy, Donald (2003). How to psychoanalyze a robot: Unconscious cognition and the evolution of intentionality. Minds and Machines 13 (2):203-212. ( Google | More links ) Abstract:   According to a common philosophical distinction, the `original' intentionality, or `aboutness' possessed by our thoughts, beliefs and desires, is categorically different from the `derived' intentionality manifested in some of our artifacts –- our words, books and pictures, for example. Those making the distinction claim that the intentionality of our artifacts is `parasitic' on the `genuine' intentionality to be found in members of the former class of things. In Kinds of Minds: Toward an Understanding of Consciousness, Daniel Dennett criticizes that claim and the distinction it rests on, and seeks to show that ``metaphysically original intentionality'' is illusory by working out the implications he sees in the practical possibility of a certain type of robot, i.e., one that generates `utterances' which are `inscrutable to the robot's designers' so that we, and they, must consult the robot to discover the meaning of its utterances. I argue that the implications Dennett finds are erroneous, regardless of whether such a robot is possible, and therefore that the real existence of metaphysically original intentionality has not been undermined by the possibility of the robot Dennett describes Additional links for this entry: http://www.kluweronline.com/article.asp?PIPS=5115347=1 http://www.springerlink.com/index/U6T815G723R22382.pdf http://www.ingentaconnect.com/content/klu/mind/2003/00000013/00000002/05115347 Lucas, John R. (1994). A view of one's own (conscious machines). Philosophical Transactions of the Royal Society, Series A 349:147-52. ( Google ) Lycan, William G. (1998). Qualitative experience in machines. In Terrell Ward Bynum James H. Moor (eds.), How Computers Are Changing Philosophy . Blackwell. ( Google ) Mackay, Donald M. (1963). Consciousness and mechanism: A reply to miss Fozzy. British Journal for the Philosophy of Science 14 (August):157-159. ( Google | More links ) Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/citation/XIV/54/157 http://bjps.oxfordjournals.org/cgi/reprint/XIV/54/157 http://www.jstor.org/stable/pdfplus/685435.pdf Mackay, Donald M. (1985). Machines, brains, and persons. Zygon 20 (December):401-412. ( Google ) Manzotti, Riccardo (2007). From artificial intelligence to artificial consciousness. In Antonio Chella Riccardo Manzotti (eds.), Artificial Consciousness . Imprint Academic. ( Google ) Margolis, Joseph (1974). Ascribing actions to machines. Behaviorism 2:85-93. ( Google ) Marras, Ausonio (1993). Pollock on how to build a person. Dialogue 32 (3):595-605. ( Cited by 1 | Google ) Maudlin, Tim (1989). Computation and consciousness. Journal of Philosophy 86 (August):407-32. ( Cited by 24 | Annotation | Google | More links ) Computational state is not sufficient for consciousness, as it can be instantiated by a mostly inert object. A nice thought-experiment, raising questions about the relevance of counterfactuals to consciousness. Additional links for this entry: http://www.jstor.org/stable/pdfplus/2026650.pdf Mayberry, Thomas C. (1970). Consciousness and robots. Personalist 51:222-236. ( Google ) McCann, Hugh J. (2005). Intentional action and intending: Recent empirical studies. Philosophical Psychology 18 (6):737-748. ( Cited by 19 | Google | More links ) Abstract: Recent empirical work calls into question the so-called Simple View that an agent who A’s intentionally intends to A. In experimental studies, ordinary speakers frequently assent to claims that, in certain cases, agents who knowingly behave wrongly intentionally bring about the harm they do; yet the speakers tend to deny that it was the intention of those agents to cause the harm. This paper reports two additional studies that at first appear to support the original ones, but argues that in fact, the evidence of all the studies considered is best understood in terms of the Simple View. Additional links for this entry: http://instruct1.cit.cornell.edu/courses/phi663/knobe McCann.pdf http://www.informaworld.com/smpp/./ftinterface~content=a727334015~fulltext=713240930 http://taylorandfrancis.metapress.com/index/RP912503N228W554.pdf http://www.informaworld.com/index/727334015.pdf http://www.ingentaconnect.com/content/routledg/cphp/2005/00000018/00000006/art00005 http://www.informaworld.com/smpp/./ftinterface~db=all~content=a727334015~fulltext=713240930 McCarthy, John (1996). Making robots conscious of their mental states. In S. Muggleton (ed.), Machine Intelligence 15 . Oxford University Press. ( Cited by 68 | Google | More links ) Abstract: In AI, consciousness of self consists in a program having certain kinds of facts about its own mental processes and state of mind. We discuss what consciousness of its own mental structures a robot will need in order to operate in the common sense world and accomplish the tasks humans will give it. It's quite a lot. Many features of human consciousness will be wanted, some will not, and some abilities not possessed by humans have already been found feasible and useful in limited contexts. We give preliminary fragments of a logical language a robot can use to represent information about its own state of mind. A robot will often have to conclude that it cannot decide a question on the basis of the information in memory and therefore must seek information externally. Gödel's idea of relative consistency is used to formalize non-knowledge. Programs with the kind of consciousness discussed in this article do not yet exist, although programs with some components of it exist. Thinking about consciousness with a view to designing it provides a new approach to some of the problems of consciousness studied by philosophers. One advantage is that it focusses on the aspects of consciousness important for intelligent behavior Additional links for this entry: http://citeseer.ist.psu.edu/81449.html http://citeseer.ist.psu.edu/mccarthy95making.html http://cogprints.ecs.soton.ac.uk/archive/00000422/ http://www-formal.stanford.edu/jmc/consciousness.ps http://portal.acm.org/citation.cfm?id=647636.733058 http://www-formal.stanford.edu/jmc/consciousness/consciousness.html http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:422 McDermott, Drew (2007). Artificial intelligence and consciousness. In Philip David Zelazo, Morris Moscovitch Evan Thompson (eds.), The Cambridge Handbook of Consciousness . Cambridge. ( Google ) McGinn, Colin (1987). Could a machine be conscious? In Colin Blakemore Susan A. Greenfield (eds.), Mindwaves . Blackwell. ( Cited by 1 | Annotation | Google ) Of course, as we are machines. But what sort of machines are conscious, and in virtue of what properties? Remarks on artefacts, life, functionalism, and computationalism. So far, we don't know what makes the brain conscious. Mele, Alfred R. (2006). The folk concept of intentional action: A commentary. Journal of Cognition and Culture . ( Cited by 1 | Google | More links ) Abstract: In this commentary, I discuss the three main articles in this volume that present survey data relevant to a search for something that might merit the label “the folk concept of intentional action” – the articles by Joshua Knobe and Arudra Burra, Bertram Malle, and Thomas Nadelhoffer. My guiding question is this: What shape might we find in an analysis of intentional action that takes at face value the results of all of the relevant surveys about vignettes discussed in these three articles?1 To simplify exposition, I assume that there is something that merits the label I mentioned Additional links for this entry: http://www.unc.edu/~knobe/MeleJCC.pdf http://www.springerlink.com/index/0P30736376436T04.pdf http://www.ingentaconnect.com/content/brill/jocc/2006/00000006/F0020001/art00016 Menant, Christophe , Proposal for an approach to artificial consciousness based on self-consciousness. ( Google | More links ) Abstract: Current research on artificial consciousness is focused on phenomenal consciousness and on functional consciousness. We propose to shift the focus to self-consciousness in order to open new areas of investigation. We use an existing scenario where self-consciousness is considered as the result of an evolution of representations. Application of the scenario to the possible build up of a conscious robot also introduces questions relative to emotions in robots. Areas of investigation are proposed as a continuation of this approach Additional links for this entry: http://cogprints.org/6013/ http://cogprints.org/6013/1/FS01MenantCh20070905074007.pdf http://www.aaai.org/Papers/Symposia/Fall/2007/FS-07-01/FS07-01-020.pdf http://cogprints.org/5866/3/FS01MenantCh20070905074007.pdf http://cogprints.org/5999/1/FS01MenantCh20070905074007.pdf Minsky, Marvin L. (1991). Conscious machines. In Machinery of Consciousness . ( Google ) Moffett, Marc A. Cole Wright, Jennifer (online). The folk on know-how: Why radical intellectualism does not over-intellectualize. ( Google ) Abstract: Philosophical discussion of the nature of know-how has focused on the relation between know-how and ability. Broadly speaking, neo-Ryleans attempt to identify know-how with a certain type of ability,1 while, traditionally, intellectualists attempt to reduce it to some form of propositional knowledge. For our purposes, however, this characterization of the debate is too crude. Instead, we prefer the following more explicit taxonomy. Anti-intellectualists, as we will use the term, maintain that knowing how to ? entails the ability to ?. Dispositionalists maintain that the ability to ? is sufficient (modulo some fairly innocuous constraints) for knowing how to ?. Intellectualists, as we will use the term, deny the anti-intellectualist claim. Finally, radical intellectualists deny both the anti-intellectualist and dispositionalist claims. Pace neo-Ryleans (who in our taxonomy are those who accept both dispositionalism and anti-intellectualism), radical intellectualists maintain that the ability to ? is neither necessary nor sufficient for knowing how to ? Nichols, Shaun (2004). Folk concepts and intuitions: From philosophy to cognitive science. Trends in Cognitive Sciences . ( Cited by 10 | Google | More links ) Abstract: Analytic philosophers have long used a priori methods to characterize folk concepts like knowledge, belief, and wrongness. Recently, researchers have begun to exploit social scientific methodologies to characterize such folk concepts. One line of work has explored folk intuitions on cases that are disputed within philosophy. A second approach, with potentially more radical implications, applies the methods of cross-cultural psychology to philosophical intuitions. Recent work suggests that people in different cultures have systematically different intuitions surrounding folk concepts like wrong, knows, and refers. A third strand of research explores the emergence and character of folk concepts in children. These approaches to characterizing folk concepts provide important resources that will supplement, and perhaps sometimes displace, a priori approaches Additional links for this entry: http://dingo.sbs.arizona.edu/~snichols/Papers/FolkConcepts.pdf http://matrix.aklab.psych.ubc.ca/uploads/Christine_FolkPhilosophy_TICS.pdf http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation http://www.ncbi.nlm.nih.gov/sites/entrez?db=pubmed=google http://linkinghub.elsevier.com/retrieve/pii/S1364661304002360 Pinker, Steven (online). Could a computer ever be conscious? ( Google ) Prinz, Jesse J. (2003). Level-headed mysterianism and artificial experience. Journal of Consciousness Studies 10 (4-5):111-132. ( Cited by 8 | Google ) Puccetti, Roland (1975). God and the robots: A philosophical fable. Personalist 56:29-30. ( Google ) Puccetti, Roland (1967). On thinking machines and feeling machines. British Journal for the Philosophy of Science 18 (May):39-51. ( Cited by 3 | Annotation | Google | More links ) Machines can think but can't feel, so aren't persons. Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/citation/18/1/39 http://bjps.oxfordjournals.org/cgi/reprint/18/1/39 http://www.jstor.org/stable/pdfplus/686888.pdf Putnam, Hilary (1964). Robots: Machines or artificially created life? Journal of Philosophy 61 (November):668-91. ( Annotation | Google ) Various arguments and counter-arguments re machine consciousness and civil liberties. Problems of machine consciousness are analogous to problems of human consciousness. The structural basis of the two may well be the same. Rhodes, Kris (ms). Vindication of the Rights of Machine. ( Google | More links ) Abstract: In this paper, I argue that certain Machines can have rights independently of whether they are sentient, or conscious, or whatever you might call it. Additional links for this entry: http://www.igradeyourpaper.com/VRMv3.doc Robinson, William S. (1998). Could a robot be qualitatively conscious? Aisb 99:13-18. ( Google ) Sanz, Ricardo ; López, Ignacio Bermejo-Alonso, Julita (2007). A rationale and vision for machine consciousness in complex controllers. In Antonio Chella Riccardo Manzotti (eds.), Artificial Consciousness . Imprint Academic. ( Google ) Schlagel, Richard H. (1999). Why not artificial consciousness or thought? Minds and Machines 9 (1):3-28. ( Cited by 6 | Google | More links ) Abstract:   The purpose of this article is to show why consciousness and thought are not manifested in digital computers. Analyzing the rationale for claiming that the formal manipulation of physical symbols in Turing machines would emulate human thought, the article attempts to show why this proved false. This is because the reinterpretation of designation and meaning to accommodate physical symbol manipulation eliminated their crucial functions in human discourse. Words have denotations and intensional meanings because the brain transforms the physical stimuli received from the microworld into a qualitative, macroscopic representation for consciousness. Lacking this capacity as programmed machines, computers have no representations for their symbols to designate and mean. Unlike human beings in which consciousness and thought, with their inherent content, have emerged because of their organic natures, serial processing computers or parallel distributed processing systems, as programmed electrical machines, lack these causal capacities Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=467237CI http://www.springerlink.com/content/k248165u513p4746/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=187743=1 http://www.springerlink.com/index/K248165U513P4746.pdf http://www.ingentaconnect.com/content/klu/mind/1998/00000009/00000001/00187743 Scriven, Michael (1953). The mechanical concept of mind. Mind 62 (April):230-240. ( Cited by 12 | Annotation | Google | More links ) To speak of a conscious machine is to commit a semantic mistake. Consciousness presupposes life and non-mechanism. Later retracted. Additional links for this entry: http://mind.oxfordjournals.org/cgi/reprint/LXII/246/230 http://www.jstor.org/stable/pdfplus/2251386.pdf Shanon, Benny (1991). Consciousness and the computer: A reply to Henley. Journal of Mind and Behavior 12 (3):371-375. ( Google ) Sharlow, Mark F. (ms). Can machines have first-person properties? ( Google ) Abstract: One of the most important ongoing debates in the philosophy of mind is the debate over the reality of the first-person character of consciousness.[1] Philosophers on one side of this debate hold that some features of experience are accessible only from a first-person standpoint. Some members of this camp, notably Frank Jackson, have maintained that epiphenomenal properties play roles in consciousness [2]; others, notably John R. Searle, have rejected dualism and regarded mental phenomena as entirely biological.[3] In the opposite camp are philosophers who hold that all mental capacities are in some sense computational - or, more broadly, explainable in terms of features of information processing systems.[4] Consistent with this explanatory agenda, members of this camp normally deny that any aspect of mind is accessible solely from a first-person standpoint. This denial sometimes goes very far - even as far as Dennett's claim that the phenomenology of conscious experience does not really exist Simon, Michael A. (1969). Could there be a conscious automaton? American Philosophical Quarterly 6 (January):71-78. ( Google ) Sloman, Aaron Chrisley, Ronald L. (2003). Virtual machines and consciousness. Journal of Consciousness Studies 10 (4-5):133-172. ( Cited by 26 | Google | More links ) Additional links for this entry: http://citeseer.ist.psu.edu/sloman02virtual.html http://www.cs.bham.ac.uk/research/cogaff/sloman-chrisley-jcs.pdf http://www.cs.bham.ac.uk/research/cogaff/sloman-chrisley-jcs03.pdf http://www.cs.memphis.edu/~franklin/documents/sloman-chrisley-jcs.pdf http://www.msci.memphis.edu/~franklin/documents/sloman-chrisley-jcs.pdf http://www.ingentaconnect.com/content/imp/jcs/2003/00000010/F0020004/1350 Smart, J. J. C. (1959). Professor Ziff on robots. Analysis 19 (April):117-118. ( Cited by 3 | Google ) Smart, Ninian (1959). Robots incorporated. Analysis 19 (April):119-120. ( Cited by 3 | Google ) Stuart, Susan A. J. (2007). Machine consciousness: Cognitive and kinaesthetic imagination. Journal of Consciousness Studies 14 (7):141-153. ( Cited by 1 | Google | More links ) Abstract: Machine consciousness exists already in organic systems and it is only a matter of time -- and some agreement -- before it will be realised in reverse-engineered organic systems and forward- engineered inorganic systems. The agreement must be over the preconditions that must first be met if the enterprise is to be successful, and it is these preconditions, for instance, being a socially-embedded, structurally-coupled and dynamic, goal-directed entity that organises its perceptual input and enacts its world through the application of both a cognitive and kinaesthetic imagination, that I shall concentrate on presenting in this paper. It will become clear that these preconditions will present engineers with a tall order, but not, I will argue, an impossible one. After all, we might agree with Freeman and Núñez's claim that the machine metaphor has restricted the expectations of the cognitive sciences (Freeman & Núñez, 1999); but it is a double-edged sword, since our limited expectations about machines also narrow the potential of our cognitive science Additional links for this entry: http://www.ingentaconnect.com/content/imp/jcs/2007/00000014/00000007/art00010 Stubenberg, Leopold (1992). What is it like to be Oscar? Synthese 90 (1):1-26. ( Cited by 1 | Annotation | Google | More links ) Argues that AI systems like Pollock's Oscar needn't be conscious. Blindsight tells us that complex perceptual processing can go on unconsciously. Abstract:   Oscar is going to be the first artificial person — at any rate, he is going to be the first artificial person to be built in Tucson's Philosophy Department. Oscar's creator, John Pollock, maintains that once Oscar is complete he will experience qualia, will be self-conscious, will have desires, fears, intentions, and a full range of mental states (Pollock 1989, pp. ix–x). In this paper I focus on what seems to me to be the most problematical of these claims, viz., that Oscar will experience qualia. I argue that we have not been given sufficient reasons to believe this bold claim. I doubt that Oscar will enjoy qualitative conscious phenomena and I maintain that it will be like nothing to be Oscar Additional links for this entry: http://www.springerlink.com/index/M60825350X345117.pdf Tagliasco, Vincenzo (2007). Artificial consciousness: A technological discipline. In Antonio Chella Riccardo Manzotti (eds.), Artificial Consciousness . Imprint Academic. ( Google ) Taylor, John G. (2007). Through machine attention to machine consciousness. In Antonio Chella Riccardo Manzotti (eds.), Artificial Consciousness . Imprint Academic. ( Google ) Thompson, David L. (1965). Can a machine be conscious? British Journal for the Philosophy of Science 16 (May):33-43. ( Annotation | Google | More links ) Accepting machine consciousness would have few philosophical consequences, whereas rejecting it would tend to commit one to epiphenomenalism. Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/citation/XVI/61/33 http://bjps.oxfordjournals.org/cgi/reprint/XVI/61/33 http://www.jstor.org/stable/pdfplus/686137.pdf Thompson, William I. (2003). The Borg or Borges? In Owen Holland (ed.), Machine Consciousness . Imprint Academic. ( Cited by 2 | Google | More links ) Additional links for this entry: http://www.ingentaconnect.com/content/imp/jcs/2003/00000010/F0020004/1352 Torrance, Steve (2007). Two conceptions of machine phenomenality. Journal of Consciousness Studies 14 (7):154-166. ( Cited by 2 | Google | More links ) Abstract: Current approaches to machine consciousness (MC) tend to offer a range of characteristic responses to critics of the enterprise. Many of these responses seem to marginalize phenomenal consciousness, by presupposing a 'thin' conception of phenomenality. This conception is, we will argue, largely shared by anti- computationalist critics of MC. On the thin conception, physiological or neural or functional or organizational features are secondary accompaniments to consciousness rather than primary components of consciousness itself. We outline an alternative, 'thick' conception of phenomenality. This shows some signposts in the direction of a more adequate approach to MC Additional links for this entry: http://www.ingentaconnect.com/content/imp/jcs/2007/00000014/00000007/art00011 Tson, M. E. (ms). From Dust to Descartes: A Mechanical and Evolutionary Explanation of Consciousness and Self-Awareness. ( Google ) van de Vete, D. (1971). The problem of robot consciousness. Philosophy and Phenomenological Research 32:149-65. ( Google ) Wallace, Rodrick (2006). Pitfalls in biological computing: Canonical and idiosyncratic dysfunction of conscious machines. Mind and Matter 4 (1):91-113. ( Cited by 7 | Google | More links ) Abstract: The central paradigm of arti?cial intelligence is rapidly shifting toward biological models for both robotic devices and systems performing such critical tasks as network management, vehicle navigation, and process control. Here we use a recent mathematical analysis of the necessary conditions for consciousness in humans to explore likely failure modes inherent to a broad class of biologically inspired computing machines. Analogs to developmental psychopathology, in which regulatory mechanisms for consciousness fail progressively and subtly understress, and toinattentional blindness, where a narrow 'syntactic band pass' de?ned by the rate distortion manifold of conscious attention results in pathological ?xation, seem inevitable. Similar problems are likely to confront other possible architectures, although their mathematical description may be far less straightforward. Computing devices constructed on biological paradigms will inevitably lack the elaborate, but poorly understood, system of control mechanisms which has evolved over the last few hundred million years to stabilize consciousness in higher animals. This will make such machines prone to insidious degradation, and, ultimately, catastrophic failure Additional links for this entry: http://www.ingentaconnect.com/content/imp/mm/2006/00000004/00000001/art00004 Ziff, P. (1959). The feelings of robots. Analysis 19 (January):64-68. ( Cited by 11 | Annotation | Google ) Of course robots can't think: they're not alive, so this gives us good reason not to rely on behavior. With replies by J.J.C. Smart, N. Smart. 6.1e Machine Mentality, Misc Albritton, Rogers (1964). Comments on Hilary Putnam's robots: Machines or artificially created life. Journal of Philosophy 61 (November):691-694. ( Google ) Ashby, W. R. (1947). The nervous system as physical machine: With special reference to the origin of adaptive behaviour. Mind 56 (January):44-59. ( Cited by 8 | Google | More links ) Additional links for this entry: http://mind.oxfordjournals.org/cgi/reprint/LVI/221/44 http://www.jstor.org/stable/pdfplus/2250675.pdf Beisecker, David (2006). Dennett's overlooked originality. Minds and Machines 16 (1):43-55. ( Google | More links ) Abstract: No philosopher has worked harder than Dan Dennett to set the possibility of machine mentality on firm philosophical footing. Dennett’s defense of this possibility has both a positive and a negative thrust. On the positive side, he has developed an account of mental activity that is tailor-made for the attribution of intentional states to purely mechanical contrivances, while on the negative side, he pillories as mystery mongering and skyhook grasping any attempts to erect barriers to the conception of machine mentality by excavating gulfs to keep us “bona fide” thinkers apart from the rest of creation. While I think he’s “won” the rhetorical tilts with his philosophical adversaries, I worry that Dennett’s negative side sometimes gets the better of him, and that this obscures advances that can be made on the positive side of his program. In this paper, I show that Dennett is much too dismissive of original intentionality in particular, and that this notion can be put to good theoretical use after all. Though deployed to distinguish different grades of mentality, it can (and should) be incorporated into a philosophical account of the mind that is recognizably Dennettian in spirit Additional links for this entry: http://www.springerlink.com/index/3M13835807176130.pdf Beloff, John (2002). Minds or machines. Truth Journal . ( Cited by 2 | Google ) Boden, Margaret A. (1995). Could a robot be creative--and would we know? In Android Epistemology . Cambridge: MIT Press. ( Cited by 6 | Google | More links ) Additional links for this entry: http://kogs-www.informatik.uni-hamburg.de/~neumann/Denkmaschinen-SS-2005/Vortrag-14.6.05.pdf Boden, Margaret A. (1969). Machine perception. Philosophical Quarterly 19 (January):33-45. ( Cited by 2 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2218186.pdf Bostrom, Nick (2003). Taking intelligent machines seriously: Reply to critics. Futures 35 (8):901-906. ( Google | More links ) Abstract: In an earlier paper in this journal[1], I sought to defend the claims that (1) substantial probability should be assigned to the hypothesis that machines will outsmart humans within 50 years, (2) such an event would have immense ramifications for many important areas of human concern, and that consequently (3) serious attention should be given to this scenario. Here, I will address a number of points made by several commentators Additional links for this entry: http://www.questia.com/PM.qst?a=o=5002030442 http://linkinghub.elsevier.com/retrieve/pii/S0016328703000466 http://www.ingentaconnect.com/content/els/00163287/2003/00000035/00000008/art00046 Brey, Philip (2001). Hubert Dreyfus: Humans versus computers. In American Philosophy of Technology: The Empirical Turn . Bloomington: Indiana University Press. ( Cited by 2 | Google ) Bringsjord, Selmer (1998). Cognition is not computation: The argument from irreversibility. Synthese 113 (2):285-320. ( Cited by 11 | Google | More links ) Abstract:   The dominant scientific and philosophical view of the mind – according to which, put starkly, cognition is computation – is refuted herein, via specification and defense of the following new argument: Computation is reversible; cognition isn't; ergo, cognition isn't computation. After presenting a sustained dialectic arising from this defense, we conclude with a brief preview of the view we would put in place of the cognition-is-computation doctrine Additional links for this entry: http://www.rpi.edu/~brings/SELPAP/irr.ps http://www.rpi.edu/~brings/SELPAP/irr.pdf http://citeseer.ist.psu.edu/bringsjord96cognition.html http://www.springerlink.com/content/n887505850j434ur/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=152330=1 http://www.springerlink.com/index/N887505850J434UR.pdf http://www.ingentaconnect.com/content/klu/synt/1997/00000113/00000002/00152330 Bringsjord, Selmer (1994). Precis of What Robots Can and Can't Be . Psycholoquy 5 (59). ( Cited by 22 | Google ) Bunge, Mario (1956). Do computers think? (I). British Journal for the Philosophy of Science 7 (26):139-148. (Cited by 1 | Google | More links ) Additional links for this entry: http://bjps.oxfordjournals.org/cgi/reprint/VII/26/139 http://www.jstor.org/stable/pdfplus/685697.pdf http://www.jstor.org/stable/pdfplus/685873.pdf Bunge, Mario (1956). Do computers think? (II). British Journal for the Philosophy of Science 7 (27):212-219. ( Google | More links ) Additional links for this entry: http://bjps.oxfordjournals.org/cgi/reprint/VII/27/212 http://www.jstor.org/stable/pdfplus/685697.pdf http://www.jstor.org/stable/pdfplus/685873.pdf Burks, Arthur W. (1973). Logic, computers, and men. Proceedings and Addresses of the American Philosophical Association 46:39-57. ( Cited by 4 | Annotation | Google ) Arguing that a finite deterministic automaton can perform all natural human functions. With remarks on the logical organization of computers. Campbell, Richmond M. Rosenberg, Alexander (1973). Action, purpose, and consciousness among the computers. Philosophy of Science 40 (December):547-557. ( Google | More links ) Additional links for this entry: http://www.journals.uchicago.edu/cgi-bin/resolve?id=doi:10.1086/288564 http://www.jstor.org/stable/pdfplus/186287.pdf Casey, Gerard (1992). Minds and machines. American Catholic Philosophical Quarterly 66 (1):57-80. ( Cited by 3 | Google ) Abstract: The emergence of electronic computers in the last thirty years has given rise to many interesting questions. Many of these questions are technical, relating to a machine’s ability to perform complex operations in a variety of circumstances. While some of these questions are not without philosophical interest, the one question which above all others has stimulated philosophical interest is explicitly non-technical and it can be expressed crudely as follows: Can a machine be said to think and, if so, in what sense? The issue has received much attention in the scholarly journals with articles and arguments appearing in great profusion, some resolutely answering this question in the affirmative, some, equally resolutely, answering this question in the negative, and others manifesting modified rapture. While the ramifications of the question are enormous I believe that the issue at the heart of the matter has gradually emerged from the forest of complications Cherry, Christopher (1991). Machines as persons? - I. In Human Beings . New York: Cambridge University Press. ( Google ) Cohen, L. Jonathan (1955). Can there be artificial minds? Analysis 16 (December):36-41. ( Cited by 3 | Annotation | Google ) Subservience to known or knowable rules is incompatible with mentality. Collins, Harry M. (2008). Response to Selinger on Dreyfus. Phenomenology and the Cognitive Sciences 7 (2). ( Google | More links ) Abstract: My claim is clear and unambiguous: no machine will pass a well-designed Turing Test unless we find some means of embedding it in lived social life. We have no idea how to do this but my argument, and all our evidence, suggests that it will not be a necessary condition that the machine have more than a minimal body. Exactly how minimal is still being worked out Additional links for this entry: http://www.springerlink.com/index/w6vp1135100t4555.pdf Copeland, B. Jack (2000). Narrow versus wide mechanism: Including a re-examination of Turing's views on the mind-machine issue. Journal of Philosophy 97 (1):5-33. ( Cited by 42 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2678472.pdf Dayre, Kenneth M. (1968). Intelligence, bodies, and digital computers. Review of Metaphysics 21 (June):714-723. ( Google ) Dembski, William A. (1999). Are we spiritual machines? First Things 96:25-31. ( Google ) Abstract: For two hundred years materialist philosophers have argued that man is some sort of machine. The claim began with French materialists of the Enlightenment such as Pierre Cabanis, Julien La Mettrie, and Baron d’Holbach (La Mettrie even wrote a book titled Man the Machine). Likewise contemporary materialists like Marvin Minsky, Daniel Dennett, and Patricia Churchland claim that the motions and modifications of matter are sufficient to account for all human experiences, even our interior and cognitive ones. Whereas the Enlightenment philosophes might have thought of humans in terms of gear mechanisms and fluid flows, contemporary materialists think of humans in terms of neurological systems and computational devices. The idiom has been updated, but the underlying impulse to reduce mind to matter remains unchanged Dennett, Daniel C. (1984). Can machines think? In M. G. Shafto (ed.), How We Know . Harper & Row. ( Cited by 24 | Annotation | Google ) Defending the Turing test as a good test for intelligence. Dennett, Daniel C. (1997). Did Hal committ murder? In D. Stork (ed.), Hal's Legacy: 2001's Computer As Dream and Reality . MIT Press. ( Google ) Abstract: The first robot homicide was committed in 1981, according to my files. I have a yellowed clipping dated 12/9/81 from the Philadelphia Inquirer--not the National Enquirer--with the headline: Robot killed repairman, Japan reports The story was an anti-climax: at the Kawasaki Heavy Industries plant in Akashi, a malfunctioning robotic arm pushed a repairman against a gearwheel-milling machine, crushing him to death. The repairman had failed to follow proper instructions for shutting down the arm before entering the workspace. Why, indeed, had this industrial accident in Japan been reported in a Philadelphia newspaper? Every day somewhere in the world a human worker is killed by one machine or another. The difference, of course, was that in the public imagination at least, this was no ordinary machine; this was a robot, a machine that might have a mind, might have evil intentions, might be capable not just of homicide but of murder Dretske, Fred (1993). Can intelligence be artificial? Philosophical Studies 71 (2):201-16. ( Cited by 3 | Annotation | Google | More links ) Intelligence requires not just action or thought, but the governance of action by thought, which requires a history. "Wired-up" systems lack the explanatory connection between thought and action, so are not intelligent. Additional links for this entry: http://www.springerlink.com/index/X3345068024H5002.pdf http://www.springerlink.com/content/x3345068024h5002/fulltext.pdf Dretske, Fred (1985). Machines and the mental. Proceedings and Addresses of the American Philosophical Association 59 (1):23-33. ( Cited by 27 | Annotation | Google ) Machines can't even add, let alone think, as the symbols they use aren't meaningful to them. They would need real information based on perceptual embodiment, and conceptual capacities, for meaning to play a real role. Drexler, Eric (1986). Thinking machines. In Engines of Creation . Fourth Estate. ( Cited by 1 | Google ) Dreyfus, Hubert L. (1972). What Computers Can't Do. Harper and Row. ( Cited by 847 | Annotation | Google ) Computers follow rules, people don't. Dreyfus, Hubert L. (1967). Why computers must have bodies in order to be intelligent. Review of Metaphysics 21 (September):13-32. ( Cited by 13 | Google ) Drozdek, Adam (1993). Computers and the mind-body problem: On ontological and epistemological dualism. Idealistic Studies 23 (1):39-48. ( Google ) Endicott, Ronald P. (1996). Searle, syntax, and observer-relativity. Canadian Journal of Philosophy 26 (1):101-22. ( Cited by 3 | Google ) Abstract: I critically examine some provocative arguments that John Searle presents in his book The Rediscovery of Mind to support the claim that the syntactic states of a classical computational system are observer relative or mind dependent or otherwise less than fully and objectively real. I begin by explaining how this claim differs from Searle's earlier and more well-known claim that the physical states of a machine, including the syntactic states, are insufficient to determine its semantics. In contrast, his more recent claim concerns the syntax, in particular, whether a machine actually has symbols to underlie its semantics. I then present and respond to a number of arguments that Searle offers to support this claim, including whether machine symbols are observer relative because the assignment of syntax is arbitrary, or linked to universal realizability, or linked to the sub-personal interpretive acts of a homunculus, or linked to a person's consciousness. I conclude that a realist about the computational model need not be troubled by such arguments. Their key premises need further support. Fisher, Mark (1983). A note on free will and artificial intelligence. Philosophia 13 (September):75-80. ( Google | More links ) Additional links for this entry: http://www.springerlink.com/index/657443W258453806.pdf Fozzy, P. J. (1963). Professor MacKay on machines. British Journal for the Philosophy of Science 14 (August):154-156. ( Google | More links ) Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/citation/XIV/54/154 http://bjps.oxfordjournals.org/cgi/reprint/XIV/54/154 http://www.jstor.org/stable/pdfplus/685434.pdf Friedland, Julian (2005). Wittgenstein and the aesthetic robot's handicap. Philosophical Investigations 28 (2):177-192. ( Google | More links ) Additional links for this entry: http://www.blackwell-synergy.com/doi/abs/10.1111/j.1467-9205.2005.00250.x http://www3.interscience.wiley.com/cgi-bin/fulltext/118651665/PDFSTART http://www.ingentaconnect.com/content/bpl/phin/2005/00000028/00000002/art00004 Fulton, James S. (1957). Computing machines and minds. Personalist 38:62-72. ( Google ) Gaglio, Salvatore (2007). Intelligent artificial systems. In Antonio Chella Riccardo Manzotti (eds.), Artificial Consciousness . Imprint Academic. ( Google ) Gams, Matjaz (ed.) (1997). Mind Versus Computer: Were Dreyfus and Winograd Right? Amsterdam: IOS Press. ( Cited by 7 | Google | More links ) Additional links for this entry: http://portal.acm.org/citation.cfm?id=294029 http://portal.acm.org/citation.cfm?id=SERIES9545.294029 http://ai.ijs.si/mezi/kopija231/STARI_PC/USR/INFORM/Vol21/Book/BOOK_97.PS Gauld, Alan (1966). Could a machine perceive? British Journal for the Philosophy of Science 17 (May):44-58. ( Cited by 3 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/686403.pdf Gogol, Daniel (1970). Determinism and the predicting machine. Philosophy and Phenomenological Research 30 (March):455-456. ( Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2105612.pdf Goldkind, Stuart (1982). Machines and mistakes. Ratio 24 (December):173-184. ( Cited by 1 | Google ) Goldberg, Sanford C. (1997). The very idea of computer self-knowledge and self-deception. Minds and Machines 7 (4):515-529. ( Cited by 5 | Google | More links ) Abstract:   Do computers have beliefs? I argue that anyone who answers in the affirmative holds a view that is incompatible with what I shall call the commonsense approach to the propositional attitudes. My claims shall be two. First,the commonsense view places important constraints on what can be acknowledged as a case of having a belief. Second, computers – at least those for which having a belief would be conceived as having a sentence in a belief box – fail to satisfy some of these constraints. This second claim can best be brought out in the context of an examination of the idea of computer self-knowledge and self-deception, but the conclusion is perfectly general: the idea that computers are believers, like the idea that computers could have self-knowledge or be self-deceived, is incompatible with the commonsense view. The significance of the argument lies in the choice it forces on us: whether to revise our notion of belief so as to accommodate the claim that computers are believers, or to give up on that claim so as to preserve our pretheoretic notion of the attitudes. We cannot have it both ways Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=316238CI http://www.springerlink.com/content/x67006n334r1n383/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=136245=1 http://www.springerlink.com/index/X67006N334R1N383.pdf http://www.ingentaconnect.com/content/klu/mind/1997/00000007/00000004/00136245 Gomila, Antoni (1995). From cognitive systems to persons. In Android Epistemology . Cambridge: MIT Press. ( Cited by 2 | Google ) Gunderson, Keith (1963). Interview with a robot. Analysis 23 (June):136-142. ( Cited by 2 | Google ) Gunderson, Keith (1985). Mentality And Machines, Second Edition. Minneapolis: University Minnesota Press. ( Google ) Hauser, Larry (1993). The sense of thinking. Minds and Machines 3 (1):21-29. ( Cited by 3 | Google | More links ) Abstract:   It will be found that the great majority, given the premiss that thought is not distinct from corporeal motion, take a much more rational line and maintain that thought is the same in the brutes as in us, since they observe all sorts of corporeal motions in them, just as in us. And they will add that the difference, which is merely one of degree, does not imply any essential difference; from this they will be quite justified in concluding that, although there may be a smaller degree of reason in the beasts than there is in us, the beasts possess minds which are of exactly the same type as ours. (Descartes 1642: 288–289.) Additional links for this entry: http://www.springerlink.com/index/H68X032616766137.pdf Hauser, Larry (1993). Why isn't my pocket calculator a thinking thing? Minds and Machines 3 (1):3-10. ( Cited by 11 | Google | More links ) Abstract: My pocket calculator (Cal) has certain arithmetical abilities: it seems Cal calculates. That calculating is thinking seems equally untendentious. Yet these two claims together provide premises for a seemingly valid syllogism whose conclusion -- Cal thinks -- most would deny. I consider several ways to avoid this conclusion, and find them mostly wanting. Either we ourselves can't be said to think or calculate if our calculation-like performances are judged by the standards proposed to rule out Cal; or the standards -- e.g., autonomy and self-consciousness -- make it impossible to verify whether anything or anyone (save myself) meets them. While appeals to the intentionality of thought or the unity of minds provide more credible lines of resistance, available accounts of intentionality and mental unity are insufficiently clear and warranted to provide very substantial arguments against Cal's title to be called a thinking thing. Indeed, considerations favoring granting that title are more formidable than generally appreciated Additional links for this entry: http://cogprints.org/242/00/wimpcatt.html http://members.aol.com/lshauser/wimpcatt.html http://cogprints.ecs.soton.ac.uk/archive/00000242/ http://cogprints.ecs.soton.ac.uk/archive/00000242/00/wimpcatt.html http://citebase.eprints.org/cgi-bin/citations?id=oai:cogprints.soton.ac.uk:242 http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:242 http://www.springerlink.com/content/t46235328605618l/fulltext.pdf http://www.springerlink.com/content/k5383t2m586208l4/fulltext.pdf http://www.springerlink.com/index/T46235328605618L.pdf http://www.springerlink.com/index/K5383T2M586208L4.pdf http://cogprints.org/242/1/wimpcatt.html http://cogprints.org/242/0/wimpcatt.html Heffernan, James D. (1978). Some doubts about Turing machine arguments. Philosophy of Science 45 (December):638-647. ( Google | More links ) Additional links for this entry: http://www.journals.uchicago.edu/cgi-bin/resolve?id=doi:10.1086/288843 http://www.jstor.org/stable/pdfplus/186977.pdf Henley, Tracy B. (1990). Natural problems and artificial intelligence. Behavior and Philosophy 18:43-55. ( Cited by 4 | Annotation | Google ) On the philosophical importance of criteria for intelligence. With remarks on Searle, the Turing test, attitudes to AI, and ethical considerations. Joske, W. D. (1972). Deliberating machines. Philosophical Papers 1 (October):57-66. ( Google ) Kary, Michael Mahner, Martin (2002). How would you know if you synthesized a thinking thing? Minds and Machines 12 (1):61-86. ( Cited by 1 | Google | More links ) Abstract:   We confront the following popular views: that mind or life are algorithms; that thinking, or more generally any process other than computation, is computation; that anything other than a working brain can have thoughts; that anything other than a biological organism can be alive; that form and function are independent of matter; that sufficiently accurate simulations are just as genuine as the real things they imitate; and that the Turing test is either a necessary or sufficient or scientific procedure for evaluating whether or not an entity is intelligent. Drawing on the distinction between activities and tasks, and the fundamental scientific principles of ontological lawfulness, epistemological realism, and methodological skepticism, we argue for traditional scientific materialism of the emergentist kind in opposition to the functionalism, behaviourism, tacit idealism, and merely decorative materialism of the artificial intelligence and artificial life communities Additional links for this entry: http://www.springerlink.com/content/h8731k7036822314/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=390424=1 http://www.springerlink.com/index/H8731K7036822314.pdf http://www.ingentaconnect.com/content/klu/mind/2002/00000012/00000001/00390424 Kearns, John T. (1997). Thinking machines: Some fundamental confusions. Minds and Machines 7 (2):269-87. ( Cited by 8 | Google | More links ) Abstract:   This paper explores Church's Thesis and related claims madeby Turing. Church's Thesis concerns computable numerical functions, whileTuring's claims concern both procedures for manipulating uninterpreted marksand machines that generate the results that these procedures would yield. Itis argued that Turing's claims are true, and that they support (the truth of)Church's Thesis. It is further argued that the truth of Turing's and Church'sTheses has no interesting consequences for human cognition or cognitiveabilities. The Theses don't even mean that computers can do as much as peoplecan when it comes to carrying out effective procedures. For carrying out aprocedure is a purposive, intentional activity. No actual machine does, orcan do, as much Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=322375CI http://www.springerlink.com/content/p457647356006413/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=116417=1 http://www.springerlink.com/index/P457647356006413.pdf http://www.ingentaconnect.com/content/klu/mind/1997/00000007/00000002/00116417 Krishna, Daya (1961). "Lying" and the compleat robot. British Journal for the Philosophy of Science 12 (August):146-149. ( Cited by 1 | Google | More links ) Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/citation/XII/46/146 http://bjps.oxfordjournals.org/cgi/reprint/XII/46/146 http://www.jstor.org/stable/pdfplus/685493.pdf Kugel, Peter (2002). Computing machines can't be intelligent (...And Turing said so). Minds and Machines 12 (4):563-579. ( Cited by 4 | Google | More links ) Abstract:   According to the conventional wisdom, Turing (1950) said that computing machines can be intelligent. I don''t believe it. I think that what Turing really said was that computing machines –- computers limited to computing –- can only fake intelligence. If we want computers to become genuinelyintelligent, we will have to give them enough initiative (Turing, 1948, p. 21) to do more than compute. In this paper, I want to try to develop this idea. I want to explain how giving computers more ``initiative'''' can allow them to do more than compute. And I want to say why I believe (and believe that Turing believed) that they will have to go beyond computation before they can become genuinely intelligent Additional links for this entry: http://www.cs.bc.edu/~kugel/Publications/Hyper.pdf http://www.cs.queensu.ca/home/akl/cisc879/papers/PAPERS_FROM_MINDS_AND_MACHINES/VOLUME_12_NO_4/P8T30NK152182338.pdf http://www.springerlink.com/content/p8t30nk152182338/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=5098145=1 http://www.springerlink.com/index/P8T30NK152182338.pdf http://www.ingentaconnect.com/content/klu/mind/2002/00000012/00000004/05098145 Lanier, Jaron (ms). Mindless thought experiments (a critique of machine intelligence). ( Google ) Abstract: Since there isn't a computer that seems conscious at this time, the idea of machine consciousness is supported by thought experiments. Here's one old chestnut: "What if you replaced your neurons one by one with neuron sized and shaped substitutes made of silicon chips that perfectly mimicked the chemical and electric functions of the originals? If you just replaced one single neuron, surely you'd feel the same. As you proceed, as more and more neurons are replaced, you'd stay conscious. Why wouldn't you still be conscious at the end of the process, when you'd reside in a brain shaped glob of silicon? And why couldn't the resulting replacement brain have been manufactured by some other means?" Lanier, Jaron (1998). Three objections to the idea of artificial intelligence. In Stuart R. Hameroff, Alfred W. Kaszniak A. C. Scott (eds.), Toward a Science of Consciousness II . MIT Press. ( Google ) Laymon, Ronald E. (1988). Some computers can add (even if the IBM 1620 couldn't): Defending eniac's accumulators against Dretske. Behaviorism 16:1-16. ( Google ) Lind, Richard W. (1986). The priority of attention: Intentionality for automata. The Monist 69 (October):609-619. ( Cited by 1 | Google ) Long, Douglas C. (1994). Why Machines Can Neither Think nor Feel. In Dale W. Jamieson (ed.), Language, Mind and Art . Kluwer. ( Cited by 1 | Google ) Abstract: Over three decades ago, in a brief but provocative essay, Paul Ziff argued for the thesis that robots cannot have feelings because they are "mechanisms, not organisms, not living creatures. There could be a broken-down robot but not a dead one. Only living creatures can literally have feelings."[i] Since machines are not living things they cannot have feelings Mackay, Donald M. (1951). Mind-life behavior in artifacts. British Journal for the Philosophy of Science 2 (August):105-21. ( Google | More links ) Additional links for this entry: http://bjps.oxfordjournals.org/cgi/reprint/II/6/105 http://www.jstor.org/stable/pdfplus/685505.pdf Mackay, Donald M. (1952). Mentality in machines. Proceedings of the Aristotelian Society 26:61-86. ( Cited by 11 | Google ) Mackay, Donald M. (1952). Mentality in machines, part III. Proceedings of the Aristotelian Society 61:61-86. ( Google ) Mackay, Donald M. (1962). The use of behavioural language to refer to mechanical processes. British Journal for the Philosophy of Science 13 (August):89-103. ( Cited by 9 | Google | More links ) Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/citation/XIII/50/89 http://bjps.oxfordjournals.org/cgi/reprint/XIII/50/89 http://www.jstor.org/stable/pdfplus/685963.pdf Manning, Rita C. (1987). Why Sherlock Holmes can't be replaced by an expert system. Philosophical Studies 51 (January):19-28. ( Cited by 3 | Annotation | Google | More links ) An expert system would lack Holmes' ability to raise the right questions, sort out relevant data, and determine what data are in need of explanation. Additional links for this entry: http://www.springerlink.com/index/NXQ656054557584R.pdf Mays, W. (1952). Can machines think? Philosophy 27 (April):148-62. ( Cited by 7 | Google ) McCarthy, John (1979). Ascribing mental qualities to machines. In Martin Ringle (ed.), Philosophical Perspectives in Artificial Intelligence . Humanities Press. ( Cited by 168 | Google | More links ) Abstract: Ascribing mental qualities like beliefs, intentions and wants to a machine is sometimes correct if done conservatively and is sometimes necessary to express what is known about its state. We propose some new definitional tools for this: definitions relative to an approximate theory and second order structural definitions Additional links for this entry: http://www-formal.stanford.edu/jmc/ascribing.ps http://www-formal.stanford.edu/jmc/ascribing.pdf http://cogprints.ecs.soton.ac.uk/archive/00000416/00/ascribing.ps http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:416 http://stinet.dtic.mil/oai/oai?verb=getRecord=ADA071423 McNamara, Paul (1993). Comments on can intelligence be artificial? Philosophical Studies 71 (2):217-222. ( Google | More links ) Additional links for this entry: http://www.springerlink.com/index/V4U3146226676203.pdf Minsky, Marvin L. (1968). Matter, minds, models. In Marvin L. Minsky (ed.), Semantic Information Processing . MIT Press. ( Cited by 18 | Google ) Minsky, Marvin L. (1982). Why people think computers can't. AI Magazine Fall 1982. ( Cited by 32 | Google | More links ) Abstract: Most people think computers will never be able to think. That is, really think. Not now or ever. To be sure, most people also agree that computers can do many things that a person would have to be thinking to do. Then how could a machine seem to think but not actually think? Well, setting aside the question of what thinking actually is, I think that most of us would answer that by saying that in these cases, what the computer is doing is merely a superficial imitation of human intelligence. It has been designed to obey certain simple commands, and then it has been provided with programs composed of those commands. Because of this, the computer has to obey those commands, but without any idea of what's happening Additional links for this entry: http://web.media.mit.edu/~minsky/papers/ComputersCantThink.txt http://www.cogsci.northwestern.edu/courses/cg207/readings/AIMag03-04-001.pdf http://cs.clarku.edu/~jbreecher/public/2005_Can_Computers_Think/Minsky-WhyPeopleThinkComputersCant.pdf http://aleph0.clarku.edu/~jbreecher/public/2005_Can_Computers_Think/Minsky-WhyPeopleThinkComputersCant.pdf http://act-r.psy.cmu.edu/~douglass/Douglass/Agents/TopicPapers/~TopCandidates~/State-of-AI/AIMag03-04-001.pdf Nanay, Bence (2006). Symmetry between the intentionality of minds and machines? The biological plausibility of Dennett's position. Minds and Machines 16 (1):57-71. ( Google | More links ) Abstract: One of the most influential arguments against the claim that computers can think is that while our intentionality is intrinsic, that of computers is derived: it is parasitic on the intentionality of the programmer who designed the computer-program. Daniel Dennett chose a surprising strategy for arguing against this asymmetry: instead of denying that the intentionality of computers is derived, he endeavours to argue that human intentionality is derived too. I intend to examine that biological plausibility of Dennett’s suggestion and show that Dennett’s argument for the claim that human intentionality is derived because it was designed by natural selection is based on the misunderstanding of how natural selection works Additional links for this entry: http://www.springerlink.com/content/a44653240255k3g2/fulltext.pdf http://www.springerlink.com/index/A44653240255K3G2.pdf Negley, Glenn (1951). Cybernetics and theories of mind. Journal of Philosophy 48 (September):574-82. ( Cited by 2 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2020606.pdf Pinsky, Leonard (1951). Do machines think about machines thinking? Mind 60 (July):397-398. ( Google | More links ) Additional links for this entry: http://mind.oxfordjournals.org/cgi/reprint/LX/239/397 http://www.jstor.org/stable/pdfplus/2251326.pdf Preston, Beth (1995). The ontological argument against the mind-machine hypothesis. Philosophical Studies 80 (2):131-57. ( Annotation | Google | More links ) Lucas, Searle, and Penrose all fall prey to "dual-description" fallacies. Additional links for this entry: http://www.springerlink.com/index/U36T1V40250183J7.pdf Proudfoot, Diane (2004). The implications of an externalist theory of rule-following behavior for robot cognition. Minds and Machines 14 (3):283-308. ( Google | More links ) Abstract:   Given (1) Wittgensteins externalist analysis of the distinction between following a rule and behaving in accordance with a rule, (2) prima facie connections between rule-following and psychological capacities, and (3) pragmatic issues about training, it follows that most, even all, future artificially intelligent computers and robots will not use language, possess concepts, or reason. This argument suggests that AIs traditional aim of building machines with minds, exemplified in current work on cognitive robotics, is in need of substantial revision Additional links for this entry: http://www.springerlink.com/content/v3gpx4k76m46705k/fulltext.pdf http://www.springerlink.com/index/V3GPX4K76M46705K.pdf http://www.ingentaconnect.com/content/klu/mind/2004/00000014/00000003/05268464 Puccetti, Roland (1966). Can humans think? Analysis 26 (June):198-202. ( Google ) Putnam, Hilary (1967). The mental life of some machines. In Hector-Neri Castaneda (ed.), Intentionality, Minds and Perception . Wayne State University Press. ( Cited by 37 | Annotation | Google ) On explaining behavior via TM states, e.g. explaining preference via utility functions. Logical behaviorism assumes rational preference functions. Functional organization is what matters, not physical make-up. Pylyshyn, Zenon W. (1975). Minds, machines and phenomenology: Some reflections on Dreyfus' What Computers Can't Do . Cognition 3:57-77. ( Cited by 7 | Google ) Rapaport, William J. (1993). Because mere calculating isn't thinking: Comments on Hauser's Why Isn't My Pocket Calculator a Thinking Thing? . Minds and Machines 3 (1):11-20. ( Cited by 5 | Google | More links ) Additional links for this entry: http://www.springerlink.com/index/L1VHLV44122W0014.pdf Rapaport, William J. (online). Computer processes and virtual persons: Comments on Cole's "artificial intelligence and personal identity". ( Cited by 7 | Google | More links ) Abstract: This is a draft of the written version of comments on a paper by David Cole, presented orally at the American Philosophical Association Central Division meeting in New Orleans, 27 April 1990. Following the written comments are 2 appendices: One contains a letter to Cole updating these comments. The other is the handout from the oral presentation Additional links for this entry: http://www.cse.buffalo.edu/tech-reports/90-13.ps http://www.cse.buffalo.edu/tech-reports/90-13.ps.Z http://www.cse.buffalo.edu/~rapaport/Papers/cole.tr.17my90.pdf http://www.cse.buffalo.edu/~rapaport/Papers/cole.tr.17my90.pdf http://historical.ncstrl.org/litesite-data/suny_buffalo_cs/90-13.ps.Z Ritchie, Graeme (2007). Some empirical criteria for attributing creativity to a computer program. Minds and Machines 17 (1). ( Google | More links ) Abstract: Over recent decades there has been a growing interest in the question of whether computer programs are capable of genuinely creative activity. Although this notion can be explored as a purely philosophical debate, an alternative perspective is to consider what aspects of the behaviour of a program might be noted or measured in order to arrive at an empirically supported judgement that creativity has occurred. We sketch out, in general abstract terms, what goes on when a potentially creative program is constructed and run, and list some of the relationships (for example, between input and output) which might contribute to a decision about creativity. Specifically, we list a number of criteria which might indicate interesting properties of a program’s behaviour, from the perspective of possible creativity. We go on to review some ways in which these criteria have been applied to actual implementations, and some possible improvements to this way of assessing creativity Additional links for this entry: http://www.springerlink.com/content/873382581451wk27/fulltext.pdf http://www.springerlink.com/index/873382581451WK27.pdf http://www.ingentaconnect.com/content/klu/mind/2007/00000017/00000001/00009066 Ronald, E. Sipper, Moshe (2001). Intelligence is not enough: On the socialization of talking machines. Minds and Machines 11 (4):567-576. ( Cited by 3 | Google | More links ) Abstract:   Since the introduction of the imitation game by Turing in 1950 there has been much debate as to its validity in ascertaining machine intelligence. We wish herein to consider a different issue altogether: granted that a computing machine passes the Turing Test, thereby earning the label of ``Turing Chatterbox'', would it then be of any use (to us humans)? From the examination of scenarios, we conclude that when machines begin to participate in social transactions, unresolved issues of trust and responsibility may well overshadow any raw reasoning ability they possess Additional links for this entry: http://books.google.com/books?hl=en=3A-d8vteZ5VxcLftqerbTWdpGLI http://books.google.com/books?hl=en=7-0Vb2LD-6m2dlj8qK05keaxOEU http://www.springerlink.com/content/l363406154q21317/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=338896=1 http://www.springerlink.com/index/L363406154Q21317.pdf http://www.ingentaconnect.com/content/klu/mind/2001/00000011/00000004/00338896 Baker, Lynne Rudder (1981). Why computers can't act. American Philosophical Quarterly 18 (April):157-163. ( Cited by 6 | Google ) Schmidt, C. T. A. (2005). Of robots and believing. Minds and Machines 15 (2):195-205. ( Cited by 6 | Google | More links ) Abstract: Discussion about the application of scientific knowledge in robotics in order to build people helpers is widespread. The issue herein addressed is philosophically poignant, that of robots that are “people”. It is currently popular to speak about robots and the image of Man. Behind this lurks the dialogical mind and the questions about the significance of an artificial version of it. Without intending to defend or refute the discourse in favour of ‘recreating’ Man, a lesser familiar question is brought forth: “and what if we were capable of creating a very convincible replica of man (constructing a robot-person), what would the consequences of this be and would we be satisfied with such technology?” Thorny topic; it questions the entire knowledge foundation upon which strong AI/Robotics is positioned. The author argues for improved monitoring of technological progress and thus favours implementing weaker techniques Additional links for this entry: http://www.springerlink.com/content/h605152238852292/fulltext.pdf http://www.springerlink.com/index/H605152238852292.pdf http://www.ingentaconnect.com/content/klu/mind/2005/00000015/00000002/00004734 Scriven, Michael (1960). The compleat robot: A prolegomena to androidology. In Sidney Hook (ed.), Dimensions of Mind . New York University Press. ( Cited by 6 | Annotation | Google ) A machine could possess every characteristic of human thought: e.g. freedom, creativity, learning, understanding, perceiving, feeling. Scriven, Michael (1963). The supercomputer as liar. British Journal for the Philosophy of Science 13 (February):313-314. ( Google | More links ) Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/citation/XIII/52/313 http://bjps.oxfordjournals.org/cgi/reprint/XIII/52/313 http://www.jstor.org/stable/pdfplus/685329.pdf Selinger, Evan (2008). Collins's incorrect depiction of Dreyfus's critique of artificial intelligence. Phenomenology and the Cognitive Sciences 7 (2). ( Google ) Abstract: Harry Collins interprets Hubert Dreyfus’s philosophy of embodiment as a criticism of all possible forms of artificial intelligence. I argue that this characterization is inaccurate and predicated upon a misunderstanding of the relevance of phenomenology for empirical scientific research Sloman, Aaron (1986). What sorts of machines can understand the symbols they use? Proceedings of the Aristotelian Society 61:61-80. ( Cited by 4 | Google ) Spilsbury, R. J. (1952). Mentality in machines. Proceedings of the Aristotelian Society 26:27-60. ( Cited by 2 | Google ) Spilsbury, R. J. (1952). Mentality in machines, part II. Proceedings of the Aristotelian Society 27:27-60. ( Google ) Srzednicki, Jan (1962). Could machines talk? Analysis 22 (April):113-117. ( Google ) Stahl, Bernd Carsten (2006). Responsible computers? A case for ascribing quasi-responsibility to computers independent of personhood or agency. Ethics and Information Technology 8 (4):205-213. ( Google | More links ) Abstract: There has been much debate whether computers can be responsible. This question is usually discussed in terms of personhood and personal characteristics, which a computer may or may not possess. If a computer fulfils the conditions required for agency or personhood, then it can be responsible; otherwise not. This paper suggests a different approach. An analysis of the concept of responsibility shows that it is a social construct of ascription which is only viable in certain social contexts and which serves particular social aims. If this is the main aspect of responsibility then the question whether computers can be responsible no longer hinges on the difficult problem of agency but on the possibly simpler question whether responsibility ascriptions to computers can fulfil social goals. The suggested solution to the question whether computers can be subjects of responsibility is the introduction of a new concept, called “quasi-responsibility” which will emphasise the social aim of responsibility ascription and which can be applied to computers Additional links for this entry: http://www.cse.dmu.ac.uk/~bstahl/publications/2006_responsible_computers_EIT.pdf http://www.springerlink.com/content/27607300gt85x46k/fulltext.pdf http://www.springerlink.com/index/27607300GT85X46K.pdf http://www.ingentaconnect.com/content/klu/etin/2006/00000008/00000004/00009112 Tallis, Raymond C. (2004). Why the Mind Is Not a Computer: A Pocket Lexicon of Neuromythology. Thorverton UK: Imprint Academic. ( Cited by 1 | Google | More links ) Abstract: Taking a series of key words such as calculation, language, information and memory, Professor Tallis shows how their misuse has lured a whole generation into... Additional links for this entry: http://imprint.co.uk/books/tallis.html Taube, M. (1961). Computers And Common Sense: The Myth Of Thinking Machines. Ny: Columbia University Press. ( Cited by 12 | Google ) Velleman, J. David (online). Artificial agency. ( Google | More links ) Abstract: I argue that participants in a virtual world such as "Second Life" exercise genuine agency via their avatars. Indeed, their avatars are fictional bodies with which they act in the virtual world, just as they act in the real world with their physical bodies. Hence their physical bodies can be regarded as their default avatars. I also discuss recent research into "believable" software agents, which are designed on principles borrowed from the character-based arts, especially cinematic animation as practiced by the artists at Disney and Warner Brothers Studios. I claim that these agents exemplify a kind of autonomy that should be of greater interest to philosophers than that exemplified by the generic agent modeled in current philosophical theory. The latter agent is autonomous by virtue of being governed by itself; but a believable agent appears to be governed by a self, which is the anima by which it appears to be animated. Putting these two discussions together, I suggest that philosophers of action should focus their attention on how we animate our bodies Additional links for this entry: http://phonline.org/paper.php?keynum=807 http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1006884 Wait, Eldon C. (2006). What computers could never do. In Analecta Husserliana: The Yearbook of Phenomenological Research, Volume XD:Artificial Intelligence;Experience;Premise;Searle, John R . Dordrecht: Springer. ( Google ) Waldrop, Mitchell (1990). Can computers think? In R. Kurzweil (ed.), The Age of Intelligent Machines . MIT Press. ( Cited by 2 | Google ) Wallace, Rodrick (ms). New mathematical foundations for AI and alife: Are the necessary conditions for animal consciousness sufficient for the design of intelligent machines? ( Google | More links ) Abstract: Rodney Brooks' call for 'new mathematics' to revitalize the disciplines of artificial intelligence and artificial life can be answered by adaptation of what Adams has called 'the informational turn in philosophy', aided by the novel perspectives that program gives regarding empirical studies of animal cognition and consciousness. Going backward from the necessary conditions communication theory imposes on animal cognition and consciousness to sufficient conditions for machine design is, however, an extraordinarily difficult engineering task. The most likely use of the first generations of conscious machines will be to model the various forms of psychopathology, since we have little or no understanding of how consciousness is stabilized in humans or other animals Additional links for this entry: http://philsci-archive.pitt.edu/archive/00002646/01/alife.pdf Weiss, Paul A. (1990). On the impossibility of artificial intelligence. Review of Metaphysics (December) 335 (December):335-341. ( Google ) Whiteley, C. H. (1956). Note on the concept of mind. Analysis 16 (January):68-70. ( Google ) Whobrey, Darren (2001). Machine mentality and the nature of the ground relation. Minds and Machines 11 (3):307-346. ( Cited by 7 | Google | More links ) Abstract:   John Searle distinguished between weak and strong artificial intelligence (AI). This essay discusses a third alternative, mild AI, according to which a machine may be capable of possessing a species of mentality. Using James Fetzer's conception of minds as semiotic systems, the possibility of what might be called ``mild AI'' receives consideration. Fetzer argues against strong AI by contending that digital machines lack the ground relationship required of semiotic systems. In this essay, the implementational nature of semiotic processes posited by Charles S. Peirce's triadic sign relation is re-examined in terms of the underlying dispositional processes and the ontological levels they would span in an inanimate machine. This suggests that, if non-human mentality can be replicated rather than merely simulated in a digital machine, the direction to pursue appears to be that of mild AI Additional links for this entry: http://www.mildai.org/papers/MAM2000-0-DJRW.pdf http://www.springerlink.com/content/tm143v51k2638028/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=323262=1 http://www.springerlink.com/index/TM143V51K2638028.pdf http://www.ingentaconnect.com/content/klu/mind/2001/00000011/00000003/00323262 Wilks, Yorick (1976). Dreyfus's disproofs. Britis Journal for the Philosophy of Science 27 (2). (Cited by 1 | Google | More links ) Additional links for this entry: http://bjps.oxfordjournals.org/cgi/reprint/27/2/177 http://www.jstor.org/stable/pdfplus/686169.pdf Wisdom, John O. (1952). Mentality in machines, part I. Proceedings of the Aristotelian Society 1:1-26. ( Google ) 6.2 Computation and Representation 6.2a Symbols and Symbol Systems Boyle, C. Franklin (2001). Transduction and degree of grounding. Psycoloquy 12 (36). (Cited by 2 | Google | More links ) Abstract: While I agree in general with Stevan Harnad's symbol grounding proposal, I do not believe "transduction" (or "analog process") PER SE is useful in distinguishing between what might best be described as different "degrees" of grounding and, hence, for determining whether a particular system might be capable of cognition. By 'degrees of grounding' I mean whether the effects of grounding go "all the way through" or not. Why is transduction limited in this regard? Because transduction is a physical process which does not speak to the issue of representation, and, therefore, does not explain HOW the informational aspects of signals impinging on sensory surfaces become embodied as symbols or HOW those symbols subsequently cause behavior, both of which, I believe, are important to grounding and to a system's cognitive capacity. Immunity to Searle's Chinese Room (CR) argument does not ensure that a particular system is cognitive, and whether or not a particular degree of groundedness enables a system to pass the Total Turing Test (TTT) may never be determined Additional links for this entry: http://cogsci.soton.ac.uk/~harnad/Temp/Think/boyle.htm http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad93.symb.anal.net.boyle.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad93.symb.anal.net.boyle.html Bringsjord, Selmer (online). People are infinitary symbol systems: No sensorimotor capacity necessary. ( Cited by 2 | Google | More links ) Abstract: Stevan Harnad and I seem to be thinking about many of the same issues. Sometimes we agree, sometimes we don't; but I always find his reasoning refreshing, his positions sensible, and the problems with which he's concerned to be of central importance to cognitive science. His "Grounding Symbols in the Analog World with Neural Nets" (= GS) is no exception. And GS not only exemplifies Harnad's virtues, it also provides a springboard for diving into Harnad- Bringsjord terrain Additional links for this entry: http://psycprints.ecs.soton.ac.uk/archive/00000167/ http://cogsci.soton.ac.uk/~harnad/Temp/Think/bringsj.htm http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad93.symb.anal.net.bringsjord.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad93.symb.anal.net.bringsjord.html http://psycprints.ecs.soton.ac.uk/archive/00000167/02/psyc.01.12.038.symbolism-connectionism.5.bringsjord http://psycprints.ecs.soton.ac.uk/archive/00000167/01/psyc.01.12.038.symbolism-connectionism.5.bringsjord.xml Clark, Andy (2006). Material symbols. Philosophical Psychology 19 (3):291-307. ( Cited by 4 | Google | More links ) Abstract: What is the relation between the material, conventional symbol structures that we encounter in the spoken and written word, and human thought? A common assumption, that structures a wide variety of otherwise competing views, is that the way in which these material, conventional symbol-structures do their work is by being translated into some kind of content-matching inner code. One alternative to this view is the tempting but thoroughly elusive idea that we somehow think in some natural language (such as English). In the present treatment I explore a third option, which I shall call the "complementarity" view of language. According to this third view the actual symbol structures of a given language add cognitive value by complementing (without being replicated by) the more basic modes of operation and representation endemic to the biological brain. The "cognitive bonus" that language brings is, on this model, not to be cashed out either via the ultimately mysterious notion of "thinking in a given natural language" or via some process of exhaustive translation into another inner code. Instead, we should try to think in terms of a kind of coordination dynamics in which the forms and structures of a language qua material symbol system play a key and irreducible role. Understanding language as a complementary cognitive resource is, I argue, an important part of the much larger project (sometimes glossed in terms of the "extended mind") of understanding human cognition as essentially and multiply hybrid: as involving a complex interplay between internal biological resources and external non-biological resources Additional links for this entry: http://www.psych.unito.it/csc/cogsci05/frame/talk/plen1-clark.pdf http://data.cstr.ed.ac.uk/internal/library/proceedings/2005/cogsci2005/docs/p1.pdf http://www.informaworld.com/smpp/./ftinterface~content=a747697830~fulltext=713240930 http://taylorandfrancis.metapress.com/index/XNRG5M77H65766ML.pdf http://www.informaworld.com/index/747697830.pdf http://www.ingentaconnect.com/content/routledg/cphp/2006/00000019/00000003/art00002 Cummins, Robert E. (1996). Why there is no symbol grounding problem? In Representations, Targets, and Attitudes . MIT Press. ( Google ) Harnad, Stevan (1992). Connecting object to symbol in modeling cognition. In A. Clark Ronald Lutz (eds.), Connectionism in Context . Springer-Verlag. ( Cited by 61 | Annotation | Google | More links ) On the limitations of symbol systems, and the potential for grounding symbols in sensory icons and categorical perception, e.g. with neural networks. Abstract: Connectionism and computationalism are currently vying for hegemony in cognitive modeling. At first glance the opposition seems incoherent, because connectionism is itself computational, but the form of computationalism that has been the prime candidate for encoding the "language of thought" has been symbolic computationalism (Dietrich 1990, Fodor 1975, Harnad 1990c; Newell 1980; Pylyshyn 1984), whereas connectionism is nonsymbolic (Fodor & Pylyshyn 1988, or, as some have hopefully dubbed it, "subsymbolic" Smolensky 1988). This paper will examine what is and is not a symbol system. A hybrid nonsymbolic/symbolic system will be sketched in which the meanings of the symbols are grounded bottom-up in the system's capacity to discriminate and identify the objects they refer to. Neural nets are one possible mechanism for learning the invariants in the analog sensory projection on which successful categorization is based. "Categorical perception" (Harnad 1987a), in which similarity space is "warped" in the service of categorization, turns out to be exhibited by both people and nets, and may mediate the constraints exerted by the analog world of objects on the formal world of symbols Additional links for this entry: http://eprints.ecs.soton.ac.uk/3374/ http://eprints.resist.ecs.soton.ac.uk/3374/ http://cogprints.ecs.soton.ac.uk/archive/00001583/ http://cogprints.soton.ac.uk/documents/disk0/00/00/15/83/ http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad92.symbol.object.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad92.symbol.object.html http://eprints.ecs.soton.ac.uk/archive/00003374/02/harnad92.symbol.object.html http://citebase.eprints.org/cgi-bin/citations?id=oai:eprints.ecs.soton.ac.uk:3374 http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1583 http://cogprints.org/1583/1/harnad92.symbol.object.html http://cogprints.org/1583/0/harnad92.symbol.object.html Harnad, Stevan (2002). Symbol grounding and the origin of language. In Matthias Scheutz (ed.), Computationalism: New Directions . MIT Press. ( Cited by 12 | Google | More links ) Abstract: What language allows us to do is to "steal" categories quickly and effortlessly through hearsay instead of having to earn them the hard way, through risky and time-consuming sensorimotor "toil" (trial-and-error learning, guided by corrective feedback from the consequences of miscategorisation). To make such linguistic "theft" possible, however, some, at least, of the denoting symbols of language must first be grounded in categories that have been earned through sensorimotor toil (or else in categories that have already been "prepared" for us through Darwinian theft by the genes of our ancestors); it cannot be linguistic theft all the way down. The symbols that denote categories must be grounded in the capacity to sort, label and interact with the proximal sensorimotor projections of their distal category-members in a way that coheres systematically with their semantic interpretations, both for individual symbols, and for symbols strung together to express truth-value-bearing propositions Additional links for this entry: http://eprints.resist.ecs.soton.ac.uk/7714/ http://eprints.resist.ecs.soton.ac.uk/6471/ http://bi.snu.ac.kr/Courses/4ai05s/Essay/2-06.pdf http://cogprints.ecs.soton.ac.uk/archive/00002133/ http://cogprints.soton.ac.uk/documents/disk0/00/00/21/33/ http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad02.symlang.htm http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad02.symlang.htm http://eprints.ecs.soton.ac.uk/archive/00007714/01/harnad02.symlang.htm http://eprints.ecs.soton.ac.uk/archive/00006471/01/harnad02.symlang.html http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:eprints.ecs.soton.ac.uk:7714 Harnad, Stevan (ms). Symbol grounding is an empirical problem: Neural nets are just a candidate component. ( Cited by 27 | Google | More links ) Abstract: "Symbol Grounding" is beginning to mean too many things to too many people. My own construal has always been simple: Cognition cannot be just computation, because computation is just the systematically interpretable manipulation of meaningless symbols, whereas the meanings of my thoughts don't depend on their interpretability or interpretation by someone else. On pain of infinite regress, then, symbol meanings must be grounded in something other than just their interpretability if they are to be candidates for what is going on in our heads. Neural nets may be one way to ground the names of concrete objects and events in the capacity to categorize them (by learning the invariants in their sensorimotor projections). These grounded elementary symbols could then be combined into symbol strings expressing propositions about more abstract categories. Grounding does not equal meaning, however, and does not solve any philosophical problems Additional links for this entry: http://eprints.resist.ecs.soton.ac.uk/3367/ http://eprints.ecs.soton.ac.uk/archive/00003367/ http://cogprints.org/1588/0/harnad93.cogsci.html http://cogprints.ecs.soton.ac.uk/archive/00001588/ http://cogprints.soton.ac.uk/documents/disk0/00/00/15/88/ http://eprints.ecs.soton.ac.uk/3367/01/harnad93.cogsci.html http://cogprints.ecs.soton.ac.uk/documents/disk0/00/00/15/88/ http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad93.cogsci.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad93.cogsci.html http://cogprints.ecs.soton.ac.uk/archive/00001588/00/harnad93.cogsci.html http://citebase.eprints.org/cgi-bin/citations?id=oai:eprints.ecs.soton.ac.uk:3367 http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1588 http://cogprints.org/1588/1/harnad93.cogsci.html Harnad, Stevan (1990). The symbol grounding problem. [Journal (Paginated)] 42:335-346. ( Cited by 1265 | Annotation | Google | More links ) AI symbols are empty and meaningless. They need to be "grounded" in something, e.g. sensory projection. Maybe connectionism can do the trick? Abstract: There has been much discussion recently about the scope and limits of purely symbolic models of the mind and about the proper role of connectionism in cognitive modeling. This paper describes the symbol grounding problem: How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) iconic representations, which are analogs of the proximal sensory projections of distal objects and events, and (2) categorical representations, which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) symbolic representations, grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., An X is a Y that is Z). Connectionism is one natural candidate for the mechanism that learns the invariant features underlying categorical representations, thereby connecting names to the proximal projections of the distal objects they stand for. In this way connectionism can be seen as a complementary component in a hybrid nonsymbolic/symbolic model of the mind, rather than a rival to purely symbolic modeling. Such a hybrid model would not have an autonomous symbolic module, however; the symbolic functions would emerge as an intrinsically dedicated symbol system as a consequence of the bottom-up grounding of categories' names in their sensory representations. Symbol manipulation would be governed not just by the arbitrary shapes of the symbol tokens, but by the nonarbitrary shapes of the icons and category invariants in which they are grounded Additional links for this entry: http://arxiv.org/abs/cs.AI/9906002 http://eprints.resist.ecs.soton.ac.uk/8175/ http://portal.acm.org/citation.cfm?id=96679 http://cogprints.org/3106/1/sgproblem1.html http://citeseer.ist.psu.edu/harnad90symbol.html http://eprints.ecs.soton.ac.uk/archive/00008175/ http://adsabs.harvard.edu/abs/1999cs........6002H http://www.ecs.soton.ac.uk/~harnad/Temp/symgro.htm http://cogprints.ecs.soton.ac.uk/archive/00003106/ http://cogprints.soton.ac.uk/documents/disk0/00/00/06/15/ http://cogprints.org/615/00/The_Symbol_Grounding_Problem.html http://www.isrl.uiuc.edu/~amag/langev/paper/harnad90theSymbol.html http://cogprints.ecs.soton.ac.uk/archive/00003106/01/sgproblem1.html http://cogsci.soton.ac.uk/~harnad/Papers/Harnad/harnad90.sgproblem.html http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad90.sgproblem.html http://wexler.free.fr/library/files/harnad (1990) the symbol grounding problem.pdf http://cogprints.ecs.soton.ac.uk/archive/00000615/00/The_Symbol_Grounding_Problem.html http://www.ecs.soton.ac.uk/%7Eharnad/Papers/Harnad/harnad90.sgproblem.html http://cogprints.org/615/1/The_Symbol_Grounding_Problem.html http://cogprints.org/615/0/The_Symbol_Grounding_Problem.html Kosslyn, Stephen M. Hatfield, Gary (1984). Representation without symbol systems. Social Research 51:1019-1045. ( Cited by 15 | Google ) Lumsden, David (2005). How can a symbol system come into being? Dialogue 44 (1):87-96. ( Google ) Abstract: One holistic thesis about symbols is that a symbol cannot exist singly, but only as apart of a symbol system. There is also the plausible view that symbol systems emerge gradually in an individual, in a group, and in a species. The problem is that symbol holism makes it hard to see how a symbol system can emerge gradually, at least if we are considering the emergence of a first symbol system. The only way it seems possible is if being a symbol can be a matter of degree, which is initially problematic. This article explains how being a cognitive symbol can be a matter of degree after all. The contrary intuition arises from the way a process of interpretation forces an all-or-nothing character on symbols, leaving room for underlying material to realize symbols to different degrees in a way that Daniel Dennett’s work can help illuminate. Holism applies to symbols as interpreted, while gradualism applies to how the underlying material realizes symbols.Selon une thèse holistique sur les symboles, un symbole ne peut exister isolément mais doit faire partie d’un systéme symbolique. Une opinion, elle aussi plausible, veut que les systèmes symboliques émergent graduellement chez un individu, un groupe ou une espèce. Le problème c’est qu’on voit mal, si le holisme des systèmes symboliques tient, comment un système symbolique peut émerger graduellement, du moins pour la première fois. Ce n’est possible, semble-t-il, que si être un symbole est affaire de degré, thèse au départ problématique. Cet article explique comment être un symbole cognitif peut après tout être affaire de degré. L’intuition contraire vient de ce que le processus d’interprétation nous force au tout ou rien, ce qui laisse un jeu dans la façon dont le matériel sous-jacent réalise les symboles à des degrés divers. Les travaux de Daniel Dennett sont à cet égard éclairants. Le holisme vaut pour les symboles tels qu’ils sont interprétés, tandis que le gradualisme vaut pour la façon dont le matériel sous-jacent réalise les symboles MacDorman, Karl F. (1997). How to ground symbols adaptively. In S. O'Nuillain, Paul McKevitt E. MacAogain (eds.), Two Sciences of Mind . John Benjamins. ( Cited by 1 | Google ) Newell, Allen Simon, Herbert A. (1981). Computer science as empirical inquiry: Symbols and search. Communications of the Association for Computing Machinery 19:113-26. ( Cited by 758 | Annotation | Google | More links ) On computer science, AI, & the Physical Symbol System Hypothesis. Additional links for this entry: http://portal.acm.org/citation.cfm?id=1283930 http://www.jdl.ac.cn/turing/pdf/p113-newell.pdf http://portal.acm.org/citation.cfm?id=216000.216007 http://home.dei.polimi.it/colombet/IA_2004/NewellandSimon.pdf http://www.elet.polimi.it/upload/colombet/IA_2004/NewellandSimon.pdf http://act-r.psy.cmu.edu/~douglass/Douglass/Agents/TopicPapers/PSSH/PSSH1.pdf http://act-r.psy.cmu.edu/people/douglass/Douglass/Agents/TopicPapers/PSSH/PSSH1.pdf http://books.google.com/books?hl=en=JepEkCn93darsdPpvcGEn7VG61E http://books.google.com/books?hl=en=5jb8G10Jz9vcHSA8SMjvAyDa79E http://books.google.com/books?hl=en=x8TqXd03sMF7Ku2tjTjKPLStp7M http://books.google.com/books?hl=en=G4uV6cXYV2x1beYCw6nX6T4sbdI http://books.google.com/books?hl=en=-ndDGHatiVA3bw4jwnF9OuPsYOE http://books.google.com/books?hl=en=bj8ATdZXgK2geid62HIRWyOdnuo http://books.google.com/books?hl=en=aHVuO1cvjfMeqsc-mx75yheObrA http://books.google.com/books?hl=en=cEaNBtdTsZioMpkk6W2Yd7_Ecf8 http://books.google.com/books?hl=en=TORphnvN5dYkzgzVFVGRZUaWdJc http://books.google.com/books?hl=en=NrAngS_FECgzGE2Iwqo6KD9OnDU http://books.google.com/books?hl=en=YUwqrPXzwnaBIYgYiu0RVv1K52I http://books.google.com/books?hl=en=0Avl1AKiXUbrSFEGnL7r0e8A8bM http://books.google.com/books?hl=en=43O3YB2BeYIwnOoSeJ-LE-V8lao http://books.google.com/books?hl=en=uTVh7cE69OpNBo9FxMib124gZ00 Newell, Allen (1980). Physical symbol systems. Cognitive Science 4:135-83. ( Cited by 469 | Google | More links ) Additional links for this entry: http://www.leaonline.com/doi/pdf/10.1207/s15516709cog0402_2 http://www.leaonline.com/doi/pdfplus/10.1207/s15516709cog0402_2 http://www.cogsci.rpi.edu/CSJarchive/1980v04/i02/p0135p0183/MAIN.PDF http://stinet.dtic.mil/oai/oai?verb=getRecord=ADA224247 Pinker, Steven (2004). Why nature & nurture won't go away. Daedalus . ( Cited by 7 | Google | More links ) Additional links for this entry: http://www.mitpressjournals.org/doi/abs/10.1162/0011526042365591 http://pinker.wjh.harvard.edu/articles/papers/nature_nurture.pdf http://pinker.wjh.harvard.edu/articles/papers/nature_nurture.pdf http://mitpress.mit.edu/catalog/item/default.asp?tid=17270=6 http://mitpress.mit.edu/catalog/item/default.asp?ttype=6=17270 http://www.mitpressjournals.org/doi/pdfplus/10.1162/0011526042365591 http://www.ingentaconnect.com/content/mitpress/dae/2004/00000133/00000004/art00001 Robinson, William S. (1995). Brain symbols and computationalist explanation. Minds and Machines 5 (1):25-44. ( Cited by 4 | Google | More links ) Abstract:   Computationalist theories of mind require brain symbols, that is, neural events that represent kinds or instances of kinds. Standard models of computation require multiple inscriptions of symbols with the same representational content. The satisfaction of two conditions makes it easy to see how this requirement is met in computers, but we have no reason to think that these conditions are satisfied in the brain. Thus, if we wish to give computationalist explanations of human cognition, without committing ourselvesa priori to a strong and unsupported claim in neuroscience, we must first either explain how we can provide multiple brain symbols with the same content, or explain how we can abandon standard models of computation. It is argued that both of these alternatives require us to explain the execution of complex tasks that have a cognition-like structure. Circularity or regress are thus threatened, unless noncomputationalist principles can provide the required explanations. But in the latter case, we do not know that noncomputationalist principles might not bear most of the weight of explaining cognition. Four possible types of computationalist theory are discussed; none appears to provide a promising solution to the problem. Thus, despite known difficulties in noncomputationalist investigations, we have every reason to pursue the search for noncomputationalist principles in cognitive theory Additional links for this entry: http://www.springerlink.com/index/L075451262856X7G.pdf Roitblat, Herbert L. (2001). Computational grounding. Psycoloquy 12 (58). (Cited by 1 | Google | More links ) Abstract: Harnad defines computation to mean the manipulation of physical symbol tokens on the basis of syntactic rules defined over the shapes of the symbols, independent of what, if anything, those symbols represent. He is, of course, free to define terms in any way that he chooses, and he is very clear about what he means by computation, but I am uncomfortable with this definition. It excludes, at least at a functional level of description, much of what a computer is actually used for, and much of what the brain/mind does. When I toss a Frisbee to the neighbor's dog, the dog does not, I think, engage in a symbolic soliloquy about the trajectory of the disc, the wind's effects on it, and formulas for including lift and the acceleration due to gravity. There are symbolic formulas for each of these relations, but the dog insofar as I can tell, does not use any of these formulas. Nevertheless, it computes these factors in order to intercept the disc in the air. I argue that determining the solution to a differential equation is at least as much computation as is processing symbols. The disagreement is over what counts as computation, I think that Harnad and I both agree that the dog solves the trajectory problem implicitly. This definition is important, because, although Harnad offers a technical definition for what he means by computation, the folk- definition of the term is probably interpreted differently, and I believe this leads to trouble Additional links for this entry: http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad93.symb.anal.net.roitblat.html Schneider, Susan (2009). Lot, ctm, and the elephant in the room. Synthese 170 (2):235-250. ( Google | More links ) Abstract: According to the language of thought (LOT) approach and the related computational theory of mind (CTM), thinking is the processing of symbols in an inner mental language that is distinct from any public language. Herein, I explore a deep problem at the heart of the LOT/CTM program—it has yet to provide a plausible conception of a mental symbol Additional links for this entry: http://www.sas.upenn.edu/~sls/SchneiderLOTSynthesesent.rtf http://www.sas.upenn.edu/~sls/documents/SchneiderSyntheseLOT.pdf http://www.springerlink.com/content/f9312g2m003630r0/fulltext.pdf Schneider, Susan (forthcoming). The nature of primitive symbols in the language of thought. Mind and Language . ( Google | More links ) Abstract: This paper provides a theory of the nature of symbols in the language of thought (LOT). My discussion consists in three parts. In part one, I provide three arguments for the individuation of primitive symbols in terms of total computational role. The first of these arguments claims that Classicism requires that primitive symbols be typed in this manner; no other theory of typing will suffice. The second argument contends that without this manner of symbol individuation, there will be computational processes that fail to supervene on syntax, together with the rules of composition and the computational algorithms. The third argument says that cognitive science needs a natural kind that is typed by total computational role. Otherwise, either cognitive science will be incomplete, or its laws will have counterexamples. Then, part two defends this view from a criticism, offered by both Jerry Fodor and Jesse Prinz, who respond to my view with the charge that because the types themselves are individuated Additional links for this entry: http://www.sas.upenn.edu/~sls/documents/SchneiderNatureSymbolsMindLang.doc http://www.sas.upenn.edu/~sls/documents/SchneiderNatureSymbolsMindLang-1-2.pdf http://www.sas.upenn.edu/~sls/documents/Schneider-MindLangSymbols.pdf Sun, Ron (2000). Symbol grounding: A new look at an old idea. Philosophical Psychology 13 (2):149-172. ( Cited by 39 | Google | More links ) Abstract: Symbols should be grounded, as has been argued before. But we insist that they should be grounded not only in subsymbolic activities, but also in the interaction between the agent and the world. The point is that concepts are not formed in isolation (from the world), in abstraction, or "objectively." They are formed in relation to the experience of agents, through their perceptual/motor apparatuses, in their world and linked to their goals and actions. This paper takes a detailed look at this relatively old issue, with a new perspective, aided by our work of computational cognitive model development. To further our understanding, we also go back in time to link up with earlier philosophical theories related to this issue. The result is an account that extends from computational mechanisms to philosophical abstractions Additional links for this entry: http://www.cogsci.rpi.edu/~rsun/sun.PP00.ps http://citeseer.ist.psu.edu/sun99symbol.html http://www.informaworld.com/smpp/./ftinterface~content=a713690432~fulltext=713240930 http://taylorandfrancis.metapress.com/index/KKLQAQTYVG4RUY7G.pdf http://www.informaworld.com/index/KKLQAQTYVG4RUY7G.pdf http://www.ingentaconnect.com/content/routledg/cphp/2000/00000013/00000002/art00001 http://www.informaworld.com/smpp/./ftinterface~db=all~content=a713690432~fulltext=713240930 Taddeo, Mariarosaria Floridi, Luciano (2008). A praxical solution of the symbol grounding problem. Minds and Machines . ( Google | More links ) Abstract: This article is the second step in our research into the Symbol Grounding Problem (SGP). In a previous work, we defined the main condition that must be satisfied by any strategy in order to provide a valid solution to the SGP, namely the zero semantic commitment condition (Z condition). We then showed that all the main strategies proposed so far fail to satisfy the Z condition, although they provide several important lessons to be followed by any new proposal. Here, we develop a new solution of the SGP. It is called praxical in order to stress the key role played by the interactions between the agents and their environment. It is based on a new theory of meaning—Action-based Semantics (AbS)—and on a new kind of artificial agents, called two-machine artificial agents (AM²). Thanks to their architecture, AM2s implement AbS, and this allows them to ground their symbols semantically and to develop some fairly advanced semantic abilities, including the development of semantically grounded communication and the elaboration of representations, while still respecting the Z condition Additional links for this entry: http://www.springerlink.com/content/f748ru5636784058/fulltext.pdf Thompson, Evan (1997). Symbol grounding: A bridge from artiﬁcial life to artiﬁcial intelligence. Brain and Cognition 34 (1):48-71. (Cited by 8 | Google | More links ) Abstract: This paper develops a bridge from AL issues about the symbol–matter relation to AI issues about symbol-grounding by focusing on the concepts of formality and syntactic interpretability. Using the DNA triplet-amino acid speciﬁcation relation as a paradigm, it is argued that syntactic properties can be grounded as high-level features of the non-syntactic interactions in a physical dynamical system. This argu- ment provides the basis for a rebuttal of John Searle’s recent assertion that syntax is observer-relative (1990, 1992). But the argument as developed also challenges the classic symbol-processing theory of mind against which Searle is arguing, as well as the strong AL thesis that life is realizable in a purely computational medium. Finally, it provides a new line of support for the autonomous systems approach in AL and AI (Varela & Bourgine 1992a, 1992b). © 1997 Academic Press Additional links for this entry: http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation http://www.ingentaconnect.com/content/ap/br/1997/00000034/00000001/art00906 6.2b Computational Semantics Akman, Varol (1998). Situations and artificial intelligence. Minds and Machines 8 (4):475-477. ( Google ) Blackburn, Patrick Bos, Johan (2003). Computational semantics. Theoria: Revista de Teoría, Historia y Fundamentos de la Ciencia 18 (1):27-45. ( Google ) Abstract: In this article we discuss what constitutes a good choice of semantic representation, compare different approaches of constructing semantic representations for fragments of natural language, and give an overview of recent methods for employing inference engines for natural language understanding tasks Blackburn, Patrick Kohlhase, Michael (2004). Inference and computational semantics. Journal of Logic, Language and Information 13 (2). ( Google ) Blackburn, Patrick (2005). Representation and Inference for Natural Language: A First Course in Computational Semantics. Center for the Study of Language and Information. ( Google ) Abstract: How can computers distinguish the coherent from the unintelligible, recognize new information in a sentence, or draw inferences from a natural language passage? Computational semantics is an exciting new field that seeks answers to these questions, and this volume is the first textbook wholly devoted to this growing subdiscipline. The book explains the underlying theoretical issues and fundamental techniques for computing semantic representations for fragments of natural language. This volume will be an essential text for computer scientists, linguists, and anyone interested in the development of computational semantics Bogdan, Radu J. (1994). By way of means and ends. In Radu J. Bogdan (ed.), Grounds for Cognition . Lawrence Erlbaum. ( Google ) Abstract: This chapter provides the teleological foundations for our analysis of guidance to goal. Its objective is to ground goal-directedness genetically. The basic suggestion is this. Organisms are small things, with few energy resources and puny physical means, battling a ruthless physical and biological nature. How do they manage to survive and multiply? CLEVERLY, BY ORGANIZING Bos, Johan (2004). Computational semantics in discourse: Underspecification, resolution, and inference. Journal of Logic, Language and Information 13 (2). ( Google ) Abstract: In this paper I introduce a formalism for natural language understandingbased on a computational implementation of Discourse RepresentationTheory. The formalism covers a wide variety of semantic phenomena(including scope and lexical ambiguities, anaphora and presupposition),is computationally attractive, and has a genuine inference component. Itcombines a well-established linguistic formalism (DRT) with advancedtechniques to deal with ambiguity (underspecification), and isinnovative in the use of first-order theorem proving techniques.The architecture of the formalism for natural language understandingthat I advocate consists of three levels of processing:underspecification, resolution, andinference. Each of these levels has a distinct function andtherefore employs a different kind of semantic representation. Themappings between these different representations define the interfacesbetween the levels Charniak, Eugene Wilks, Yorick (eds.) (1976). Computational Semantics: An Introduction to Artificial Intelligence and Natural Language Comprehension. Distributors for the U.S.A. And Canada, Elsevier/North Holland. ( Google ) Szymanik, Jakub Zajenkowski, Marcin (2009). Comprehension of Simple Quantifiers. Empirical Evaluation of a Computational Model. Cognitive Science: A Multidisciplinary Journal 34 (3):521-532. ( Google ) Abstract: We examine the verification of simple quantifiers in natural language from a computational model perspective. We refer to previous neuropsychological investigations of the same problem and suggest extending their experimental setting. Moreover, we give some direct empirical evidence linking computational complexity predictions with cognitive reality. In the empirical study we compare time needed for understanding different types of quantifiers. We show that the computational distinction between quantifiers recognized by finite-automata and push-down automata is psychologically relevant. Our research improves upon hypothesis and explanatory power of recent neuroimaging studies as well as provides evidence Dennett, Daniel C. (2003). The Baldwin Effect: A Crane, Not a Skyhook. In Bruce H. Weber D.J. Depew (eds.), And Learning: The Baldwin Effect Reconsidered . MIT Press. ( Cited by 6 | Google | More links ) Abstract: In 1991, I included a brief discussion of the Baldwin effect in my account of the evolution of human consciousness, thinking I was introducing to non-specialist readers a little-appreciated, but no longer controversial, wrinkle in orthodox neo-Darwinism. I had thought that Hinton and Nowlan (1987) and Maynard Smith (1987) had shown clearly and succinctly how and why it worked, and restored the neglected concept to grace. Here is how I put it then Additional links for this entry: http://ase.tufts.edu/cogstud/papers/baldwincranefin.htm http://books.google.com/books?hl=en=urmY-aQvjQsgpb_W81dI6SjJvGc Fodor, Jerry A. (1979). In reply to Philip Johnson-Laird's What's Wrong with Grandma's Guide to Procedural Semantics: A Reply to Jerry Fodor . Cognition 7 (March):93-95. ( Google ) Fodor, Jerry A. (1978). Tom swift and his procedural grandmother. Cognition 6 (September):229-47. ( Cited by 24 | Annotation | Google ) Against procedural semantics; it's a rerun of verificationism. Hadley, Robert F. (1990). Truth conditions and procedural semantics. In Philip P. Hanson (ed.), Information, Language and Cognition . University of British Columbia Press. ( Cited by 2 | Google ) Harnad, Stevan (2002). Darwin, Skinner, Turing and the mind. Magyar Pszichologiai Szemle 57 (4):521-528. ( Google | More links ) Abstract: Darwin differs from Newton and Einstein in that his ideas do not require a complicated or deep mind to understand them, and perhaps did not even require such a mind in order to generate them in the first place. It can be explained to any school-child (as Newtonian mechanics and Einsteinian relativity cannot) that living creatures are just Darwinian survival/reproduction machines. They have whatever structure they have through a combination of chance and its consequences: Chance causes changes in the genetic blueprint from which organisms' bodies are built, and if those changes are more successful in helping their owners survive and reproduce than their predecessors or their rivals, then, by definition, those changes are reproduced, and thereby become more prevalent in succeeding generations: Whatever survives/reproduces better survives/reproduces better. That is the tautological force that shaped us Additional links for this entry: http://cogprints.org/3016/01/darwin.htm http://eprints.resist.ecs.soton.ac.uk/7715/ http://eprints.ecs.soton.ac.uk/7715/01/darwin.htm http://cogprints.ecs.soton.ac.uk/archive/00003016/ http://www.ecs.soton.ac.uk/~harnad/Temp/darwin.htm http://eprints.ecs.soton.ac.uk/7715/01/darwin.htm http://www.ecs.soton.ac.uk/~harnad/Temp/darwin.htm http://cogprints.ecs.soton.ac.uk/archive/00003016/ http://citebase.eprints.org/cgi-bin/citations?id=oai:cogprints.soton.ac.uk:3016 http://citebase.eprints.org/cgi-bin/citations?id=oai:cogprints.soton.ac.uk:3016 http://citebase.eprints.org/cgi-bin/citations?id=oai:eprints.ecs.soton.ac.uk:7715 Johnson-Laird, Philip N. (1977). Procedural semantics. Cognition 5:189-214. ( Cited by 37 | Google ) Johnson-Laird, Philip N. (1978). What's wrong with grandma's guide to procedural semantics: A reply to Jerry Fodor. Cognition 9 (September):249-61. ( Cited by 1 | Google ) McDermott, Drew (1978). Tarskian semantics, or no notation without denotation. Cognitive Science 2:277-82. ( Cited by 33 | Annotation | Google | More links ) On the virtues of denotational semantics for AI. Notation without denotation, as found in many AI systems, leads to castles in the air. Additional links for this entry: http://www.leaonline.com/doi/abs/10.1207/s15516709cog0203_5 http://www.leaonline.com/doi/pdf/10.1207/s15516709cog0203_5 http://www.leaonline.com/doi/pdfplus/10.1207/s15516709cog0203_5 Papineau, David (2006). The cultural origins of cognitive adaptations. Royal Institute of Philosophy Supplement . ( Google | More links ) Abstract: According to an influential view in contemporary cognitive science, many human cognitive capacities are innate. The primary support for this view comes from ‘poverty of stimulus’ arguments. In general outline, such arguments contrast the meagre informational input to cognitive development with its rich informational output. Consider the ease with which humans acquire languages, become facile at attributing psychological states (‘folk psychology’), gain knowledge of biological kinds (‘folk biology’), or come to understand basic physical processes (‘folk physics’). In all these cases, the evidence available to a growing child is far too thin and noisy for it to be plausible that the underlying principles involved are derived from general learning mechanisms. This only alternative hypothesis seems to be that the child’s grasp of these principles is innate. (Cf. Laurence and Margolis, 2001.) Additional links for this entry: http://journals.cambridge.org/abstract_S1358246105056134 Perlis, Donald R. (1991). Putting one's foot in one's head -- part 1: Why. Noûs 25 (September):435-55. ( Cited by 12 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2216073.pdf Perlis, Donald R. (1994). Putting one's foot in one's head -- part 2: How. In Eric Dietrich (ed.), Thinking Computers and Virtual Persons . Academic Press. ( Google ) Rapaport, William J. (1988). Syntactic semantics: Foundations of computational natural language understanding. In James H. Fetzer (ed.), Aspects of AI . Kluwer. ( Cited by 44 | Google ) Rapaport, William J. (1995). Understanding understanding: Syntactic semantics and computational cognition. Philosophical Perspectives 9:49-88. ( Cited by 22 | Google | More links ) Additional links for this entry: http://www.cse.buffalo.edu/~rapaport/Papers/understanding.ps http://historical.ncstrl.org/litesite-data/suny_buffalo_cs/94-28.ps.Z http://www.jstor.org/stable/pdfplus/2214212.pdf Smith, B. (1988). On the semantics of clocks. In James H. Fetzer (ed.), Aspects of AI . Kluwer. ( Cited by 7 | Google ) Smith, B. (1987). The correspondence continuum. Csli 87. ( Cited by 34 | Google ) Szymanik, Jakub Zajenkowski, Marcin (2009). Understanding Quantifiers in Language. In N. A. Taatgen H. van Rijn (eds.), Proceedings of the 31st Annual Conference of the Cognitive Science Society . ( Google ) Tin, Erkan Akman, Varol (1994). Computational situation theory. ACM SIGART Bulletin 5 (4):4-17. (Cited by 15 | Google | More links ) Abstract: Situation theory has been developed over the last decade and various versions of the theory have been applied to a number of linguistic issues. However, not much work has been done in regard to its computational aspects. In this paper, we review the existing approaches towards `computational situation theory' with considerable emphasis on our own research Additional links for this entry: http://citeseer.ist.psu.edu/599996.html http://cogprints.ecs.soton.ac.uk/archive/00000200/ http://portal.acm.org/citation.cfm?id=191604.191608 http://citeseer.ist.psu.edu/tin94computational.html http://www.cs.bilkent.edu.tr/~akman/jour-papers/sigart/sigart1994.pdf http://citebase.eprints.org/cgi-bin/citations?id=oai:cogprints.soton.ac.uk:200 http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:200 http://cogprints.org/200/2/sig.ps http://cogprints.org/200/0/sig.ps Wilks, Y. (1990). Form and content in semantics. Synthese 82 (3):329-51. ( Cited by 10 | Annotation | Google | More links ) Criticism of McDermott's views on semantics, logic and natural language. Abstract:   This paper continues a strain of intellectual complaint against the presumptions of certain kinds of formal semantics (the qualification is important) and their bad effects on those areas of artificial intelligence concerned with machine understanding of human language. After some discussion of the use of the term epistemology in artificial intelligence, the paper takes as a case study the various positions held by McDermott on these issues and concludes, reluctantly, that, although he has reversed himself on the issue, there was no time at which he was right Additional links for this entry: http://www.springerlink.com/index/N51T8HT6V72G63M4.pdf Wilks, Y. (1982). Some thoughts on procedural semantics. In W. Lehnert (ed.), Strategies for Natural Language Processing . Lawrence Erlbaum. ( Cited by 12 | Google ) Winograd, Terry (1985). Moving the semantic fulcrum. Linguistics and Philosophy 8 (February):91-104. ( Cited by 16 | Google | More links ) Additional links for this entry: http://www.springerlink.com/index/N7W873341553W800.pdf Woods, W. (1986). Problems in procedural semantics. In Zenon W. Pylyshyn W. Demopolous (eds.), Meaning and Cognitive Structure . Ablex. ( Cited by 2 | Annotation | Google ) With commentaries by Haugeland, J. D. Fodor. Woods, W. (1981). Procedural semantics as a theory of meaning. In A. Joshi, Bruce H. Weber Ivan A. Sag (eds.), Elements of Discourse Understanding . Cambridge University Press. ( Cited by 33 | Google ) 6.2c Implicit/Explicit Rules and Representations Bechtel, William P. (forthcoming). Explanation: Mechanism, modularity, and situated cognition. In P. Robbins M. Aydede (eds.), Cambridge Handbook of Situated Cognition . Cambridge University Press. ( Google ) Abstract: The situated cognition movement has emerged in recent decades (although it has roots in psychologists working earlier in the 20 th century including Vygotsky, Bartlett, and Dewey) largely in reaction to an approach to explaining cognition that tended to ignore the context in which cognitive activities typically occur. Fodor’s (1980) account of the research strategy of methodological solipsism, according to which only representational states within the mind are viewed as playing causal roles in producing cognitive activity, is an extreme characterization of this approach. (As Keith Gunderson memorably commented when Fodor first presented this characterization, it amounts to reversing behaviorism by construing the mind as a white box in a black world). Critics as far back as the 1970s and 1980s objected to many experimental paradigms in cognitive psychology as not being ecologically valid; that is, they maintained that the findings only applied to the artificial circumstances created in the laboratory and did not generalize to real world settings (Neisser, 1976; 1987). The situated cognition movement, however, goes much further than demanding ecologically valid experiments—it insists that an agent’s cognitive activities are inherently embedded and supported by dynamic interactions with the agent’s body and features of its environment Clark, Andy (1991). In defense of explicit rules. In William Ramsey, Stephen P. Stich D. Rumelhart (eds.), Philosophy and Connectionist Theory . Lawrence Erlbaum. ( Cited by 11 | Annotation | Google ) Argues that we need explicit rules for flexibility, adaptibility, and representational redescription. With remarks on eliminativism. Cummins, Robert E. (1986). Inexplicit information. In Myles Brand Robert M. Harnish (eds.), The Representation of Knowledge and Belief . University of Arizona Press. ( Cited by 13 | Annotation | Google ) On various kinds of representation of knowledge or belief without explicit tokens: control-implicit, domain-implicit, and procedural information. The key distinction is representation vs. execution of a rule. Davies, Martin (1995). Two notions of implicit rules. Philosophical Perspectives 9:153-83. ( Cited by 14 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2214216.pdf Dennett, Daniel C. (1993). Review of F. Varela, E. Thompson and E. Rosch, The Embodied Mind . American Journal of Psychology 106:121-126. ( Google | More links ) Abstract: Cognitive science, as an interdisciplinary school of thought, may have recently moved beyond the bandwagon stage onto the throne of orthodoxy, but it does not make a favorable first impression on many people. Familiar reactions on first encounters range from revulsion to condescending dismissal--very few faces in the crowd light up with the sense of "Aha! So that's how the mind works! Of course!" Cognitive science leaves something out, it seems; moreover, what it apparently leaves out is important, even precious. Boiled down to its essence, cognitive science proclaims that in one way or another our minds are computers, and this seems so mechanistic, reductionistic, intellectualistic, dry, philistine, unbiological. It leaves out emotion, or what philosophers call qualia, or value, or mattering, or . . . the soul. It doesn't explain what minds are so much as attempt to explain minds away Additional links for this entry: http://ase.tufts.edu/cogstud/papers/varela.htm http://cogprints.ecs.soton.ac.uk/archive/00000273/ http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:273 Fulda, Joseph S. (2000). The logic of “improper cross”. Artificial Intelligence and Law 8 (4):337-341. ( Google ) G. , Nagarjuna (2009). Collaborative creation of teaching-learning sequences and an Atlas of knowledge. Mathematics Teaching-Research Journal Online 3 (N3):23-40. ( Google | More links ) Abstract: Our focus in the article is to introduce a simple methodology of generating teaching-learning sequences using the semantic network techinque, followed by the emergent properties of such a network and their implications for the teaching-learning process (didactics) with marginal notes on epistemological implications. A collaborative portal for teachers, which publishes a network of prerequisites for teaching/learning any concept or an activity is introduced. The article ends with an appeal to the global community to contribute prerequisites of any subject to complete the global roadmap for an altas being built on similar lines as Wikipedia. The portal is launched and waiting for community participation at http://www.gnowledge.org. Additional links for this entry: http://cogprints.org/6588/1/collaborative-LTS-wpversion-preprint.pdf http://cogprints.org/6588/1/collaborative%2DLTS%2Dwpversion%2Dpreprint.pdf Hadley, Robert F. (1993). Connectionism, explicit rules, and symbolic manipulation. Minds and Machines 3 (2):183-200. ( Cited by 13 | Google | More links ) Abstract:   At present, the prevailing Connectionist methodology forrepresenting rules is toimplicitly embody rules in neurally-wired networks. That is, the methodology adopts the stance that rules must either be hard-wired or trained into neural structures, rather than represented via explicit symbolic structures. Even recent attempts to implementproduction systems within connectionist networks have assumed that condition-action rules (or rule schema) are to be embodied in thestructure of individual networks. Such networks must be grown or trained over a significant span of time. However, arguments are presented herein that humanssometimes follow rules which arevery rapidly assignedexplicit internal representations, and that humans possessgeneral mechanisms capable of interpreting and following such rules. In particular, arguments are presented that thespeed with which humans are able to follow rules ofnovel structure demonstrates the existence of general-purpose rule following mechanisms. It is further argued that the existence of general-purpose rule following mechanisms strongly indicates that explicit rule following is not anisolated phenomenon, but may well be a common and important aspect of cognition. The relationship of the foregoing conclusions to Smolensky''s view of explicit rule following is also explored. The arguments presented here are pragmatic in nature, and are contrasted with thekind of arguments developed by Fodor and Pylyshyn in their recent, influential paper Additional links for this entry: http://www.springerlink.com/index/P6445N4854J7T407.pdf Hadley, Robert F. (1990). Connectionism, rule-following, and symbolic manipulation. Proc AAAI 3 (2):183-200. ( Cited by 10 | Annotation | Google ) Some rules are learnt so quickly that representation must be explicit. Hadley, Robert F. (1995). The 'explicit-implicit' distinction. Minds and Machines 5 (2):219-42. ( Cited by 25 | Google | More links ) Abstract:   Much of traditional AI exemplifies the explicit representation paradigm, and during the late 1980''s a heated debate arose between the classical and connectionist camps as to whether beliefs and rules receive an explicit or implicit representation in human cognition. In a recent paper, Kirsh (1990) questions the coherence of the fundamental distinction underlying this debate. He argues that our basic intuitions concerning explicit and implicit representations are not only confused but inconsistent. Ultimately, Kirsh proposes a new formulation of the distinction, based upon the criterion ofconstant time processing.The present paper examines Kirsh''s claims. It is argued that Kirsh fails to demonstrate that our usage of explicit and implicit is seriously confused or inconsistent. Furthermore, it is argued that Kirsh''s new formulation of the explicit-implicit distinction is excessively stringent, in that it banishes virtually all sentences of natural language from the realm of explicit representation. By contrast, the present paper proposes definitions for explicit and implicit which preserve most of our strong intuitions concerning straightforward uses of these terms. It is also argued that the distinction delineated here sustains the meaningfulness of the abovementioned debate between classicists and connectionists Additional links for this entry: http://www.springerlink.com/content/content/kl736k107182317h/fulltext.pdf http://www.springerlink.com/content/kl736k107182317h/fulltext.pdf http://www.springerlink.com/index/KL736K107182317H.pdf Kirsh, David (1990). When is information explicitly represented? In Philip P. Hanson (ed.), Information, Language and Cognition . University of British Columbia Press. ( Cited by 62 | Google ) Martínez, Fernando Ezquerro Martínez, Jesús (1998). Explicitness with psychological ground. Minds and Machines 8 (3):353-374. ( Cited by 1 | Google | More links ) Abstract:   Explicitness has usually been approached from two points of view, labelled by Kirsh the structural and the process view, that hold opposite assumptions to determine when information is explicit. In this paper, we offer an intermediate view that retains intuitions from both of them. We establish three conditions for explicit information that preserve a structural requirement, and a notion of explicitness as a continuous dimension. A problem with the former accounts was their disconnection with psychological work on the issue. We review studies by Karmiloff-Smith, and Shanks and St. John to show that the proposed conditions have psychological grounds. Finally, we examine the problem of explicit rules in connectionist systems in the light of our framework Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=381463CI http://www.springerlink.com/content/j4u0011h11660267/fulltext.pdf http://www.springerlink.com/index/J4U0011H11660267.pdf http://www.ingentaconnect.com/content/klu/mind/1998/00000008/00000003/00160303 Shapiro, Lawrence A. (ms). The embodied cognition research program. ( Cited by 1 | Google | More links ) Abstract: Unifying traditional cognitive science is the idea that thinking is a process of symbol manipulation, where symbols lead both a syntactic and a semantic life. The syntax of a symbol comprises those properties in virtue of which the symbol undergoes rule-dictated transformations. The semantics of a symbol constitute the symbolsÕ meaning or representational content. Thought consists in the syntactically determined manipulation of symbols, but in a way that respects their semantics. Thus, for instance, a calculating computer sensitive only to the shape of symbols might produce the symbol Ô5Õ in response to the inputs Ô2Õ, Ô+Õ, and Ô3Õ. As far as the computer is concerned, these symbols have no meaning, but because of its program it will produce outputs that, to the user, Òmake senseÓ given the meanings the user attributes to the symbols Additional links for this entry: http://philosophy.wisc.edu/shapiro/HomePage/PhilComp.htm http://philosophy.wisc.edu/shapiro/HomePage/embodiedcognition.pdf Skokowski, Paul G. (1994). Can computers carry content "inexplicitly"? Minds and Machines 4 (3):333-44. ( Cited by 2 | Annotation | Google | More links ) Cummins' account of inexplicit information fails, as even "executed" rules must be represented in the system. With remarks on the Chinese room. Abstract:   I examine whether it is possible for content relevant to a computer''s behavior to be carried without an explicit internal representation. I consider three approaches. First, an example of a chess playing computer carrying emergent content is offered from Dennett. Next I examine Cummins response to this example. Cummins says Dennett''s computer executes a rule which is inexplicitly represented. Cummins describes a process wherein a computer interprets explicit rules in its program, implements them to form a chess-playing device, then this device executes the rules in a way that exhibits them inexplicitly. Though this approach is intriguing, I argue that the chess-playing device cannot exist as imagined. The processes of interpretation and implementation produce explicit representations of the content claimed to be inexplicit. Finally, the Chinese Room argument is examined and shown not to save the notion of inexplicit information. This means the strategy of attributing inexplicit content to a computer which is executing a rule, fails Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=0148099CI http://www.springerlink.com/content/j821320368ht5863/fulltext.pdf http://www.springerlink.com/index/J821320368HT5863.pdf Slezak, Peter (1999). Situated cognition. Perspectives on Cognitive Science . ( Cited by 22 | Google ) Abstract: The self-advertising, at least, suggests that 'situated cognition' involves the most fundamental conceptual re-organization in AI and cognitive science, even appearing to deny that cognition is to be explained by mental representations. In their defence of the orthodox symbolic representational theory, A. Vera and H. Simon (1993) have rebutted many of these claims, but they overlook an important reading of situated arguments which may, after all, involve a revolutionary insight. I show that the whole debate turns on puzzles familiar from the history of philosophy and psychology and these may serve to clarify the current disputes Sutton, John (2000). The body and the brain. In S. Gaukroger, J. Schuster J. Sutton (eds.), Descartes' Natural Philosophy . Routledge. ( Google ) Abstract: Does self?knowledge help? A rationalist, presumably, thinks that it does: both that self?knowledge is possible and that, if gained through appropriate channels, it is desirable. Descartes notoriously claimed that, with appropriate methods of enquiry, each of his readers could become an expert on herself or himself. As well as the direct, first?person knowledge of self to which we are led in the Meditationes , we can also seek knowledge of our own bodies, and of the union of our minds and our bodies: the latter forms of self?knowledge are inevitably imperfect, but are no less important in guiding our conduct in the search after truth van Gelder, Tim (1998). Review: Being There: Body and World Together Again , by Andy Clark. Philosophical Review 107 (4):647-650. ( Google ) Abstract: Are any nonhuman animals rational? What issues are we raising when we ask this question? Are there different kinds or levels of rationality, some of which fall short of full human rationality? Should any behaviour by nonhuman animals be regarded as rational? What kinds of tasks can animals successfully perform? What kinds of processes control their performance at these tasks, and do they count as rational processes? Is it useful or theoretically justified to raise questions about the rationality of animals at all? Should we be interested in whether they are rational? Why does it matter? 6.2d AI without Representation? Andrews, Kristin (web). Critter psychology: On the possibility of nonhuman animal folk psychology. In Daniel D. Hutto Matthew Ratcliffe (eds.), Folk Psychology Re-Assessed . Kluwer/Springer Press. ( Google | More links ) Abstract: Humans have a folk psychology, without question. Paul Churchland used the term to describe “our commonsense conception of psychological phenomena” (Churchland 1981, p. 67), whatever that may be. When we ask the question whether animals have their own folk psychology, we’re asking whether any other species has a commonsense conception of psychological phenomenon as well. Different versions of this question have been discussed over the past 25 years, but no clear answer has emerged. Perhaps one reason for this lack of progress is that we don’t clearly understand the question. In asking whether animals have folk psychology, I hope to help clarify the concept of folk psychology itself, and in the process, to gain a greater understanding of the role of belief and desire attribution in human social interaction Additional links for this entry: http://www.springerlink.com/index/w2506611815711g1.pdf Bechtel, William P. (1996). Yet another revolution? Defusing the dynamical system theorists' attack on mental representations. Presidential Address to Society of Philosophy and Psychology . ( Cited by 1 | Google ) Brooks, Rodney (1991). Intelligence without representation. Artificial Intelligence 47:139-159. ( Cited by 2501 | Annotation | Google | More links ) We don't need explicit representation; the world can do the job instead. Use embodied, complete systems, starting simple and working incrementally. Abstract: Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporateeverything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environments Additional links for this entry: http://people.csail.mit.edu/brooks/papers/representation.pdf http://people.csail.mit.edu/~brooks/papers/representation.pdf http://www.ai.mit.edu/people/brooks/papers/representation.pdf http://www.cs.iastate.edu/~honavar/Courses/cs673/spring00/brooks2.pdf http://www.valentiniweb.com/Piermo/robotica/doc/Brooks/representation.pdf http://www.scs.ryerson.ca/~aferworn/courses/CPS607/CLASSES/subsumption.pdf http://hugo.csie.ntu.edu.tw/~yjhsu/courses/u1760/Online/papers/brooks2.pdf http://agents.csie.ntu.edu.tw/~yjhsu/courses/u1760/Online/papers/brooks2.pdf http://www.ee.pdx.edu/~mperkows/ML_LAB/Giant_Hexapod/brooks.representation.pdf http://www.inf.ufrgs.br/~alvares/CMP124SMA/IntelligenceWithoutRepresentation.pdf http://courses.media.mit.edu/2004spring/mas966/Brooks Int wo rep (orig version).pdf http://www.csa.com/partners/viewrecord.php?requester=gs=2506361CI http://act-r.psy.cmu.edu/~douglass/Douglass/Agents/TopicPapers/~TopCandidates~/Robotics/brooks87intelligence.pdf http://books.google.com/books?hl=en=605f19sD3y7t4DoispNMxvmyy3Y http://books.google.com/books?hl=en=DAI5FtzBm8byWUcnf5U3qBR0ihk http://books.google.com/books?hl=en=HHybOULxfTdFwfwaliV_tWAF-rY http://books.google.com/books?hl=en=qiqJVUCetwl0cyAg5v_FQkGQePw http://books.google.com/books?hl=en=CcPsaAISyDSP5mOKB52AxYOWprI http://books.google.com/books?hl=en=kaOJuTY3bDJqBoxZyS0StZghsM4 http://books.google.com/books?hl=en=ZOLRXpLrB5DFvnDlSf6wNaw3gYg http://books.google.com/books?hl=en=wE7wq6m3Inq7Z4qwmXLucc7RI00 http://books.google.com/books?hl=en=LYLowQNnoYxvhtwzFNt0X9gEQ4Y http://books.google.com/books?hl=en=eFSDUHWbsnHEh9S4Zn_paKguMWc http://books.google.com/books?hl=en=lN7SP76JP7P7VOlJEKQ4NEl3MkU http://books.google.com/books?hl=en=WBYA3src9nEuAgdYTkk4CjPHrqQ http://books.google.com/books?hl=en=gzFoWrIH5zSH1JxwjugtFKvBPxs Clark, Andy Toribio, Josefa (1994). Doing without representing. Synthese 101 (3):401-31. ( Cited by 97 | Annotation | Google | More links ) A discussion of anti-representationalism in situated robotics and the dynamic systems movement (Brooks, Beer, van Gelder). These arguments appeal to overly simple domains, and a modest notion of representation survives. Abstract:   Connectionism and classicism, it generally appears, have at least this much in common: both place some notion of internal representation at the heart of a scientific study of mind. In recent years, however, a much more radical view has gained increasing popularity. This view calls into question the commitment to internal representation itself. More strikingly still, this new wave of anti-representationalism is rooted not in armchair theorizing but in practical attempts to model and understand intelligent, adaptive behavior. In this paper we first present, and then critically assess, a variety of recent anti-representationalist treatments. We suggest that so far, at least, the sceptical rhetoric outpaces both evidence and argument. Some probable causes of this premature scepticism are isolated. Nonetheless, the anti-representationalist challenge is shown to be both important and progressive insofar as it forces us to see beyond the bare representational/non-representational dichotomy and to recognize instead a rich continuum of degrees and types of representationality Additional links for this entry: http://wexler.free.fr/library/files/clark (1994) doing without representing.pdf http://www.springerlink.com/content/w02t5nm8j7164v10/fulltext.pdf http://www.springerlink.com/index/W02T5NM8J7164V10.pdf Dennett, Daniel C. (1989). Cognitive ethology. In Goals, No-Goals and Own Goals . Unwin Hyman. ( Cited by 15 | Google ) Abstract: The field of Artificial Intelligence has produced so many new concepts--or at least vivid and more structured versions of old concepts--that it would be surprising if none of them turned out to be of value to students of animal behavior. Which will be most valuable? I will resist the temptation to engage in either prophecy or salesmanship; instead of attempting to answer the question: "How might Artificial Intelligence inform the study of animal behavior?" I will concentrate on the obverse: "How might the study of animal behavior inform research in Artificial Intelligence?" Millikan, Ruth G. (online). On reading signs. (Cited by 1 | Google | More links ) Abstract: On Reading Signs; Some Differences between Us and The Others If there are certain kinds of signs that an animal cannot learn to interpret, that might be for any of a number of reasons. It might be, first, because the animal cannot discriminate the signs from one another. For example, although human babies learn to discriminate human speech sounds according to the phonological structures of their native languages very easily, it may be that few if any other animals are capable of fully grasping the phonological structures of human languages. If an animal cannot learn to interpret certain signs it might be, second, because the decoding is too difficult for it. It could be, for example, that some animals are incapable of decoding signs that exhibit syntactic embedding, or signs that are spread out over time as opposed to over space. Problems of these various kinds might be solved by using another sign system, say, gestures rather than noises, or visual icons laid out in spatial order, or by separating out embedded propositions and presenting each separately. But a more interesting reason that an animal might be incapable of understanding a sign would be that it lacked mental representations of the necessary kind. It might be incapable of representing mentally what the sign conveys. When discussing what signs animals can understand or Additional links for this entry: http://www.ucc.uconn.edu/~wwwphil/lorenz.pdf http://www.philosophy.uconn.edu/department/millikan/lorenz.pdf http://www.philosophy.uconn.edu/department/millikan/lorenz.pdf http://books.google.com/books?hl=en=uhILZgZBxuWhg03CdjzvarNZ8wc Keijzer, Fred A. (1998). Doing without representations which specify what to do. Philosophical Psychology 11 (3):269-302. ( Cited by 15 | Google ) Abstract: A discussion is going on in cognitive science about the use of representations to explain how intelligent behavior is generated. In the traditional view, an organism is thought to incorporate representations. These provide an internal model that is used by the organism to instruct the motor apparatus so that the adaptive and anticipatory characteristics of behavior come about. So-called interactionists claim that this representational specification of behavior raises more problems than it solves. In their view, the notion of internal representational models is to be dispensed with. Instead, behavior is to be explained as the intricate interaction between an embodied organism and the specific make up of an environment. The problem with a non-representational interactive account is that it has severe difficulties with anticipatory, future oriented behavior. The present paper extends the interactionist conceptual framework by drawing on ideas derived from the study of morphogenesis. This extended interactionist framework is based on an analysis of anticipatory behavior as a process which involves multiple spatio-temporal scales of neural, bodily and environmental dynamics. This extended conceptual framework provides the outlines for an explanation of anticipatory behavior without involving a representational specification of future goal states Kirsh, David (1991). Today the earwig, tomorrow man? Artificial Intelligence 47:161-184. ( Cited by 111 | Google | More links ) Abstract: A startling amount of intelligent activity can be controlled without reasoning or thought. By tuning the perceptual system to task relevant properties a creature can cope with relatively sophisticated environments without concepts. There is a limit, however, to how far a creature without concepts can go. Rod Brooks, like many ecologically oriented scientists, argues that the vast majority of intelligent behaviour is concept-free. To evaluate this position I consider what special benefits accrue to concept-using creatures. Concepts are either necessary for certain types of perception, learning, and control, or they make those processes computationally simpler. Once a creature has concepts its capacities are vastly multiplied. Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=2506371CI Müller, Vincent C. (2007). Is there a future for AI without representation? Minds and Machines 17 (1). ( Google | More links ) Abstract: This paper investigates the prospects of Rodney Brooks’ proposal for AI without representation. It turns out that the supposedly characteristic features of “new AI” (embodiment, situatedness, absence of reasoning, and absence of representation) are all present in conventional systems: “New AI” is just like old AI. Brooks proposal boils down to the architectural rejection of central control in intelligent agents—Which, however, turns out to be crucial. Some of more recent cognitive science suggests that we might do well to dispose of the image of intelligent agents as central representation processors. If this paradigm shift is achieved, Brooks’ proposal for cognition without representation appears promising for full-blown intelligent agents—Though not for conscious agents Additional links for this entry: http://portal.acm.org/citation.cfm?id=1285561.1285568 http://www.springerlink.com/index/T65JK1H2705383L8.pdf http://www.typos.de/pdf/2007_AI_without_representation_M http://www.springerlink.com/content/t65jk1h2705383l8/fulltext.pdf van Gelder, Tim (1995). What might cognition be if not computation? Journal of Philosophy 92 (7):345-81. ( Cited by 266 | Annotation | Google | More links ) Argues for a dynamic-systems conception of the mind that is non-computational and non-representational. Uses an analogy with the Watt steam governor to argue for a new kind of dynamic explanation. Additional links for this entry: http://books.google.com/books?hl=en=_-hLgEKPrvzRGqiHU3WB-ByPxG4 http://books.google.com/books?hl=en=9bvnduaEm9LGyoZvLb9vrkbzLwc http://books.google.com/books?hl=en=nbynhfg_nmoW_oos5qyFfZKaq_w http://www.jstor.org/stable/pdfplus/2941061.pdf Wallis, Peter (2004). Intention without representation. Philosophical Psychology 17 (2):209-223. ( Cited by 3 | Google | More links ) Abstract: A mechanism for planning ahead would appear to be essential to any creature with more than insect level intelligence. In this paper it is shown how planning, using full means-ends analysis, can be had while avoiding the so called symbol grounding problem. The key role of knowledge representation in intelligence has been acknowledged since at least the enlightenment, but the advent of the computer has made it possible to explore the limits of alternate schemes, and to explore the nature of our everyday understanding of the world around us. In particular, artificial intelligence (AI) and robotics has forced a close examination, by people other than philosophers, of what it means to say for instance that "snow is white." One interpretation of the "new AI" is that it is questioning the need for representation altogether. Brooks and others have shown how a range of intelligent behaviors can be had without representation, and this paper goes one step further showing how intending to do things can be achieved without symbolic representation. The paper gives a concrete example of a mechanism in terms of robots that play soccer. It describes a belief, desire and intention (BDI) architecture that plans in terms of activities. The result is a situated agent that plans to do things with no more ontological commitment than the reactive systems Brooks described in his seminal paper, "Intelligence without Representation." Additional links for this entry: http://www.informaworld.com/smpp/./ftinterface~content=a713630032~fulltext=713240930 http://taylorandfrancis.metapress.com/index/T86XG5YMR7KE123J.pdf http://www.informaworld.com/index/713630032.pdf http://www.ingentaconnect.com/content/routledg/cphp/2004/00000017/00000002/art00004 http://www.informaworld.com/smpp/./ftinterface~db=all~content=a713630032~fulltext=713240930 Webber, Jonathan (2002). Doing without representation: Coping with Dreyfus. Philosophical Explorations 5 (1):82-88. ( Google | More links ) Abstract: Hubert Dreyfus argues that the traditional and currently dominant conception of an action, as an event initiated or governed by a mental representation of a possible state of affairs that the agent is trying to realise, is inadequate. If Dreyfus is right, then we need a new conception of action. I argue, however, that the considerations that Dreyfus adduces show only that an action need not be initiated or governed by a conceptual representation, but since a representation need not be conceptually structured, do not show that we need a conception of action that does not involve representation Additional links for this entry: http://www.bristol.ac.uk/philosophy/department/staff/JW/DoingWithoutRepresentation.pdf http://www.informaworld.com/smpp/./ftinterface~content=a782351921~fulltext=713240930 http://www.informaworld.com/index/782351921.pdf http://www.informaworld.com/smpp/./ftinterface~db=all~content=a782351921~fulltext=713240930 6.2e Computation and Representation, Misc Akman, Varol ten Hagen, Paul J. W. (1989). The power of physical representations. AI Magazine 10 (3):49-65. (Cited by 10 | Google | More links ) Additional links for this entry: http://portal.acm.org/citation.cfm?id=65965.69627 http://www.cs.bilkent.edu.tr/~akman/jour-papers/aimag/aimag1989-2.pdf http://www.csa.com/partners/viewrecord.php?requester=gs=2227604CI http://www.csa.com/partners/viewrecord.php?requester=gs=N8920612AH Bailey, Andrew R. (1994). Representations versus regularities: Does computation require representation? Eidos 12 (1):47-58. ( Google ) Chalmers, David J. ; French, Robert M. Hofstadter, Douglas R. (1992). High-level perception, representation, and analogy:A critique of artificial intelligence methodology. Journal of Experimental and Theoretical Artificial Intellige 4 (3):185 - 211. ( Cited by 123 | Google | More links ) Abstract: High-level perception--”the process of making sense of complex data at an abstract, conceptual level--”is fundamental to human cognition. Through high-level perception, chaotic environmen- tal stimuli are organized into the mental representations that are used throughout cognitive pro- cessing. Much work in traditional artificial intelligence has ignored the process of high-level perception, by starting with hand-coded representations. In this paper, we argue that this dis- missal of perceptual processes leads to distorted models of human cognition. We examine some existing artificial-intelligence models--”notably BACON, a model of scientific discovery, and the Structure-Mapping Engine, a model of analogical thought--”and argue that these are flawed pre- cisely because they downplay the role of high-level perception. Further, we argue that perceptu- al processes cannot be separated from other cognitive processes even in principle, and therefore that traditional artificial-intelligence models cannot be defended by supposing the existence of a --œrepresentation module--� that supplies representations ready-made. Finally, we describe a model of high-level perception and analogical thought in which perceptual processing is integrated with analogical mapping, leading to the flexible build-up of representations appropriate to a given context Additional links for this entry: http://citeseer.ist.psu.edu/49715.html http://portal.acm.org/citation.cfm?id=175346.175347 http://www.u.arizona.edu/~chalmers/papers/highlevel.pdf http://www.nbu.bg/cogs/personal/kokinov/COG501/hightlevel.pdf http://www.csa.com/partners/viewrecord.php?requester=gs=2767459CI http://www.informaworld.com/index/778787585.pdf Dartnall, Terry (2000). Reverse psychologism, cognition and content. Minds and Machines 10 (1):31-52. ( Cited by 32 | Google | More links ) Abstract:   The confusion between cognitive states and the content of cognitive states that gives rise to psychologism also gives rise to reverse psychologism. Weak reverse psychologism says that we can study cognitive states by studying content – for instance, that we can study the mind by studying linguistics or logic. This attitude is endemic in cognitive science and linguistic theory. Strong reverse psychologism says that we can generate cognitive states by giving computers representations that express the content of cognitive states and that play a role in causing appropriate behaviour. This gives us strong representational, classical AI (REPSCAI), and I argue that it cannot succeed. This is not, as Searle claims in his Chinese Room Argument, because syntactic manipulation cannot generate content. Syntactic manipulation can generate content, and this is abundantly clear in the Chinese Room scenano. REPSCAI cannot succeed because inner content is not sufficient for cognition, even when the representations that carry the content play a role in generating appropriate behaviour Additional links for this entry: http://www98.griffith.edu.au/dspace/handle/10072/3245 http://www.csa.com/partners/viewrecord.php?requester=gs=475243CI http://www.springerlink.com/content/content/r64783586474k030/fulltext.pdf http://www.springerlink.com/content/r64783586474k030/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=231754=1 http://www.springerlink.com/index/R64783586474K030.pdf http://www.ingentaconnect.com/content/klu/mind/2000/00000010/00000001/00231754 Dietrich, Eric (1988). Computers, intentionality, and the new dualism. Computers and Philosophy Newsletter . ( Google ) Dreyfus, Hubert L. (1979). A framework for misrepresenting knowledge. In Martin Ringle (ed.), Philosophical Perspectives in Artificial Intelligence . Humanities Press. ( Cited by 7 | Annotation | Google ) On the problems with context-free symbolic representation. Echavarria, Ricardo Restrepo (2009). Russell's structuralism and the supposed death of computational cognitive science. Minds and Machines 19 (2). ( Google ) Abstract: John Searle believes that computational properties are purely formal and that consequently, computational properties are not intrinsic, empirically discoverable, nor causal; and therefore, that an entity’s having certain computational properties could not be sufficient for its having certain mental properties. To make his case, Searle’s employs an argument that had been used before him by Max Newman, against Russell’s structuralism; one that Russell himself considered fatal to his own position. This paper formulates a not-so-explored version of Searle’s problem with computational cognitive science, and refutes it by suggesting how our understanding of computation is far from implying the structuralism Searle vitally attributes to it. On the way, I formulate and argue for a thesis that strengthens Newman’s case against Russell’s structuralism, and thus raises the apparent risk for computational cognitive science too Fields, Christopher A. (1994). Real machines and virtual intentionality: An experimentalist takes on the problem of representational content. In Eric Dietrich (ed.), Thinking Computers and Virtual Persons . Academic Press. ( Google ) Franklin, James , The representation of context: Ideas from artiﬁcial intelligence. ( Google ) Abstract: To move beyond vague platitudes about the importance of context in legal reasoning or natural language understanding, one must take account of ideas from artiﬁcial intelligence on how to represent context formally. Work on topics like prior probabilities, the theory-ladenness of observation, encyclopedic knowledge for disambiguation in language translation and pathology test diagnosis has produced a body of knowledge on how to represent context in artiﬁcial intelligence applications Fulda, Joseph S. (2000). The logic of “improper cross”. Artificial Intelligence and Law 8 (4):337-341. ( Google ) Garzon, Francisco Calvo Rodriguez, Angel Garcia (2009). Where is cognitive science heading? Minds and Machines . ( Google ) Abstract: According to Ramsey (Representation reconsidered, Cambridge University Press, New York, 2007), only classical cognitive science, with the related notions of input–output and structural representations, meets the job description challenge (the challenge to show that a certain structure or process serves a representational role at the subpersonal level). By contrast, connectionism and other nonclassical models, insofar as they exploit receptor and tacit notions of representation, are not genuinely representational. As a result, Ramsey submits, cognitive science is taking a U-turn from representationalism back to behaviourism, thus presupposing that (1) the emergence of cognitivism capitalized on the concept of representation, and that (2) the materialization of nonclassical cognitive science involves a return to some form of pre-cognitivist behaviourism. We argue against both (1) and (2), by questioning Ramsey’s divide between classical and representational, versus nonclassical and nonrepresentational, cognitive models. For, firstly, connectionist and other nonclassical accounts have the resources to exploit the notion of a structural isomorphism, like classical accounts (the beefing-up strategy); and, secondly, insofar as input–output and structural representations refer to a cognitive agent, classical explanations fail to meet the job description challenge (the deflationary strategy). Both strategies work independently of each other: if the deflationary strategy succeeds, contra (1), cognitivism has failed to capitalize on the relevant concept of representation; if the beefing-up strategy is sound, contra (2), the return to a pre-cognitivist era cancels out. Guvenir, Halil A. Akman, Varol (1992). Problem representation for refinement. Minds and Machines 2 (3):267-282. ( Google | More links ) Abstract:   In this paper we attempt to develop a problem representation technique which enables the decomposition of a problem into subproblems such that their solution in sequence constitutes a strategy for solving the problem. An important issue here is that the subproblems generated should be easier than the main problem. We propose to represent a set of problem states by a statement which is true for all the members of the set. A statement itself is just a set of atomic statements which are binary predicates on state variables. Then, the statement representing the set of goal states can be partitioned into its subsets each of which becomes a subgoal of the resulting strategy. The techniques involved in partitioning a goal into its subgoals are presented with examples Additional links for this entry: http://www.springerlink.com/index/9522322854447002.pdf Haugeland, John (1981). Semantic engines: An introduction to mind design. In J. Haugel (ed.), Mind Design . MIT Press. ( Cited by 92 | Google ) Marsh, Leslie (2005). Review Essay: Andy Clark's Natural-Born Cyborgs: Minds, Technologies, and the Future of Human Intelligence_. Cognitive Systems Research 6:405-409. ( Google ) Prem, Erich (2000). Changes of representational AI concepts induced by embodied autonomy. Communication and Cognition-Artificial Intelligence 17 (3-4):189-208. ( Cited by 4 | Google ) Robinson, William S. (1995). Direct representation. Philosophical Studies 80 (3):305-22. ( Cited by 3 | Annotation | Google | More links ) On Searle's critique of computational explanation, contrasted with Gallistel's use thereof. The real issue is computation on indirect vs. direct representations; direct computationalism is an attractive view. Additional links for this entry: http://www.springerlink.com/index/N84X1826K3573762.pdf Shani, Itay (2005). Computation and intentionality: A recipe for epistemic impasse. Minds and Machines 15 (2):207-228. ( Cited by 1 | Google | More links ) Abstract: Searle’s celebrated Chinese room thought experiment was devised as an attempted refutation of the view that appropriately programmed digital computers literally are the possessors of genuine mental states. A standard reply to Searle, known as the “robot reply” (which, I argue, reflects the dominant approach to the problem of content in contemporary philosophy of mind), consists of the claim that the problem he raises can be solved by supplementing the computational device with some “appropriate” environmental hookups. I argue that not only does Searle himself casts doubt on the adequacy of this idea by applying to it a slightly revised version of his original argument, but that the weakness of this encoding-based approach to the problem of intentionality can also be exposed from a somewhat different angle. Capitalizing on the work of several authors and, in particular, on that of psychologist Mark Bickhard, I argue that the existence of symbol-world correspondence is not a property that the cognitive system itself can appreciate, from its own perspective, by interacting with the symbol and therefore, not a property that can constitute intrinsic content. The foundational crisis to which Searle alluded is, I conclude, very much alive Additional links for this entry: http://www.springerlink.com/content/k26t830883g84041/fulltext.pdf http://www.springerlink.com/index/K26T830883G84041.pdf http://www.ingentaconnect.com/content/klu/mind/2005/00000015/00000002/00002004 Stanley, Jason (2005). Review of Robyn Carston, Thoughts and Utterances . Mind and Language 20 (3). ( Google ) Abstract: Relevance Theory is the influential theory of linguistic interpretation first championed by Dan Sperber and Deirdre Wilson. Relevance theorists have made important contributions to our understanding of a wide range of constructions, especially constructions that tend to receive less attention in semantics and philosophy of language. But advocates of Relevance Theory also have had a tendency to form a rather closed community, with an unwillingness to translate their own special vocabulary and distinctions into more neutral vernacular. Since Robyn Carston has long been the advocate of Relevance Theory most able to communicate with a broader philosophical and linguistic audience, it is with particular interest that the emergence of her long-awaited volume, Thoughts and Utterances has been greeted. The volume exhibits many of the strengths, but also some of the weaknesses, of this well-known program Thornton, Chris (1997). Brave mobots use representation: Emergence of representation in fight-or-flight learning. Minds and Machines 7 (4):475-494. ( Cited by 10 | Google | More links ) Abstract:   The paper uses ideas from Machine Learning, Artificial Intelligence and Genetic Algorithms to provide a model of the development of a fight-or-flight response in a simulated agent. The modelled development process involves (simulated) processes of evolution, learning and representation development. The main value of the model is that it provides an illustration of how simple learning processes may lead to the formation of structures which can be given a representational interpretation. It also shows how these may form the infrastructure for closely-coupled agent/environment interaction Additional links for this entry: http://portal.acm.org/citation.cfm?id=596763 http://www.cogs.susx.ac.uk/users/christ/fight-or-flight.html http://www.cogs.susx.ac.uk/users/christ/papers/brave-mobots.ps http://www.cogs.sussex.ac.uk/users/christ/papers/brave-mobots.ps http://www.informatics.sussex.ac.uk/users/christ/papers/brave-mobots.ps http://www.csa.com/partners/viewrecord.php?requester=gs=316236CI http://www.springerlink.com/content/q228732u4h164l35/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=136246=1 http://www.springerlink.com/index/Q228732U4H164L35.pdf http://www.ingentaconnect.com/content/klu/mind/1997/00000007/00000004/00136246 6.3 Philosophy of Connectionism 6.3a Connectionism and Compositionality 54 / 55 entries displayed Aizawa, Kenneth (1997). Explaining systematicity. Mind and Language 12 (2):115-36. ( Cited by 48 | Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/119154896/PDFSTART http://www.blackwell-synergy.com/links/doi/10.1111/1468-0017.00039 http://www.ingentaconnect.com/content/bpl/mila/1997/00000012/00000002/art00039 Aizawa, Kenneth (1997). Exhibiting verses explaining systematicity: A reply to Hadley and Hayward. Minds and Machines 7 (1):39-55. ( Google | More links ) Additional links for this entry: http://www.springerlink.com/content/tuq4w37067306080/fulltext.pdf Aizawa, Kenneth (1997). The role of the systematicity argument in classicism and connectionism. In S. O'Nuallain (ed.), Two Sciences of Mind . John Benjamins. ( Cited by 4 | Google ) Aizawa, Kenneth (2003). The Systematicity Arguments. Kluwer. (Cited by 4 | Google ) Abstract: The Systematicity Arguments is the only book-length treatment of the systematicity and productivity arguments. Antony, Michael V. (1991). Fodor and Pylyshyn on connectionism. Minds and Machines 1 (3):321-41. ( Cited by 3 | Annotation | Google | More links ) Fodor and Pylyshyn's argument is an invalid instance of inference to the best explanation, as there is much to explain than systematicity. Connectionism and classicism may be compatible even without implementation, in any case. Abstract:   Fodor and Pylyshyn (1988) have argued that the cognitive architecture is not Connectionist. Their argument takes the following form: (1) the cognitive architecture is Classical; (2) Classicalism and Connectionism are incompatible; (3) therefore the cognitive architecture is not Connectionist. In this essay I argue that Fodor and Pylyshyn's defenses of (1) and (2) are inadequate. Their argument for (1), based on their claim that Classicalism best explains the systematicity of cognitive capacities, is an invalid instance of inference to the best explanation. And their argument for (2) turns out to be question-begging. The upshot is that, while Fodor and Pylyshyn have presented Connectionists with the important empirical challenge of explaining systematicity, they have failed to provide sufficient reason for inferring that the cognitive architecture is Classical and not Connectionist Additional links for this entry: http://www.springerlink.com/content/u316462310k73495/fulltext.pdf http://www.springerlink.com/index/U316462310K73495.pdf Aydede, Murat (1997). Language of thought: The connectionist contribution. Minds and Machines 7 (1):57-101. ( Cited by 15 | Google | More links ) Abstract:   Fodor and Pylyshyn's critique of connectionism has posed a challenge to connectionists: Adequately explain such nomological regularities as systematicity and productivity without postulating a "language of thought" (LOT). Some connectionists like Smolensky took the challenge very seriously, and attempted to meet it by developing models that were supposed to be non-classical. At the core of these attempts lies the claim that connectionist models can provide a representational system with a combinatorial syntax and processes sensitive to syntactic structure. They are not implementation models because, it is claimed, the way they obtain syntax and structure sensitivity is not "concatenative," hence "radically different" from the way classicists handle them. In this paper, I offer an analysis of what it is to physically satisfy/realize a formal system. In this context, I examine the minimal truth-conditions of LOT Hypothesis. From my analysis it will follow that concatenative realization of formal systems is irrelevant to LOTH since the very notion of LOT is indifferent to such an implementation level issue as concatenation. I will conclude that to the extent to which they can explain the law-like cognitive regularities, a certain class of connectionist models proposed as radical alternatives to the classical LOT paradigm will in fact turn out to be LOT models, even though new and potentially very exciting ones Additional links for this entry: http://www.springerlink.com/content/content/v8365g5vp26x7x75/fulltext.pdf http://www.springerlink.com/content/v8365g5vp26x7x75/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=115552=1 http://www.springerlink.com/index/V8365G5VP26X7X75.pdf http://www.ingentaconnect.com/content/klu/mind/1997/00000007/00000001/00115552 Butler, Keith (1993). Connectionism, classical cognitivism, and the relation between cognitive and implementational levels of analysis. Philosophical Psychology 6 (3):321-33. ( Cited by 6 | Annotation | Google ) Contra Chalmers 1993, F's argument doesn't apply at the implementational level. Contra Chater and Oaksford 1990, connectionism can't be purely implementational, but some implementational details can be relevant. Abstract: This paper discusses the relation between cognitive and implementational levels of analysis. Chalmers (1990, 1993) argues that a connectionist implementation of a classical cognitive architecture possesses a compositional semantics, and therefore undercuts Fodor and Pylyshyn's (1988) argument that connectionist networks cannot possess a compositional semantics. I argue that Chalmers argument misconstrues the relation between cognitive and implementational levels of analysis. This paper clarifies the distinction, and shows that while Fodor and Pylyshyn's argument survives Chalmers' critique, it cannot be used to establish the irrelevance of neurophysiological implementation to cognitive modeling; some aspects of Chater and Oaksford's (1990) response to Fodor and Pylyshyn, though not all, are therefore cogent Butler, Keith (1995). Compositionality in cognitive models: The real issue. Philosophical Studies 78 (2):153-62. ( Cited by 3 | Google | More links ) Additional links for this entry: http://www.springerlink.com/index/T05Q0506V1349189.pdf Butler, Keith (1993). On Clark on systematicity and connectionism. British Journal for the Philosophy of Science 44 (1):37-44. ( Cited by 1 | Annotation | Google | More links ) Argues against Clark on holism and the conceptual truth of systematicity. Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/citation/44/1/37 http://bjps.oxfordjournals.org/cgi/reprint/44/1/37 http://www.jstor.org/stable/pdfplus/687848.pdf Butler, Keith (1991). Towards a connectionist cognitive architecture. Mind and Language 6 (3):252-72. ( Cited by 12 | Annotation | Google | More links ) Connectionism can make do with unstructured representations, as long have they have the right causal relations between them. Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/119352985/PDFSTART Chater, Nick Oaksford, Mike (1990). Autonomy, implementation and cognitive architecture: A reply to Fodor and Pylyshyn. Cognition 34:93-107. ( Cited by 63 | Annotation | Google ) Implementation can make a difference at the algorithmic level. Chalmers, David J. (1993). Connectionism and compositionality: Why Fodor and Pylyshyn were wrong. Philosophical Psychology 6 (3):305-319. ( Annotation | Google ) Points out a structural flaw in F's argument, and traces the problem to a lack of appreciation of distributed representation. With some empirical results on structure sensitive processing, and some remarks on explanation. Abstract: This paper offers both a theoretical and an experimental perspective on the relationship between connectionist and Classical (symbol-processing) models. Firstly, a serious flaw in Fodor and Pylyshyn’s argument against connectionism is pointed out: if, in fact, a part of their argument is valid, then it establishes a conclusion quite different from that which they intend, a conclusion which is demonstrably false. The source of this flaw is traced to an underestimation of the differences between localist and distributed representation. It has been claimed that distributed representations cannot support systematic operations, or that if they can, then they will be mere implementations of traditional ideas. This paper presents experimental evidence against this conclusion: distributed representations can be used to support direct structure-sensitive operations, in a man- ner quite unlike the Classical approach. Finally, it is argued that even if Fodor and Pylyshyn’s argument that connectionist models of compositionality must be mere implementations were correct, then this would still not be a serious argument against connectionism as a theory of mind Chalmers, David J. (online). Deep systematicity and connectionist representation. ( Google | More links ) Abstract: 1. I think that by emphasizing theoretical spaces of representations, Andy has put his finger on an issue that is key to connectionism's success, and whose investigation will be a key determinant of the field's further progress. I also think that if we look at representational spaces in the right way, we can see that they are deeply related to classical phenomenon of systematicity in representation. I want to argue that the key to understanding representational spaces, and in particular their ability to capture the deep organization underlying various problems, lies in the idea of what I will call Additional links for this entry: http://consc.net/notes/clark-comments.html Chalmers, David J. (1990). Syntactic transformations on distributed representations. Connection Science 2:53-62. ( Cited by 180 | Annotation | Google | More links ) An experimental demonstration that connectionist models can handle structure-sensitive operations in a non-classical way, transforming structured representations of active sentences to passive sentences. Abstract: There has been much interest in the possibility of connectionist models whose representations can be endowed with compositional structure, and a variety of such models have been proposed. These models typically use distributed representations that arise from the functional composition of constituent parts. Functional composition and decomposition alone, however, yield only an implementation of classical symbolic theories. This paper explores the possibility of moving beyond implementation by exploiting holistic structure-sensitive operations on distributed representations. An experiment is performed using Pollack’s Recursive Auto-Associative Memory. RAAM is used to construct distributed representations of syntactically structured sentences. A feed-forward network is then trained to operate directly on these representations, modeling syn- tactic transformations of the represented sentences. Successful training and generalization is obtained, demonstrating that the implicit structure present in these representations can be used for a kind of structure-sensitive processing unique to the connectionist domain Additional links for this entry: http://citeseer.ist.psu.edu/517265.html http://citeseer.ist.psu.edu/chalmers90syntactic.html http://www.u.arizona.edu/~chalmers/papers/transformations.pdf http://www.informaworld.com/index/776705615.pdf Christiansen, M. H. Chater, Nick (1994). Generalization and connectionist language learning. Mind and Language 9:273-87. ( Cited by 45 | Google | More links ) Additional links for this entry: http://www.blackwell-synergy.com/doi/abs/10.1111/j.1468-0017.1994.tb00226.x http://www3.interscience.wiley.com/cgi-bin/fulltext/119275792/PDFSTART Cummins, Robert E. (1996). Systematicity. Journal of Philosophy 93 (12):591-614. ( Cited by 14 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2941118.pdf Davis, Wayne A. (2005). On begging the systematicity question. Journal of Philosophical Research 30:399-404. ( Google ) Fetzer, James H. (1992). Connectionism and cognition: Why Fodor and Pylyshyn are wrong. In A. Clark Ronald Lutz (eds.), Connectionism in Context . Springer-Verlag. ( Cited by 7 | Google ) Fodor, Jerry A. Pylyshyn, Zenon W. (1988). Connectionism and cognitive architecture. Cognition 28:3-71. ( Cited by 1496 | Annotation | Google | More links ) Connectionist models can't explain cognitive systematicity and productivity, as their representations lack compositional structure. The allures of connectionism are illusory; it's best used as an implementation strategy. Abstract: This paper explores the difference between Connectionist proposals for cognitive a r c h i t e c t u r e a n d t h e s o r t s o f m o d e l s t hat have traditionally been assum e d i n c o g n i t i v e s c i e n c e . W e c l a i m t h a t t h e m a j o r d i s t i n c t i o n i s t h a t , w h i l e b o t h Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a ‘language of thought’: i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the ‘systematicity’ of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or ‘abstract neurological’) structures in which Classical cognitive architecture is implemented. We survey a n u m b e r o f t h e s t a n d a r d a r g u m e n t s t h a t h a v e b e e n o f f e r e d i n f a v o r o f Connectionism, and conclude that they are coherent only on this interpretation Additional links for this entry: http://portal.acm.org/citation.cfm?id=58067 http://www.blutner.de/philom/connect/jaf.pdf http://ruccs.rutgers.edu/ftp/pub/papers/jaf.pdf http://portal.acm.org/citation.cfm?id=190704.190740 http://www.ii.metu.edu.tr/~tekman/fodor http://citeseer.ist.psu.edu/fodor88connectionism.html http://books.google.com/books?hl=en=1PDJYThCtCF24M7R6UJK4T http://watarts.uwaterloo.ca/~celiasmi/courses/Phil256/papers/fodor.pylyshyn.1988.Connectionism cog architec.bbs.pdf http://books.google.com/books?hl=en=lvbjst___yAcJlOgVzyRdzEV09k http://books.google.com/books?hl=en=bfPrm5H3Z-QZFIh2WX-XvqgjqHE http://books.google.com/books?hl=en=W9wLTgrsP59cPqZhSN0ysQbSVoM http://books.google.com/books?hl=en=ic0bVVM1PfuvCnadV8OCRGBbNA4 http://books.google.com/books?hl=en=UkXTSX19hHzvmMUKvLT-lq0r50I http://books.google.com/books?hl=en=LBgHuNyf_KzzIi7ocvlxiFsIWIE http://books.google.com/books?hl=en=-rBwtgEcuMHfIKQJzOHWYIrBX9E http://books.google.com/books?hl=en=00wE8Svuh3W8kqsUo0UKOYCfcAI http://books.google.com/books?hl=en=5IusXYTvApV1_rtpQEkaBet0xn4 http://books.google.com/books?hl=en=HFGXyuxr_1mxH5ACFOe-zyaTw4o http://books.google.com/books?hl=en=CAuCWiDdEmwTntCDA3H1Zyfn5a0 http://books.google.com/books?hl=en=e9QEFe5nD4bGAK5W4dDIRZBKNuM http://books.google.com/books?hl=en=4TMgHlnpnDyehNFJT-NG470Bbz8 http://books.google.com/books?hl=en=9UEpXL5XYxZdQa3NvX7yU9iikyw http://books.google.com/books?hl=en=7sjHbKdagqCd6sUT-Kk-fcITZlc http://books.google.com/books?hl=en=9ypKOA18BrlaitMu_it-TVCBfUw http://books.google.com/books?hl=en=tdUqyz4ARusp_kokrKNOJedNknk http://books.google.com/books?hl=en=cUrJjY2sKfVLW5Zp1ZtlEsdhcaM http://books.google.com/books?hl=en=Qe8qR4Go6tnV95LlvTT6ELC_0Ds http://books.google.com/books?hl=en=iCTIyYq8EOvBdYu-srEXgyt-HoM http://books.google.com/books?hl=en=7hnYM8j4zdtpN4THjVSpzpGFEDs http://books.google.com/books?hl=en=F0CONzdT49KnmoIp41iOngjBa_A http://books.google.com/books?hl=en=Zks6FRmma9XZGHW2EO1kvwIxesQ http://books.google.com/books?hl=en=ohgYrPGd2Ut0_gQqvfJPXg0_xJo http://books.google.com/books?hl=en=snID_Ptc5j-K-DxZvSjAScHkD6s http://books.google.com/books?hl=en=YnoEHTZY6rwzlL8DcfCj45uUQo0 http://books.google.com/books?hl=en=XBIWzRchQULcgPTlX6HyBgOg_pQ http://books.google.com/books?hl=en=35GDmDcPG526574Ar-pZaGlfYXA http://books.google.com/books?hl=en=OzlrEEZ2Td9CGt5mgUD99UXGCWE http://books.google.com/books?hl=en=ZsqjFV8LrCGXI-3jf8Pliu_sVsc http://books.google.com/books?hl=en=1yq1WHwwotr3qVSaqN0K4StIWc8 http://books.google.com/books?hl=en=lDwkN-dMTxlLX_1XELpsD4ONtM4 http://books.google.com/books?hl=en=s7d-fmzg598bYH-IpAMNrcyO8eo http://books.google.com/books?hl=en=6AJnryXo_R8vj8qseWkK5DD31zQ http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation Fodor, Jerry A. McLaughlin, Brian P. (1990). Connectionism and the problem of systematicity: Why Smolensky's solution doesn't work. Cognition 35:183-205. ( Cited by 193 | Annotation | Google ) Smolensky's weak compositionality is useless; and tensor product architecture can't support systematicity, as nonexistent tokens can't play a causal role. Fodor, Jerry A. (1997). Connectionism and the problem of systematicity (continued): Why Smolensky's solution still doesn't work. Cognition 62:109-19. ( Cited by 25 | Google | More links ) Additional links for this entry: http://www.ingentaconnect.com/content/els/00100277/1997/00000062/00000001/art00780 Garfield, Jay L. (1997). Mentalese not spoken here: Computation, cognition, and causation. Philosophical Psychology 10 (4):413-35. ( Cited by 38 | Google ) Abstract: Classical computational modellers of mind urge that the mind is something like a von Neumann computer operating over a system of symbols constituting a language of thought. Such an architecture, they argue, presents us with the best explanation of the compositionality, systematicity and productivity of thought. The language of thought hypothesis is supported by additional independent arguments made popular by Jerry Fodor. Paul Smolensky has developed a connectionist architecture he claims adequately explains compositionality, systematicity and productivity without positing any language of thought, and without positing any operations over a set of symbols. This architecture encodes the information represented in linguistic trees without explicitly representing those trees or their constituents, and indeed without employing any representational vehicles with constituent structure. In a recent article, Fodor (1997; Connectionism and systematicity, Cognition , 62, 109-119) argues that Smolensky's proposal does not work. I defend Smolensky against Fodor's attack, and use this interchange as a vehicle for exploring and criticising the “Language of Thought” hypothesis more generally and the arguments Fodor adduces on its behalf Garcia-Carpintero, Manuel (1996). Two spurious varieties of compositionality. Minds and Machines 6 (2):159-72. ( Google | More links ) Abstract:   The paper examines an alleged distinction claimed to exist by Van Gelder between two different, but equally acceptable ways of accounting for the systematicity of cognitive output (two varieties of compositionality): concatenative compositionality vs. functional compositionality. The second is supposed to provide an explanation alternative to the Language of Thought Hypothesis. I contend that, if the definition of concatenative compositionality is taken in a different way from the official one given by Van Gelder (but one suggested by some of his formulations) then there is indeed a different sort of compositionality; however, the second variety is not an alternative to the language of thought in that case. On the other hand, if the concept of concatenative compositionality is taken in a different way, along the lines of Van Gelder's explicit definition, then there is no reason to think that there is an alternative way of explaining systematicity Additional links for this entry: http://www.springerlink.com/content/j74132418g368g21/fulltext.pdf http://www.springerlink.com/index/J74132418G368G21.pdf Guarini, Marcello (1996). Tensor products and split-level architecture: Foundational issues in the classicism-connectionism debate. Philosophy of Science 63 (3):S239-S247. ( Google | More links ) Additional links for this entry: http://links.jstor.org/sici?sici=0031-8248(199609)63 2.0.CO;2-T http://www.jstor.org/stable/pdfplus/188532.pdf Hadley, Robert F. (1997). Cognition, systematicity, and nomic necessity. Mind and Language 12 (2):137-53. ( Cited by 12 | Google | More links ) Additional links for this entry: http://www.blackwell-synergy.com/doi/abs/10.1111/j.1468-0017.1997.tb00066.x http://www3.interscience.wiley.com/cgi-bin/fulltext/119154897/PDFSTART http://www.blackwell-synergy.com/links/doi/10.1111/1468-0017.00040 http://www.blackwell-synergy.com/doi/abs/10.1111/1468-0017.00040 http://www.ingentaconnect.com/content/bpl/mila/1997/00000012/00000002/art00002 http://www.ingentaconnect.com/content/bpl/mila/1997/00000012/00000002/art00040 Hadley, Robert F. (1997). Explaining systematicity: A reply to Kenneth Aizawa. Minds and Machines 12 (4):571-79. ( Cited by 3 | Google | More links ) Abstract:   In his discussion of results which I (with Michael Hayward) recently reported in this journal, Kenneth Aizawa takes issue with two of our conclusions, which are: (a) that our connectionist model provides a basis for explaining systematicity within the realm of sentence comprehension, and subject to a limited range of syntax (b) that the model does not employ structure-sensitive processing, and that this is clearly true in the early stages of the network''s training. Ultimately, Aizawa rejects both (a) and (b) for reasons which I think are ill-founded. In what follows, I offer a defense of our position. In particular, I argue (1) that Aizawa adopts a standard of explanation that many accepted scientific explanations could not meet, and (2) that Aizawa misconstrues the relevant meaning of structure-sensitive process Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=316242CI http://www.springerlink.com/content/m395128140134l86/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=122810=1 http://www.springerlink.com/index/M395128140134L86.pdf http://www.ingentaconnect.com/content/klu/mind/1997/00000007/00000004/00122810 Hadley, Robert F. (1994). Systematicity in connectionist language learning. Mind and Language 9:247-72. ( Cited by 74 | Annotation | Google | More links ) Argues that existing connectionist models do not achieve an adequate systematicity in learning; they fail to generalize to handle structures with novel constituents. Additional links for this entry: http://www.blackwell-synergy.com/doi/abs/10.1111/j.1468-0017.1994.tb00225.x http://www3.interscience.wiley.com/cgi-bin/fulltext/119275791/PDFSTART Hadley, Robert F. (1994). Systematicity revisited. Mind and Language 9:431-44. ( Cited by 34 | Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/119970226/PDFSTART Hadley, Robert F. Hayward, M. B. (1997). Strong semantic systematicity from Hebbian connectionist learning. Minds and Machines 7 (1):1-55. ( Cited by 46 | Google | More links ) Abstract:   Fodor's and Pylyshyn's stand on systematicity in thought and language has been debated and criticized. Van Gelder and Niklasson, among others, have argued that Fodor and Pylyshyn offer no precise definition of systematicity. However, our concern here is with a learning based formulation of that concept. In particular, Hadley has proposed that a network exhibits strong semantic systematicity when, as a result of training, it can assign appropriate meaning representations to novel sentences (both simple and embedded) which contain words in syntactic positions they did not occupy during training. The experience of researchers indicates that strong systematicity in any form is difficult to achieve in connectionist systems.Herein we describe a network which displays strong semantic systematicity in response to Hebbian, connectionist training. During training, two-thirds of all nouns are presented only in a single syntactic position (either as grammatical subject or object). Yet, during testing, the network correctly interprets thousands of sentences containing those nouns in novel positions. In addition, the network generalizes to novel levels of embedding. Successful training requires a, corpus of about 1000 sentences, and network training is quite rapid. The architecture and learning algorithms are purely connectionist, but classical insights are discernible in one respect, viz, that complex semantic representations spatially contain their semantic constituents. However, in other important respects, the architecture is distinctly non-classical Additional links for this entry: http://portal.acm.org/citation.cfm?id=596704.596727 http://www.springerlink.com/content/k4l352163l5777l2/fulltext.pdf http://www.springerlink.com/index/K4L352163L5777L2.pdf http://www.ingentaconnect.com/content/klu/mind/1997/00000007/00000001/00112501 Haselager, W. F. G. Van Rappard, J. F. H. (1998). Connectionism, systematicity, and the frame problem. Minds and Machines 8 (2):161-179. ( Cited by 11 | Google | More links ) Abstract:   This paper investigates connectionism's potential to solve the frame problem. The frame problem arises in the context of modelling the human ability to see the relevant consequences of events in a situation. It has been claimed to be unsolvable for classical cognitive science, but easily manageable for connectionism. We will focus on a representational approach to the frame problem which advocates the use of intrinsic representations. We argue that although connectionism's distributed representations may look promising from this perspective, doubts can be raised about the potential of distributed representations to allow large amounts of complexly structured information to be adequately encoded and processed. It is questionable whether connectionist models that are claimed to effectively represent structured information can be scaled up to a realistic extent. We conclude that the frame problem provides a difficulty to connectionism that is no less serious than the obstacle it constitutes for classical cognitive science Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=371129CI http://www.springerlink.com/content/um0x474450532866/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=150466=1 http://www.springerlink.com/index/UM0X474450532866.pdf http://www.ingentaconnect.com/content/klu/mind/1998/00000008/00000002/00150466 Hawthorne, John (1989). On the compatibility of connectionist and classical models. Philosophical Psychology 2 (1):5-16. ( Cited by 9 | Annotation | Google ) Localist connectionist models may not be able to handle structured presentation, but appropriate distributed models can. Abstract: This paper presents considerations in favour of the view that traditional (classical) architectures can be seen as emergent features of connectionist networks with distributed representation. A recent paper by William Bechtel (1988) which argues for a similar conclusion is unsatisfactory in that it fails to consider whether the compositional syntax and semantics attributed to mental representations by classical models can emerge within a connectionist network. The compatibility of the two paradigms hinges largely, I suggest, on how this question is answered. Focusing on the issue of syntax, I argue that while such structure is lacking in connectionist models with local representation, it can be accommodated within networks where representation is distributed. I discuss an important paper by Smolenski (1988) which attempts to show how connectionists can incorporate the relevant syntactic structure, suggesting that some criticisms levelled against that paper by Fodor & Pylyshyn (1988) are wanting. I then go on to indicate a strategy by which a compositional syntax and semantics can be defined for the sort of network that Smolenski describes. I conclude that since the connectionist can respect the central tenets of classicism, the two approaches are compatible with one another Horgan, Terence E. Tienson, John L. (1991). Structured representations in connectionist systems? In S. Davis (ed.), Connectionism: Theorye and Practice . Oup. ( Cited by 9 | Annotation | Google ) A discussion of how connectionism might achieve "effective syntax" without implementing a classical system. Macdonald, Cynthia (1995). Classicism V connectionism. In C. Macdonald Graham F. Macdonald (eds.), Connectionism: Debates on Psychological Explanation . Cambridge: Blackwell. ( Google ) Matthews, Robert J. (1997). Can connectionists explain systematicity? Mind and Language 12 (2):154-77. ( Cited by 9 | Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/119154898/PDFSTART http://www.blackwell-synergy.com/links/doi/10.1111/1468-0017.00041 http://www.blackwell-synergy.com/doi/abs/10.1111/1468-0017.00041 http://www.ingentaconnect.com/content/bpl/mila/1997/00000012/00000002/art00003 http://www.ingentaconnect.com/content/bpl/mila/1997/00000012/00000002/art00041 Matthews, Robert J. (1994). Three-concept Monte: Explanation, implementation, and systematicity. Synthese 101 (3):347-63. ( Cited by 12 | Annotation | Google | More links ) F deal a sucker bet: on their terms, connectionism could never give a a non-implementational explanation of systematicity, as the notions are construed in a manner specific to classical architectures. Abstract:   Fodor and Pylyshyn (1988), Fodor and McLaughlin (1990) and McLaughlin (1993) challenge connectionists to explain systematicity without simply implementing a classical architecture. In this paper I argue that what makes the challenge difficult for connectionists to meet has less to do with what is to be explained than with what is to count as an explanation. Fodor et al. are prepared to admit as explanatory, accounts of a sort that only classical models can provide. If connectionists are to meet the challenge, they are going to have to insist on the propriety of changing what counts as an explanation of systematicity. Once that is done, there would seem to be as yet no reason to suppose that connectionists are unable to explain systematicity Additional links for this entry: http://www.springerlink.com/index/L3656473H0382735.pdf McLaughlin, Brian P. (1992). Systematicity, conceptual truth, and evolution. Philosophy and the Cognitive Sciences 34:217-234. ( Cited by 13 | Annotation | Google ) Against responses to Fodor and Pylyshyn claiming that cognitive theories needn't explain systematicity. Contra Clark, the conceptual truth of systematicity won't help. Contra others, nor will evolution. McLaughlin, Brian P. (1993). The connectionism/classicism battle to win souls. Philosophical Studies 71 (2):163-190. ( Cited by 19 | Annotation | Google | More links ) Argues that no connectionist model so far has come close to explaining systematicity. Considers the models of Elman, Chalmers, and Smolensky. Additional links for this entry: http://www.springerlink.com/index/N20X21753V2565J1.pdf Niklasson, L. F. van Gelder, Tim (1994). On being systematically connectionist. Mind and Language 9:288-302. ( Cited by 42 | Google | More links ) Abstract: In 1988 Fodor and Pylyshyn issued a challenge to the newly-popular connectionism: explain the systematicity of cognition without merely implementing a so-called classical architecture. Since that time quite a number of connectionist models have been put forward, either by their designers or by others, as in some measure demonstrating that the challenge can be met (e.g., Pollack, 1988, 1990; Smolensky, 1990; Chalmers, 1990; Niklasson and Sharkey, 1992; Brousse, 1993). Unfortu- nately, it has generally been unclear whether these models actually do have this implication (see, for instance, the extensive philosophical debate in Smolensky, 1988; Fodor and McLaughlin, 1990; van Gelder, 1990, 1991; McLaughlin, 1993a, 1993b; Clark, 1993). Indeed, we know of no major supporter of classical orthodoxy who has felt compelled, by connectionist models and argu- ments, to concede in print that connectionists have in fact delivered a non-classical explanation of systematicity Additional links for this entry: http://citeseer.ist.psu.edu/niklasson94being.html http://www.blackwell-synergy.com/doi/abs/10.1111/j.1468-0017.1994.tb00227.x Petersen, Steven E. Roskies, Adina L. (2001). Visualizing human brain function. In E. Bizzi, P. Calissano V. Volterra (eds.), Frontiers of Life, Vol III: The Intelligent Systems, Part One: The Brain of Homo Sapiens . Academic Press. ( Google ) Abstract: Running head: Functional neuroimaging Abstract Several recently developed techniques enable the investigation of the neural basis of cognitive function in the human brain. Two of these, PET and fMRI, yield whole-brain images reflecting regional neural activity associated with the performance of specific tasks. This article explores the spatial and temporal capabilities and limitations of these techniques, and discusses technical, biological, and cognitive issues relevant to understanding the goals and methods of neuroimaging studies. The types of advances in understanding cognitive and brain function made possible with these methods are illustrated with examples from the neuroimaging literature Phillips, Stephen H. (2002). Does classicism explain universality? Minds and Machines 12 (3):423-434. ( Cited by 1 | Google | More links ) Abstract:   One of the hallmarks of human cognition is the capacity to generalize over arbitrary constituents. Recently, Marcus (1998, 1998a, b; Cognition 66, p. 153; Cognitive Psychology 37, p. 243) argued that this capacity, called universal generalization (universality), is not supported by Connectionist models. Instead, universality is best explained by Classical symbol systems, with Connectionism as its implementation. Here it is argued that universality is also a problem for Classicism in that the syntax-sensitive rules that are supposed to provide causal explanations of mental processes are either too strict, precluding possible generalizations; or too lax, providing no information as to the appropriate alternative. Consequently, universality is not explained by a Classical theory Additional links for this entry: http://www.springerlink.com/content/9lycmrkn91j71qqe/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=408595=1 http://www.springerlink.com/index/9LYCMRKN91J71QQE.pdf Plate, Tony A. (2003). Holographic Reduced Representation: Distributed Representation for Cognitive Structures. Center for the Study of Language and Information. (Cited by 18 | Google ) Pollack, Jordan B. (1990). Recursive distributed representations. Artificial Intelligence 46:77-105. ( Cited by 539 | Annotation | Google | More links ) Develops a connectionist architecture -- recursive auto-associative memory -- that can recursively represent compositional structures in distributed form. Additional links for this entry: http://demo.cs.brandeis.edu/papers/raam.pdf http://portal.acm.org/citation.cfm?id=102423 http://www.ing.unisi.it/~monica/Paper/p19.ps.gz http://www.uow.edu.au/~markus/apods/doc/papers/raam.ps http://www.blutner.de/philom/connect/pollack90recursive.pdf http://www.blutner.de/NeuralNets/Texts/pollack90recursive.pdf http://www.idi.ntnu.no/emner/it3708/archive/2004/files/raam.pdf http://ftp.uni-koeln.de/neural-nets/papers/pollack.newraam.ps.gz http://www.cse.ohio-state.edu/~jj/pub/papers/89-JP-RECURSIVE.ps.Z http://www.umiacs.umd.edu/~ymarton/ling849/pollack90recursive.pdf http://ftp.funet.fi/pub/sci/neural/neuroprose/pollack.newraam.ps.Z http://www.funet.fi/pub/sci/neural/neuroprose/pollack.newraam.ps.Z http://www.ftp.funet.fi/ftp/pub/sci/ai/neural/neuroprose/pollack.newraam.ps.Z http://www.csa.com/partners/viewrecord.php?requester=gs=2470980CI Rowlands, Mark (1994). Connectionism and the language of thought. British Journal for the Philosophy of Science 45 (2):485-503. ( Annotation | Google | More links ) F's argument confuses constituent structure with logical/sentential structure. Connectionism is a psychotechtonic project, whereas propositional description is a psychosemantic project. Abstract: In an influential critique, Jerry Fodor and Zenon Pylyshyn point to the existence of a potentially devastating dilemma for connectionism (Fodor and Pylyshyn [1988]). Either connectionist models consist in mere associations of unstructured representations, or they consist in processes involving complex representations. If the former, connectionism is mere associationism, and will not be capable of accounting for very much of cognition. If the latter, then connectionist models concern only the implementation of cognitive processes, and are, therefore, not informative at the level of cognition. I shall argue that Fodor and Pylyshyn's argument is based on a crucial misunderstanding, the same misunderstanding which motivates the entire language of thought hypothesis Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/abstract/45/2/485 http://bjps.oxfordjournals.org/cgi/reprint/45/2/485 http://www.jstor.org/stable/pdfplus/687678.pdf Schroder, Jurgen (1998). Knowledge of rules, causal systematicity, and the language of thought. Synthese 117 (3):313-330. ( Google | More links ) Abstract:   Martin Davies' criterion for the knowledge of implicit rules, viz. the causal systematicity of cognitive processes, is first exposed. Then the inference from causal systematicity of a process to syntactic properties of the input states is examined. It is argued that Davies' notion of a syntactic property is too weak to bear the conclusion that causal systematicity implies a language of thought as far as the input states are concerned. Next, it is shown that Davies' criterion leads to a counterintuitive consequence: it groups together distributed connectionist systems with look-up tables. To avoid this consequence, a modified construal of causal systematicity is proposed and Davies' argument for the causal systematicity of thought is shown to be question-begging. It is briefly sketched how the modified construal links up with multiple dispositions of the same categorical base. Finally, the question of the causal efficacy of single rules is distinguished from the question of their psychological reality: implicit rules might be psychologically real without being causally efficacious Additional links for this entry: http://www.kluweronline.com/article.asp?PIPS=188458=1 http://www.springerlink.com/index/W8735T5747313V32.pdf http://www.ingentaconnect.com/content/klu/synt/1998/00000117/00000003/00188458 Smolensky, Paul (1991). Connectionism, constituency and the language of thought. In Barry M. Loewer Georges Rey (eds.), Meaning in Mind: Fodor and His Critics . Blackwell. ( Cited by 68 | Annotation | Google ) Connectionism can do compositionality its own way, including both weak compositionality (with context effects) or strong compositionality (via tensor products). Smolensky, Paul (1995). Constituent structure and explanation in an integrated connectionist/symbolic cognitive architecture. In C. Macdonald (ed.), Connectionism: Debates on Psychological Explanation . Blackwell. ( Cited by 51 | Google ) Smolensky, Paul (1987). The constituent structure of connectionist mental states. Southern Journal of Philosophy Supplement 26:137-60. ( Cited by 2 | Annotation | Google ) F ignore distributed representation and interaction effects. Smolensky, Paul (1990). Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence 46:159-216. ( Cited by 335 | Annotation | Google | More links ) Develops a connectionist architecture that represents compositional structures as tensor products of distributed representations. Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=2470919CI van Gelder, Tim (1990). Compositionality: A connectionist variation on a classical theme. Cognitive Science 14:355-84. ( Cited by 187 | Annotation | Google | More links ) Connectionism can do compositionality functionally. All one needs is the right functional relation between representations; physical concatenation is not necessary. Additional links for this entry: http://www.leaonline.com/doi/pdfplus/10.1207/s15516709cog1403_2 http://www.cogsci.rpi.edu/CSJarchive/1990v14/i03/p0355p0384/MAIN.PDF van Gelder, Tim (online). Can connectionist models exhibit non-classical structure sensitivity? ( Google | More links ) Abstract: Department of Computer Science Philosophy Program, Research School of Social Sciences University of Skövde, S-54128, SWEDEN Australian National University, Canberra ACT 0200 Additional links for this entry: http://citeseer.ist.psu.edu/niklasson94can.html van Gelder, Tim (1991). Classical questions, radical answers. In Terence E. Horgan John L. Tienson (eds.), Connectionism and the Philosophy of Mind . Kluwer. ( Cited by 20 | Annotation | Google ) On connectionism as a Kuhnian paradigm shift in cognitive science, with emphasis on the implications of functional compositionality and distributed representations. van Gelder, Tim (1994). On being systematically connectionist. Mind and Language 9:288-30. ( Cited by 2 | Google | More links ) Abstract: In 1988 Fodor and Pylyshyn issued a challenge to the newly-popular connectionism: explain the systematicity of cognition without merely implementing a so-called classical architecture. Since that time quite a number of connectionist models have been put forward, either by their designers or by others, as in some measure demonstrating that the challenge can be met (e.g., Pollack, 1988, 1990; Smolensky, 1990; Chalmers, 1990; Niklasson and Sharkey, 1992; Brousse, 1993). Unfortu- nately, it has generally been unclear whether these models actually do have this implication (see, for instance, the extensive philosophical debate in Smolensky, 1988; Fodor and McLaughlin, 1990; van Gelder, 1990, 1991; McLaughlin, 1993a, 1993b; Clark, 1993). Indeed, we know of no major supporter of classical orthodoxy who has felt compelled, by connectionist models and argu- ments, to concede in print that connectionists have in fact delivered a non-classical explanation of systematicity Additional links for this entry: http://citeseer.ist.psu.edu/niklasson94being.html Waskan, Jonathan A. Bechtel, William P. (1997). Directions in connectionist research: Tractable computations without syntactically structured representations. Metaphilosophy 28 (1-2):31-62. ( Cited by 1 | Google | More links ) Abstract: Figure 1 : A pr ototyp ical exa mple of a three-layer feed forward network, used by Plunkett and M archm an (1 991 ) to simulate learning the past-tense of En glish verbs. The inpu t units encode representations of the three phonemes of the present tense of the artificial words used in this simulation. Th e netwo rk is trained to produce a representation of the phonemes employed in the past tense form and the suffix (/d/, /ed/, or /t/) used on regular verbs. To run the network, each input unit is assigned an activation value o f 0 or 1 , dep ending on whethe r the featu re is present or not. Eac h input unit is conne cted to each of the 30 hidden units by a we ighted conn ection and p rovid es an inp ut to each hidden unit equal to the product of the input unit's activation and the weight. Each hidd en unit's activation is then determined by summing ov er the va lues co ming fro m each inp ut unit to deter mine a netinput, and then applying a non-linear function (e.g., the logistic function 1/(1+e netinput ). Th is whole proced ure is Additional links for this entry: http://www.blackwell-synergy.com/links/doi/10.1111/1467-9973.00040 http://www.blackwell-synergy.com/doi/abs/10.1111/1467-9973.00040 http://www.ingentaconnect.com/content/bpl/meta/1997/00000028/F0020001/art00002 http://www.ingentaconnect.com/content/bpl/meta/1997/00000028/f0020001 Young, Robert M. (1970). Mind, Brain and Adaptation. ( Cited by 7 | Google | More links ) Additional links for this entry: http://human-nature.com/mba/mba1.html http://human-nature.com/mba/chap1.html 6.3b Representation in Connectionism Bechtel, William P. (1994). Natural deduction in connectionist systems. Synthese 101 (3):433-463. ( Cited by 7 | Google | More links ) Abstract:   The relation between logic and thought has long been controversial, but has recently influenced theorizing about the nature of mental processes in cognitive science. One prominent tradition argues that to explain the systematicity of thought we must posit syntactically structured representations inside the cognitive system which can be operated upon by structure sensitive rules similar to those employed in systems of natural deduction. I have argued elsewhere that the systematicity of human thought might better be explained as resulting from the fact that we have learned natural languages which are themselves syntactically structured. According to this view, symbols of natural language are external to the cognitive processing system and what the cognitive system must learn to do is produce and comprehend such symbols. In this paper I pursue that idea by arguing that ability in natural deduction itself may rely on pattern recognition abilities that enable us to operate on external symbols rather than encodings of rules that might be applied to internal representations. To support this suggestion, I present a series of experiments with connectionist networks that have been trained to construct simple natural deductions in sentential logic. These networks not only succeed in reconstructing the derivations on which they have been trained, but in constructing new derivations that are only similar to the ones on which they have been trained Additional links for this entry: http://citeseer.ist.psu.edu/bechtel94natural.html http://www.blutner.de/NeuralNets/Texts/DERIVATI.pdf http://mechanism.ucsd.edu/~bill/research/DERIVATI.pdf http://www.springerlink.com/content/q71158t2342jl517/fulltext.pdf http://www.springerlink.com/index/Q71158T2342JL517.pdf Butler, Keith (1995). Representation and computation in a deflationary assessment of connectionist cognitive science. Synthese 104 (1):71-97. ( Google | More links ) Abstract:   Connectionism provides hope for unifying work in neuroscience, computer science, and cognitive psychology. This promise has met with some resistance from Classical Computionalists, which may have inspired Connectionists to retaliate with bold, inflationary claims on behalf of Connectionist models. This paper demonstrates, by examining three intimately connected issues, that these inflationary claims made on behalf of Connectionism are wrong. This should not be construed as an attack on Connectionism, however, since the inflated claims made on its behalf have the look of cures for which there are no ailments. There is nothing wrong with Connectionism for its failure to solve illusory problems Additional links for this entry: http://www.springerlink.com/index/R347125074346147.pdf Calvo Garzón, Francisco (2000). A connectionist defence of the inscrutability thesis. Mind and Language 15 (5):465-480. ( Google ) Calvo Garzón, Francisco (2003). Connectionist semantics and the collateral information challenge. Mind and Language 18 (1):77-94. ( Google ) Calvo Garzón, Francisco (2000). State space semantics and conceptual similarity: Reply to Churchland. Philosophical Psychology 13 (1):77-95. ( Google ) Abstract: Jerry Fodor and Ernest Lepore [(1992) Holism: a shopper's guide, Oxford: Blackwell; (1996) in R. McCauley (Ed.) The Churchlands and their critics , Cambridge: Blackwell] have launched a powerful attack against Paul Churchland's connectionist theory of semantics--also known as state space semantics. In one part of their attack, Fodor and Lepore argue that the architectural and functional idiosyncrasies of connectionist networks preclude us from articulating a notion of conceptual similarity applicable to state space semantics. Aarre Laakso and Gary Cottrell [(1998) in M. A. Gernsbacher & S. Derry (Eds) Proceedings of the 20th Annual Conference of the Cognitive Science Society, Mahway, NJ: Erlbaum; Philosophical Psychology ] 13, 47-76 have recently run a number of simulations on simple feedforward networks and applied a mathematical technique for measuring conceptual similarity in the representational spaces of those networks. Laakso and Cottrell contend that their results decisively refute Fodor and Lepore's criticisms. Paul Churchland [(1998) Journal of Philosophy, 95, 5-32 ] goes further. He uses Laakso and Cottrell's neurosimulations to argue that connectionism does furnish us with all we need to construct a robust theory of semantics and a robust theory of translation. In this paper I shall argue that whereas Laakso and Cottrell's neurocomputational results may provide us with a rebuttal of Fodor and Lepore's argument, Churchland's conclusion is far too optimistic. In particular, I shall try to show that connectionist modelling does not provide any objective criterion for achieving a one-to-one accurate translational mapping across networks Cilliers, F. P. (1991). Rules and relations: Some connectionist implications for cognitive science and language. South African Journal of Philosophy 49 (May):49-55. ( Cited by 1 | Google ) Clark, Andy (1993). Associative Engines: Connectionism, Concepts, and Representational Change. MIT Press. ( Cited by 222 | Google | More links ) Additional links for this entry: http://portal.acm.org/citation.cfm?id=164767 http://www.reiters.com/index.cgi?ISBN=0262032104=p Clark, Andy (ms). Connectionism, nonconceptual content, and representational redescription. ( Annotation | Google ) On some troubles connectionism has with higher-order knowledge. Contrasts Cussins, Karmiloff-Smith on development. Subsymbols without symbols are blind. Clark, Andy Karmiloff-Smith, Annette (1994). The cognizer's innards: A psychological and philosophical perspective on the development of thought. Mind and Language 8 (4):487-519. ( Cited by 196 | Annotation | Google | More links ) On the importance of representational redescription, and on the limits of connectionist networks in cross-domain knowledge transfer. What does a true believer need, above behavior: conceptual combination, real-world fluency? Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/119302072/PDFSTART Cummins, Robert E. (1991). The role of representation in connectionist explanation of cognitive capacities. In William Ramsey, Stephen P. Stich D. Rumelhart (eds.), Philosophy and Connectionist Theory . Lawrence Erlbaum. ( Cited by 8 | Annotation | Google ) Connectionism isn't really radical. There's no new concept of representation or of learning, and cognition can still be the manipulation of semantically structured representations. Cussins, Adrian (1990). The connectionist construction of concepts. In Margaret A. Boden (ed.), The Philosophy of AI . Oxford University Press. ( Cited by 107 | Annotation | Google ) Connectionism builds up concepts from the nonconceptual level. From nonconceptual content (e.g. perceptual experiences) to the emergence of objectivity. Abstract: The character of computational modelling of cognition depends on an underlying theory of representation. Classical cognitive science has exploited the syntax/semantics theory of representation that derives from logic. But this has had the consequence that the kind of psychological explanation supported by classical cognitive science is _conceptualist_: psychological phenomena are modelled in terms of relations that hold between concepts, and between the sensors/effectors and concepts. This kind of explanation is inappropriate for the Proper Treatment of Connectionism (Smolensky 1988) Eliasmith, Chris (online). Structure without symbols: Providing a distributed account of high-level cognition. ( Google ) Abstract: There has been a long-standing debate between symbolicists and connectionists concerning the nature of representation used by human cognizers. In general, symbolicist commitments have allowed them to provide superior models of high-level cognitive function. In contrast, connectionist distributed representations are preferred for providing a description of low-level cognition. The development of Holographic Reduced Representations (HRRs) has opened the possibility of one representational medium unifying both low-level and high-level descriptions of cognition. This paper describes the relative strengths and weaknesses of symbolic and distributed representations. HRRs are shown to capture the important strengths of both types of representation. These properties of HRRs allow a rebuttal of Fodor and McLaughlin's (1990) criticism that distributed representations are not adequately structure sensitive to provide a full account of human cognition Garzon, Francisco Calvo (2000). A connectionist defence of the inscrutability thesis. Mind and Language 15 (5):465-480. ( Cited by 4 | Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/119041838/PDFSTART http://www.blackwell-synergy.com/doi/abs/10.1111/1468-0017.00145 http://www.ingentaconnect.com/content/bpl/mila/2000/00000015/00000005/art00145 Garzon, Francisco Calvo (2000). State space semantics and conceptual similarity: Reply to Churchland. Philosophical Psychology 13 (1):77-96. ( Cited by 8 | Google | More links ) Abstract: Jerry Fodor and Ernest Lepore [(1992) Holism: a shopper's guide, Oxford: Blackwell; (1996) in R. McCauley (Ed.) The Churchlands and their critics , Cambridge: Blackwell] have launched a powerful attack against Paul Churchland's connectionist theory of semantics--also known as state space semantics. In one part of their attack, Fodor and Lepore argue that the architectural and functional idiosyncrasies of connectionist networks preclude us from articulating a notion of conceptual similarity applicable to state space semantics. Aarre Laakso and Gary Cottrell [(1998) in M. A. Gernsbacher & S. Derry (Eds) Proceedings of the 20th Annual Conference of the Cognitive Science Society, Mahway, NJ: Erlbaum; Philosophical Psychology ] 13, 47-76 have recently run a number of simulations on simple feedforward networks and applied a mathematical technique for measuring conceptual similarity in the representational spaces of those networks. Laakso and Cottrell contend that their results decisively refute Fodor and Lepore's criticisms. Paul Churchland [(1998) Journal of Philosophy, 95, 5-32 ] goes further. He uses Laakso and Cottrell's neurosimulations to argue that connectionism does furnish us with all we need to construct a robust theory of semantics and a robust theory of translation. In this paper I shall argue that whereas Laakso and Cottrell's neurocomputational results may provide us with a rebuttal of Fodor and Lepore's argument, Churchland's conclusion is far too optimistic. In particular, I shall try to show that connectionist modelling does not provide any objective criterion for achieving a one-to-one accurate translational mapping across networks Additional links for this entry: http://taylorandfrancis.metapress.com/index/U2D6G36BN8KQEMXH.pdf http://www.informaworld.com/index/U2D6G36BN8KQEMXH.pdf http://www.ingentaconnect.com/content/routledg/cphp/2000/00000013/00000001/art00003 Gauker, Christopher (2007). A critique of the similarity space theory of concepts. Mind and Language 22 (4):317345. ( Google | More links ) Abstract: A similarity space is a hyperspace in which the dimensions represent various dimensions on which objects may differ. The similarity space theory of concepts is the thesis that concepts are regions of similarity spaces that are somehow realized in the brain. Proponents of such a theory of concepts include Paul Churchland and Peter Gärdenfors. This paper argues that the similarity space theory of concepts is mistaken because regions of similarity spaces cannot serve as the components of judgments. It emerges that although similarity spaces cannot model concepts, they may model a kind of nonconceptual representation Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/117998149/PDFSTART Goschke, T. Koppelberg, Dirk (1990). Connectionism and the semantic content of internal representation. Review of International Philosophy 44 (172):87-103. ( Google ) Goschke, T. Koppelberg, Dirk (1991). The concept of representation and the representation of concepts in connectionist models. In William Ramsey, Stephen P. Stich D. Rumelhart (eds.), Philosophy and Connectionist Theory . Lawrence Erlbaum. ( Cited by 17 | Annotation | Google ) On correlational semantics and context-dependent representations. Hadley, Robert F. (2004). On the proper treatment of semantic systematicity. Minds and Machines 14 (2):145-172. ( Cited by 7 | Google | More links ) Abstract:   The past decade has witnessed the emergence of a novel stance on semantic representation, and its relationship to context sensitivity. Connectionist-minded philosophers, including Clark and van Gelder, have espoused the merits of viewing hidden-layer, context-sensitive representations as possessing semantic content, where this content is partially revealed via the representations'' position in vector space. In recent work, Bodén and Niklasson have incorporated a variant of this view of semantics within their conception of semantic systematicity. Moreover, Bodén and Niklasson contend that they have produced experimental results which not only satisfy a kind of context-based, semantic systematicity, but which, to the degree that reality permits, effectively deals with challenges posed by Fodor and Pylyshyn (1988), and Hadley (1994a). The latter challenge involved well-defined criteria for strong semantic systematicity. This paper examines the relevant claims and experiments of Bodén and Niklasson. It is argued that their case fatally involves two fallacies of equivocation; one concerning ''semantic content'' and the other concerning ''novel test sentences''. In addition, it is argued that their ultimate construal of context sensitive semantics contains serious confusions. These confusions are also found in certain publications dealing with "latent semantic analysis". Thus, criticisms presented here have relevance beyond the work of Bodén and Niklasson Additional links for this entry: http://www.springerlink.com/content/p4312417q27100p0/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=5142070=1 http://www.springerlink.com/index/P4312417Q27100P0.pdf http://www.ingentaconnect.com/content/klu/mind/2004/00000014/00000002/05142070 Haselager, W. F. G. (1999). On the potential of non-classical constituency. Acta Analytica 22 (22):23-42. ( Cited by 4 | Google | More links ) Additional links for this entry: http://www.nici.kun.nl/~haselag/publications/1999c.pdf http://www.socsci.kun.nl/~haselag/publications/1999c.pdf http://www.nici.ru.nl/~haselag/publications/NonClassConst99.pdf http://www.nici.kun.nl/~haselag/publications/NonClassConst99.pdf http://www.socsci.kun.nl/~haselag/publications/NonClassConst99.pdf Hatfield, Gary (1991). Representation and rule-instantiation in connectionist systems. In Terence E. Horgan John L. Tienson (eds.), Connectionism and the Philosophy of Mind . Kluwer. ( Cited by 11 | Annotation | Google ) Some remarks on psychology & physiology. Even connectionism uses psychological concepts. Hatfield, Gary (1991). Representation in perception and cognition: Connectionist affordances. In William Ramsey, Stephen P. Stich D. Rumelhart (eds.), Philosophy and Connectionist Theory . Lawrence Erlbaum. ( Cited by 49 | Google ) Haybron, Daniel M. (2000). The causal and explanatory role of information stored in connectionist networks. Minds and Machines 10 (3):361-380. ( Cited by 2 | Google | More links ) Abstract:   In this paper I defend the propriety of explaining the behavior of distributed connectionist networks by appeal to selected data stored therein. In particular, I argue that if there is a problem with such explanations, it is a consequence of the fact that information storage in networks is superpositional, and not because it is distributed. I then develop a ``proto-account'''' of causation for networks, based on an account of Andy Clark''s, that shows even superpositionality does not undermine information-based explanation. Finally, I argue that the resulting explanations are genuinely informative and not vacuous Additional links for this entry: http://www.slu.edu/users/users/haybrond/Connectionist information storage.pdf http://www.csa.com/partners/viewrecord.php?requester=gs=536479CI http://www.springerlink.com/content/content/t061056966k44051/fulltext.pdf http://www.springerlink.com/content/t061056966k44051/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=258827=1 http://www.springerlink.com/index/T061056966K44051.pdf http://www.ingentaconnect.com/content/klu/mind/2000/00000010/00000003/00258827 Laakso, Aarre Cottrell, Garrison W. (2000). Content and cluster analysis: Assessing representational similarity in neural systems. Philosophical Psychology 13 (1):47-76. ( Cited by 18 | Google | More links ) Abstract: If connectionism is to be an adequate theory of mind, we must have a theory of representation for neural networks that allows for individual differences in weighting and architecture while preserving sameness, or at least similarity, of content. In this paper we propose a procedure for measuring sameness of content of neural representations. We argue that the correct way to compare neural representations is through analysis of the distances between neural activations, and we present a method for doing so. We then use the technique to demonstrate empirically that different artificial neural networks trained by backpropagation on the same categorization task, even with different representational encodings of the input patterns and different numbers of hidden units, reach states in which representations at the hidden units are similar. We discuss how this work provides a rebuttal to Fodor and Lepore's critique of Paul Churchland's state space semantics Additional links for this entry: http://www.laakshmi.com/aarre/cv/phipsy.pdf http://www.cs.ucsd.edu/groups/guru/docs/laakso2000.pdf http://www-cse.ucsd.edu/groups/guru/docs/laakso2000.pdf http://www.cse.ucsd.edu/groups/guru/docs/laakso2000.pdf http://www.informaworld.com/smpp/./ftinterface~content=a713690421~fulltext=713240930 http://taylorandfrancis.metapress.com/index/RY0JBTH3PY9B6GF2.pdf http://www.informaworld.com/index/RY0JBTH3PY9B6GF2.pdf http://www.ingentaconnect.com/content/routledg/cphp/2000/00000013/00000001/art00002 Lormand, Eric (ms). Connectionist content. ( Google ) Mandik, Pete (2003). Varieties of representation in evolved and embodied neural networks. Biology and Philosophy 18 (1):95-130. ( Cited by 6 | Google | More links ) Abstract:   In this paper I discuss one of the key issuesin the philosophy of neuroscience:neurosemantics. The project of neurosemanticsinvolves explaining what it means for states ofneurons and neural systems to haverepresentational contents. Neurosemantics thusinvolves issues of common concern between thephilosophy of neuroscience and philosophy ofmind. I discuss a problem that arises foraccounts of representational content that Icall ``the economy problem'': the problem ofshowing that a candidate theory of mentalrepresentation can bear the work requiredwithin in the causal economy of a mind and anorganism. My approach in the current paper isto explore this and other key themes inneurosemantics through the use of computermodels of neural networks embodied and evolvedin virtual organisms. The models allow for thelaying bare of the causal economies of entireyet simple artificial organisms so that therelations between the neural bases of, forinstance, representation in perception andmemory can be regarded in the context of anentire organism. On the basis of thesesimulations, I argue for an account ofneurosemantics adequate for the solution of theeconomy problem Additional links for this entry: http://citeseer.ist.psu.edu/mandik02varieties.html http://www.cogs.indiana.edu/cogx/Mandik_var_rep_03.pdf http://www.petemandik.com/philosophy/papers/varrep.pdf http://testweb.wpunj.edu/cohss/philosophy/FACULTY/mandik/papers/vreenn.pdf http://www.springerlink.com/content/m8023w0720l50p81/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=5109389=1 http://www.springerlink.com/index/M8023W0720L50P81.pdf http://www.ingentaconnect.com/content/klu/biph/2003/00000018/00000001/05109389 Markic, Olga (1995). Finding the right level for connectionist representations (a critical note on Ramsey's paper). Acta Analytica 14 (14):27-35. ( Google ) O'Brien, Gerard (1989). Connectionism, analogicity and mental content. Acta Analytica 22 (22):111-31. ( Google | More links ) Abstract: In Connectionism and the Philosophy of Psychology, Horgan and Tienson (1996) argue that cognitive processes, pace classicism, are not governed by exceptionless, representation-level rules; they are instead the work of defeasible cognitive tendencies subserved by the non-linear dynamics of the brains neural networks. Many theorists are sympathetic with the dynamical characterisation of connectionism and the general (re)conception of cognition that it affords. But in all the excitement surrounding the connectionist revolution in cognitive science, it has largely gone unnoticed that connectionism adds to the traditional focus on computational processes, a new focus  one on the vehicles of mental representation, on the entities that carry content through the mind. Indeed, if Horgan and Tiensons dynamical characterisation of connectionism is on the right track, then so intimate is the relationship between computational processes and representational vehicles, that connectionist cognitive science is committed to a resemblance theory of mental content Additional links for this entry: http://cogprints.org/1675/3/Connectionism_Analogicity_and_Mental_Content.pdf http://cogprints.org/1675/0/Connectionism_Analogicity_and_Mental_Content.pdf O'Brien, Gerard Opie, Jonathan (2004). Notes toward a structuralist theory of mental representation. In Hugh Clapin (ed.), Representation in Mind . Elsevier. ( Google ) Abstract: Any creature that must move around in its environment to find nutrients and mates, in order to survive and reproduce, faces the problem of sensorimotor control. A solution to this problem requires an on-board control mechanism that can shape the creature’s behaviour so as to render it “appropriate” to the conditions that obtain. There are at least three ways in which such a control mechanism can work, and Nature has exploited them all. The first and most basic way is for a creature to bump into the things in its environment, and then, depending on what has been encountered, seek to modify its behaviour accordingly. Such an approach is risky, however, since some things in the environment are distinctly unfriendly. A second and better way, therefore, is for a creature to exploit ambient forms of energy that carry information about the distal structure of the environment. This is an improvement on the first method since it enables the creature to respond to the surroundings without actually bumping into anything. Nonetheless, this second method also has its limitations, one of which is that the information conveyed by such ambient energy is often impoverished, ambiguous and intermittent Place, Ullin T. (1989). Toward a connectionist version of the causal theory of reference. Acta Analytica 4 (5):71-97. ( Google ) Potrc, Matjaz (1999). Morphological content. Acta Analytica 22 (22):133-149. ( Google ) Prinz, Jesse J. (2006). Empiricism and state-space semantics. In Brian L Keeley (ed.), Paul Churchland . Cambridge: Cambridge University Press. ( Google ) Ramsey, William (1997). Do connectionist representations earn their explanatory keep? Mind and Language 12 (1):34-66. ( Cited by 16 | Annotation | Google | More links ) Argues that talk of representations has no explanatory role in connectionist theory, and can be discarded. It can't be understood along the lines of the teleo-informational or classical frameworks. Additional links for this entry: http://www.blackwell-synergy.com/doi/abs/10.1111/1468-0017.00035 http://www.ingentaconnect.com/content/bpl/mila/1997/00000012/00000001/art00035 http://www.ingentaconnect.com/content/bpl/mila/1997/00000012/00000001/art00003 Ramsey, William (1995). Rethinking distributed representation. Acta Analytica 10 (14):9-25. ( Cited by 1 | Google ) Schopman, Joop Shawky, A. (1999). Remarks on the impact of connectionism on our thinking about concepts. In Peter Millican A. Clark (eds.), Connectionism, Concepts and Folk Psychology . Oxford University Press. ( Google ) Shea, Nicholas (2007). Content and its vehicles in connectionist systems. Mind and Language 22 (3):246–269. ( Google | More links ) Abstract: This paper advocates explicitness about the type of entity to be considered as content- bearing in connectionist systems; it makes a positive proposal about how vehicles of content should be individuated; and it deploys that proposal to argue in favour of representation in connectionist systems. The proposal is that the vehicles of content in some connectionist systems are clusters in the state space of a hidden layer. Attributing content to such vehicles is required to vindicate the standard explanation for some classificatory networks’ ability to generalise to novel samples their correct classification of the samples on which they were trained Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/117998145/PDFSTART http://www.ingentaconnect.com/content/bpl/mila/2007/00000022/00000003/art00002 Stone, Tony Davies, Martin (2000). Autonomous psychology and the moderate neuron doctrine. Behavioral and Brain Sciences 22 (5):849-850. ( Cited by 4 | Google | More links ) Abstract: _Two notions of autonomy are distinguished. The respective_ _denials that psychology is autonomous from neurobiology are neuron_ _doctrines, moderate and radical. According to the moderate neuron_ _doctrine, inter-disciplinary interaction need not aim at reduction. It is_ _proposed that it is more plausible that there is slippage from the_ _moderate to the radical neuron doctrine than that there is confusion_ _between the radical neuron doctrine and the trivial version._ Additional links for this entry: http://philrsss.anu.edu.au/~mdavies/papers/aut.pdf http://journals.cambridge.org/action/displayFulltext?type=1=31693 http://www.journals.cambridge.org/abstract_S0140525X99452197 http://www.journals.cambridge.org/abstract_S0140525X99452197 Tiffany, Evan (1999). Semantics San Diego style. Journal of Philosophy 96 (8):416-429. ( Cited by 6 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2564630.pdf Tye, Michael (1987). Representation in pictorialism and connectionism. Southern Journal of Philosophy Supplement 26:163-184. ( Annotation | Google ) Pictorialism isn't compatible with language of thought, but connectionism might be. van Gelder, Tim (1999). Distributed vs. local representation. In R.A. Wilson F.C. Keil (eds.), The MIT Encyclopedia of the Cognitive Sciences . MIT Press. ( Cited by 6 | Google ) Abstract: been to define various notions of distribution in terms of represented by one and the same distributed pattern (Mur- structures of correspondence between the represented items dock 1979). For example, it is standard in feedforward and the representational resources (e.g., van Gelder 1992). connectionist networks for one and the same set of synap- This approach may be misguided; the essence of this alter- tic weights to represent many associations between input native category of representation might be some other prop- and output. erty entirely. For example, Haugeland (1991) has suggested • Equipotentiality In some cases, an item is represented by van Gelder, Tim (1990). Why distributed representation is inherently non-symbolic. In G. Dorffner (ed.), Konnektionismus in Artificial Intelligence Und Kognitionsforschung . Berlin: Springer-Verlag. ( Cited by 4 | Google ) Abstract: There are many conflicting views concerning the nature of distributed representation, its compatibility or otherwise with symbolic representation, and its importance in characterizing the nature of connectionist models and their relationship to more traditional symbolic approaches to understanding cognition. Many have simply assumed that distribution is merely an implementation issue, and that symbolic mechanisms can be designed to take advantage of the virtues of distribution if so desired. Others, meanwhile, see the use of distributed representation as marking a fundamental difference between the two approaches. One reason for this diversity of opinion is the fact that the relevant notions - especially that of van Gelder, Tim (1991). What is the D in PDP? In William Ramsey, Stephen P. Stich D. Rumelhart (eds.), Philosophy and Connectionist Theory . Lawrence Erlbaum. ( Cited by 65 | Annotation | Google ) Argues that distributed representation is best analyzed in terms of superposition of representation, not in terms of extendedness. Von Eckardt, Barbara (2003). The explanatory need for mental representations in cognitive science. Mind and Language 18 (4):427-439. ( Cited by 1 | Google | More links ) Abstract:   Ramsey (1997) argues that connectionist representations 'do not earn their explanatory keep'. The aim of this paper is to examine the argument Ramsey gives to support that conclusion. In doing so, I identify two kinds of explanatory need—need relative to a possible explanation and need relative to a true explanation and argue that internal representations are not needed for either connectionist or nonconnectionist possible explanations but that it is quite likely that they are needed for true explanations. However, to show that the latter is the case requires more than a consideration of the form of explanation involved Additional links for this entry: http://www.blackwell-synergy.com/doi/abs/10.1111/1468-0017.00235 http://www.ingentaconnect.com/content/bpl/mila/2003/00000018/00000004/art00006 Waskan, Jonathan A. (2001). A critique of connectionist semantics. Connection Science 13 (3):277-292. ( Google | More links ) Additional links for this entry: http://www.informaworld.com/index/WGXAHC9FUKFKEVME.pdf http://www.ingentaconnect.com/content/tandf/ccos/2001/00000013/00000003/art00004 6.3c Connectionism and Eliminativism Bickle, John (1993). Connectionism, eliminativism, and the semantic view of theories. Erkenntnis 39 (3):359-382. ( Cited by 5 | Annotation | Google | More links ) Outlines the semantic view of scientific theories, and applies it to the connectionism/eliminativism debate. There's no reason why folk psychology shouldn't be reducible, in a homogeneous or heterogeneous way. Abstract:   Recently some philosophers have urged that connectionist artificial intelligence is (potentially) eliminative for the propositional attitudes of folk psychology. At the same time, however, these philosophers have also insisted that since philosophy of science has failed to provide criteria distinguishing ontologically retentive from eliminative theory changes, the resulting eliminativism is not principled. Application of some resources developed within the semantic view of scientific theories, particularly recent formal work on the theory reduction relation, reveals these philosophers to be wrong in this second contention, yet by and large correct in the first Additional links for this entry: http://www.springerlink.com/index/PK525K2222273073.pdf Botterill, George (1994). Beliefs, functionally discrete states, and connectionist networks. British Journal for the Philosophy of Science 45 (3):899-906. ( Cited by 2 | Annotation | Google | More links ) Distinguishes active from dispositional beliefs: the former are realized discretely in activation patterns, the latter nondiscretely in weights, which is all that folk psychology needs. Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/citation/45/3/899 http://bjps.oxfordjournals.org/cgi/reprint/45/3/899 http://www.jstor.org/stable/pdfplus/687799.pdf Chemero, Anthony (2007). Asking what's inside the head: Neurophilosophy meets the extended mind. Minds and Machines 17 (3). ( Google | More links ) Abstract: In their historical overview of cognitive science, Bechtel, Abraham- son and Graham (1999) describe the ﬁeld as expanding in focus be- ginning in the mid-1980s. The ﬁeld had spent the previous 25 years on internalist, high-level GOFAI (“good old fashioned artiﬁcial intelli- gence” [Haugeland 1985]), and was ﬁnally moving “outwards into the environment and downards into the brain” (Bechtel et al, 1999, p.75). One important force behind the downward movement was Patricia Churchland’s Neurophilosophy (1986). This book began a movement bearing its name, one that truly came of age in 1999 when Kath- leen Akins won a million-dollar fellowship to begin the McDonnell Project in Philosophy and the Neurosciences. The McDonnell Project put neurophilosophy at the forefront of philosophy of mind and cogni- tive science, yielding proliferating articles, conferences, special journal issues and books. In two major new books, neurophilosophers Patricia Churchland (2002) and John Bickle (2003) clearly feel this newfound prominence: Churchland mocks those who do not apply ﬁndings in neuroscience to philosophical problems as “no-brainers”; Bickle mocks anyone with traditional philosophical concerns, including “naturalistic philosophers of mind” and other neurophilosophers Additional links for this entry: http://edisk.fandm.edu/tony.chemero/papers/bickchurchrev.pdf http://www.springerlink.com/content/m812557162027h08/fulltext.pdf Clark, Andy (1989). Beyond eliminativism. Mind and Language 4 (4):251-79. ( Cited by 5 | Annotation | Google | More links ) Connectionism needn't imply eliminativism, as higher levels may have a causal role, if not causal completeness. Also, it may not tell the whole story. Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/120006482/PDFSTART Clapin, Hugh (1991). Connectionism isn't magic. Minds and Machines 1 (2):167-84. ( Cited by 3 | Annotation | Google | More links ) Commentary on Ramsey/Stich/Garon. Connectionism has symbols that interact, and has propositional modularity in processing if not in storage. Abstract:   Ramsey, Stich and Garon's recent paper Connectionism, Eliminativism, and the Future of Folk Psychology claims a certain style of connectionism to be the final nail in the coffin of folk psychology. I argue that their paper fails to show this, and that the style of connectionism they illustrate can in fact supplement, rather than compete with, the claims of a theory of cognition based in folk psychology's ontology. Ramsey, Stich and Garon's argument relies on the lack of easily identifiable symbols inside the connectionist network they discuss, and they suggest that the existence of a system which behaves in a cognitively interesting way, but which cannot be explained by appeal to internal symbol processing, falsifies central assumptions of folk psychology. My claim is that this argument is flawed, and that the theorist need not discard folk psychology in order to accept that the network illustrated exhibits cognitively interesting behaviour, even if it is conceded that symbols cannot be readily identified within the network Additional links for this entry: http://www.springerlink.com/content/j71587tqw5g8141r/fulltext.pdf http://www.springerlink.com/index/J71587TQW5G8141R.pdf Clark, Andy (1990). Connectionist minds. Proceedings of the Aristotelian Society 90:83-102. ( Cited by 10 | Annotation | Google ) Responding to eliminativist challenge via cluster analysis and recurrence. Davies, Martin (1991). Concepts, connectionism, and the language of thought. Philosophy and connectionist theory. Lawrence Erlbaum . ( Annotation | Google ) Argues that our conception of thought requires causal systematicity, which requires a language of thought. Connectionist systems are not causally systematic, so connectionism leads to eliminativism. Egan, Frances (1995). Folk psychology and cognitive architecture. Philosophy of Science 62 (2):179-96. ( Cited by 7 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/188429.pdf Forster, M. Saidel, Eric (1994). Connectionism and the fate of folk psychology. Philosophical Psychology 7 (4):437-52. ( Cited by 6 | Annotation | Google | More links ) Contra Ramsey, Stich, and Garon, connectionist representations can be seen to be functionally discrete on an appropriate analysis of causal relevance. Additional links for this entry: http://philosophy.wisc.edu/forster/papers/Connection.pdf Horgan, Terence E. Tienson, John L. (1995). Connectionism and the commitments of folk psychology. Philosophical Perspectives 9:127-52. ( Cited by 4 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2214215.pdf Macdonald, Cynthia (1995). Connectionism and eliminativism. In C. Macdonald Graham F. Macdonald (eds.), Connectionism: Debates on Psychological Explanation . Cambridge: Blackwell. ( Google ) O'Brien, Gerard (1991). Is connectionism commonsense? Philosophical Psychology 4 (2):165-78. ( Google | More links ) Abstract: In this paper I critically examine the line of reasoning that has recently appeared in the literature that connects connectionism with eliminativism. This line of reasoning has it that if connectionist models turn out accurately to characterize our cognition, then beliefs, desires and the other intentional entities of commonsense psychology will be eliminated from our theoretical ontology. In complete contrast I argue (1) that not only is this line of reasoning mistaken about the eliminativist tendencies of connectionist models, but (2) that these models have the potential to provide a more robust vindication of commonsense psychology than classical computational models Additional links for this entry: http://www.informaworld.com/smpp/./ftinterface~db=all~content=a793921812~fulltext=713240930 O'Leary-Hawthorne, John (1994). On the threat of eliminativism. Philosophical Studies 74 (3):325-46. ( Annotation | Google ) A dispositional construal of beliefs and desires can distinguish the relevant active states (via counterfactuals) and is compatible with FP, so internals can't threaten FP. With remarks on Davidson, overdetermination, etc. Place, Ullin T. (1992). Eliminative connectionism: Its implications for a return to an empiricist/behaviorist linguistics. Behavior and Philosophy 20 (1):21-35. ( Google ) Ramsey, William ; Stich, Stephen P. Garon, J. (1991). Connectionism, eliminativism, and the future of folk psychology. In William Ramsey, Stephen P. Stich D. Rumelhart (eds.), Philosophy and Connectionist Theory . Lawrence Erlbaum. ( Cited by 85 | Annotation | Google | More links ) If connectionism is true, then eliminativism is true, as you can't isolate the causal role of individual beliefs in a connectionist system. Additional links for this entry: http://www.jstor.org/stable/pdfplus/2214202.pdf Ramsey, William (1994). Distributed representation and causal modularity: A rejoinder to Forster and Saidel. Philosophical Psychology 7 (4):453-61. ( Cited by 2 | Annotation | Google ) Upon examination, the model of Forster and Saidel 1994 does not exhibit features that are both distributed and causally discrete. Abstract: In “Connectionism and the fats of folk psychology”, Forster and Saidel argue that the central claim of Ramsey, Stich and Garon (1991)—that distributed connectionist models are incompatible with the causal discreteness of folk psychology—is mistaken. To establish their claim, they offer an intriguing model which allegedly shows how distributed representations can function in a causally discrete manner. They also challenge our position regarding projectibility of folk psychology. In this essay, I offer a response to their account and show how their model fails to demonstrate that our original argument was mistaken. While I will discuss several difficulties with their model, my primary criticism will be that the features of their model that are causally discrete are not truly distributed, while the features that are distributed are not really discrete. Concerning the issue of projectibility, I am more inclined to agree with Forster and Saidel and I offer a revised account of what we should have said originally Skokowski, Paul G. (ms). Belief in networks. ( Google ) Smolensky, Paul (1995). On the projectable predicates of connectionist psychology: A case for belief. In C. Macdonald Graham F. Macdonald (eds.), Connectionism: Debates on Psychological Explanation . Blackwell. ( Cited by 4 | Google ) Stich, Stephen P. Warfield, Ted A. (1995). Reply to Clark and Smolensky: Do connectionist minds have beliefs? In C. Macdonald Graham F. Macdonald (eds.), Connectionism: Debates on Psychological Explanation . Blackwell. ( Cited by 5 | Google ) Von Eckhardt, Barbara (2004). Connectionism and the propositional attitudes. In Christina E. Erneling David Martel Johnson (eds.), Mind As a Scientific Object . Oxford University Press. ( Google ) 6.3d The Connectionist/Classical Debate Adams, Frederick R. ; Aizawa, Kenneth Fuller, Gary (1992). Rules in programming languages and networks. In J. Dinsmore (ed.), The Symbolic and Connectionist Paradigms: Closing the Gap . Lawrence Erlbaum. ( Cited by 3 | Annotation | Google | More links ) The distinction between programming languages and networks is neutral on rule-following, etc, so there's nothing really new about connectionism. Additional links for this entry: http://books.google.com/books?hl=en=mhv9JiNUtz2joCKf_iL9HXjfIOk Aizawa, Kenneth (1994). Representations without rules, connectionism, and the syntactic argument. Synthese 101 (3):465-92. ( Cited by 10 | Google | More links ) Abstract:   Terry Horgan and John Tienson have suggested that connectionism might provide a framework within which to articulate a theory of cognition according to which there are mental representations without rules (RWR) (Horgan and Tienson 1988, 1989, 1991, 1992). In essence, RWR states that cognition involves representations in a language of thought, but that these representations are not manipulated by the sort of rules that have traditionally been posited. In the development of RWR, Horgan and Tienson attempt to forestall a particular line of criticism, theSyntactic Argument, which would show RWR to be inconsistent with connectionism. In essence, the argument claims that the node-level rules of connectionist networks, along with the semantic interpretations assigned to patterns of activation, serve to determine a set of representation-level rules incompatible with the RWR conception of cognition. The present paper argues that the Syntactic Argument can be made to show that RWR is inconsistent with connectionism Additional links for this entry: http://www.springerlink.com/index/W30780736G40M071.pdf Aydede, Murat (1995). Connectionism and the language of thought. CSLI Technical Report . ( Cited by 4 | Google ) Abstract: Fodor and Pylyshyn's (F) critique of connectionism has posed a challenge to connectionists: Adequately explain such nomological regularities as systematicity and productivity without postulating a "language of thought'' (LOT). Some connectionists declined to meet the challenge on the basis that the alleged regularities are somehow spurious. Some, like Smolensky, however, took the challenge very seriously, and attempted to meet it by developing models that are supposed to be non-classical beim Graben, Peter (2004). Incompatible implementations of physical symbol systems. Mind and Matter 2 (2):29-51. ( Google ) Abstract: Classical cognitive science assumes that intelligently behaving systems must be symbol processors that are implemented in physical systems such as brains or digital computers. By contrast, connectionists suppose that symbol manipulating systems could be approximations of neural networks dynamics. Both classicists and connectionists argue that symbolic computation and subsymbolic dynamics are incompatible, though on different grounds. While classicists say that connectionist architectures and symbol processors are either incompatible or the former are mere implementations of the latter, connectionists reply that neural networks might be incompatible with symbol processors because the latter cannot be implementations of the former. In this contribution, the notions of 'incompatibility' and 'implementation' will be criticized to show that they must be revised in the context of the dynamical system approach to cognitive science. Examples for implementations of symbol processors that are incompatible with respect to contextual topologies will be discussed Bringsjord, Selmer (1991). Is the connectionist-logicist debate one of ai's wonderful red herrings? Journal of Theoretical and Experimental Artificial Intelligence 3:319-49. ( Cited by 16 | Annotation | Google | More links ) A detailed analysis purporting to show that connectionism and "logicism" are compatible, as Turing machines can do everything a neural network can. Entertaining, but misunderstands subsymbolic processing. Additional links for this entry: http://kryten.mm.rpi.edu/connectionist_logicist_clash.pdf http://www.informaworld.com/index/776648445.pdf Broadbent, D. (1985). A question of levels: Comment on McClelland and rumelhart. Journal of Experimental Psychology 114:189-92. ( Cited by 29 | Annotation | Google ) Distributed models are at the implementational, not computational, level. Chandrasekaran, B. ; Goel, A. Allemang, D. (1988). Connectionism and information-processing abstractions. AI Magazine 24. ( Cited by 15 | Annotation | Google | More links ) Connectionism won't affect AI too much, as AI is concerned with the information-processing (task) level. With greater modularity, connectionism will look more like traditional AI. Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=1914723CI Christensen, Wayne D. Tomassi, Luca (2006). Neuroscience in context: The new flagship of the cognitive sciences. Biological Theory 1 (1):78-83. ( Google | More links ) Additional links for this entry: http://www.arts.adelaide.edu.au/humanities/people/philosophy/christensen_preprints/Christensen_Tommasi_2006_Neuroscience_in_context.pdf Corbi, Josep E. (1993). Classical and connectionist models: Levels of description. Synthese 95 (2):141-68. ( Google ) Davies, Martin (1991). Concepts, connectionism, and the language of thought. In W Ramsey, Stephen P. Stich D. Rumelhart (eds.), Philosophy and Connectionist Theory . Hillsdale, NJ: Lawrence Erlbaum Associates. ( Cited by 40 | Google ) Abstract: The aim of this paper is to demonstrate a _prima facie_ tension between our commonsense conception of ourselves as thinkers and the connectionist programme for modelling cognitive processes. The language of thought hypothesis plays a pivotal role. The connectionist paradigm is opposed to the language of thought; and there is an argument for the language of thought that draws on features of the commonsense scheme of thoughts, concepts, and inference. Most of the paper (Sections 3-7) is taken up with the argument for the language of thought hypothesis. The argument for an opposition between connectionism and the language of thought comes towards the end (Section 8), along with some discussion of the potential eliminativist consequences (Sections 9 and Dawson, Michael R. W. ; Medler, D. A. Berkeley, Istvan S. N. (1997). PDP networks can provide models that are not mere implementations of classical theories. Philosophical Psychology 10 (1):25-40. ( Cited by 17 | Google ) Abstract: There is widespread belief that connectionist networks are dramatically different from classical or symbolic models. However, connectionists rarely test this belief by interpreting the internal structure of their nets. A new approach to interpreting networks was recently introduced by Berkeley et al. (1995). The current paper examines two implications of applying this method: (1) that the internal structure of a connectionist network can have a very classical appearance, and (2) that this interpretation can provide a cognitive theory that cannot be dismissed as a mere implementation Dennett, Daniel C. (1991). Mother nature versus the walking encyclopedia. In William Ramsey, Stephen P. Stich D. Rumelhart (eds.), Philosophy and Connectionist Theory . Lawrence Erlbaum. ( Cited by 23 | Annotation | Google | More links ) Reiterating the value of connectionism, especially biological plausibility. Abstract: In 1982, Feldman and Ballard published "Connectionist models and their properties" in Cognitive Science , helping to focus attention on a family of similarly inspired research strategies just then under way, by giving the family a name: "connectionism." Now, seven years later, the connectionist nation has swelled to include such subfamilies as "PDP" and "neural net models." Since the ideological foes of connectionism are keen to wipe it out in one fell swoop aimed at its "essence", it is worth noting the diversity of not only the models but also the aspirations of the modelers. There is no good reason to suppose that they all pledge allegiance to any one principle.. Additional links for this entry: http://ase.tufts.edu/cogstud/papers/motherna.htm http://cogprints.ecs.soton.ac.uk/archive/00000433/ http://cogprints.ecs.soton.ac.uk/archive/00000433/00/motherna.htm http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:433 Dennett, Daniel C. (1986). The logical geography of computational approaches: A view from the east pole. In Myles Brand Robert M. Harnish (eds.), The Representation of Knowledge and Belief . University of Arizona Press. ( Cited by 21 | Annotation | Google ) Drawing the battle-lines: High Church Computationalism at the "East Pole", New Connectionism, Zen Holism, etc, at various locations on the "West Coast". With remarks on connectionism, and on AI as thought-experimentation. DeVries, Willem A. (1993). Who sees with equal eye,... Atoms or systems into ruin hurl'd? Philosophical Studies 71 (2):191-200. ( Google | More links ) Additional links for this entry: http://www.springerlink.com/index/L7664W2T0U211253.pdf Dinsmore, J. (ed.) (1992). The Symbolic and Connectionist Paradigms: Closing the Gap. Lawrence Erlbaum. ( Cited by 18 | Google ) Abstract: This book records the thoughts of researchers -- from both computer science and philosophy -- on resolving the debate between the symbolic and connectionist... Dyer, Michael G. (1991). Connectionism versus symbolism in high-level cognition. In Terence E. Horgan John L. Tienson (eds.), Connectionism and the Philosophy of Mind . Kluwer. ( Cited by 7 | Google ) Eliasmith, Chris (2000). Is the brain analog or digital? Cognitive Science Quarterly 1 (2):147-170. ( Google | More links ) Abstract: It will always remain a remarkable phenomenon in the history of philosophy, that there was a time, when even mathematicians, who at the same time were philosophers, began to doubt, not of the accuracy of their geometrical propositions so far as they concerned space, but of their objective validity and the applicability of this concept itself, and of all its corollaries, to nature. They showed much concern whether a line in nature might not consist of physical points, and consequently that true space in the object might consist of simple [discrete] parts, while the space which the geometer has in his mind [being continuous] cannot be such Additional links for this entry: http://www.arts.uwaterloo.ca/~celiasmi/Papers/ce.2000.continuity.debate.csq.html Eliasmith, Chris Clark, Andy (2002). Philosophical issues in brain theory and connectionism. In M. Arbib (ed.), The Handbook of Brain Theory and Neural Networks . MIT Press. ( Google | More links ) Abstract: In this article, we highlight three questions: (1) Does human cognition rely on structured internal representations? (2) How should theories, models and data relate? (3) In what ways might embodiment, action and dynamics matter for understanding the mind and the brain? Additional links for this entry: http://www.arts.uwaterloo.ca/~celiasmi/Papers/withclark.html http://www.philosophy.ed.ac.uk/staff/clark/pubs/clarkeliasmith2.pdf Fodor, Jerry A. Pylyshyn, Zenon W. (1988). Connectionism and cognitive architecture. Cognition 28:3-71. ( Cited by 1496 | Annotation | Google | More links ) Connectionist models can't explain cognitive systematicity and productivity, as their representations lack compositional structure. The allures of connectionism are illusory; it's best used as an implementation strategy. Abstract: This paper explores the difference between Connectionist proposals for cognitive a r c h i t e c t u r e a n d t h e s o r t s o f m o d e l s t hat have traditionally been assum e d i n c o g n i t i v e s c i e n c e . W e c l a i m t h a t t h e m a j o r d i s t i n c t i o n i s t h a t , w h i l e b o t h Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a ‘language of thought’: i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the ‘systematicity’ of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or ‘abstract neurological’) structures in which Classical cognitive architecture is implemented. We survey a n u m b e r o f t h e s t a n d a r d a r g u m e n t s t h a t h a v e b e e n o f f e r e d i n f a v o r o f Connectionism, and conclude that they are coherent only on this interpretation Additional links for this entry: http://portal.acm.org/citation.cfm?id=58067 http://www.blutner.de/philom/connect/jaf.pdf http://ruccs.rutgers.edu/ftp/pub/papers/jaf.pdf http://portal.acm.org/citation.cfm?id=190704.190740 http://www.ii.metu.edu.tr/~tekman/fodor http://citeseer.ist.psu.edu/fodor88connectionism.html http://books.google.com/books?hl=en=1PDJYThCtCF24M7R6UJK4T http://watarts.uwaterloo.ca/~celiasmi/courses/Phil256/papers/fodor.pylyshyn.1988.Connectionism cog architec.bbs.pdf http://books.google.com/books?hl=en=lvbjst___yAcJlOgVzyRdzEV09k http://books.google.com/books?hl=en=bfPrm5H3Z-QZFIh2WX-XvqgjqHE http://books.google.com/books?hl=en=W9wLTgrsP59cPqZhSN0ysQbSVoM http://books.google.com/books?hl=en=ic0bVVM1PfuvCnadV8OCRGBbNA4 http://books.google.com/books?hl=en=UkXTSX19hHzvmMUKvLT-lq0r50I http://books.google.com/books?hl=en=LBgHuNyf_KzzIi7ocvlxiFsIWIE http://books.google.com/books?hl=en=-rBwtgEcuMHfIKQJzOHWYIrBX9E http://books.google.com/books?hl=en=00wE8Svuh3W8kqsUo0UKOYCfcAI http://books.google.com/books?hl=en=5IusXYTvApV1_rtpQEkaBet0xn4 http://books.google.com/books?hl=en=HFGXyuxr_1mxH5ACFOe-zyaTw4o http://books.google.com/books?hl=en=CAuCWiDdEmwTntCDA3H1Zyfn5a0 http://books.google.com/books?hl=en=e9QEFe5nD4bGAK5W4dDIRZBKNuM http://books.google.com/books?hl=en=4TMgHlnpnDyehNFJT-NG470Bbz8 http://books.google.com/books?hl=en=9UEpXL5XYxZdQa3NvX7yU9iikyw http://books.google.com/books?hl=en=7sjHbKdagqCd6sUT-Kk-fcITZlc http://books.google.com/books?hl=en=9ypKOA18BrlaitMu_it-TVCBfUw http://books.google.com/books?hl=en=tdUqyz4ARusp_kokrKNOJedNknk http://books.google.com/books?hl=en=cUrJjY2sKfVLW5Zp1ZtlEsdhcaM http://books.google.com/books?hl=en=Qe8qR4Go6tnV95LlvTT6ELC_0Ds http://books.google.com/books?hl=en=iCTIyYq8EOvBdYu-srEXgyt-HoM http://books.google.com/books?hl=en=7hnYM8j4zdtpN4THjVSpzpGFEDs http://books.google.com/books?hl=en=F0CONzdT49KnmoIp41iOngjBa_A http://books.google.com/books?hl=en=Zks6FRmma9XZGHW2EO1kvwIxesQ http://books.google.com/books?hl=en=ohgYrPGd2Ut0_gQqvfJPXg0_xJo http://books.google.com/books?hl=en=snID_Ptc5j-K-DxZvSjAScHkD6s http://books.google.com/books?hl=en=YnoEHTZY6rwzlL8DcfCj45uUQo0 http://books.google.com/books?hl=en=XBIWzRchQULcgPTlX6HyBgOg_pQ http://books.google.com/books?hl=en=35GDmDcPG526574Ar-pZaGlfYXA http://books.google.com/books?hl=en=OzlrEEZ2Td9CGt5mgUD99UXGCWE http://books.google.com/books?hl=en=ZsqjFV8LrCGXI-3jf8Pliu_sVsc http://books.google.com/books?hl=en=1yq1WHwwotr3qVSaqN0K4StIWc8 http://books.google.com/books?hl=en=lDwkN-dMTxlLX_1XELpsD4ONtM4 http://books.google.com/books?hl=en=s7d-fmzg598bYH-IpAMNrcyO8eo http://books.google.com/books?hl=en=6AJnryXo_R8vj8qseWkK5DD31zQ http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation Garson, James W. (1994). Cognition without classical architecture. Synthese 100 (2):291-306. ( Cited by 10 | Google | More links ) Abstract:   Fodor and Pylyshyn (1988) argue that any successful model of cognition must use classical architecture; it must depend upon rule-based processing sensitive to constituent structure. This claim is central to their defense of classical AI against the recent enthusiasm for connectionism. Connectionist nets, they contend, may serve as theories of the implementation of cognition, but never as proper theories of psychology. Connectionist models are doomed to describing the brain at the wrong level, leaving the classical view to account for the mind.This paper considers whether recent results in connectionist research weigh against Fodor and Pylyshyn's thesis. The investigation will force us to develop criteria for determining exactly when a net is capable of systematic processing. Fodor and Pylyshyn clearly intend their thesis to affect the course of research in psychology. I will argue that when systematicity is defined in a way that makes the thesis relevant in this way, the thesis is challenged by recent progress in connectionism Additional links for this entry: http://www.springerlink.com/index/P4414695610528G3.pdf Garson, James W. (1994). No representations without rules: The prospects for a compromise between paradigms in cognitive science. Mind and Language 9 (1):25-37. ( Cited by 7 | Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/119275779/PDFSTART Garson, James W. (1991). What connectionists cannot do: The threat to classical AI. In Terence E. Horgan John L. Tienson (eds.), Connectionism and the Philosophy of Mind . Kluwer. ( Cited by 1 | Annotation | Google ) Connectionism and classicism aren't necessarily incompatible on symbolic discreteness, causal role, functional discreteness, constituency, representation of rules. Guarini, Marcello (2001). A defence of connectionism against the "syntactic" argument. Synthese 128 (3):287-317. ( Cited by 2 | Google | More links ) Abstract:   In "Representations without Rules, Connectionism and the Syntactic Argument'', Kenneth Aizawa argues against the view that connectionist nets can be understood as processing representations without the use of representation-level rules, and he provides a positive characterization of how to interpret connectionist nets as following representation-level rules. He takes Terry Horgan and John Tienson to be the targets of his critique. The present paper marshals functional and methodological considerations, gleaned from the practice of cognitive modelling, to argue against Aizawa's characterization of how connectionist nets may be understood as making use of representation-level rules Additional links for this entry: http://www.kluweronline.com/article.asp?PIPS=320358=1 http://www.springerlink.com/index/X30362P60J65P226.pdf http://www.ingentaconnect.com/content/klu/synt/2001/00000128/00000003/00320358 Horgan, Terence E. Tienson, John L. (2006). Cognition needs syntax but not rules. In Robert J. Stainton (ed.), Contemporary Debates in Cognitive Science . Malden MA: Blackwell Publishing. ( Cited by 1 | Google ) Horgan, Terence E. Tienson, John L. (1994). Representations don't need rules: Reply to James Garson. Mind and Language 9 (1):1-24. ( Cited by 6 | Google ) Horgan, Terence E. Tienson, John L. (1989). Representation without rules. Philosophical Perspectives 17 (1):147-74. ( Annotation | Google ) Cognition uses structured representations without high-level rules, and connectionism is better at accounting for this. With remarks on exceptions to psychological laws, and the crisis in traditional AI. Horgan, Terence E. Tienson, John L. (1987). Settling into a new paradigm. Southern Journal of Philosophy Supplement 26:97-113. ( Annotation | Google ) On connectionism, basketball, and representation without rules. Responses to the "syntactic" and "semantic" arguments against connectionism. Nice. Lormand, Eric (1991). Classical and Connectionist Models. Dissertation, Mit ( Google ) Lormand, Eric (ms). Connectionist languages of thought. ( Cited by 1 | Google ) Abstract: Fodor and Pylyshyn (1988) have presented an influential argument to the effect that any viable connectionist account of human cognition must implement a language of thought. Their basic strategy is to argue that connectionist models that do not implement a language of thought fail to account for the systematic relations among propositional attitudes. Several critics of the LOT hypothesis have tried to pinpoint flaws in Fodor and Pylyshyn’s argument (Smolensky 1989; Clark, 1989; Chalmers, 1990; Braddon-Mitchell and Fitzpatrick, 1990). One thing I will try to show is that the argument can be rescued from these criticisms. (Score: LOT 1, Visitors 0.) However, I agree that the argument fails, and I will provide a new account of how it goes wrong. (The score becomes tied.) Of course, the failure of Fodor and Pylyshyn’s argument does not mean that their conclusion is false. Consequently, some connectionist criticisms of Fodor and Pylyshyn’s article take the form of direct counterexamples to their conclusion (Smolensky 1989; van Gelder, 1990; Chalmers, 1990). I will argue, however, that Fodor and Pylyshyn’s conclusion survives confrontation with the alleged counterexamples. Finally, I provide an alternative argument that may succeed where Fodor and Pylyshyn’s fails. (Final Score: LOT 3, Visitors 1.) Markic, Olga (1999). Connectionism and the language of thought: The cross-context stability of representations. Acta Analytica 22 (22):43-57. ( Cited by 1 | Google ) McClelland, J. L. Rumelhart, D. E. (1985). Levels indeed! A response to Broadbent. Journal of Experimental Psychology 114:193-7. ( Annotation | Google ) Response to Broadbent 1985: Distributed models are at the algorithmic level. Elucidating the low-level/high-level relation via various analogies. McLaughlin, Brian P. Warfield, F. (1994). The allure of connectionism reexamined. Synthese 101 (3):365-400. ( Cited by 11 | Annotation | Google | More links ) Argues that symbolic systems such as decision trees are as good at learning and pattern recognition as connectionist networks, and it is just as plausible that they are implemented in the brain. Abstract:   There is currently a debate over whether cognitive architecture is classical or connectionist in nature. One finds the following three comparisons between classical architecture and connectionist architecture made in the pro-connectionist literature in this debate: (1) connectionist architecture is neurally plausible and classical architecture is not; (2) connectionist architecture is far better suited to model pattern recognition capacities than is classical architecture; and (3) connectionist architecture is far better suited to model the acquisition of pattern recognition capacities by learning than is classical architecture. If true, (1)–(3) would yield a compelling case against the view that cognitive architecture is classical, and would offer some reason to think that cognitive architecture may be connectionist. We first present the case for (1)–(3) in the very words of connectionist enthusiasts. We then argue that the currently available evidence fails to support any of (1)–(3) Additional links for this entry: http://www.springerlink.com/index/V2103368752NX552.pdf Rey, Georges (1991). An explanatory budget for connectionism and eliminativism. In Terence E. Horgan John L. Tienson (eds.), Connectionism and the Philosophy of Mind . Kluwer. ( Cited by 11 | Annotation | Google ) Challenges connectionism to explain things that the classical approach seems to handle better: the structure, systematicity, causal role, and grain of propositional attitudes, their rational relations, and conceptual stability. Schneider, Susan (2009). The language of thought. In John Symons Paco Calvo (eds.), Routledge Companion to Philosophy of Psychology . Routledge. ( Google ) Abstract: According to the language of thought (or Schneider, Susan (forthcoming). The nature of primitive symbols in the language of thought. Mind and Language . ( Google | More links ) Abstract: This paper provides a theory of the nature of symbols in the language of thought (LOT). My discussion consists in three parts. In part one, I provide three arguments for the individuation of primitive symbols in terms of total computational role. The first of these arguments claims that Classicism requires that primitive symbols be typed in this manner; no other theory of typing will suffice. The second argument contends that without this manner of symbol individuation, there will be computational processes that fail to supervene on syntax, together with the rules of composition and the computational algorithms. The third argument says that cognitive science needs a natural kind that is typed by total computational role. Otherwise, either cognitive science will be incomplete, or its laws will have counterexamples. Then, part two defends this view from a criticism, offered by both Jerry Fodor and Jesse Prinz, who respond to my view with the charge that because the types themselves are individuated Additional links for this entry: http://www.sas.upenn.edu/~sls/documents/SchneiderNatureSymbolsMindLang.doc http://www.sas.upenn.edu/~sls/documents/SchneiderNatureSymbolsMindLang-1-2.pdf http://www.sas.upenn.edu/~sls/documents/Schneider-MindLangSymbols.pdf ter Hark, Michel (1995). Connectionism, behaviourism, and the language of thought. In Cognitive Patterns in Science and Common Sense . Amsterdam: Rodopi. ( Google ) 6.3e Subsymbolic Computation Berkeley, Istvan S. N. (2006). Moving the goal posts: A reply to Dawson and Piercey. Minds and Machines 16 (4):471-478. ( Google | More links ) Abstract: Berkeley [Minds Machines 10 (2000) 1] described a methodology that showed the subsymbolic nature of an artificial neural network system that had been trained on a logic problem, originally described by Bechtel and Abrahamsen [Connectionism and the mind. Blackwells, Cambridge, MA, 1991]. It was also claimed in the conclusion of this paper that the evidence was suggestive that the network might, in fact, count as a symbolic system. Dawson and Piercey [Minds Machines 11 (2001) 197] took issue with this latter claim. They described some lesioning studies that they argued showed that Berkeley’s (2000) conclusions were premature. In this paper, these lesioning studies are replicated and it is shown that the effects that Dawson and Piercey rely upon for their argument are merely an artifact of a threshold function they chose to employ. When a threshold function much closer to that deployed in the original studies is used, the significant effects disappear Additional links for this entry: http://www.springerlink.com/content/g46288063247n5nv/fulltext.pdf http://www.springerlink.com/index/G46288063247N5NV.pdf Berkeley, Istvan S. N. (2000). What the #$*%! Is a subsymbol? Minds and Machines 10 (1):1-13. ( Cited by 5 | Google | More links ) Abstract:   In 1988, Smolensky proposed that connectionist processing systems should be understood as operating at what he termed the `subsymbolic'' level. Subsymbolic systems should be understood by comparing them to symbolic systems, in Smolensky''s view. Up until recently, there have been real problems with analyzing and interpreting the operation of connectionist systems which have undergone training. However, recently published work on a network trained on a set of logic problems originally studied by Bechtel and Abrahamsen (1991) seems to offer the potential to provide a detailed, empirically based answer to questions about the nature of subsymbols. In this paper, a network analysis procedure and the results obtained using it are discussed. This provides the basis for an insight into the nature of subsymbols, which is surprising Additional links for this entry: http://www.ucs.louisiana.edu/~isb9112/dept/phil341/subsymbol/subsymbol.html http://www.springerlink.com/content/content/g81p084q505n1210/fulltext.pdf http://www.springerlink.com/content/g81p084q505n1210/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=258824=1 http://www.springerlink.com/index/G81P084Q505N1210.pdf http://www.ingentaconnect.com/content/klu/mind/2000/00000010/00000001/00258824 Chalmers, David J. (1992). Subsymbolic computation and the chinese room. In J. Dinsmore (ed.), The Symbolic and Connectionist Paradigms: Closing the Gap . Lawrence Erlbaum. ( Cited by 29 | Annotation | Google | More links ) Gives an account of symbolic vs. subsymbolic computation, and argues that the latter is less vulnerable to the Chinese-room intuition, as representations there are not computational tokens. Abstract: More than a decade ago, philosopher John Searle started a long-running controversy with his paper “Minds, Brains, and Programs” (Searle, 1980a), an attack on the ambitious claims of artificial intelligence (AI). With his now famous _Chinese Room_ argument, Searle claimed to show that despite the best efforts of AI researchers, a computer could never recreate such vital properties of human mentality as intentionality, subjectivity, and understanding. The AI research program is based on the underlying assumption that all important aspects of human cognition may in principle be captured in a computational model. This assumption stems from the belief that beyond a certain level, implementational details are irrelevant to cognition. According to this belief, neurons, and biological wetware in general, have no preferred status as the substrate for a mind. As it happens, the best examples of minds we have at present have arisen from a carbon-based substrate, but this is due to constraints of evolution and possibly historical accidents, rather than to an absolute metaphysical necessity. As a result of this belief, many cognitive scientists have chosen to focus not on the biological substrate of the mind, but instead on the abstract causal structure_ _that the mind embodies (at an appropriate level of abstraction). The view that it is abstract causal structure that is essential to mentality has been an implicit assumption of the AI research program since Turing (1950), but was first articulated explicitly, in various forms, by Putnam (1960), Armstrong (1970) and Lewis (1970), and has become known as _functionalism_. From here, it is a very short step to _computationalism_, the view that computational structure is what is important in capturing the essence of mentality. This step follows from a belief that any abstract causal structure can be captured computationally: a belief made plausible by the Church–Turing Thesis, which articulates the power Additional links for this entry: http://consc.net/papers/subsymbolic.pdf http://citeseer.ist.psu.edu/chalmers92subsymbolic.html http://www.u.arizona.edu/~chalmers/papers/subsymbolic.pdf Clark, Andy (1993). Superpositional connectionism: A reply to Marinov. Minds and Machines 3 (3):271-81. ( Cited by 2 | Google | More links ) Abstract:   Marinov''s critique I argue, is vitiated by its failure to recognize the distinctive role of superposition within the distributed connectionist paradigm. The use of so-called subsymbolic distributed encodings alone is not, I agree, enough to justify treating distributed connectionism as a distinctive approach. It has always been clear that microfeatural decomposition is both possible and actual within the confines of recognizably classical approaches. When such approaches also involve statistically-driven learning algorithms — as in the case of ID3 — the fundamental differences become even harder to spot. To see them, it is necessary to consider not just the nature of an acquired input-output function but the nature of the representational scheme underlying it. Differences between such schemes make themselves best felt outside the domain of immediate problem solving. It is in the more extended contexts of performance DURING learning and cognitive change as a result of SUBSEQUENT training on new tasks (or simultaneous training on several tasks) that the effects of superpositional storage techniques come to the fore. I conclude that subsymbols, distribution and statistically driven learning alone are indeed not of the essence. But connectionism is not just about subsymbols and distribution. It is about the generation of whole subsymbol SYSTEMS in which multiple distributed representations are created and superposed Additional links for this entry: http://www.springerlink.com/content/l11l1130450x265x/fulltext.pdf http://www.springerlink.com/index/L11L1130450X265X.pdf Cleeremans, Axel (1998). The other hard problem: How to bridge the gap between subsymbolic and symbolic cognition. Behavioral and Brain Sciences 21 (1):22-23. ( Google | More links ) Abstract: The constructivist notion that features are purely functional is incompatible with the classical computational metaphor of mind. I suggest that the discontent expressed by Schyns, Goldstone and Thibaut about fixed-features theories of categorization reflects the growing impact of connectionism, and show how their perspective is similar to recent research on implicit learning, consciousness, and development. A hard problem remains, however: How to bridge the gap between subsymbolic and symbolic cognition Additional links for this entry: http://srsc.ulb.ac.be/axcWWW/papers/BBS-schyns98.html http://srsc.ulb.ac.be/axcwww/papers/pdf/98-C-on-SGT.pdf http://srsc.ulb.ac.be/axcWWW/papers/pdf/98-C-on-SGT.pdf http://srsc.ulb.ac.be/axcWWW/papers/pdf/98-C-on-SGT.pdf http://srsc-mac1.ulb.ac.be/axcWWW/papers/pdf/98-C-on-SGT.pdf http://journals.cambridge.org/action/displayFulltext?type=1=30265 http://www.journals.cambridge.org/abstract_S0140525X98290108 Hofstadter, Douglas R. (1983). Artificial intelligence: Subcognition as computation. In Fritz Machlup (ed.), The Study of Information: Interdisciplinary Messages . Wiley. ( Cited by 12 | Annotation | Google ) AI needs statistical emergence. For real semantics, symbols must be decomposable, complex, autonomous -- i.e. active. Marinov, Marin (1993). On the spuriousness of the symbolic/subsymbolic distinction. Minds and Machines 3 (3):253-70. ( Cited by 2 | Annotation | Google | More links ) Argues with Smolensky: symbolic systems such as decision trees have all the positive features of neural networks (flexibility, lack of brittleness), and can represent concepts as sets of subconcepts. With a reply by Clark. Abstract:   The article criticises the attempt to establish connectionism as an alternative theory of human cognitive architecture through the introduction of thesymbolic/subsymbolic distinction (Smolensky, 1988). The reasons for the introduction of this distinction are discussed and found to be unconvincing. It is shown that thebrittleness problem has been solved for a large class ofsymbolic learning systems, e.g. the class oftop-down induction of decision-trees (TDIDT) learning systems. Also, the process of articulating expert knowledge in rules seems quite practical for many important domains, including common sense knowledge.The article discusses several experimental comparisons betweenTDIDT systems and artificial neural networks using the error backpropagation algorithm (ANNs usingBP). The properties of one of theTDIDT systemsID3 (Quinlan, 1986a) are examined in detail. It is argued that the differences in performance betweenANNs usingBP andTDIDT systems reflect slightly different inductive biases but are not systematic; these differences do not support the view that symbolic and subsymbolic systems are fundamentally incompatible. It is concluded, that thesymbolic/subsymbolic distinction is spurious. It cannot establish connectionism as an alternative cognitive architecture Additional links for this entry: http://www.springerlink.com/index/NH755160272VV7JH.pdf Rosenberg, Jay F. (1990). Treating connectionism properly: Reflections on Smolensky. Psychological Research 52. ( Cited by 5 | Annotation | Google ) Rejects Smolensky's PTC, as the proper interaction of the microscopic and macroscopic levels would take a "miracle". Smolensky, Paul (1987). Connectionist, symbolic, and the brain. AI Review 1:95-109. ( Cited by 18 | Annotation | Google ) On connectionist networks as subsymbolic dynamic systems. Smolensky, Paul (1988). On the proper treatment of connectionism. Behavioral and Brain Sciences 11:1-23. ( Cited by 902 | Annotation | Google ) Connectionism offers a complete account at the subsymbolic level, rather than an approximate account at the symbolic level. 6.3f Philosophy of Connectionism, Misc Abrahamsen, Adele A. (1993). Cognizers' innards and connectionist nets: A holy alliance? Mind and Language 8 (4):520-530. ( Cited by 2 | Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/119302073/PDFSTART Aizawa, Kenneth (1999). Connectionist rules: A rejoinder to Horgan and Tienson's connectionism and the philosophy of psychology. Acta Analytica 22 (22):59-85. ( Cited by 3 | Google ) Bechtel, William P. (1985). Are the new PDP models of cognition cognitivist or associationist? Behaviorism 13:53-61. ( Google ) Bechtel, William P. Abrahamson, A. (1990). Beyond the exclusively propositional era. Synthese 82 (2):223-53. ( Cited by 9 | Annotation | Google | More links ) An account of the shift from propositions to pattern recognition in the study of cognition: knowing-how, imagery, categorization, connectionism. Abstract:   Contemporary epistemology has assumed that knowledge is represented in sentences or propositions. However, a variety of extensions and alternatives to this view have been proposed in other areas of investigation. We review some of these proposals, focusing on (1) Ryle's notion of knowing how and Hanson's and Kuhn's accounts of theory-laden perception in science; (2) extensions of simple propositional representations in cognitive models and artificial intelligence; (3) the debate concerning imagistic versus propositional representations in cognitive psychology; (4) recent treatments of concepts and categorization which reject the notion of necessary and sufficient conditions; and (5) parallel distributed processing (connectionist) models of cognition. This last development is especially promising in providing a flexible, powerful means of representing information nonpropositionally, and carrying out at least simple forms of inference without rules. Central to several of the proposals is the notion that much of human cognition might consist in pattern recognition rather than manipulation of rules and propositions Additional links for this entry: http://www.springerlink.com/index/WW512705U27062RV.pdf Bechtel, William P. (1988). Connectionism and rules and representation systems: Are they compatible? Philosophical Psychology 1 (1):5-16. ( Cited by 43 | Annotation | Google ) There's room for both styles within a single mind. The rule-based level needn't be autonomous; the connectionist level plays a role in pattern recognition, concepts, and so on. Abstract: The introduction of connectionist or parallel distributed processing (PDP) systems to model cognitive functions has raised the question of the possible relations between these models and traditional information processing models which employ rules to manipulate representations. After presenting a brief account of PDP models and two ways in which they are commonly interpreted by those seeking to use them to explain cognitive functions, I present two ways one might relate these models to traditional information processing models and so not totally repudiate the tradition of modelling cognition through systems of rules and representations. The proposal that seems most promising is that PDP-type structures might provide the underlying framework in which a rule and representation model might be implemented. To show how one might pursue such a strategy, I discuss recent research by Barsalou on the instability of concepts and show how that might be accounted for in a system whose microstructure had a PDP architecture. I also outline how adopting a multi-leveled view of the mind, where on one level the mind employed a PDP-type system and at another level constituted a rule processing system, would allow researchers to relocate some problems which seemed difficult to explain at one level, such as the capacity for concept learning, to another level where it could be handled in a straightforward manner Bechtel, William P. (1987). Connectionism and the philosophy of mind. Southern Journal of Philosophy Supplement 26:17-41. ( Cited by 18 | Annotation | Google ) Lots of questions about connectionism. Bechtel, William P. Abrahamsen, Adele A. (1992). Connectionism and the future of folk psychology. In Robert G. Burton (ed.), Minds: Natural and Artificial . SUNY Press. ( Cited by 3 | Google | More links ) Additional links for this entry: http://books.google.com/books?hl=en=QFXNQv5o0oQLrXrlzp1rPvcShxo Bechtel, William P. (1985). Contemporary connectionism: Are the new parallel distributed processing models cognitive or associationist? Behaviorism 13:53-61. ( Cited by 8 | Google ) Bechtel, William P. (1993). The case for connectionism. Philosophical Studies 71 (2):119-54. ( Cited by 5 | Google | More links ) Additional links for this entry: http://www.springerlink.com/index/UH58178Q9414W317.pdf Bechtel, William P. (1993). The path beyond first-order connectionism. Mind and Language 8 (4):531-539. ( Cited by 6 | Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/119302074/PDFSTART Bechtel, William P. (1986). What happens to accounts of mind-brain relations if we forgo an architecture of rules and representations? Philosophy of Science Association 1986. ( Annotation | Google ) On the relationship between connectionism, symbol processing, psychology and neuroscience. Bechtel, William P. (1996). What should a connectionist philosophy of science look like? In The Churchlands and Their Critics . Oup. ( Cited by 5 | Google | More links ) Abstract: The reemergence of connectionism 2 has profoundly altered the philosophy of mind. Paul Churchland has argued that it should equally transform the philosophy of science. He proposes that connectionism offers radical and useful new ways of understanding theories and explanations Additional links for this entry: http://citeseer.ist.psu.edu/134759.html http://mechanism.ucsd.edu/~bill/research/CHURCHLA.pdf Berkeley, Istvan S. N. (ms). A revisionist history of connectionism. ( Cited by 1 | Google ) Abstract: According to the standard (recent) history of connectionism (see for example the accounts offered by Hecht-Nielsen (1990: pp. 14-19) and Dreyfus and Dreyfus (1988), or Papert's (1988: pp. 3-4) somewhat whimsical description), in the early days of Classical Computational Theory of Mind (CCTM) based AI research, there was also another allegedly distinct approach, one based upon network models. The work on network models seems to fall broadly within the scope of the term 'connectionist' (see Aizawa 1992), although the term had yet to be coined at the time. These two approaches were "two daughter sciences" according to Papert (1988: p. 3). The fundamental difference between these two 'daughters', lay (according to Dreyfus and Dreyfus (1988: p. 16)) in what they took to be the paradigm of intelligence. Whereas the early connectionists took learning to be fundamental, the traditional school concentrated upon problem solving Berkeley, István S. N. (online). Some myths of connectionism. (Cited by 1 | Google ) Abstract: Since the emergence of what Fodor and Pylyshyn (1988) call 'new connectionism', there can be little doubt that connectionist research has become a significant topic for discussion in the Philosophy of Cognitive Science and the Philosophy of Mind. In addition to the numerous papers on the topic in philosophical journals, almost every recent book in these areas contain at least a brief reference to, or discussion of, the issues raised by connectionist research (see Sterelny 1990, Searle, 1992, and O Nualláin, 1995, for example). Other texts have focused almost exclusively upon connectionist issues (see Clark, 1993, Bechtel and Abrahamsen, 1991 and Lloyd, 1989, for example). Regrettably the discussions of connectionism found in the philosophical literature suffer from a number of deficiencies. My purpose in this paper is to highlight one particular problem and attempt to take a few steps to remedy the situation Berkeley, Istvan S. N. (online). What is connectionism? ( Google ) Abstract: Connectionism is a style of modeling based upon networks of interconnected simple processing devices. This style of modeling goes by a number of other names too. Connectionist models are also sometimes referred to as 'Parallel Distributed Processing' (or PDP for short) models or networks.1 Connectionist systems are also sometimes referred to as 'neural networks' (abbreviated to NNs) or 'artificial neural networks' (abbreviated to ANNs). Although there may be some rhetorical appeal to this neural nomenclature, it is in fact misleading as connectionist networks are commonly significantly dissimilar to neurological systems. For this reason, I will avoid using this terminology, other than in direct quotations. Instead, I will follow the practice I have adopted above and use 'connectionist' as my primary term for systems of this kind Bickle, John (1995). Connectionism, reduction, and multiple realizability. Behavior and Philosophy 23 (2):29-39. ( Cited by 3 | Google ) Blackmore, Susan J. (2003). The case of the mysterious mind: Review of Radiant Cool , by Dan Lloyd. New Scientist 13:36-39. ( Cited by 3 | Google | More links ) Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=250536AN Bradshaw, Denny E. (1991). Connectionism and the specter of representationalism. In Terence E. Horgan John L. Tienson (eds.), Connectionism and the Philosophy of Mind . Kluwer. ( Cited by 4 | Annotation | Google ) Argues that connectionism allows for a more plausible epistemology of perception, compatible with direct realism rather than representationalism. With remarks on Fodor and Pylshyn's argument against Gibson. Christie, Drew (1993). Comments on Bechtel's The Case for Connectionism . Philosophical Studies 71 (2):155-162. ( Cited by 1 | Google | More links ) Additional links for this entry: http://www.springerlink.com/index/LQ37582576241K87.pdf Churchland, Patricia S. Sejnowski, Terrence J. (1989). Neural representation and neural computation. In L. Nadel (ed.), Neural Connections, Mental Computations . MIT Press. ( Cited by 78 | Annotation | Google | More links ) Implications of connectionism and neuroscience for our concept of mind. Additional links for this entry: http://www.jstor.org/stable/pdfplus/2214198.pdf Churchland, Paul M. (1989). On the nature of explanation: A PDP approach. In A Neurocomputational Perspective . MIT Press. ( Cited by 9 | Annotation | Google | More links ) We achieve explanatory understanding not through the manipulation of propositions but through the activation of prototypes. Additional links for this entry: http://portal.acm.org/citation.cfm?id=87498.87581 http://adsabs.harvard.edu/abs/1990PhyD...42..281C http://books.google.com/books?hl=en=yMyj0EhkKs802wSnrOjoUyDdztg Churchland, Paul M. (1989). On the nature of theories: A neurocomputational perspective. Minnesota Studies in the Philosophy of Science 14. ( Cited by 22 | Annotation | Google ) Clark, Andy (1990). Connectionism, competence and explanation. British Journal for the Philosophy of Science 41 (June):195-222. ( Cited by 25 | Annotation | Google | More links ) Abstract: A competence model describes the abstract structure of a solution to some problem. or class of problems, facing the would-be intelligent system. Competence models can be quite derailed, specifying far more than merely the function to be computed. But for all that, they are pitched at some level of abstraction from the details of any particular algorithm or processing strategy which may be said to realize the competence. Indeed, it is the point and virtue of such models to specify some equivalence class of algorithms/processing strategies so that the common properties highlighted by the chosen class may feature in psychologically interesting accounts. A question arises concerning the type of relation a theorist might expect to hold between such a competence model and a psychologically real processing strategy. Classical work in cognitive science expects the actual processing to depend on explicit or tacit knowledge of the competence theory. Connectionist work, for reasons to be explained, represents a departure from this norm. But the precise way in which a connectionist approach may disturb the satisfying classical symmetry of competence and processing has yet to be properly specified. A standard ?Newtonian? connectionist account, due to Paul Smolensky, is discussed and contrasted with a somewhat different ?rogue? account. A standard connectionist understanding has it that a classical competence theory describes an idealized subset of a network's behaviour. But the network's behaviour is not to be explained by its embodying explicit or tacit knowledge of the information laid out in the competence theory. A rogue model, by contrast, posits either two systems, or two aspects of a single system, such that one system does indeed embody the knowledge laid out in the competence theory Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/abstract/41/2/195 http://bjps.oxfordjournals.org/cgi/reprint/41/2/195 http://www.jstor.org/stable/pdfplus/687772.pdf Clark, Andy (1995). Connectionist minds. In Connectionism: Debates on Psychological Explanation . Cambridge: Blackwell. ( Cited by 10 | Google ) Clark, Andy (1989). Microcognition. MIT Press. ( Cited by 300 | Annotation | Google | More links ) All kinds of stuff on connectionism and philosophy. Additional links for this entry: http://portal.acm.org/citation.cfm?id=SERIES9429.70109 http://books.google.com/books?hl=en=-6Hu6F83CquItCX-Lxms0YhuGlU http://books.google.com/books?hl=en=TrQYGdxKRMudEvqmpbDjxjPwIaI http://books.google.com/books?hl=en=WxqyIRmYNeyowDGBx4FLlmfv08k http://books.google.com/books?hl=en=faEp603Cxj7uK7mYqVrZXJWpOco http://books.google.com/books?hl=en=Qq7mNE3j0WBCdbrqfwaSqfW5LJ8 Clark, Andy (1989). Microfunctionalism: Connectionism and the Scientific Explanation of Mental States. In A. Clark (ed.), Microcognition: Philosophy, Cognitive Science, and Parallel Distributed Processing . MIT Press. ( Google | More links ) Abstract: This is an amended version of material that first appeared in A. Clark, Microcognition: Philosophy, Cognitive Science, and Parallel Distributed Processing (MIT Press, Cambridge, MA, 1989), Ch. 1, 2, and 6. It appears in German translation in Metzinger,T (Ed) DAS LEIB-SEELE-PROBLEM IN DER ZWEITEN HELFTE DES 20 JAHRHUNDERTS (Frankfurt am Main: Suhrkamp. 1999) Additional links for this entry: http://www.cogs.indiana.edu/andy/microfx.pdf http://www.philosophy.ed.ac.uk/staff/clark/pubs/microfx.pdf Clark, Andy (1991). Microcognition: Philosophy, Cognitive Science, and Parallel Distributed Processing. Cambridge: MIT Press. ( Cited by 224 | Google | More links ) Additional links for this entry: http://portal.acm.org/citation.cfm?id=SERIES9429.70109 http://books.google.com/books?hl=en=-6Hu6F83CquItCX-Lxms0YhuGlU http://books.google.com/books?hl=en=GCwUSyOMg9UCDXJEGf1L3cfzo-g http://books.google.com/books?hl=en=-kb67YkQLwQCZ538E07C2vpuH-4 Clark, Andy Eliasmith, Chris (2002). Philosophical issues in brain theory and connectionism. In Michael A. Arbib (ed.), The Handbook of Brain Theory and Neural Networks, Second Edition . MIT Press. ( Cited by 7 | Google | More links ) Additional links for this entry: http://watarts.uwaterloo.ca/~celiasmi/Papers/withclark.html Collier, Mark (1999). Filling the Gaps: Hume and Connectionism on the Continued Existence of Unperceived Objects. Hume Studies 25 (1 and 2):155-170. ( Google ) Copeland, Jack (1996). On Alan Turing's anticipation of connectionism. Synthese 108 (3):361-377. ( Cited by 20 | Google | More links ) Abstract:   It is not widely realised that Turing was probably the first person to consider building computing machines out of simple, neuron-like elements connected together into networks in a largely random manner. Turing called his networks unorganised machines. By the application of what he described as appropriate interference, mimicking education an unorganised machine can be trained to perform any task that a Turing machine can carry out, provided the number of neurons is sufficient. Turing proposed simulating both the behaviour of the network and the training process by means of a computer program. We outline Turing's connectionist project of 1948 Additional links for this entry: http://citeseer.ist.psu.edu/copeland96alan.html http://www.springerlink.com/content/v3571g00l3504874/fulltext.pdf http://www.springerlink.com/index/V3571G00L3504874.pdf Cummins, Robert E. (1995). Connectionist and the rationale constraint on cognitive explanations. Philosophical Perspectives 9:105-25. ( Cited by 3 | Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2214214.pdf Cummins, Robert E. Schwarz, Georg (1991). Connectionism, computation, and cognition. In Terence E. Horgan John L. Tienson (eds.), Connectionism and the Philosophy of Mind . Kluwer. ( Cited by 55 | Annotation | Google ) Explicates computationalism, and discusses ways in which connectionism might end up non-computational: if causal states cross-classify representational states, or if transitions between representations aren't computable. Cummins, Robert E. Schwarz, Georg (1987). Radical connectionism. Southern Journal of Philosophy Supplement 26:43-61. ( Cited by 8 | Annotation | Google ) On computation and representation in AI and connectionism, and on problems for radical connectionism in reconciling these without denying representation or embracing mystery. Davies, Martin (1989). Connectionism, modularity and tacit knowledge. British Journal for the Philosophy of Science 40 (December):541-55. ( Cited by 11 | Annotation | Google | More links ) Argues that connectionist networks don't have tacit knowledge of modular theories (as representations lack the appropriate structure, etc.). Abstract: In this paper, I define tacit knowledge as a kind of causal-explanatory structure, mirroring the derivational structure in the theory that is tacitly known. On this definition, tacit knowledge does not have to be explicitly represented. I then take the notion of a modular theory, and project the idea of modularity to several different levels of description: in particular, to the processing level and the neurophysiological level. The fundamental description of a connectionist network lies at a level between the processing level and the physiological level. At this level, connectionism involves a characteristic departure from modularity, and a correlative absence of syntactic structure. This is linked to the fact that tacit knowledge descriptions of networks are only approximately true. A consequence is that strict causal systematicity in cognitive processes poses a problem for the connectionist programme Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/abstract/40/4/541 http://bjps.oxfordjournals.org/cgi/reprint/40/4/541 http://www.jstor.org/stable/pdfplus/687740.pdf Duran, Jane Doell, Ruth (1993). Naturalized epistemology, connectionism and biology. Dialectica 47 (4):327-336. ( Google ) García-Carpintero, Manuel (1995). The philosophical import of connectionism: A critical notice of Andy Clark's associative engines. Mind and Language 10 (4):370-401. ( Cited by 1 | Google ) Globus, Gordon G. (1992). Derrida and connectionism: Differance in neural nets. Philosophical Psychology 5 (2):183-97. ( Cited by 2 | Google ) Abstract: A possible relation between Derrida's deconstruction of metaphysics and connectionism is explored by considering diff rance in neural nets terms. First diff rance , as the crossing of Saussurian difference and Freudian deferral, is modeled and then the fuller 'sheaf of diff rance is taken up. The metaphysically conceived brain has two versions: in the traditional computational version the brain processes information like a computer and in the connectionist version the brain computes input vector to output vector transformations non-symbolically. The 'deconstructed brain' neither processes information nor computes functions but is spontaneously economical Hadley, Robert F. (1999). Connectionism and novel combinations of skills: Implications for cognitive architecture. Minds and Machines 9 (2):197-221. ( Cited by 11 | Google | More links ) Abstract:   In the late 1980s, there were many who heralded the emergence of connectionism as a new paradigm – one which would eventually displace the classically symbolic methods then dominant in AI and Cognitive Science. At present, there remain influential connectionists who continue to defend connectionism as a more realistic paradigm for modeling cognition, at all levels of abstraction, than the classical methods of AI. Not infrequently, one encounters arguments along these lines: given what we know about neurophysiology, it is just not plausible to suppose that our brains are digital computers. Thus, they could not support a classical architecture. I argue here for a middle ground between connectionism and classicism. I assume, for argument's sake, that some form(s) of connectionism can provide reasonably approximate models – at least for lower-level cognitive processes. Given this assumption, I argue on theoretical and empirical grounds that most human mental skills must reside in separate connectionist modules or sub-networks. Ultimately, it is argued that the basic tenets of connectionism, in conjunction with the fact that humans often employ novel combinations of skill modules in rule following and problem solving, lead to the plausible conclusion that, in certain domains, high level cognition requires some form of classical architecture. During the course of argument, it emerges that only an architecture with classical structure could support the novel patterns of information flow and interaction that would exist among the relevant set of modules. Such a classical architecture might very well reside in the abstract levels of a hybrid system whose lower-level modules are purely connectionist Additional links for this entry: http://portal.acm.org/citation.cfm?id=596712.596817 http://www.springerlink.com/content/l64416n38j41q278/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=206285=1 http://www.springerlink.com/index/L64416N38J41Q278.pdf http://www.ingentaconnect.com/content/klu/mind/1999/00000009/00000002/00206285 Hatfield, Gary (1990). Gibsonian representations and connectionist symbol-processing: Prospects for unification. Psychological Research 52:243-52. ( Cited by 5 | Annotation | Google ) Gibson is compatible with connectionism. In both, we can have rule-instantiation without rule-following. Horgan, Terence E. Tienson, John L. (1999). Authors' replies. Acta Analytica 22 (22):275-287. ( Google ) Horgan, Dianne D. Hacker, Douglas J. (1999). Beginning a theoretician-practitioner dialogue about connectionism. Acta Analytica 22 (22):261-273. ( Google ) Horgan, Terence E. Tienson, John L. (eds.) (1991). Connectionism and the Philosophy of Mind. Kluwer. ( Cited by 30 | Google ) Abstract: A third of the papers in this volume originated at the 1987 Spindel Conference ... at Memphis State University--Pref. Horgan, Terence E. Tienson, John L. (1996). Connectionism and the Philosophy of Psychology. MIT Press. ( Cited by 123 | Google ) Abstract: In Connectionism and the Philosophy of Psychology, Horgan and Tienson articulate and defend a new view of cognition. Horgan, Terence E. (1997). Connectionism and the philosophical foundations of cognitive science. Metaphilosophy 28 (1-2):1-30. ( Cited by 5 | Google | More links ) Additional links for this entry: http://www.blackwell-synergy.com/links/doi/10.1111/1467-9973.00039 http://www.blackwell-synergy.com/doi/abs/10.1111/1467-9973.00039 http://www.ingentaconnect.com/content/bpl/meta/1997/00000028/F0020001/art00001 http://www.ingentaconnect.com/content/bpl/meta/1997/00000028/f0020001 Horgan, Terence E. (1997). Modelling the noncomputational mind: Reply to Litch. Philosophical Psychology 10 (3):365-371. ( Google ) Abstract: I explain why, within the nonclassical framework for cognitive science we describe in the book, cognitive-state transitions can fail to be tractably computable even if they are subserved by a discrete dynamical system whose mathematical-state transitions are tractably computable. I distinguish two ways that cognitive processing might conform to programmable rules in which all operations that apply to representation-level structure are primitive, and two corresponding constraints on models of cognition. Although Litch is correct in maintaining that classical cognitive science is not committed to the first constraint, it is committed to the second. This fact constitutes an illuminating gloss on our claim that one foundational assumption of classicism is that human cognition conforms to programmable, representation-level, rules Horgan, Terence E. (1999). Short prcis of connectionism and the philosophy of psychology. Acta Analytica 22 (22):9-21. ( Cited by 4 | Google ) Humphreys, Glyn W. (1986). Information-processing systems which embody computational rules: The connectionist approach. Mind and Language 1:201-12. ( Cited by 2 | Google ) Kirsh, David (1992). PDP Learnability and Innate Knowledge of Language. In S. Davis (ed.), Connectionism: Theory and practice (Volume III of The Vancouver Studies in Cognitive Science . Oxford University press. ( Google ) Laakso, Aarre Cottrell, Garrison W. (2006). Churchland on connectionism. In Brian L. Keeley (ed.), Paul Churchland . Cambridge: Cambridge University Press. ( Google ) Legg, C. R. (1988). Connectionism and physiological psychology: A marriage made in heaven? Philosophical Psychology 1:263-78. ( Google ) Litch, Mary (1997). Computation, connectionism and modelling the mind. Philosophical Psychology 10 (3):357-364. ( Google ) Abstract: Any analysis of the concept of computation as it occurs in the context of a discussion of the computational model of the mind must be consonant with the philosophic burden traditionally carried by that concept as providing a bridge between a physical and a psychological description of an agent. With this analysis in hand, one may ask the question: are connectionist-based systems consistent with the computational model of the mind? The answer depends upon which of several versions of connectionism one presupposes: non-learning connectionist-based systems as simulated on digital computers are consistent with the computational model of the mind, whereas connectionist-based systems (/dynamical systems) qua analog systems are not Litch, Mary (1999). Learning connectionist networks and the philosophy of psychology. Acta Analytica 22 (22):87-110. ( Google ) Lloyd, Dan (1994). Connectionist hysteria: Reducing a Freudian case study to a network model. Philosophy, Psychiatry, and Psychology 1 (2):69-88. ( Cited by 10 | Google ) Lloyd, Dan (1989). Parallel distributed processing and cognition: Only connect? In Simple Minds . MIT Press. ( Annotation | Google ) An overview: local/distributed/featural representations; explanation in connectionism (how to avoid big mush); relation to neuroscience; explicit representations of rules vs weight matrices. Lycan, William G. (1991). Homuncular functionalism meets PDP. In William Ramsey, Stephen P. Stich D. Rumelhart (eds.), Philosophy and Connectionist Theory . Lawrence Erlbaum. ( Cited by 7 | Annotation | Google ) On various ways in which connectionism relates to representational homuncular functionalism, e.g. on implementation, eliminativism, and explanation. Macdonald, C. (ed.) (1995). Connectionism: Debates on Psychological Explanation. Blackwell. ( Cited by 36 | Google ) McLaughlin, Brian P. (1987). Tye on connectionism. Southern Journal of Philosophy (Suppl.) 185:185-193. ( Cited by 2 | Google ) Mills, Stephen L. (1993). Wittgenstein and connectionism: A significant complementarity? Philosophy 34:137-157. ( Cited by 4 | Google ) Miscevic, Nenad (1994). Connectionism and epistemic value. Acta Analytica 12 (12):19-37. ( Google ) Nenon, Thomas J. (1994). Connectionism and phenomenology. In Phenomenology of the Cultural Disciplines . Dordrecht: Kluwer. ( Google ) Niklasson, L. F. van Gelder, Tim (online). Can connectionist models exhibit non-classical structure sensitivity? ( Cited by 30 | Google | More links ) Abstract: Department of Computer Science Philosophy Program, Research School of Social Sciences University of Skövde, S-54128, SWEDEN Australian National University, Canberra ACT 0200 Additional links for this entry: http://citeseer.ist.psu.edu/niklasson94can.html O'Brien, Gerard Opie, Jonathan (2002). Radical connectionism: Thinking with (not in) language. Language and Communication 22 (3):313-329. ( Cited by 12 | Google | More links ) Abstract: In this paper we defend a position we call radical connectionism. Radical connectionism claims that cognition _never_ implicates an internal symbolic medium, not even when natural language plays a part in our thought processes. On the face of it, such a position renders the human capacity for abstract thought quite mysterious. However, we argue that connectionism is committed to an analog conception of neural computation, and that representation of the abstract is no more problematic for a system of analog vehicles than for a symbol system. Natural language is therefore not required as a representational medium for abstract thought. Since natural language is arguably not a representational medium _at all_, but a conventionally governed scheme of communicative signals, we suggest that the role of internalised (i.e., self- directed) language is best conceived in terms of the coordination and control of cognitive activities within the brain Additional links for this entry: http://linkinghub.elsevier.com/retrieve/pii/S0271530902000101 http://www.ingentaconnect.com/content/els/02715309/2002/00000022/00000003/art00010 Piccinini, Gualtiero (2007). Connectionist computation. In Gualtiero Piccinini (ed.), Proceedings of the 2007 International Joint Conference on Neural Networks . ( Google ) Abstract: The following three theses are inconsistent: (1) (Paradigmatic) connectionist systems perform computations. (2) Performing computations requires executing programs. (3) Connectionist systems do not execute programs. Many authors embrace (2). This leads them to a dilemma: either connectionist systems execute programs or they don't compute. Accordingly, some authors attempt to deny (1), while others attempt to deny (3). But as I will argue, there are compelling reasons to accept both (1) and (3). So, we should replace (2) with a more satisfactory account of computation. Once we do, we can see more clearly what is peculiar to connectionist computation. Place, Ullin T. (1999). Connectionism and the problem of consciousness. Acta Analytica 22 (22):197-226. ( Google ) Plunkett, Kim (2001). Connectionism today. Synthese 129 (2):185-194. ( Cited by 2 | Google | More links ) Abstract:   Connectionist networks have been used to model a wide range of cognitivephenomena, including developmental, neuropsychological and normal adultbehaviours. They have offered radical alternatives to traditional accounts ofwell-established facts about cognition. The primary source of the success ofthese models is their sensitivity to statistical regularities in their trainingenvironment. This paper provides a brief description of the connectionisttoolbox and how this has developed over the past 2 decades, with particularreference to the problem of reading aloud Additional links for this entry: http://www.kluweronline.com/article.asp?PIPS=322207=1 http://www.springerlink.com/index/K0X25844K059H4W2.pdf http://www.ingentaconnect.com/content/klu/synt/2001/00000129/00000002/00322207 Ramsey, William Stich, Stephen P. (1990). Connectionism and three levels of nativism. Synthese 82 (2):177-205. ( Cited by 14 | Annotation | Google | More links ) How connectionism bears on the nativism debate. Conclusion: not too much. Abstract:   Along with the increasing popularity of connectionist language models has come a number of provocative suggestions about the challenge these models present to Chomsky's arguments for nativism. The aim of this paper is to assess these claims. We begin by reconstructing Chomsky's argument from the poverty of the stimulus and arguing that it is best understood as three related arguments, with increasingly strong conclusions. Next, we provide a brief introduction to connectionism and give a quick survey of recent efforts to develop networks that model various aspects of human linguistic behavior. Finally, we explore the implications of this research for Chomsky's arguments. Our claim is that the relation between connectionism and Chomsky's views on innate knowledge is more complicated than many have assumed, and that even if these models enjoy considerable success the threat they pose for linguistic nativism is small Additional links for this entry: http://www.springerlink.com/content/p66n338hjq44332p/fulltext.pdf http://www.springerlink.com/index/P66N338HJQ44332P.pdf Ramsey, William ; Stich, Stephen P. Rumelhart, D. M. (eds.) (1991). Philosophy and Connectionist Theory. Lawrence Erlbaum. ( Cited by 46 | Google ) Abstract: The philosophy of cognitive science has recently become one of the most exciting and fastest growing domains of philosophical inquiry and analysis. Until the early 1980s, nearly all of the models developed treated cognitive processes -- like problem solving, language comprehension, memory, and higher visual processing -- as rule-governed symbol manipulation. However, this situation has changed dramatically over the last half dozen years. In that period there has been an enormous shift of attention toward connectionist models of cognition that are inspired by the network-like architecture of the brain. Because of their unique architecture and style of processing, connectionist systems are generally regarded as radically different from the more traditional symbol manipulation models. This collection was designed to provide philosophers who have been working in the area of cognitive science with a forum for expressing their views on these recent developments. Because the symbol-manipulating paradigm has been so important to the work of contemporary philosophers, many have watched the emergence of connectionism with considerable interest. The contributors take very different stands toward connectionism, but all agree that the potential exists for a radical shift in the way many philosophers think of various aspects of cognition. Exploring this potential and other philosophical dimensions of connectionist research is the aim of this volume Rosenberg, Jay F. (1989). Connectionism and cognition. Bielefeld Report . ( Cited by 7 | Annotation | Google ) Criticism of Churchland's connectionist epistemology. Sehon, Scott R. (1998). Connectionism and the causal theory of action explanation. Philosophical Psychology 11 (4):511-532. ( Cited by 2 | Google ) Abstract: It is widely assumed that common sense psychological explanations of human action are a species of causal explanation. I argue against this construal, drawing on Ramsey et al.'s paper, “Connectionism, eliminativism, and the future of folk psychology”. I argue that if certain connec-tionist models are correct, then mental states cannot be identified with functionally discrete causes of behavior, and I respond to some recent attempts to deny this claim. However, I further contend that our common sense psychological practices are not committed to the falsity of such connectionist models. The paper concludes that common sense psychology is not committed to the identification of mental states with functionally discrete causes of behavior, and hence that common sense psychology is not committed to the causal account of action explanation Shanon, Benny (1992). Are connectionist models cognitive? Philosophical Psychology 5 (3):235-255. ( Cited by 5 | Annotation | Google ) In some senses of "cognitive", yes; in other senses, no. Phenomenological, theoretical, and sociological perspectives. Toward meaning-laden models. Abstract: In their critique of connectionist models Fodor and Pylyshyn (1988) dismiss such models as not being cognitive or psychological. Evaluating Fodor and Pylyshyn's critique requires examining what is required in characterizating models as 'cognitive'. The present discussion examines the various senses of this term. It argues the answer to the title question seems to vary with these different senses. Indeed, by one sense of the term, neither representa-tionalism nor connectionism is cognitive. General ramifications of such an appraisal are discussed and alternative avenues for cognitive research are suggested Smith, Barry (1997). The connectionist mind: A study of Hayekian psychology. In Stephen F. Frowen (ed.), Hayek: Economist and Social Philosopher: A Critical Retrospect . St. Martin's Press. (Cited by 16 | Google | More links ) Abstract: Introduction I shall begin my remarks with some discussion of recent work in cognitive science, and the participants in this meeting might find it useful to note that I might equally well have chosen as title of my paper something like 'Artificial Intelligence and the Free Market Order'. They might care to note also that I am, as far as the achievements and goals of research in artificial intelligence are concerned, something of a sceptic. My appeal to cognitive science in what follows is designed to serve clarificatory ends, and to raise new questions, of a sort which will become clear as the paper progresses Additional links for this entry: http://ontology.buffalo.edu/smith/articles/HAYEK.HTM http://wings.buffalo.edu/philosophy/faculty/smith/articles/HAYEK.HTM http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:306 http://wings.buffalo.edu/academic/department/philosophy/faculty/smith/articles/HAYEK.HTM http://cogprints.org/306/1/connect.html http://cogprints.org/306/0/connect.html Stark, Herman E. (1994). Connectionism and the form of rational norms. Acta Analytica 12 (12):39-53. ( Cited by 2 | Google ) Sterelny, Kim (1990). Connectionism. In The Representational Theory of Mind . Blackwell. ( Google ) Thagard, Paul R. (1989). Connectionism and epistemology: Goldman on Winner-take-all networks. Philosophia 19 (2-3):189-196. ( Cited by 1 | Google | More links ) Additional links for this entry: http://www.springerlink.com/index/4416534216015862.pdf Tienson, John L. (1987). Introduction to connectionism. Southern Journal of Philosophy (Suppl.) 1:1-16. ( Cited by 15 | Google ) van Gelder, Tim (1993). Connectionism and the mind-body problem: Exposing the distinction between mind and cognition. Artificial Intelligence Review 7:355-369. ( Google ) 6.3g Philosophy of Connectionism, Foundational Empirical Issues Aizawa, Kenneth (1992). Connectionism and artificial intelligence: History and philosophical interpretation. Journal for Experimental and Theoretical Artificial Intelligence 4:1992. (Cited by 1 | Google | More links ) Additional links for this entry: http://www.informaworld.com/index/777592363.pdf Beaman, C. Philip (2000). Neurons amongst the symbols? Behavioral and Brain Sciences 23 (4):468-470. ( Google ) Abstract: Page's target article presents an argument for the use of localist, connectionist models in future psychological theorising. The “manifesto” marshalls a set of arguments in favour of localist connectionism and against distributed connectionism, but in doing so misses a larger argument concerning the level of psychological explanation that is appropriate to a given domain Berkeley, Istvan S. N. (ms). Connectionism reconsidered: Minds, machines and models. ( Cited by 1 | Google | More links ) Abstract: In this paper the issue of drawing inferences about biological cognitive systems on the basis of connectionist simulations is addressed. In particular, the justification of inferences based on connectionist models trained using the backpropagation learning algorithm is examined. First it is noted that a justification commonly found in the philosophical literature is inapplicable. Then some general issues are raised about the relationships between models and biological systems. A way of conceiving the role of hidden units in connectionist networks is then introduced. This, in combination with an assumption about the way evolution goes about solving problems, is then used to suggest a means of justifying inferences about biological systems based on connectionist research Additional links for this entry: http://www.ucs.louisiana.edu/~isb9112/dept/phil341/conrecon/conrecon.html http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:1975 http://cogprints.org/1975/3/CReconF.pdf http://cogprints.org/1975/0/CReconF.pdf Clark, Andy (1994). Representational trajectories in connectionist learning. Minds and Machines 4 (3):317-32. ( Cited by 5 | Annotation | Google | More links ) On how to get connectionist networks to learn about structured task domains. Concentrates on incremental learning, and other developmental/scaffolding strategies. With remarks on systematicity. Abstract:   The paper considers the problems involved in getting neural networks to learn about highly structured task domains. A central problem concerns the tendency of networks to learn only a set of shallow (non-generalizable) representations for the task, i.e., to miss the deep organizing features of the domain. Various solutions are examined, including task specific network configuration and incremental learning. The latter strategy is the more attractive, since it holds out the promise of a task-independent solution to the problem. Once we see exactly how the solution works, however, it becomes clear that it is limited to a special class of cases in which (1) statistically driven undersampling is (luckily) equivalent to task decomposition, and (2) the dangers of unlearning are somehow being minimized. The technique is suggestive nonetheless, for a variety of developmental factors may yield the functional equivalent of both statistical AND informed undersampling in early learning Additional links for this entry: http://www.springerlink.com/content/content/r7812132kh676544/fulltext.pdf http://www.springerlink.com/content/r7812132kh676544/fulltext.pdf http://www.springerlink.com/index/R7812132KH676544.pdf Clark, Andy Thornton, S. (1997). Trading spaces: Computation, representation, and the limits of uninformed learning. Behavioral and Brain Sciences 20 (1):57-66. ( Cited by 204 | Google | More links ) Additional links for this entry: http://www.bbsonline.org/documents/a/00/00/04/44/index.html http://www.bbsonline.org/Preprints/OldArchive/bbs.clark.html http://www.psych.nyu.edu/gary/marcusArticles/Marcus 1997 BBS.pdf http://www.denizyuret.com/ref/clark_andy/clark.trading-spaces.ps http://bbsonline.cup.cam.ac.uk/Preprints/OldArchive/bbs.clark.html http://neuron-ai.tuke.sk/~hudecm/d/TradeSpacesComputationRepresentationAndTheLimitsOfUninformedLearning_clark95trading.ps http://journals.cambridge.org/action/displayFulltext?type=1=29217 http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation http://www.journals.cambridge.org/abstract_S0140525X97000022 http://journals.cambridge.org/article_S0140525X97240021 http://journals.cambridge.org/article_S0140525X97300028 Clark, Andy Thornton, Chris (1997). Relational learning re-examined. Behavioral and Brain Sciences 20 (1):83-90. ( Google ) Cliff, D. (1990). Computational Neuroethology: A Provisional Manifesto. In Jean-Arcady Meyer Stewart W. Wilson (eds.), From Animals to Animats: Proceedings of The First International Conference on Simulation of Adaptive Behavior (Complex Adaptive Systems) . Cambridge University Press. ( Cited by 103 | Annotation | Google | More links ) Criticizes connectionism for not being sufficiently rooted in neuroscience, and for not being grounded in the world. Additional links for this entry: http://portal.acm.org/citation.cfm?id=116521 http://citeseer.ist.psu.edu/cliff91computational.html Dawson, Michael R. W. Schopflocher, D. P. (1992). Autonomous processing in parallel distributed processing networks. Philosophical Psychology 5 (2):199-219. ( Google ) Abstract: This paper critically examines the claim that parallel distributed processing (PDP) networks are autonomous learning systems. A PDP model of a simple distributed associative memory is considered. It is shown that the 'generic' PDP architecture cannot implement the computations required by this memory system without the aid of external control. In other words, the model is not autonomous. Two specific problems are highlighted: (i) simultaneous learning and recall are not permitted to occur as would be required of an autonomous system; (ii) connections between processing units cannot simultaneously represent current and previous network activation as would be required if learning is to occur. Similar problems exist for more sophisticated networks constructed from the generic PDP architecture. We argue that this is because these models are not adequately constrained by the properties of the functional architecture assumed by PDP modelers. It is also argued that without such constraints, PDP researchers cannot claim to have developed an architecture radically different from that proposed by the Classical approach in cognitive science Franklin, James (1996). How a neural net grows symbols. Proc 7. ( Cited by 1 | Google ) Abstract: Brains, unlike artiﬁcial neural nets, use sym- bols to summarise and reason about percep- tual input. But unlike symbolic AI, they “ground” the symbols in the data: the sym- bols have meaning in terms of data, not just meaning imposed by the outside user. If neu- ral nets could be made to grow their own sym- bols in the way that brains do, there would be a good prospect of combining neural networks and symbolic AI, in such a way as to combine the good features of each Graham, George (1987). Connectionism in Pavlovian harness. Southern Journal of Philosophy (Suppl.) 73:73-91. ( Cited by 2 | Google ) Hanson, Susan Burr, D. (1990). What connectionist models learn. Behavioral and Brain Sciences . ( Cited by 81 | Annotation | Google ) What's new to connectionism is not learning or representation but the way that learning and representation interact. Kaplan, S. ; Weaver, M. French, Robert M. (1990). Active symbols and internal models: Towards a cognitive connectionism. AI and Society . ( Cited by 18 | Annotation | Google | More links ) Addresses behaviorist/associationist charges. Connectionism needs recurrent circuits to support active symbols. Additional links for this entry: http://www.springerlink.com/index/G674K4JTV61G1510.pdf Kirsh, David (1987). Putting a price on cognition. Southern Journal of Philosophy Supplement 26:119-35. ( Cited by 9 | Annotation | Google ) On the importance of variable binding, and why it's hard with connectionism. Lachter, J. Bever, Thomas G. (1988). The relation between linguistic structure and associative theories of language learning. Cognition 28:195-247. ( Cited by 66 | Annotation | Google | More links ) Criticism of connectionist language models. They build in too much. Additional links for this entry: http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation Mills, Stephen L. (1989). Connectionism, the classical theory of cognition, and the hundred step constraint. Acta Analytica 4 (4):5-38. ( Google ) Nelson, Raymond J. (1989). Philosophical issues in Edelman's neural darwinism. Journal of Experimental and Theoretical Artificial Intelligence 1:195-208. ( Cited by 2 | Annotation | Google | More links ) On the relationship between ND, PDP and AI. All are computational. Additional links for this entry: http://www.informaworld.com/index/778705900.pdf Oaksford, Mike ; Chater, Nick Stenning, Keith (1990). Connectionism, classical cognitive science and experimental psychology. AI and Society . ( Cited by 11 | Annotation | Google | More links ) Connectionism is better at explaining empirical findings about mind. Additional links for this entry: http://www.springerlink.com/index/L20P853057770738.pdf O'Brien, Gerard (1998). The role of implementation in connectionist explanation. Psycoloquy 9 (6). ( Cited by 8 | Google | More links ) Additional links for this entry: http://psycprints.ecs.soton.ac.uk/archive/00000555/ http://psycprints.ecs.soton.ac.uk/archive/00000555/#html Pinker, Steven Prince, Alan (1988). On language and connectionism. Cognition 28:73-193. ( Cited by 612 | Annotation | Google | More links ) Extremely thorough criticism of the R past-tense-learning model, with arguments on why connectionism can't handle linguistic rules. Additional links for this entry: http://portal.acm.org/citation.cfm?id=58068 http://cogsci.soton.ac.uk/harnad/Papers/Py104/pinker.conn.html http://cogsci.soton.ac.uk/~harnad/Papers/Py104/pinker.conn.html http://books.google.com/books?hl=en=Eg_CAnM1Mg4n2ZmvA2OlpN9m9sw http://books.google.com/books?hl=en=v5UAf_38SNcEICx0yEG3cIhTzJo http://books.google.com/books?hl=en=88u9kHQCTGaVdnM7m8iNb2PPKTY http://books.google.com/books?hl=en=4kAym1qCPEcgjt8JlqshIRKhIDU http://books.google.com/books?hl=en=xDNu_Z09OZAy27O-n3WfUmMLQUs http://books.google.com/books?hl=en=5sN8KnPgqLSBs0KApMSatX82EJg http://books.google.com/books?hl=en=KeLD5m7m1-u6IfRCZkbe94UL1Es http://books.google.com/books?hl=en=cRtyKLds-QhYetV0FyVJPWnziVI http://books.google.com/books?hl=en=E7BrmcOGPhpsKDfBsYC2IVCxHn0 http://books.google.com/books?hl=en=HMW-DysLVZiCJI1ORuFT8UdXKHM http://books.google.com/books?hl=en=ydGhKMgRwpBcRx4at94y5lYz9mE http://books.google.com/books?hl=en=rWgaifux53r8Vm_6Rj2-68EJMjE http://books.google.com/books?hl=en=SVeTbYG1MwT2L1atC0yVUgMx0ys http://books.google.com/books?hl=en=-djAWMRY1P5GSdsHAJzC8nqpsAY http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation Potrc, Matjaz (1995). Consciousness and connectionism--the problem of compatability of type identity theory and of connectionism. Acta Analytica 13 (13):175-190. ( Google ) Ross, Don (1998). Internal recurrence. Dialogue 37 (1):155-161. ( Google ) Roth, Martin (2005). Program execution in connectionist networks. Mind and Language 20 (4):448-467. ( Cited by 1 | Google | More links ) Abstract: Recently, connectionist models have been developed that seem to exhibit structuresensitive cognitive capacities without executing a program. This paper examines one such model and argues that it does execute a program. The argument proceeds by showing that what is essential to running a program is preserving the functional structure of the program. It has generally been assumed that this can only be done by systems possessing a certain temporalcausal organization. However, counterfactualpreserving functional architecture can be instantiated in other ways, for example geometrically, which are realizable by connectionist networks Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/118696238/PDFSTART 6.4 Special Topics in AI Rhodes, Kris (ms). Vindication of the Rights of Machine. ( Google | More links ) Abstract: In this paper, I argue that certain Machines can have rights independently of whether they are sentient, or conscious, or whatever you might call it. Additional links for this entry: http://www.igradeyourpaper.com/VRMv3.doc Bostrom, Nick (ms). Ethical issues in advanced artificial intelligence. ( Google | More links ) Abstract: The ethical issues related to the possible future creation of machines with general intellectual capabilities far outstripping those of humans are quite distinct from any ethical problems arising in current automation and information systems. Such superintelligence would not be just another technological development; it would be the most important invention ever made, and would lead to explosive progress in all scientific and technological fields, as the superintelligence would conduct research with superhuman efficiency. To the extent that ethics is a cognitive pursuit, a superintelligence could also easily surpass humans in the quality of its moral thinking. However, it would be up to the designers of the superintelligence to specify its original motivations. Since the superintelligence may become unstoppably powerful because of its intellectual superiority and the technologies it could develop, it is crucial that it be provided with human-friendly motivations. This paper surveys some of the unique ethical issues in creating superintelligence, and discusses what motivations we ought to give a superintelligence, and introduces some cost-benefit considerations relating to whether the development of superintelligent machines ought to be accelerated or retarded Additional links for this entry: http://www.nickbostrom.com/ethics/ai.html Bostrom, Nick (1998). How long before superintelligence? International Journal of Futures Studies 2. ( Cited by 22 | Google ) Abstract: _This paper outlines the case for believing that we will have superhuman artificial intelligence_ _within the first third of the next century. It looks at different estimates of the processing power of_ _the human brain; how long it will take until computer hardware achieve a similar performance;_ _ways of creating the software through bottom-up approaches like the one used by biological_ _brains; how difficult it will be for neuroscience figure out enough about how brains work to_ _make this approach work; and how fast we can expect superintelligence to be developed once_ _there is human-level artificial intelligence._ Bostrom, Nick (2003). Taking intelligent machines seriously: Reply to critics. Futures 35 (8):901-906. ( Google | More links ) Abstract: In an earlier paper in this journal[1], I sought to defend the claims that (1) substantial probability should be assigned to the hypothesis that machines will outsmart humans within 50 years, (2) such an event would have immense ramifications for many important areas of human concern, and that consequently (3) serious attention should be given to this scenario. Here, I will address a number of points made by several commentators Additional links for this entry: http://www.questia.com/PM.qst?a=o=5002030442 http://linkinghub.elsevier.com/retrieve/pii/S0016328703000466 http://www.ingentaconnect.com/content/els/00163287/2003/00000035/00000008/art00046 Bostrom, Nick (ms). When machines outsmart humans. ( Google ) Abstract: Artificial intelligence is a possibility that should not be ignored in any serious thinking about the future, and it raises many profound issues for ethics and public policy that philosophers ought to start thinking about. This article outlines the case for thinking that human-level machine intelligence might well appear within the next half century. It then explains four immediate consequences of such a development, and argues that machine intelligence would have a revolutionary impact on a wide range of the social, political, economic, commercial, technological, scientific and environmental issues that humanity will face over the coming decades Chalmers, David J. , The singularity: A philosophical analysis. ( Google ) Abstract: What happens when machines become more intelligent than humans? One view is that this event will be followed by an explosion to ever-greater levels of intelligence, as each generation of machines creates more intelligent machines in turn. This intelligence explosion is now often known as the “singularity”. The basic argument here was set out by the statistician I.J. Good in his 1965 article “Speculations Concerning the First Ultraintelligent Machine”: Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion”, and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make. The key idea is that a machine that is more intelligent than humans will be better than humans at designing machines. So it will be capable of designing a machine more intelligent than the most intelligent machine that humans can design. So if it is itself designed by humans, it will be capable of designing a machine more intelligent than itself. By similar reasoning, this next machine will also be capable of designing a machine more intelligent than itself. If every machine in turn does what it is capable of, we should expect a sequence of ever more intelligent machines. This intelligence explosion is sometimes combined with another idea, which we might call the “speed explosion”. The argument for a speed explosion starts from the familiar observation that computer processing speed doubles at regular intervals. Suppose that speed doubles every two years and will do so indefinitely. Now suppose that we have human-level artificial intelligence 1 designing new processors. Then faster processing will lead to faster designers and an ever-faster design cycle, leading to a limit point soon afterwards. The argument for a speed explosion was set out by the artificial intelligence researcher Ray Solomonoff in his 1985 article “The Time Scale of Artificial Intelligence”.1 Eliezer Yudkowsky gives a succinct version of the argument in his 1996 article “Staring at the Singularity”: “Computing speed doubles every two subjective years of work.. Hall, John Storrs (forthcoming). Self-improving AI: An analysis. Minds and Machines . ( Google ) Abstract: Self-improvement was one of the aspects of AI proposed for study in the 1956 Dartmouth conference. Turing proposed a “child machine” which could be taught in the human manner to attain adult human-level intelligence. In latter days, the contention that an AI system could be built to learn and improve itself indefinitely has acquired the label of the bootstrap fallacy. Attempts in AI to implement such a system have met with consistent failure for half a century. Technological optimists, however, have maintained that a such system is possible, producing, if implemented, a feedback loop that would lead to a rapid exponential increase in intelligence. We examine the arguments for both positions and draw some conclusions Hanson, Robin , Is a singularity just around the corner? ( Google ) Abstract: Economic growth is determined by the supply and demand of investment capital; technology determines the demand for capital, while human nature determines the supply. The supply curve has two distinct parts, giving the world economy two distinct modes. In the familiar slow growth mode, rates of return are limited by human discount rates. In the fast growth mode, investment is limited by the world's wealth. Historical trends suggest that we may transition to the fast mode in roughly another century and a half Vinge, Vernor (online). The technological singularity. ( Cited by 43 | Google | More links ) Additional links for this entry: http://wholeearthmag.com/ArticleBin/111-3.pdf http://adsabs.harvard.edu/abs/1993vise.nasa...11V http://www.aleph.se/Trans/Global/Singularity/sing.html http://www.cs.ucsd.edu/users/goguen/misc/singularity.html http://www.kurzweilai.net/articles/art0092.html?printable=1 http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html http://www.csa.com/partners/viewrecord.php?requester=gs=N9427359AH Yudkowsky, Eliezer (online). Staring into the singularity. ( Google ) Abstract: 1: The End of History 2: The Beyondness of the Singularity 2.1: The Definition of Smartness 2.2: Perceptual Transcends 2.3: Great Big Numbers 2.4: Smarter Than We Are 3: Sooner Than You Think 4: Uploading 5: The Interim Meaning of Life 6: Getting to the Singularity Chalmers, David J. , The singularity: A philosophical analysis. ( Google ) Abstract: What happens when machines become more intelligent than humans? One view is that this event will be followed by an explosion to ever-greater levels of intelligence, as each generation of machines creates more intelligent machines in turn. This intelligence explosion is now often known as the “singularity”. The basic argument here was set out by the statistician I.J. Good in his 1965 article “Speculations Concerning the First Ultraintelligent Machine”: Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion”, and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make. The key idea is that a machine that is more intelligent than humans will be better than humans at designing machines. So it will be capable of designing a machine more intelligent than the most intelligent machine that humans can design. So if it is itself designed by humans, it will be capable of designing a machine more intelligent than itself. By similar reasoning, this next machine will also be capable of designing a machine more intelligent than itself. If every machine in turn does what it is capable of, we should expect a sequence of ever more intelligent machines. This intelligence explosion is sometimes combined with another idea, which we might call the “speed explosion”. The argument for a speed explosion starts from the familiar observation that computer processing speed doubles at regular intervals. Suppose that speed doubles every two years and will do so indefinitely. Now suppose that we have human-level artificial intelligence 1 designing new processors. Then faster processing will lead to faster designers and an ever-faster design cycle, leading to a limit point soon afterwards. The argument for a speed explosion was set out by the artificial intelligence researcher Ray Solomonoff in his 1985 article “The Time Scale of Artificial Intelligence”.1 Eliezer Yudkowsky gives a succinct version of the argument in his 1996 article “Staring at the Singularity”: “Computing speed doubles every two subjective years of work.. 6.4a Cyborgs 6.4b Transhumanism 6.4c Cybernetics Mazis, Glen (2008). Cyborg Life: The In-Between of Humans and Machines. PhaenEx 3 (2):14-36. ( Google ) 6.4d Dynamical Systems Abrahamsen, Adele A. Bechtel, William P. (2006). Phenomena and mechanisms: Putting the symbolic, connectionist, and dynamical systems debate in broader perspective. In R. Stainton (ed.), Contemporary Debates in Cognitive Science . Basil Blackwell. ( Google | More links ) Abstract: Cognitive science is, more than anything else, a pursuit of cognitive mechanisms. To make headway towards a mechanistic account of any particular cognitive phenomenon, a researcher must choose among the many architectures available to guide and constrain the account. It is thus fitting that this volume on contemporary debates in cognitive science includes two issues of architecture, each articulated in the 1980s but still unresolved: • Just how modular is the mind? (section 1) – a debate initially pitting encapsulated mechanisms (Fodorian modules that feed their ultimate outputs to a nonmodular central cognition) against highly interactive ones (e.g., connectionist networks that continuously feed streams of output to one another). • Does the mind process language-like representations according to formal rules? (this section) – a debate initially pitting symbolic architectures (such as Chomsky’s generative grammar or Fodor’s language of thought) against less language-like architectures (such as connectionist or dynamical ones). Our project here is to consider the second issue within the broader context of where cognitive science has been and where it is headed. The notion that cognition in general—not just language processing—involves rules operating on language-like representations actually predates cognitive science. In traditional philosophy of mind, mental life is construed as involving propositional attitudes—that is, such attitudes towards propositions as believing, fearing, and desiring that they be true—and logical inferences from them. On this view, if a person desires that a proposition be true and believes that if she performs a certain action it will become true, she will make the inference and (absent any overriding consideration) perform the action Additional links for this entry: http://books.google.com/books?hl=en=BPLpA8As8Lm_kuVh5kiz4L2Xxuc Bechtel, William P. (online). Dynamics and decomposition: Are they compatible? ( Cited by 4 | Google ) Abstract: Much of cognitive neuroscience as well as traditional cognitive science is engaged in a quest for mechanisms through a project of decomposition and localization of cognitive functions. Some advocates of the emerging dynamical systems approach to cognition construe it as in opposition to the attempt to decompose and localize functions. I argue that this case is not established and rather explore how dynamical systems tools can be used to analyze and model cognitive functions without abandoning the use of decomposition and localization to understand mechanisms of cognition Bechtel, William P. (1998). Representations and cognitive explanations: Assessing the dynamicist challenge in cognitive science. Cognitive Science 22 (3):295-317. ( Cited by 64 | Google | More links ) Abstract: Advocates of dynamical systems theory (DST) sometimes employ revolutionary rhetoric. In an attempt to clarify how DST models differ from others in cognitive science, I focus on two issues raised by DST: the role for representations in mental models and the conception of explanation invoked. Two features of representations are their role in standing-in for features external to the system and their format. DST advocates sometimes claim to have repudiated the need for stand-ins in DST models, but I argue that they are mistaken. Nonetheless, DST does offer new ideas as to the format of representations employed in cognitive systems. With respect to explanation, I argue that some DST models are better seen as conforming to the covering-law conception of explanation than to the mechanistic conception of explanation implicit in most cognitive science research. But even here, I argue, DST models are a valuable complement to more mechanistic cognitive explanations Additional links for this entry: http://www.leaonline.com/doi/abs/10.1207/s15516709cog2203_2 http://www.leaonline.com/doi/pdfplus/10.1207/s15516709cog2203_2 http://www.cogsci.rpi.edu/CSJarchive/1998v22/i03/p0295p0318/MAIN.PDF http://www.ingentaconnect.com/content/els/03640213/1998/00000022/00000003/art80042 Bedau, Mark A. (1997). Emergent models of supple dynamics in life and mind. Brain and Cognition 34:5-27. (Cited by 12 | Google | More links ) Abstract: The dynamical patterns in mental phenomena have a characteristic supplenessa looseness or softness that persistently resists precise formulationwhich apparently underlies the frame problem of artificial intelligence. This suppleness also undermines contemporary philosophical functionalist attempts to define mental capacities. Living systems display an analogous form of supple dynamics. However, the supple dynamics of living systems have been captured in recent artificial life models, due to the emergent architecture of those models. This suggests that analogous emergent models might be able to explain supple dynamics of mental phenomena. These emergent models of the supple mind, if successful, would refashion the nature of contemporary functionalism in the philosophy of mind Additional links for this entry: http://people.reed.edu/~mab/publications/papers/emergent-models.html http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation http://www.ingentaconnect.com/content/ap/br/1997/00000034/00000001/art00904 Chemero, Anthony (2000). Anti-representationalism and the dynamical stance. Philosophy of Science 67 (4):625-647. ( Cited by 12 | Google | More links ) Abstract: Arguments in favor of anti-representationalism in cognitive science often suffer from a lack of attention to detail. The purpose of this paper is to fill in the gaps in these arguments, and in so doing show that at least one form of anti- representationalism is potentially viable. After giving a teleological definition of representation and applying it to a few models that have inspired anti- representationalist claims, I argue that anti-representationalism must be divided into two distinct theses, one ontological, one epistemological. Given the assumptions that define the debate, I give reason to think that the ontological thesis is false. I then argue that the epistemological thesis might, in the end, turn out to be true, despite a potentially serious difficulty. Along the way, there will be a brief detour to discuss a controversy from early twentieth century physics Additional links for this entry: http://www.journals.uchicago.edu/cgi-bin/resolve?id=doi:10.1086/392858 http://www.jstor.org/stable/pdfplus/188710.pdf Chemero, Tony (2001). Dynamical explanation and mental representations. Trends in Cognitive Sciences 5 (4):141-142. ( Cited by 4 | Google | More links ) Abstract: Markman and Dietrich 1 recently recommended extending our understanding of representation to incorporate insights from some “alternative” theories of cognition: perceptual symbol systems, situated action, embodied cognition, and dynamical systems. In particular, they suggest that allowances be made for new types of representation which had been previously under-emphasized in cognitive science. The amendments they recommend are based upon the assumption that the alternative positions each agree with the classical view that cognition requires representations, internal mediating states that bear information. 2 In the case of one of the alternatives, dynamical systems 3 , this is simply false: many dynamically-oriented cognitive scientists are anti-representationalists. 4,5,6 Additional links for this entry: http://edisk.fandm.edu/tony.chemero/papers/mand.pdf http://citeseer.ist.psu.edu/chemero01dynamical.html http://linkinghub.elsevier.com/retrieve/pii/S1364661300016272 http://www.ingentaconnect.com/content/els/13646613/2001/00000005/00000004/art01627 Chemero, Anthony Cordeiro, William (online). Dynamical, ecological sub-persons. ( Cited by 1 | Google ) Abstract: Scientific and Philosophical Studies of Mind Franklin and Marshall College Lancaster, PA 17604-3003 USA Clark, Andy (online). Commentary on "the modularity of dynamic systems". ( Google | More links ) Abstract: 1. Throughout the paper, and especially in the section called "LISP vs. DST", I worried that there was not enough focus on EXPLANATION. For the real question, it seems to me, is not whether some dynamical system can implement human cognition, but whether the dynamical description of the system is more explanatorily potent than a computational/representational one. Thus we know, for example, that a purely physical specification can fix a system capable of computing any LISP function. But from this it doesn't follow that the physical description is the one we need to understand the power of the system considered as an information processing device. In the same way, I don't think your demonstration that bifurcating attractor sets can yield the same behavior as a LISP program goes any way towards showing that we should not PREFER the LISP description. To reduce symbolic stories to a subset of DST (as hinted in that section) requires MORE than showing this kind of equivalence: it requires showing that there is explanatory gain, or at the very least, no explanatory loss, at that level. I append an extract from a recent paper of mine that touches on these issues, in case it helps clarify what I am after here Additional links for this entry: http://users.california.com/~mcmf/clark.html Clark, Andy (1998). Time and mind. Journal of Philosophy 95 (7):354-76. ( Cited by 10 | Google | More links ) Abstract: Mind, it has recently been argued1, is a thoroughly temporal phenomenon: so temporal, indeed, as to defy description and analysis using the traditional computational tools of cognitive scientific understanding. The proper explanatory tools, so the suggestion goes, are instead the geometric constructs and differential equations of Dynamical Systems Theory. I consider various aspects of the putative temporal challenge to computational understanding, and show that the root problem turns on the presence of a certain kind of causal web: a web that involves multiple components (both inner and outer) linked by chains of continuous and reciprocal causal influence. There is, however, no compelling route from such facts about causal and temporal complexity to the radical anti- computationalist conclusion. This is because, interactive complexities notwithstanding, the computational approach provides a kind of explanatory understanding that cannot (I suggest) be recreated using the alternative resources of pure Dynamical Systems Theory. In particular, it provides a means of mapping information flow onto causal structure -- a mapping that is crucial to understanding the distinctive kinds of flexibility and control characteristic of truly mindful engagements with the world. Where we confront especially complex interactive causal webs, however, it does indeed become harder to isolate the syntactic vehicles required by the computational approach. Dynamical Systems Theory, I conclude, may play a vital role in recovering such vehicles from the burgeoning mass of real-time interactive complexity Additional links for this entry: http://www.philosophy.ed.ac.uk/staff/clark/pubs/time.pdf http://www.jstor.org/stable/pdfplus/2564539.pdf Cruz, Joe (online). Psychological explanation and noise in modeling. Comments on Whit Schonbein's "cognition and the power of continuous dynamical systems". ( Google ) Abstract: I find myself ambivalent with respect to the line of argument that Schonbein offers. I certainly want to acknowledge and emphasize at the outset that Schonbein’s discussion has brought to the fore a number of central, compelling and intriguing issues regarding the nature of the dynamical approach to cognition. Though there is much that seems right in this essay, perhaps my view is that the paper invites more questions than it answers. My remarks here then are in the spirit of scouting some of the surrounding terrain in order to see just what Schonbein’s claim is and what arguments or options may be open to the dynamicist Eiser, J. Richard (1994). Attitudes, Chaos, and the Connectionist Mind. Cambridge: Blackwell. ( Cited by 67 | Google ) Eliasmith, Chris (2001). Attractive and in-discrete: A critique of two putative virtues of the dynamicist theory of mind. Minds And Machines 11 (3):417-426. ( Cited by 12 | Google | More links ) Abstract:   I argue that dynamicism does not provide a convincing alternative to currently available cognitive theories. First, I show that the attractor dynamics of dynamicist models are inadequate for accounting for high-level cognition. Second, I argue that dynamicist arguments for the rejection of computation and representation are unsound in light of recent empirical findings. This new evidence provides a basis for questioning the importance of continuity to cognitive function, challenging a central commitment of dynamicism. Coupled with a defense of current connectionist theory, these two critiques lead to the conclusion that dynamicists have failed to achieve their goal of providing a new paradigm for understanding cognition Additional links for this entry: http://watarts.uwaterloo.ca/~celiasmi/Papers/ce.press.attractive.mm.html http://www.springerlink.com/content/w7777h0413p32864/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=338891=1 http://www.springerlink.com/index/W7777H0413P32864.pdf http://www.ingentaconnect.com/content/klu/mind/2001/00000011/00000003/00338891 Eliasmith, Chris (1997). Computation and dynamical models of mind. Minds and Machines 7 (4):531-41. ( Cited by 10 | Google | More links ) Abstract:   Van Gelder (1995) has recently spearheaded a movement to challenge the dominance of connectionist and classicist models in cognitive science. The dynamical conception of cognition is van Gelder's replacement for the computation bound paradigms provided by connectionism and classicism. He relies on the Watt governor to fulfill the role of a dynamicist Turing machine and claims that the Motivational Oscillatory Theory (MOT) provides a sound empirical basis for dynamicism. In other words, the Watt governor is to be the theoretical exemplar of the class of systems necessary for cognition and MOT is an empirical instantiation of that class. However, I shall argue that neither the Watt governor nor MOT successfully fulfill these prescribed roles. This failure, along with van Gelder's peculiar use of the concept of computation and his struggle with representationalism, prevent him from providing a convincing alternative to current cognitive theories Additional links for this entry: http://cogprints.org/325/ http://cogprints.ecs.soton.ac.uk/archive/00000325/ http://cogprints.ecs.soton.ac.uk/archive/00000325/ http://portal.acm.org/citation.cfm?id=596707.596756 http://www.arts.uwaterloo.ca/~celiasmi/Papers/dynamics.mm.html http://www.arts.uwaterloo.ca/~celiasmi/Papers/dynamics.mm.html http://www.csa.com/partners/viewrecord.php?requester=gs=316239CI http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:325 http://www.csa.com/partners/viewrecord.php?requester=gs=316239CI http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:325 http://www.springerlink.com/content/content/u013j75041272360/fulltext.pdf http://www.springerlink.com/content/u013j75041272360/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=125137=1 http://www.kluweronline.com/article.asp?PIPS=125137=1 http://www.springerlink.com/index/U013J75041272360.pdf http://www.springerlink.com/index/U013J75041272360.pdf http://www.ingentaconnect.com/content/klu/mind/1997/00000007/00000004/00125137 http://www.ingentaconnect.com/content/klu/mind/1997/00000007/00000004/00125137 http://cogprints.org/325/1/dynamics.mm.html http://cogprints.org/325/0/dynamics.mm.html Eliasmith, Chris (1998). Dynamical models and Van gelder's dynamicism. Behavioral and Brain Sciences 21 (5):639-639. ( Cited by 1 | Google | More links ) Abstract: Van Gelder has presented a position which he ties closely to a broad class of models known as dynamical models. While supporting many of his broader claims about the importance of this class (as has been argued by connectionists for quite some time), I note that there are a number of unique characteristics of his brand of dynamicism. I suggest that these characteristics engender difficulties for his view Additional links for this entry: http://journals.cambridge.org/action/displayFulltext?type=1=30437 http://www.journals.cambridge.org/abstract_S0140525X98341734 Eliasmith, Chris (2003). Moving beyond metaphors: Understanding the mind for what it is. Journal of Philosophy 100 (10):493-520. ( Cited by 21 | Google | More links ) Additional links for this entry: http://www.arts.uwaterloo.ca/~doneill/cogsci600/Eliasmith.pdf http://www.arts.uwaterloo.ca/~raha/CogSci600_web/Readings/eliasmith1.pdf http://watarts.uwaterloo.ca/~celiasmi/Papers/eliasmith.moving beyond metaphors.jphil.pdf http://www.arts.uwaterloo.ca/~celiasmi/Papers/eliasmith.moving beyond metaphors.jphil.pdf http://www.journalofphilosophy.org/articles/../issues/100/10/1.pdf Eliasmith, Chris (1996). The third contender: A critical examination of the dynamicist theory of cognition. [Journal (Paginated)] 9 (4):441-63. ( Cited by 79 | Google | More links ) Abstract: In a recent series of publications, dynamicist researchers have proposed a new conception of cognitive functioning. This conception is intended to replace the currently dominant theories of connectionism and symbolicism. The dynamicist approach to cognitive modeling employs concepts developed in the mathematical field of dynamical systems theory. They claim that cognitive models should be embedded, low-dimensional, complex, described by coupled differential equations, and non-representational. In this paper I begin with a short description of the dynamicist project and its role as a cognitive theory. Subsequently, I determine the theoretical commitments of dynamicists, critically examine those commitments and discuss current examples of dynamicist models. In conclusion, I determine dynamicism's relation to symbolicism and connectionism and find that the dynamicist goal to establish a new paradigm has yet to be realized Additional links for this entry: http://cogprints.org/324/0/thirdcontender.html http://cogprints.ecs.soton.ac.uk/archive/00000324/ http://cogprints.soton.ac.uk/documents/disk0/00/00/03/24/ http://www.arts.uwaterloo.ca/~celiasmi/Papers/thirdcontender.html http://cogprints.ecs.soton.ac.uk/archive/00000324/00/thirdcontender.html http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:324 http://www.informaworld.com/smpp/./ftinterface~db=all~content=a793914984~fulltext=713240930 http://cogprints.org/324/1/thirdcontender.html Foss, Jeffrey E. (1992). Introduction to the epistemology of the brain: Indeterminacy, micro-specificity, chaos, and openness. Topoi 11 (1):45-57. ( Cited by 7 | Annotation | Google | More links ) On the brain as a vector-processing system, and the problems raised by indeterminacy, chaos, and so on. With morals for cognitive science. Abstract:   Given that the mind is the brain, as materialists insist, those who would understand the mind must understand the brain. Assuming that arrays of neural firing frequencies are highly salient aspects of brain information processing (the vector functional account), four hurdles to an understanding of the brain are identified and inspected: indeterminacy, micro-specificity, chaos, and openness Additional links for this entry: http://www.springerlink.com/index/T3R56W7785930446.pdf Freeman, Walter J. (1997). Nonlinear neurodynamics of intentionality. Journal of Mind and Behavior 18 (2-3):291-304. ( Cited by 9 | Google | More links ) Additional links for this entry: http://sulcus.berkeley.edu/wjf/CL_DynamicsOfIntention.doc http://lecture.berkeley.edu/wjf/CL_DynamicsOfIntention.doc French, Robert M. Thomas, Elizabeth (2001). The dynamical hypothesis in cognitive science: A review essay of Mind As Motion . Minds and Machines 11 (1):101-111. ( Google | More links ) Additional links for this entry: http://portal.acm.org/citation.cfm?id=596903 http://citeseer.ist.psu.edu/french01dynamical.html http://www.u-bourgogne.fr/LEAD/people/french/mind_as_motion.pdf http://www.springerlink.com/content/content/xv2x648r418194mv/fulltext.pdf http://www.springerlink.com/content/xv2x648r418194mv/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=310383=1 http://www.springerlink.com/index/XV2X648R418194MV.pdf French, Robert M. Thomas, Elizabeth (1998). The dynamical hypothesis: One battle behind. Behavioral and Brain Sciences 21 (5):640-641. (Cited by 4 | Google | More links ) Abstract: What new implications does the dynamical hypothesis have for cognitive science? The short answer is: None. The _Behavior and Brain Sciences _target article, “The dynamical hypothesis in cognitive science” by Tim Van Gelder is basically an attack on traditional symbolic AI and differs very little from prior connectionist criticisms of it. For the past ten years, the connectionist community has been well aware of the necessity of using (and understanding) dynamically evolving, recurrent network models of cognition Additional links for this entry: http://citeseer.ist.psu.edu/french98dynamical.html http://journals.cambridge.org/action/displayFulltext?type=1=30497 http://journals.cambridge.org/abstract_S0140525X98361737 Garson, James W. (1998). Chaotic emergence and the language of thought. Philosophical Psychology 11 (3):303-315. ( Cited by 7 | Google ) Abstract: The purpose of this paper is to explore the merits of the idea that dynamical systems theory (also known as chaos theory) provides a model of the mind that can vindicate the language of thought (LOT). I investigate the nature of emergent structure in dynamical systems to assess its compatibility with causally efficacious syntactic structure in the brain. I will argue that anyone who is committed to the idea that the brain's functioning depends on emergent features of dynamical systems should have serious reservations about the LOT. First, dynamical systems theory casts doubt on one of the strongest motives for believing in the LOT: principle P, the doctrine that structure found in an effect must also be found in its cause. Second, chaotic emergence is a double-edged sword. Its tendency to cleave the psychological from the neurological undermines foundations for belief in the existence of causally efficacious representations. Overall, a dynamic conception of the brain sways us away from realist conclusions about the causal powers of representations with constituent structure Garson, James W. (1996). Cognition poised at the edge of chaos: A complex alternative to a symbolic mind. Philosophical Psychology 9 (3):301-22. ( Cited by 16 | Google ) Abstract: This paper explores a line of argument against the classical paradigm in cognitive science that is based upon properties of non-linear dynamical systems, especially in their chaotic and near-chaotic behavior. Systems of this kind are capable of generating information-rich macro behavior that could be useful to cognition. I argue that a brain operating at the edge of chaos could generate high-complexity cognition in this way. If this hypothesis is correct, then the symbolic processing methodology in cognitive science faces serious obstacles. A symbolic description of the mind will be extremely difficult, and even if it is achieved to some approximation, there will still be reasons for rejecting the hypothesis that the brain is in fact a symbolic processor Garson, James W. (1997). Syntax in a dynamic brain. Synthese 110 (3):343-55. ( Cited by 8 | Annotation | Google | More links ) There are no good arguments for LOT of the form "The brain needs to do X, and X entails LOT". Considers X = concatenation, logical form, tracking, combinatorial encoding. Either LOT is weakened deeply or is unnecessary. Additional links for this entry: http://www.ingentaconnect.com/content/klu/synt/1997/00000110/00000003/00125530 Giunti, Marco (1996). Computers, Dynamical Systems, and the Mind. Oxford University Press. ( Google ) Giunti, Marco (1995). Dynamic models of cognition. In T. van Gelder Robert Port (eds.), Mind As Motion . MIT Press. ( Google ) Globus, Gordon G. (1992). Toward a noncomputational cognitive science. Journal of Cognitive Neuroscience 4:299-310. ( Google ) Haney, Mitchell R. (1999). Dynamical cognition, soft laws, and moral theorizing. Acta Analytica 22 (22):227-240. ( Cited by 5 | Google ) Harcum, E. Rae (1991). Behavioral paradigm for a psychological resolution of the free will issue. Journal of Mind and Behavior 93:93-114. ( Cited by 1 | Google ) Hooker, Cliff A. Christensen, Wayne D. (1998). Towards a new science of the mind: Wide content and the metaphysics of organizational properties in nonlinear dynamic models. Mind and Language 13 (1):98-109. ( Cited by 5 | Google | More links ) Additional links for this entry: http://www.blackwell-synergy.com/doi/abs/10.1111/1468-0017.00067 http://www.ingentaconnect.com/content/bpl/mila/1998/00000013/00000001/art00008 http://www.ingentaconnect.com/content/bpl/mila/1998/00000013/00000001/art00067 Horgan, Terence E. Tienson, John L. (1994). A nonclassical framework for cognitive science. Synthese 101 (3):305-45. ( Cited by 12 | Google | More links ) Abstract:   David Marr provided a useful framework for theorizing about cognition within classical, AI-style cognitive science, in terms of three levels of description: the levels of (i) cognitive function, (ii) algorithm and (iii) physical implementation. We generalize this framework: (i) cognitive state transitions, (ii) mathematical/functional design and (iii) physical implementation or realization. Specifying the middle, design level to be the theory of dynamical systems yields a nonclassical, alternative framework that suits (but is not committed to) connectionism. We consider how a brain's (or a network's) being a dynamical system might be the key both to its realizing various essential features of cognition — productivity, systematicity, structure-sensitive processing, syntax — and also to a non-classical solution of (frame-type) problems plaguing classical cognitive science Additional links for this entry: http://www.springerlink.com/index/T0T37187T0861785.pdf Horgan, Terence E. Tienson, John L. (1992). Cognitive systems as dynamic systems. Topoi 11 (1):27-43. ( Cited by 16 | Google | More links ) Additional links for this entry: http://www.springerlink.com/index/H86M83605H84710V.pdf Keijzer, Fred A. Bem, Sacha (1996). Behavioral systems interpreted as autonomous agents and as coupled dynamical systems: A criticism. Philosophical Psychology 9 (3):323-46. ( Cited by 34 | Google ) Abstract: Cognitive science's basic premises are under attack. In particular, its focus on internal cognitive processes is a target. Intelligence is increasingly interpreted, not as a matter of reclusive thought, but as successful agent-environment interaction. The critics claim that a major reorientation of the field is necessary. However, this will only occur when there is a distinct alternative conceptual framework to replace the old one. Whether or not a serious alternative is provided is not clear. Among the critics there is some consensus, however, that this role could be fulfilled by the concept of a 'behavioral system'. This integrates agent and environment into one encompassing general system. We will discuss two contexts in which the behavioral systems idea is being developed. Autonomous Agents Research is the enterprise of building behavior-based robots. Dynamical Systems Theory provides a mathematical framework well suited for describing the interactions between complex systems. We will conclude that both enterprises provide important contributions to the behavioral systems idea. But neither turns it into a full conceptual alternative which will initiate a major paradigm switch in cognitive science. The concept will need a lot of fleshing out before it can assume that role Mills, Stephen L. (1999). Noncomputable dynamical cognitivism: An eliminativist perspective. Acta Analytica 22 (22):151-168. ( Google ) Morton, Adam (1988). The chaology of mind. Analysis 48 (June):135-142. ( Cited by 4 | Google ) O’Brien, Gerard (1998). Digital computers versus dynamical systems: A conflation of distinctions. Behavioral and Brain Sciences 21:648-649. ( Google | More links ) Abstract: The distinction at the heart of van Gelder’s target article is one between digital computers and dynamical systems. But this distinction conflates two more fundamental distinctions in cognitive science that should be keep apart. When this conflation is undone, it becomes apparent that the “computational hypothesis” (CH) is not as dominant in contemporary cognitive science as van Gelder contends; nor has the “dynamical hypothesis” (DH) been neglected Additional links for this entry: http://www.arts.adelaide.edu.au/humanities/philosophy/publications/Commentary_on_Van_Gelder_BBS.pdf http://www.arts.adelaide.edu.au/humanities/philosophy/publications/Commentary_on_Van_Gelder_BBS.pdf http://www.journals.cambridge.org/abstract_S0140525X98451732 Rietveld, Erik (2008). The Skillful Body as a Concernful System of Possible Actions: Phenomena and Neurodynamics. Theory & Psychology 18 (3):341-361. ( Google ) Abstract: For Merleau-Ponty,consciousness in skillful coping is a matter of prereflective ‘I can’ and not explicit ‘I think that.’ The body unifies many domain-specific capacities. There exists a direct link between the perceived possibilities for action in the situation (‘affordances’) and the organism’s capacities. From Merleau-Ponty’s descriptions it is clear that in a flow of skillful actions, the leading ‘I can’ may change from moment to moment without explicit deliberation. How these transitions occur, however, is less clear. Given that Merleau-Ponty suggested that a better understanding of the self-organization of brain and behavior is important, I will re-read his descriptions of skillful coping in the light of recent ideas on neurodynamics. Affective processes play a crucial role in evaluating the motivational significance of objects and contribute to the individual’s prereflective responsiveness to relevant affordances. Robbins, Stephen E. (2002). Semantics, experience and time. Cognitive Systems Research 3 (3):301-337. ( Cited by 3 | Google | More links ) Additional links for this entry: http://linkinghub.elsevier.com/retrieve/pii/S1389041702000451 http://www.ingentaconnect.com/content/els/13890417/2002/00000003/00000003/art00045 Rockwell, Teed (2005). Attractor spaces as modules: A semi-eliminative reduction of symbolic AI to dynamic systems theory. Minds and Machines 15 (1):23-55. ( Google | More links ) Abstract: I propose a semi-eliminative reduction of Fodors concept of module to the concept of attractor basin which is used in Cognitive Dynamic Systems Theory (DST). I show how attractor basins perform the same explanatory function as modules in several DST based research program. Attractor basins in some organic dynamic systems have even been able to perform cognitive functions which are equivalent to the If/Then/Else loop in the computer language LISP. I suggest directions for future research programs which could find similar equivalencies between organic dynamic systems and other cognitive functions. This type of research could help us discover how (and/or if) it is possible to use Dynamic Systems Theory to more accurately model the cognitive functions that are now being modeled by subroutines in Symbolic AI computer models. If such a reduction of subroutines to basins of attraction is possible, it could free AI from the limitations that prompted Fodor to say that it was impossible to model certain higher level cognitive functions Additional links for this entry: http://www.springerlink.com/content/n6h75482x0qvt016/fulltext.pdf http://www.springerlink.com/index/N6H75482X0QVT016.pdf http://www.ingentaconnect.com/content/klu/mind/2005/00000015/00000001/00001344 Rockwell, Teed (online). Reply to Clark and Van gelder. ( Google | More links ) Abstract: Clark ends his appendix with a description of what he calls "dynamic computationalism", which he describes as an interesting hybrid between DST and GOFAI. My 'horseLISP" example could be described as an example of dynamic computationalism. It is clearly not as eliminativist as Van Gelder's computational governor example, for I am trying to come up with something like identities between computational entities and dynamic ones. Thus unlike other dynamicists, I am not doing what Clark calls "embracing a different vocabulary for the understanding and analysis of brain events". I think we probably can keep much of the computational vocabulary, although the meanings of many of its terms will probably shift as much as the meaning of 'atom' has shifted since Dalton's time. The label of "dynamic computationalism" is perhaps as good a description of my position as any, but I think I would mean something slightly different by it than Clark would. (For the following, please insert the mantra "of course, this is an empirical question" (OCTEQ) every paragraph or so.) Additional links for this entry: http://www.california.com/~mcmf/modreply.html http://users.california.com/~mcmf/modreply.html Rockwell, Teed (1998). The modularity of dynamic systems. Colloquia Manilana 6. ( Cited by 1 | Google | More links ) Abstract: To some degree, Fodor's claim that Cognitive science divides the mind into modules tells us more about the minds doing the studying than the mind being studied. The knowledge game is played by analyzing an object of study into parts, and then figuring out how those parts are related to each other. This is the method regardless of whether the object being studied is a mind or a solar system. If a module is just another name for a part, then to say that the mind consists of modules is simply to say that it is comprehensible. Fodor comes close to acknowledging this in the following passage Additional links for this entry: http://www.california.com/~mcmf/mod.html http://users.california.com/~mcmf/mod.html Schonbein, W. (2005). Cognition and the power of continuous dynamical systems. Minds and Machines 15 (1):57-71. ( Google | More links ) Abstract: Traditional approaches to modeling cognitive systems are computational, based on utilizing the standard tools and concepts of the theory of computation. More recently, a number of philosophers have argued that cognition is too subtle or complex for these tools to handle. These philosophers propose an alternative based on dynamical systems theory. Proponents of this view characterize dynamical systems as (i) utilizing continuous rather than discrete mathematics, and, as a result, (ii) being computationally more powerful than traditional computational automata. Indeed, the logical possibility of such super-powerful systems has been demonstrated in the form of analog artificial neural networks. In this paper I consider three arguments against the nomological possibility of these automata. While the first two arguments fail, the third succeeds. In particular, the presence of noise reduces the computational power of analog networks to that of traditional computational automata, and noise is a pervasive feature of information processing in biological systems. Consequently, as an empirical thesis, the proposed dynamical alternative is under-motivated: What is required is an account of how continuously valued systems could be realized in physical systems despite the ubiquity of noise Additional links for this entry: http://www.springerlink.com/content/xtx321861761117r/fulltext.pdf http://www.springerlink.com/index/XTX321861761117R.pdf http://www.ingentaconnect.com/content/klu/mind/2005/00000015/00000001/00001345 Sloman, Aaron (1993). The mind as a control system. In Christopher Hookway Donald M. Peterson (eds.), Philosophy and Cognitive Science . Cambridge University Press. ( Cited by 66 | Google | More links ) Additional links for this entry: http://citeseer.ist.psu.edu/sloman93mind.html http://www.cs.bham.ac.uk/research/cogaff/Aaron.Sloman_Mind.as.controlsystem.ps http://www.cs.bham.ac.uk/~axs/cog_affect/Aaron.Sloman_Mind.as.controlsystem.ps.gz Stark, Herman E. (1999). What the dynamical cognitive scientist said to the epistemologist. Acta Analytica 22 (22):241-260. ( Google ) Symons, John (2001). Explanation, representation and the dynamical hypothesis. Minds and Machines 11 (4):521-541. ( Cited by 1 | Google | More links ) Abstract:   This paper challenges arguments that systematic patterns of intelligent behavior license the claim that representations must play a role in the cognitive system analogous to that played by syntactical structures in a computer program. In place of traditional computational models, I argue that research inspired by Dynamical Systems theory can support an alternative view of representations. My suggestion is that we treat linguistic and representational structures as providing complex multi-dimensional targets for the development of individual brains. This approach acknowledges the indispensability of the intentional or representational idiom in psychological explanation without locating representations in the brains of intelligent agents Additional links for this entry: http://www.springerlink.com/content/u55n058l05q58r53/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=352659=1 http://www.springerlink.com/index/U55N058L05Q58R53.pdf http://www.ingentaconnect.com/content/klu/mind/2001/00000011/00000004/00352659 Treur, Jan (2005). States of change: Explaining dynamics by anticipatory state properties. Philosophical Psychology 18 (4):441-471. ( Cited by 4 | Google | More links ) Abstract: In cognitive science, the dynamical systems theory (DST) has recently been advocated as an approach to cognitive modeling that is better suited to the dynamics of cognitive processes than the symbolic/computational approaches are. Often, the differences between DST and the symbolic/computational approach are emphasized. However, alternatively their commonalities can be analyzed and a unifying framework can be sought. In this paper, the possibility of such a unifying perspective on dynamics is analyzed. The analysis covers dynamics in cognitive disciplines, as well as in physics, mathematics and computer science. The unifying perspective warrants the development of integrated approaches covering both DST aspects and symbolic/computational aspects. The concept of a state-determined system, which is based on the assumption that properties of a given state fully determine the properties of future states, lies at the heart of DST. Taking this assumption as a premise, the explanatory problem of dynamics is analyzed in more detail. The analysis of four cases within different disciplines (cognitive science, physics, mathematics, computer science) shows how in history this perspective led to numerous often used concepts within them. In cognitive science, the concepts desire and intention were introduced, and in classical mechanics the concepts momentum, energy and force. Similarly, in mathematics a number of concepts have been developed to formalize the state-determined system assumption [e.g. derivatives (of different orders) of a function, Taylor approximations]. Furthermore, transition systems - a currently popular format for specification of dynamical systems within computer science - can also be interpreted from this perspective. One of the main contributions of the paper is that the case studies provide a unified view on the explanation of dynamics across the chosen disciplines. All approaches to dynamics analyzed in this paper share the state-determined system assumption and the (explicit or implicit) use of anticipatory state properties. Within cognitive science, realism is one of the problems identified for the symbolic/computational approach - i.e. how do internal states described by symbols relate to the real world in a natural manner. As DST is proposed as an alternative to the symbolic/computational approach, a natural question is whether, for DST, realism of the states can be better guaranteed. As a second main contribution, the paper provides an evaluation of DST compared to the symbolic/computational approach, which shows that, in this respect (i.e. for the realism problem), DST does not provide a better solution than the other approaches. This shows that DST and the symbolic/computational approach not only have the state-determined system assumption and the use of anticipatory state properties in common, but also the realism problem Additional links for this entry: http://www.few.vu.nl/~wai/Papers/PP04soc.pdf http://www.cs.vu.nl/~wai/Papers/PP04socprel.pdf http://www.phil.uu.nl/preprints/ckipreprints/PREPRINTS/preprint051.pdf http://www.informaworld.com/smpp/./ftinterface~content=a723861123~fulltext=713240930 http://taylorandfrancis.metapress.com/index/HU51317GJ73622Q1.pdf http://www.informaworld.com/index/723861123.pdf http://www.ingentaconnect.com/content/routledg/cphp/2005/00000018/00000004/art00003 http://www.informaworld.com/smpp/./ftinterface~db=all~content=a723861123~fulltext=713240930 van Gelder, Tim (1997). Connectionism, dynamics, and the philosophy of mind. In Martin Carrier Peter K. Machamer (eds.), Mindscapes: Philosophy, Science, and the Mind . Pittsburgh University Press. ( Cited by 3 | Google ) van Gelder, Tim (1998). Disentangling dynamics, computation, and cognition. Behavioral and Brain Sciences 21 (5):654-661. ( Cited by 4 | Google | More links ) Abstract: The nature of the dynamical hypothesis in cognitive science (the DH) is further clarified in responding to various criticisms and objections raised in commentaries. Major topics addressed include the definitions of “dynamical system” and “digital computer;” the DH as Law of Qualitative Structure; the DH as an ontological claim; the multiple-realizability of dynamical models; the level at which the DH is pitched; the nature of dynamics; the role of representations in dynamical cognitive science; the falsifiability of the DH; the extent to which the DH is open; the role of temporal and implementation considerations; and the novelty or importance of the DH. The basic formulation and defense of the DH in the target article survives intact, though some refinements are recommended Additional links for this entry: http://www.philosophy.unimelb.edu.au/tgelder/papers/DHResp.pdf http://www.phil.canterbury.ac.nz/tom_bestor/e-texts/van Gelder - Dynamtical Hypothesis in Cog Sci Authors Responses.pdf http://journals.cambridge.org/action/displayFulltext?type=1=30875 http://www.journals.cambridge.org/abstract_S0140525X98521735 http://journals.cambridge.org/abstract_S0140525X98521735 van Gelder, Tim (1999). Defending the dynamic hypothesis. In Wolfgang Tschacher J-P Dauwalder (eds.), Dynamics, Synergetics, Autonomous Agents: Nonlinear Systems Approaches to Cognitive Psychology and Cognitive Science . Singapore: World Scientific. ( Cited by 16 | Google | More links ) Abstract: Cognitive science has always been dominated by the idea that cognition is _computational _in a rather strong and clear sense. Within the mainstream approach, cognitive agents are taken to be what are variously known as _physical symbol_ _systems, digital computers_, _syntactic engines_, or_ symbol manipulators_. Cognitive operations are taken to consist in the shuffling of symbol tokens according to strict rules (programs). Models of cognition are themselves digital computers, implemented on general purpose electronic machines. The basic mathematical framework for understanding cognition is the theory of discrete computation, and the core theoretical tools for developing and understanding models of cognition are those of computer science Additional links for this entry: http://citeseer.ist.psu.edu/292698.html http://citeseer.ist.psu.edu/vangelder98defending.html http://depts.washington.edu/edtech/VanGelder_revisiting.pdf http://books.google.com/books?hl=en=0xdkpD2AVxZ3VXmohG8YuqJieNg van Gelder, Tim Port, Robert (eds.) (1995). Mind As Motion: Explorations in the Dynamics of Cognition. MIT Press. ( Cited by 30 | Google ) Van Leeuwen, Marco (2005). Questions for the dynamicist: The use of dynamical systems theory in the philosophy of cognition. Minds and Machines 15 (3-4):271-333. ( Google ) Abstract: The concepts and powerful mathematical tools of Dynamical Systems Theory (DST) yield illuminating methods of studying cognitive processes, and are even claimed by some to enable us to bridge the notorious explanatory gap separating mind and matter. This article includes an analysis of some of the conceptual and empirical progress Dynamical Systems Theory is claimed to accomodate. While sympathetic to the dynamicist program in principle, this article will attempt to formulate a series of problems the proponents of the approach in question will need to face if they wish to prolong their optimism. The main points to be addressed involve the reductive tendencies inherent in Dynamical Systems Theory, its somewhat muddled position relative to connectionism, the metaphorical nature DST-C exhibits which hinders its explanatory potential, and DST-C's problematic account of causality. Brief discussions of the mathematical and philosophical backgrounds of DST, seminal experimental work and possible adaptations of the theory or alternative suggestions (dynamicist connectionism, neurophenomenology, R theory) are included van Gelder, Tim (1999). Revisiting the dynamic hypothesis. Preprint 2. ( Cited by 11 | Google | More links ) Abstract: “There is a familiar trio of reactions by scientists to a purportedly radical hypothesis: (a) “You must be our of your mind!”, (b) “What else is new? Everybody knows _that_!”, and, later—if the hypothesis is still standing—(c) “Hmm. You _might _be on to something!” ((Dennett, 1995) p. 283) Additional links for this entry: http://depts.washington.edu/edtech/VanGelder_revisiting.pdf van Gelder, Tim (1998). The dynamical hypothesis in cognitive science. Behavioral and Brain Sciences 21 (5):615-28. ( Cited by 307 | Google | More links ) Abstract: The dynamical hypothesis is the claim that cognitive agents are dynamical systems. It stands opposed to the dominant computational hypothesis, the claim that cognitive agents are digital computers. This target article articulates the dynamical hypothesis and defends it as an open empirical alternative to the computational hypothesis. Carrying out these objectives requires extensive clarification of the conceptual terrain, with particular focus on the relation of dynamical systems to computers Additional links for this entry: http://www.irit.fr/SIGCHI/old/docs/debat/DH.pdf http://bbsonline.cup.cam.ac.uk/Preprints/OldArchive/bbs.vangelder.html http://www.bbsonline.org/documents/a/00/00/04/68/bbs00000468-00/bbs.vangelder.html http://journals.cambridge.org/action/displayFulltext?type=1=30003 http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation http://www.ncbi.nlm.nih.gov/sites/entrez?db=pubmed=google http://www.journals.cambridge.org/abstract_S0140525X98001733 http://journals.cambridge.org/abstract_S0140525X98001733 http://journals.cambridge.org/article_S0140525X98301739 http://journals.cambridge.org/article_S0140525X98331738 van Gelder, Tim (1995). What might cognition be if not computation? Journal of Philosophy 92 (7):345-81. ( Cited by 266 | Annotation | Google | More links ) Argues for a dynamic-systems conception of the mind that is non-computational and non-representational. Uses an analogy with the Watt steam governor to argue for a new kind of dynamic explanation. Additional links for this entry: http://books.google.com/books?hl=en=_-hLgEKPrvzRGqiHU3WB-ByPxG4 http://books.google.com/books?hl=en=9bvnduaEm9LGyoZvLb9vrkbzLwc http://books.google.com/books?hl=en=nbynhfg_nmoW_oos5qyFfZKaq_w http://www.jstor.org/stable/pdfplus/2941061.pdf Weiskopf, Daniel A. (2004). The place of time in cognition. British Journal for the Philosophy of Science 55 (1):87-105. ( Cited by 2 | Google | More links ) Abstract: models of cognition are essentially incomplete because they fail to capture the temporal properties of mental processing. I present two possible interpretations of the dynamicists' argument from time and show that neither one is successful. The disagreement between dynamicists and symbolic theorists rests not on temporal considerations per se, but on differences over the multiple realizability of cognitive states and the proper explanatory goals of psychology. The negative arguments of dynamicists against symbolic models fail, and it is doubtful whether pursuing dynamicists' explanatory goals will lead to a robust psychological theory. Introduction Elements of the symbolic theory Elements of dynamical systems theory The argument from time 4.1 First interpretation of the argument from time 4.2 Second interpretation of the argument from time Limits of dynamical systems theory Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/abstract/55/1/87 http://bjps.oupjournals.org/cgi/content/abstract/55/1/87 http://bjps.oxfordjournals.org/cgi/reprint/55/1/87 http://www.ingentaconnect.com/searching/Expand?pub=infobike://oup/phisci/2004/00000055/00000001/art00087= http://www.ingentaconnect.com/content/oup/phisci/2004/00000055/00000001/art00087 Werning, Markus (2005). The temporal dimension of thought: Cortical foundations of predicative representation. Synthese 146 (1-2):203-224. ( Cited by 6 | Google | More links ) Abstract: The paper argues that cognitive states of biological systems are inherently temporal. Three adequacy conditions for neuronal models of representation are vindicated: the compositionality of meaning, the compositionality of content, and the co-variation with content. Classicist and connectionist approaches are discussed and rejected. Based on recent neurobiological data, oscillatory networks are introduced as a third alternative. A mathematical description in a Hilbert space framework is developed. The states of this structure can be regarded as conceptual representations satisfying the three conditions Additional links for this entry: http://service.phil-fak.uni-duesseldorf.de/ezpublish/index.php/filemanager/download/282/syntcog12.pdf http://www.springerlink.com/content/r522585100067t0p/fulltext.pdf http://www.springerlink.com/index/R522585100067T0P.pdf http://www.ingentaconnect.com/content/klu/synt/2005/00000146/F0020001/00009089 Yoshimi, Jeffrey (2009). Husserl's theory of belief and the heideggerean critique. Husserl Studies 25 (2). ( Google ) Abstract: I develop a “two-systems” interpretation of Husserl’s theory of belief. On this interpretation, Husserl accounts for our sense of the world in terms of (1) a system of embodied horizon meanings and passive synthesis, which is involved in any experience of an object, and (2) a system of active synthesis and sedimentation, which comes on line when we attend to an object’s properties. I use this account to defend Husserl against several forms of Heideggerean critique. One line of critique, recently elaborated by Taylor Carman, says that Husserl wrongly loads everyday perception with explicit beliefs about things. A second, earlier line of critique, due to Hubert Dreyfus, charges Husserl with thinking of belief on a problematic Artificial Intelligence (AI) model which involves explicit rules applied to discrete symbol structures. I argue that these criticisms are based on a conflation of Husserl’s two systems of belief. The conception of Husserlian phenomenology which emerges is compatible with Heideggerean phenomenology and associated approaches to cognitive science (in particular, dynamical systems theory) Yoshimi, Jeffrey (2007). Mathematizing phenomenology. Phenomenology and the Cognitive Sciences 6 (3). ( Google | More links ) Abstract: Husserl is well known for his critique of the “mathematizing tendencies” of modern science, and is particularly emphatic that mathematics and phenomenology are distinct and in some sense incompatible. But Husserl himself uses mathematical methods in phenomenology. In the first half of the paper I give a detailed analysis of this tension, showing how those Husserlian doctrines which seem to speak against application of mathematics to phenomenology do not in fact do so. In the second half of the paper I focus on a particular example of Husserl’s “mathematized phenomenology”: his use of concepts from what is today called dynamical systems theory Additional links for this entry: http://www.springerlink.com/index/A22585111621Q121.pdf http://www.ingentaconnect.com/content/klu/phen/2007/00000006/00000003/00009052 6.4e The Nature of AI Buchanan, Bruce G. (1988). AI as an experimental science. In James H. Fetzer (ed.), Aspects of AI . Kluwer. ( Google ) Bundy, A. (1990). What kind of field is AI? In Derek Partridge Y. Wilks (eds.), The Foundations of Artificial Intelligence: A Sourcebook . Cambridge University Press. ( Cited by 6 | Google ) Dennett, Daniel C. (1978). AI as philosophy and as psychology. In Martin Ringle (ed.), Philosophical Perspectives on Artificial Intelligence . Humanities Press. ( Annotation | Google ) AI as detailed armchair psychology and as thought-experimental epistemology. Implications for mind: e.g. a solution to the problem of homuncular regress. Glymour, C. (1988). AI is philosophy. In James H. Fetzer (ed.), Aspects of AI . D. ( Cited by 1 | Google ) Harre, Rom (1990). Vigotsky and artificial intelligence: What could cognitive psychology possibly be about? Midwest Studies in Philosophy 15:389-399. ( Google ) Kukla, André (1989). Is AI an empirical science? Analysis 49 (March):56-60. ( Cited by 4 | Annotation | Google ) No, AI is an a priori science that uses empirical methods; its status is similar to that of mathematics. Kukla, André (1994). Medium AI and experimental science. Philosophical Psychology 7 (4):493-5012. ( Cited by 4 | Annotation | Google ) On the status of "medium AI", the study of intelligence in computational systems (not just humans). Contra to many, this is not an empirical science, but a combination of (experimental) mathematics and engineering. Abstract: It has been claimed that a great deal of AI research is an attempt to discover the empirical laws describing a new type of entity in the world—the artificial computing system. I call this enterprise 'medium AI', since it is in some respects stronger than Searle's 'weak AI', and in other respects weaker than 'strong AI'. Bruce Buchanan, among others, conceives of medium AI as an empirical science entirely on a par with psychology or chemistry. I argue that medium AI is not an empirical science at all. Depending on how artificial computing systems are categorized, it is either an a priori science like mathematics, or a branch of engineering McCarthy, John (online). What is artificial intelligence? ( Cited by 38 | Google | More links ) Additional links for this entry: http://www-formal.stanford.edu/jmc/whatisai.pdf http://airobo.persiangig.com/document/whatisai.pdf http://www.formal.stanford.edu/jmc/whatisai/whatisai.html http://www.kurzweilai.net/articles/art0088.html?printable=1 http://www.fredbf.com/disciplinas/unibratec/dsi/whatisai.pdf http://www.inf.furb.br/~jomi/ia/introducao/mccarthy-whatisai.ps http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:412 Minsky, Marvin L. (online). From pain to suffering. ( Google ) Abstract: “Great pain urges all animals, and has urged them during endless generations, to make the most violent and diversified efforts to escape from the cause of suffering. Even when a limb or other separate part of the body is hurt, we often see a tendency to shake it, as if to shake off the cause, though this may obviously be impossible.” —Charles Darwin[1] Nakashima, H. (1999). AI as complex information processing. Minds and Machines 9 (1):57-80. ( Cited by 2 | Google | More links ) Abstract:   In this article, I present a software architecture for intelligent agents. The essence of AI is complex information processing. It is impossible, in principle, to process complex information as a whole. We need some partial processing strategy that is still somehow connected to the whole. We also need flexible processing that can adapt to changes in the environment. One of the candidates for both of these is situated reasoning, which makes use of the fact that an agent is in a situation, so it only processes some of the information – the part that is relevant to that situation. The combination of situated reasoning and context reflection leads to the idea of organic programming, which introduces a new building block of programs called a cell. Cells contain situated programs and the combination of cells is controlled by those programs Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=467235CI http://www.springerlink.com/content/ju27445787217334/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=187742=1 http://www.springerlink.com/index/JU27445787217334.pdf http://www.ingentaconnect.com/content/klu/mind/1998/00000009/00000001/00187742 Sloman, Aaron (2002). The irrelevance of Turing machines to AI. In Matthias Scheutz (ed.), Computationalism: New Directions . MIT Press. ( Cited by 9 | Google | More links ) Additional links for this entry: http://citeseer.ist.psu.edu/sloman02irrelevance.html http://www.cs.bham.ac.uk/research/cogaff/sloman.turing.irrelevant.ps http://www.cs.bham.ac.uk/research/cogaff/sloman.turing.irrelevant.pdf http://www-lehre.informatik.uni-osnabrueck.de/~yzhao/tcn/sloman.turing.pdf http://www.cs.bham.ac.uk/research/projects/cogaff/sloman.turing.irrelevant.pdf Sufka, Kenneth J. Polger, Thomas W. (2005). Closing the gap on pain. In Murat Aydede (ed.), Pain: New Essays on its Nature and the Methodology of its Study . MIT Press. ( Google | More links ) Abstract: A widely accepted theory holds that emotional experiences occur mainly in a part of the human brain called the amygdala. A different theory asserts that color sensation is located in a small subpart of the visual cortex called V4. If these theories are correct, or even approximately correct, then they are remarkable advances toward a scientific explanation of human conscious experience. Yet even understanding the claims of such theories—much less evaluating them—raises some puzzles. Conscious experience does not present itself as a brain process. Indeed experience seems entirely unlike neural activity. For example, to some people it seems that an exact physical duplicate of you could have different sensations than you do, or could have no sensations at all. If so, then how is it even possible that sensations could turn out to be brain processes? Additional links for this entry: http://oz.uc.edu/~polgertw/PolgerSufka-ClosingGapPain.pdf http://homepages.uc.edu/~polgertw/PolgerSufka-ClosingGapPain.pdf Yudkowsky, Eliezer (online). General intelligence and seed AI. ( Google ) 6.4f The Frame Problem Anselme, Patrick French, Robert M. (1999). Interactively converging on context-sensitive representations: A solution to the frame problem. Revue Internationale de Philosophie 53 (209):365-385. ( Google ) Abstract: While we agree that the frame problem, as initially stated by McCarthy and Hayes (1969), is a problem that arises because of the use of representations, we do not accept the anti-representationalist position that the way around the problem is to eliminate representations. We believe that internal representations of the external world are a necessary, perhaps even a defining feature, of higher cognition. We explore the notion of dynamically created context-dependent representations that emerge from a continual interaction between working memory, external input, and long-term memory. We claim that only this kind of representation, necessary for higher cognitive abilities such as counterfactualization, will allow the combinatorial explosion inherent in the frame problem to be avoided Clark, Andy (2002). Global abductive inference and authoritative sources, or, how search engines can save cognitive science. Cognitive Science Quarterly 2 (2):115-140. ( Cited by 2 | Google | More links ) Abstract: Kleinberg (1999) describes a novel procedure for efficient search in a dense hyper-linked environment, such as the world wide web. The procedure exploits information implicit in the links between pages so as to identify patterns of connectivity indicative of “authorative sources”. At a more general level, the trick is to use this second-order link-structure information to rapidly and cheaply identify the knowledge- structures most likely to be relevant given a specific input. I shall argue that Kleinberg’s procedure is suggestive of a new, viable, and neuroscientifically plausible solution to at least (one incarnation of) the so-called “Frame Problem” in cognitive science viz the problem of explaining global abductive inference. More accurately, I shall argue that Additional links for this entry: http://www.philosophy.ed.ac.uk/staff/clark/pubs/GlobalReason.pdf Crockett, L. (1994). The Turing Test and the Frame Problem: AI's Mistaken Understanding of Intelligence. Ablex. ( Cited by 19 | Google ) Dennett, Daniel C. (1984). Cognitive wheels: The frame problem of AI. In C. Hookway (ed.), Minds, Machines and Evolution . Cambridge University Press. ( Cited by 139 | Annotation | Google ) General overview. Dreyfus, Hubert L. Dreyfus, Stuart E. (1987). How to stop worrying about the frame problem even though it's computationally insoluble. In Zenon W. Pylyshyn (ed.), The Robot's Dilemma . Ablex. ( Annotation | Google ) FP is an artifact of computational explicitness. Contrast human commonsense know-how, with relevance built in. Comparison to expert/novice distinction. Fetzer, James H. (1990). The frame problem: Artificial intelligence meets David Hume. International Journal of Expert Systems 3:219-232. ( Cited by 13 | Google | More links ) Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=2737634CI Fodor, Jerry A. (1987). Modules, frames, fridgeons, sleeping dogs, and the music of the spheres. In Zenon W. Pylyshyn (ed.), The Robot's Dilemma . Ablex. ( Cited by 56 | Google ) Fodor, Jerry A. (1989). Modules, frames, fridgeons, sleeping dogs. In Modularity in Knowledge Representation and Natural-Language Understanding . Cambridge: MIT Press. ( Google ) Fodor, Jerry A. (1987). Modules, frames, fridgeons. In Modularity In Knowledge Representation And Natural-Language Understanding . Cambridge: Mit Press. ( Google ) Haselager, W. F. G. Van Rappard, J. F. H. (1998). Connectionism, systematicity, and the frame problem. Minds and Machines 8 (2):161-179. ( Cited by 11 | Google | More links ) Abstract:   This paper investigates connectionism's potential to solve the frame problem. The frame problem arises in the context of modelling the human ability to see the relevant consequences of events in a situation. It has been claimed to be unsolvable for classical cognitive science, but easily manageable for connectionism. We will focus on a representational approach to the frame problem which advocates the use of intrinsic representations. We argue that although connectionism's distributed representations may look promising from this perspective, doubts can be raised about the potential of distributed representations to allow large amounts of complexly structured information to be adequately encoded and processed. It is questionable whether connectionist models that are claimed to effectively represent structured information can be scaled up to a realistic extent. We conclude that the frame problem provides a difficulty to connectionism that is no less serious than the obstacle it constitutes for classical cognitive science Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=371129CI http://www.springerlink.com/content/um0x474450532866/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=150466=1 http://www.springerlink.com/index/UM0X474450532866.pdf http://www.ingentaconnect.com/content/klu/mind/1998/00000008/00000002/00150466 Haugeland, John (1987). An overview of the frame problem. In Zenon W. Pylyshyn (ed.), The Robot's Dilemma . Ablex. ( Cited by 17 | Annotation | Google ) The FP may be a consequence of the explicit/implicit rep distinction. Use "complicit" reps instead, and changes will be carried along intrinsically. Hayes, Patrick (1987). What the frame problem is and isn't. In Zenon W. Pylyshyn (ed.), The Robot's Dilemma . Ablex. ( Cited by 25 | Annotation | Google ) FP is a relatively narrow problem, Some, e.g. Fodor, wrongly equate FP with the "Generalized AI Problem". Hendricks, Scott (2006). The frame problem and theories of belief. Philosophical Studies 129 (2):317-33. ( Google | More links ) Abstract: The frame problem is the problem of how we selectively apply relevant knowledge to particular situations in order to generate practical solutions. Some philosophers have thought that the frame problem can be used to rule out, or argue in favor of, a particular theory of belief states. But this is a mistake. Sentential theories of belief are no better or worse off with respect to the frame problem than are alternative theories of belief, most notably, the “map” theory of belief Additional links for this entry: http://www.springerlink.com/content/p846610503j81603/fulltext.pdf http://www.springerlink.com/index/P846610503J81603.pdf http://www.ingentaconnect.com/content/klu/phil/2006/00000129/00000002/00001644 Horgan, Terry Timmons, Mark (2009). What does the frame problem tell us about moral normativity? Ethical Theory and Moral Practice 12 (1). ( Google ) Abstract: Within cognitive science, mental processing is often construed as computation over mental representations—i.e., as the manipulation and transformation of mental representations in accordance with rules of the kind expressible in the form of a computer program. This foundational approach has encountered a long-standing, persistently recalcitrant, problem often called the frame problem; it is sometimes called the relevance problem. In this paper we describe the frame problem and certain of its apparent morals concerning human cognition, and we argue that these morals have significant import regarding both the nature of moral normativity and the human capacity for mastering moral normativity. The morals of the frame problem bode well, we argue, for the claim that moral normativity is not fully systematizable by exceptionless general principles, and for the correlative claim that such systematizability is not required in order for humans to master moral normativity Janlert, Lars-Erik (1987). Modeling change: The frame problem. In Zenon W. Pylyshyn (ed.), The Robot's Dilemma . Ablex. ( Cited by 23 | Google ) Korb, Kevin B. (1998). The frame problem: An AI fairy tale. Minds and Machines 8 (3):317-351. ( Cited by 1 | Google | More links ) Abstract:   I analyze the frame problem and its relation to other epistemological problems for artificial intelligence, such as the problem of induction, the qualification problem and the "general" AI problem. I dispute the claim that extensions to logic (default logic and circumscriptive logic) will ever offer a viable way out of the problem. In the discussion it will become clear that the original frame problem is really a fairy tale: as originally presented, and as tools for its solution are circumscribed by Pat Hayes, the problem is entertaining, but incapable of resolution. The solution to the frame problem becomes available, and even apparent, when we remove artificial restrictions on its treatment and understand the interrelation between the frame problem and the many other problems for artificial epistemology. I present the solution to the frame problem: an adequate theory and method for the machine induction of causal structure. Whereas this solution is clearly satisfactory in principle, and in practice real progress has been made in recent years in its application, its ultimate implementation is in prospect only for future generations of AI researchers Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=381462CI http://www.springerlink.com/content/q30166435n81x313/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=168538=1 http://www.springerlink.com/index/Q30166435N81X313.pdf http://www.ingentaconnect.com/content/klu/mind/1998/00000008/00000003/00168538 Lormand, Eric (1990). Framing the frame problem. Synthese 82 (3):353-74. ( Cited by 9 | Annotation | Google | More links ) Criticizes Dennett's, Haugeland's and Fodor's construals of the FP. Abstract:   The frame problem is widely reputed among philosophers to be one of the deepest and most difficult problems of cognitive science. This paper discusses three recent attempts to display this problem: Dennett's problem of ignoring obviously irrelevant knowledge, Haugeland's problem of efficiently keeping track of salient side effects, and Fodor's problem of avoiding the use of kooky concepts. In a negative vein, it is argued that these problems bear nothing but a superficial similarity to the frame problem of AI, so that they do not provide reasons to disparage standard attempts to solve it. More positively, it is argued that these problems are easily solved by slight variations on familiar AI themes. Finally, some discussion is devoted to more difficult problems confronting AI Additional links for this entry: http://www.springerlink.com/index/R122137P20775140.pdf Lormand, Eric (1998). The frame problem. In Robert A. Wilson Frank F. Keil (eds.), MIT Encyclopedia of the Cognitive Sciences (MITECS) . MIT Press. ( Google ) Abstract: From its humble origins labeling a technical annoyance for a particular AI formalism, the term "frame problem" has grown to cover issues confronting broader research programs in AI. In philosophy, the term has come to encompass allegedly fundamental, but merely superficially related, objections to computational models of mind in AI and beyond Lormand, Eric (1994). The holorobophobe's dilemma. In Kenneth M. Ford Z. Pylylshyn (eds.), The Robot's Dilemma Revisited . Ablex. ( Cited by 2 | Google | More links ) Abstract: Much research in AI (and cognitive science, more broadly) proceeds on the assumption that there is a difference between being well-informed and being smart. Being well-informed has to do, roughly, with the content of one’s representations--with their truth and the range of subjects they cover. Being smart, on the other hand, has to do with one’s ability to process these representations and with packaging them in a form that allows them to be processed efficiently. The main theoretical concern of artificial intelligence research is to solve "process-and-form" problems: problems with finding processes and representational formats that enable us to understand how a computer could be smart Additional links for this entry: http://www-personal.umich.edu/~lormand/phil/cogsci/holorobophobe.htm http://books.google.com/books?hl=en=hkiEKXzQKsG9Gm3nxwiIFbbaoPw Maloney, J. Christopher (1988). In praise of narrow minds. In James H. Fetzer (ed.), Aspects of AI . D. ( Google ) McCarthy, John Hayes, Patrick (1969). Some philosophical problems from the standpoint of artificial intelligence. In B. Meltzer Donald Michie (eds.), Machine Intelligence 4 . Edinburgh University Press. ( Cited by 1919 | Google | More links ) Additional links for this entry: http://www-formal.stanford.edu/jmc/mcchay69.pdf http://www.inf.ufsc.br/~mauro/ine6102/leituras/mcchay69.pdf http://www.dfki.uni-sb.de/imedia/lidos/bibtex/GINSBERG_a5141-152.html http://www.ida.liu.se/ext/brs/A/McCarthy,John/J.McCarthy69A/descrip.adl http://www.spatial.maine.edu/~worboys/processes/mccarthy hayes situation calculus.pdf McDermott, Drew (1987). We've been framed: Or, why AI is innocent of the frame problem. In Zenon W. Pylyshyn (ed.), The Robot's Dilemma . Ablex. ( Cited by 15 | Annotation | Google ) Solve frame problem by using the sleeping-dog strategy -- keeping things fixed unless there's a reason to suppose otherwise. Murphy, Dominic (2001). Folk psychology meets the frame problem. Studies in History and Philosophy of Modern Physics 32 (3):565-573. ( Google ) Murphy, D. (2001). Folk psychology meets the frame problem - W. F. G. Haselager, cognitive science and folk psychology (london: Sage publications, 1997), X + 165 pp. ISBN 0-761-95425-2 hardback £55.00; ISBN 0-761-95426-0 paperback £17.99. Studies in History and Philosophy of Science Part C 32 (3):565-573. ( Google ) Pollock, John L. (1997). Reasoning about change and persistence: A solution to the frame problem. Noûs 31 (2):143-169. ( Cited by 4 | Google | More links ) Additional links for this entry: http://www.blackwell-synergy.com/links/doi/10.1111/0029-4624.00040 http://www.blackwell-synergy.com/doi/abs/10.1111/0029-4624.00040 http://www.jstor.org/stable/pdfplus/2216189.pdf http://www.ingentaconnect.com/content/bpl/nous/1997/00000031/00000002/art00001 http://www.ingentaconnect.com/content/bpl/nous/1997/00000031/00000002/art00040 Pylyshyn, Zenon (1996). The frame problem blues. Once more, with feeling. In K. M. Ford Z. W. Pylyshyn (eds.), The Robot's Dilemma Revisited: The Frame Problem in Artificial Intelligence . Ablex. (Cited by 2 | Google | More links ) Abstract: For many of the authors in this volume, this is the second attempt to explore what McCarthy and Hayes (1969) ﬁrst called the “Frame Problem”. Since the ﬁrst compendium (Pylyshyn, 1987), nicely summarized here by Ronald Loui, there have been several conferences and books on the topic. Their goals range from providing a clariﬁcation of the problem by breaking it down into subproblems (and sometimes declaring the hard subproblems to not be the_ real_ Frame Problem), to providing formal “solutions” to certain aspects of the problem. But more often the message has been that the problem is not solvable except in a piecemeal way in special circumstances by some sort of heuristic approximations. It has sometimes also been said that solving the Frame Problem is not only an unachievable goal, but it is also an unnecessary one since_ humans_ do not solve it either; we simply get along as best we can and deal with the problem of planning in ways that, to use Dennett’s phrase, is “good enough for government work” Additional links for this entry: http://ruccs.rutgers.edu/ftp/pub/papers/frame.pdf http://portal.acm.org/citation.cfm?id=279932.279934 Pylyshyn, Zenon W. (ed.) (1987). The Robot's Dilemma. Ablex. ( Cited by 148 | Annotation | Google | More links ) Lots of papers on the frame problem. Additional links for this entry: http://portal.acm.org/citation.cfm?id=535989 Shanahan, Murray Baars, Bernard J. (2005). Applying global workspace theory to the frame problem. Cognition 98 (2):157-176. ( Cited by 28 | Google | More links ) Additional links for this entry: http://eric.ed.gov/ERICWebPortal/recordDetail?accno=EJ724310 http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation http://linkinghub.elsevier.com/retrieve/pii/S0010027704002288 Shanahan, Murray (online). The frame problem. Stanford Encyclopedia of Philosophy . ( Google ) Sperber, Dan Wilson, Deirdre (1996). Fodor's frame problem and relevance theory (reply to chiappe & kukla). [Journal (Paginated)] . ( Google | More links ) Abstract: Chiappe and Kukla argue that relevance theory fails to solve the frame problem as defined by Fodor. They are right. They are wrong, however, to take Fodors frame problem too seriously. Fodors concerns, on the other hand, even though they are wrongly framed, are worth addressing. We argue that Relevance thoery helps address them Additional links for this entry: http://cogprints.org/2004/1/frame.htm http://cogprints.org/2029/1/frame.htm http://cogprints.org/2004/0/frame.htm Sprevak, Mark , The frame problem and the treatment of prediction. ( Google ) Abstract: The frame problem is a problem in artificial intelligence that a number of philosophers have claimed has philosophical relevance. The structure of this paper is as follows: (1) An account of the frame problem is given; (2) The frame problem is distinguished from related problems; (3) The main strategies for dealing with the frame problem are outlined; (4) A difference between commonsense reasoning and prediction using a scientific theory is argued for; (5) Some implications for the.. Waskan, Jonathan A. (2000). A virtual solution to the frame problem. Proceedings of the First IEEE-RAS International Conference on Humanoid Robots . ( Cited by 1 | Google ) Abstract: We humans often respond effectively when faced with novel circumstances. This is because we are able to predict how particular alterations to the world will play out. Philosophers, psychologists, and computational modelers have long favored an account of this process that takes its inspiration from the truth-preserving powers of formal deduction techniques. There is, however, an alternative hypothesis that is better able to account for the human capacity to predict the consequences worldly alterations. This alternative takes its inspiration from the powers of truth preservation exhibited by scale models and leads to a determinate computational solution to the frame problem Wheeler, Michael (2008). Cognition in context: Phenomenology, situated robotics and the frame problem. International Journal of Philosophical Studies 16 (3):323 349. ( Google | More links ) Abstract: The frame problem is the difficulty of explaining how non-magical systems think and act in ways that are adaptively sensitive to context-dependent relevance. Influenced centrally by Heideggerian phenomenology, Hubert Dreyfus has argued that the frame problem is, in part, a consequence of the assumption (made by mainstream cognitive science and artificial intelligence) that intelligent behaviour is representation-guided behaviour. Dreyfus' Heideggerian analysis suggests that the frame problem dissolves if we reject representationalism about intelligence and recognize that human agents realize the property of thrownness (the property of being always already embedded in a context). I argue that this positive proposal is incomplete until we understand exactly how the properties in question may be instantiated in machines like us. So, working within a broadly Heideggerian conceptual framework, I pursue the character of a representation-shunning thrown machine. As part of this analysis, I suggest that the frame problem is, in truth, a two-headed beast. The intra-context frame problem challenges us to say how a purely mechanistic system may achieve appropriate, flexible and fluid action within a context. The inter-context frame problem challenges us to say how a purely mechanistic system may achieve appropriate, flexible and fluid action in worlds in which adaptation to new contexts is open-ended and in which the number of potential contexts is indeterminate. Drawing on the field of situated robotics, I suggest that the intra-context frame problem may be neutralized by systems of special-purpose adaptive couplings, while the inter-context frame problem may be neutralized by systems that exhibit the phenomenon of continuous reciprocal causation. I also defend the view that while continuous reciprocal causation is in conflict with representational explanation, special-purpose adaptive coupling, as well as its associated agential phenomenology, may feature representations. My proposal has been criticized recently by Dreyfus, who accuses me of propagating a cognitivist misreading of Heidegger, one that, because it maintains a role for representation, leads me seriously astray in my handling of the frame problem. I close by responding to Dreyfus' concerns Additional links for this entry: http://www.informaworld.com/smpp/./ftinterface~db=all~content=a794481656~fulltext=713240930 Wilkerson, William S. (2001). Simulation, theory, and the frame problem: The interpretive moment. Philosophical Psychology 14 (2):141-153. ( Cited by 5 | Google | More links ) Abstract: The theory-theory claims that the explanation and prediction of behavior works via the application of a theory, while the simulation theory claims that explanation works by putting ourselves in others' places and noting what we would do. On either account, in order to develop a prediction or explanation of another person's behavior, one first needs to have a characterization of that person's current or recent actions. Simulation requires that I have some grasp of the other person's behavior to project myself upon; whereas theorizing requires a subject matter to theorize about. The frame problem shows that multiple, true characterizations are possible for any behavior or situation. However, only one or a few of these characterizations are relevant to explaining or predicting behavior. Since different characterizations of a behavior lead to different predictions or explanations, much of the work of interpersonal interpretation is done in the process of finding this characterization - that is, prior to either theorizing or simulating. Moreover, finding this characterization involves extensive knowledge of the physical, cultural, and social worlds of the persons involved Additional links for this entry: http://taylorandfrancis.metapress.com/index/2C8FMEEMEUG504KF.pdf http://www.informaworld.com/index/2C8FMEEMEUG504KF.pdf http://www.ingentaconnect.com/content/routledg/cphp/2001/00000014/00000002/art00001 http://www.informaworld.com/smpp/./ftinterface~db=all~content=a713690495~fulltext=713240930 6.4g AI Methodology Bickhard, Mark H. (2000). Motivation and Emotion: An Interactive Process Model. In Ralph D. Ellis Natika Newton (eds.), The Caldron of Consciousness: Motivation, Affect and Self-Organization . John Benjamins. ( Cited by 19 | Google | More links ) Abstract: In this chapter, I outline dynamic models of motivation and emotion. These turn out not to be autonomous subsystems, but, instead, are deeply integrated in the basic interactive dynamic character of living systems. Motivation is a crucial aspect of particular kinds of interactive systems -- systems for which representation is a sister aspect. Emotion is a special kind of partially reflective interaction process, and yields its own emergent motivational aspects. In addition, the overall model accounts for some of the crucial properties of consciousness Additional links for this entry: http://www.lehigh.edu/~mhb0/motemotion.html http://www.lehigh.edu/~mhb0/MotandEmotion.pdf Birnbaum, L. (1991). Rigor mortis: A response to Nilsson's 'logic and artificial intelligence'. Artificial Intelligence 47:57-78. ( Cited by 106 | Google | More links ) Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=2460287CI Chalmers, David J. ; French, Robert M. Hofstadter, Douglas R. (1992). High-level perception, representation, and analogy: A critique of AI methodology. Journal of Experimental and Theoretical Artificial Intelligence . ( Cited by 1 | Annotation | Google ) AI must integrate perception and cognition in the interest of flexible representation. Current models ignore perception and the development of representation, but this cannot be separated from later cognitive processes. Chalmers, David J. ; French, Robert M. Hofstadter, Douglas R. (1992). High-level perception, representation, and analogy:A critique of artificial intelligence methodology. Journal of Experimental and Theoretical Artificial Intellige 4 (3):185 - 211. ( Cited by 123 | Google | More links ) Abstract: High-level perception--”the process of making sense of complex data at an abstract, conceptual level--”is fundamental to human cognition. Through high-level perception, chaotic environmen- tal stimuli are organized into the mental representations that are used throughout cognitive pro- cessing. Much work in traditional artificial intelligence has ignored the process of high-level perception, by starting with hand-coded representations. In this paper, we argue that this dis- missal of perceptual processes leads to distorted models of human cognition. We examine some existing artificial-intelligence models--”notably BACON, a model of scientific discovery, and the Structure-Mapping Engine, a model of analogical thought--”and argue that these are flawed pre- cisely because they downplay the role of high-level perception. Further, we argue that perceptu- al processes cannot be separated from other cognitive processes even in principle, and therefore that traditional artificial-intelligence models cannot be defended by supposing the existence of a --œrepresentation module--� that supplies representations ready-made. Finally, we describe a model of high-level perception and analogical thought in which perceptual processing is integrated with analogical mapping, leading to the flexible build-up of representations appropriate to a given context Additional links for this entry: http://citeseer.ist.psu.edu/49715.html http://portal.acm.org/citation.cfm?id=175346.175347 http://www.u.arizona.edu/~chalmers/papers/highlevel.pdf http://www.nbu.bg/cogs/personal/kokinov/COG501/hightlevel.pdf http://www.csa.com/partners/viewrecord.php?requester=gs=2767459CI http://www.informaworld.com/index/778787585.pdf Clark, Andy (1986). A biological metaphor. Mind and Language 1:45-64. ( Cited by 6 | Annotation | Google | More links ) AI should look at biology. Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/119496832/PDFSTART Clark, Andy (1987). The kludge in the machine. Mind and Language 2:277-300. ( Cited by 12 | Google | More links ) Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/120021149/PDFSTART Colombetti, Giovanna (2007). Enactive appraisal. Phenomenology and the Cognitive Sciences . ( Cited by 4 | Google | More links ) Abstract: Emotion theorists tend to separate “arousal” and other bodily events such as “actions” from the evaluative component of emotion known as “appraisal.” This separation, I argue, implies phenomenologically implausible accounts of emotion elicitation and personhood. As an alternative, I attempt a reconceptualization of the notion of appraisal within the so-called “enactive approach.” I argue that appraisal is constituted by arousal and action, and I show how this view relates to an embodied and affective notion of personhood Additional links for this entry: http://polorovereto.unitn.it/~colombetti/docs/GC_EnactiveAppraisal06.pdf http://polorovereto.unitn.it/~colombetti/docs/GC_EnactiveAppraisal06.pdf http://www.springerlink.com/content/y7543g7p14826726/fulltext.pdf http://www.springerlink.com/index/Y7543G7P14826726.pdf Colombetti, Giovanna Thompson, Evan (forthcoming). The feeling body: Towards an enactive approach to emotion. In W. F. Overton, U. Mueller J. Newman (eds.), Body in Mind, Mind in Body: Developmental Perspectives on Embodiment and Consciousness . Erlbaum. ( Cited by 3 | Google | More links ) Abstract: For many years emotion theory has been characterized by a dichotomy between the head and the body. In the golden years of cognitivism, during the nineteen-sixties and seventies, emotion theory focused on the cognitive antecedents of emotion, the so-called “appraisal processes.” Bodily events were seen largely as byproducts of cognition, and as too unspecific to contribute to the variety of emotion experience. Cognition was conceptualized as an abstract, intellectual, “heady” process separate from bodily events. Although current emotion theory has moved beyond this disembodied stance by conceiving of emotions as involving both cognitive processes (perception, attention, and evaluation) and bodily events (arousal, behavior, and facial expressions), the legacy of cognitivism persists in the tendency to treat cognitive and bodily events as separate constituents of emotion. Thus the cognitive aspects of emotion are supposedly distinct and separate from the bodily ones. This separation indicates that cognitivism’s disembodied conception of cognition continues to shape the way emotion theorists conceptualize emotion Additional links for this entry: http://individual.utoronto.ca/evant/FeelingBody.pdf http://people.exeter.ac.uk/gc243/index/GC_FeelingBody.pdf http://polorovereto.unitn.it/~colombetti/docs/GC_FeelingBody.pdf Dascal, M. (1992). Why does language matter to artificial intelligence? Minds and Machines 2 (2):145-174. ( Cited by 7 | Google | More links ) Abstract:   Artificial intelligence, conceived either as an attempt to provide models of human cognition or as the development of programs able to perform intelligent tasks, is primarily interested in theuses of language. It should be concerned, therefore, withpragmatics. But its concern with pragmatics should not be restricted to the narrow, traditional conception of pragmatics as the theory of communication (or of the social uses of language). In addition to that, AI should take into account also the mental uses of language (in reasoning, for example) and the existential dimensions of language as a determiner of the world we (and our computers) live in. In this paper, the relevance of these three branches of pragmatics-sociopragmatics, psychopragmatics, and ontopragmatics-for AI are explored Additional links for this entry: http://www.springerlink.com/content/w371m4055304hp68/fulltext.pdf http://www.springerlink.com/index/W371M4055304HP68.pdf Dietrich, Eric (1994). AI and the tyranny of Galen, or why evolutionary psychology and cognitive ethology are important to artificial intelligence. Journal of Experimental And Theoretical Artificial Intelligence 6 (4):325-330. ( Google | More links ) Abstract: Concern over the nature of AI is, for the tastes many AI scientists, probably overdone. In this they are like all other scientists. Working scientists worry about experiments, data, and theories, not foundational issues such as what their work is really about or whether their discipline is methodologically healthy. However, most scientists aren’t in a field that is approximately fifty years old. Even relatively new fields such as nonlinear dynamics or branches of biochemistry are in fact advances in older established sciences and are therefore much more settled. Of course, by stretching things, AI can be said to have a history reaching back t o Charles Babbage, and possibly back beyond that to Leibnitz. However, all of that is best viewed as prelude. AI’s history is punctuated with the invention of the computer (and, if one wants t o stretch our history back to the 1930s, the development of the notion of computation by Turing, Church, and others). Hence, AI really began (or began in earnest) sometime in the late 1940s or early 1950s (some mark the conference a t Dartmouth in the summer of 1957 as the moment of our birth). And since those years we simply have not had time to settle into a routine science attacking reasonably well understood questions (for example, many of the questions some of us regard as supreme are regarded by others as inconsequential or mere excursions) Additional links for this entry: http://www.informaworld.com/index/777915170.pdf Dreyfus, Hubert L. (1981). From micro-worlds to knowledge: AI at an impasse. In J. Haugel (ed.), Mind Design . MIT Press. ( Annotation | Google ) Micro-worlds don't test true understanding, and frames and scripts are doomed to leave out too much. Explicit representation can't capture intelligence. Dreyfus, Hubert L. Dreyfus, Stuart E. (1988). Making a mind versus modeling the brain: AI at a crossroads. Daedalus . ( Cited by 6 | Annotation | Google ) History of AI (boo) and connectionism (qualified hooray). And Husserl/ Heidegger/Wittgenstein. Quite nice. Dreyfus, Hubert L. (2007). Why Heideggerian ai failed and how fixing it would require making it more Heideggerian. Philosophical Psychology 20 (2):247 268. (Cited by 2 | Google | More links ) Additional links for this entry: http://www.informaworld.com/smpp/./ftinterface~content=a777583844~fulltext=713240930 http://www.informaworld.com/index/777583844.pdf http://www.informaworld.com/smpp/./ftinterface~db=all~content=a777583844~fulltext=713240930 Elster, Jon (1996). Rationality and the emotions. Economic Journal 106:1386-97. ( Cited by 63 | Google | More links ) Abstract: In an earlier paper (Elster, 1989 a), I discussed the relation between rationality and social norms. Although I did mention the role of the emotions in sustaining social norms, I did not focus explicitly on the relation between rationality and the emotions. That relation is the main topic of the present paper, with social norms in a subsidiary part Additional links for this entry: http://ideas.repec.org/a/ecj/econjl/v106y1996i438p1386-97.html http://culturalheritage.ceistorvergata.it/virtual_library/Art. - Rationality and emotions - J. ELSTER.pdf http://orton.catie.ac.cr/cgi-bin/wxis.exe/?IsisScript=IDEA.xis=mfn=001739 Flach, P. A. (ed.) (1991). Future Directions in Artificial Intelligence. New York: Elsevier Science. ( Cited by 2 | Google ) Fulda, Joseph S. (2006). A Plea for Automated Language-to-Logical-Form Converters. RASK: Internationalt tidsskrift for sprog og kommuinkation 24 (--):87-102. ( Google ) Griffiths, Paul E. Scarantino, Andrea (2005). Emotions in the Wild: The Situated Perspective on Emotion. In P. Robbins Murat Aydede (eds.), The Cambridge Handbook of Situated Cognition . Cambridge University Press. ( Cited by 2 | Google | More links ) Abstract: Paul E Griffiths Biohumanities Project University of Queensland St Lucia 4072 Australia paul.griffiths@uq.edu.au Additional links for this entry: http://philsci-archive.pitt.edu/archive/00002448/ http://www.uq.edu.au/biohumanities/webpdfs/EmotionsintheWild.PDF http://www.uq.edu.au/biohumanities/webpdfs/EmotionsintheWild.PDF http://paul.representinggenes.org/webpdfs/Griff.Scara.IP.EmotionsWild.pdf http://philsci-archive.pitt.edu/archive/00002448/01/Emotions_in_the_Wild.PDF Hadley, Robert F. (1991). The many uses of 'belief' in AI. Minds and Machines 1 (1):55-74. ( Cited by 2 | Annotation | Google | More links ) Various AI approaches to belief: syntactic, propositional/meaning-based, information, tractability, discoverability, and degree of confidence. Abstract:   Within AI and the cognitively related disciplines, there exist a multiplicity of uses of belief. On the face of it, these differing uses reflect differing views about the nature of an objective phenomenon called belief. In this paper I distinguish six distinct ways in which belief is used in AI. I shall argue that not all these uses reflect a difference of opinion about an objective feature of reality. Rather, in some cases, the differing uses reflect differing concerns with special AI applications. In other cases, however, genuine differences exist about the nature of what we pre-theoretically call belief. To an extent the multiplicity of opinions about, and uses of belief, echoes the discrepant motivations of AI researchers. The relevance of this discussion for cognitive scientists and philosophers arises from the fact that (a) many regard theoretical research within AI as a branch of cognitive science, and (b) even if theoretical AI is not cognitive science, trends within AI influence theories developed within cognitive science. It should be beneficial, therefore, to unravel the distinct uses and motivations surrounding belief, in order to discover which usages merely reflect differing pragmatic concerns, and which usages genuinely reflect divergent views about reality Additional links for this entry: http://www.springerlink.com/content/qm781236u5642248/fulltext.pdf http://www.springerlink.com/index/QM781236U5642248.pdf Haugeland, John (1979). Understanding natural language. Journal of Philosophy 76 (November):619-32. ( Cited by 12 | Annotation | Google | More links ) AI will need holism: interpretational, common-sense, situational, existential. Additional links for this entry: http://www.jstor.org/stable/pdfplus/2025695.pdf Kirsh, David (1991). Foundations of AI: The big issues. Artificial Intelligence 47:3-30. ( Cited by 46 | Annotation | Google | More links ) Identifying the dividing lines: pre-eminence of knowledge, embodiment, language-like kinematics, role of learning, uniformity of architecture. Abstract: The objective of research in the foundations of Al is to explore such basic questions as: What is a theory in Al? What are the most abstract assumptions underlying the competing visions of intelligence? What are the basic arguments for and against each assumption? In this essay I discuss five foundational issues: (1) Core Al is the study of conceptualization and should begin with knowledge level theories. (2) Cognition can be studied as a disembodied process without solving the symbol grounding problem. (3) Cognition is nicely described in propositional terms. (4) We can study cognition separately from learning. (5) There is a single architecture underlying virtually all cognition. I explain what each of these implies and present arguments from both outside and inside Al why each has been seen as right or wrong. Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=2460285CI Kobsa, Alfred (1987). What is explained by AI models. In Artificial Intelligence . St Martin's Press. (Cited by 2 | Google ) Labuschagne, Willem A. Heidema, Johannes (2005). Natural and artificial cognition: On the proper place of reason. South African Journal of Philosophy 24 (2):137-149. ( Cited by 1 | Google | More links ) Additional links for this entry: http://www.ajol.info/viewarticle.php?jid=211=24184 http://www.cs.otago.ac.nz/homepages/willem/publications/SAJP paper.rtf Marr, David (1977). Artificial intelligence: A personal view. Artificial Intelligence 9 (September):37-48. ( Cited by 131 | Annotation | Google | More links ) AI usually comes up with Type 2 (algorithmic) theories, when Type 1 (info processing) theories might be more useful -- at least if they exist. Additional links for this entry: https://dspace.mit.edu/handle/1721.1/5776?mode=simple http://www.inf.ed.ac.uk/teaching/modules/irm/marr.ps.gz http://www.inf.ed.ac.uk/teaching/courses/irm/marr.ps.gz http://books.google.com/books?hl=en=TFXshtTLSQeKKpJCvrg1gvOCbDg McDermott, Drew (1987). A critique of pure reason. Computational Intelligence 3:151-60. ( Cited by 141 | Annotation | Google | More links ) Criticism of logicism (i.e. reliance on deduction) in AI. Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/120022352/PDFSTART McDermott, Drew (1981). Artificial intelligence meets natural stupidity. In J. Haugel (ed.), Mind Design . MIT Press. ( Cited by 99 | Annotation | Google ) Problems in AI methodology: wishful mnemonics, oversimplifying natural language concepts, and never implementing programs. Entertaining. Nilsson, Neil (1991). Logic and artificial intelligence. Artificial Intelligence 47:31-56. ( Cited by 123 | Google | More links ) Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=2460286CI http://books.google.com/books?hl=en=6iT-5H55h1UY0sB27skFDMVYdDc Partridge, Derek Wilks, Y. (eds.) (1990). The Foundations of Artificial Intelligence: A Sourcebook. Cambridge University Press. ( Cited by 19 | Annotation | Google | More links ) Lots of papers on various aspects of AI methodology. Quite thorough. Abstract: This outstanding collection is designed to address the fundamental issues and principles underlying the task of Artificial Intelligence. Additional links for this entry: http://dannyreviews.com/h/The_Foundations_of_Artificial_Intelligence.html http://www.anatomy.usyd.edu.au/danny/book-reviews/h/The_Foundations_of_Artificial_Intelligence.html Petersen, Stephen (2004). Functions, creatures, learning, emotion. Hudlicka and Canamero . ( Cited by 2 | Google ) Preston, Beth (1993). Heidegger and artificial intelligence. Philosophy and Phenomenological Research 53 (1):43-69. ( Cited by 4 | Annotation | Google | More links ) On the non-represented background to everyday activity, and environmental interaction in cognition. Criticizes cognitivism, connectionism, looks at Agre/Chapman/Brooks, ethology, anthropology for support. Additional links for this entry: http://www.jstor.org/stable/pdfplus/2108053.pdf Pylyshyn, Zenon W. (1979). Complexity and the study of artificial and human intelligence. In Martin Ringle (ed.), Philosophical Perspectives in Artificial Intelligence . Humanities Press. ( Cited by 15 | Google ) Ringle, Martin (ed.) (1979). Philosophical Perspectives in Artificial Intelligence. Humanities Press. ( Cited by 5 | Annotation | Google ) 10 papers on philosophy of AI, psychology and knowledge representation. Robinson, William S. (1991). Rationalism, expertise, and the dreyfuses' critique of AI research. Southern Journal of Philosophy 29:271-90. ( Annotation | Google ) Defending limited rationalism: i.e. a theory of intelligence below the conceptual level but above the neuronal level. Shaffer, Michael J. (2009). Decision theory, intelligent planning and counterfactuals. Minds and Machines 19 (1):61-92. ( Google ) Abstract: The ontology of decision theory has been subject to considerable debate in the past, and discussion of just how we ought to view decision problems has revealed more than one interesting problem, as well as suggested some novel modifications of classical decision theory. In this paper it will be argued that Bayesian, or evidential, decision-theoretic characterizations of decision situations fail to adequately account for knowledge concerning the causal connections between acts, states, and outcomes in decision situations, and so they are incomplete. Second, it will be argues that when we attempt to incorporate the knowledge of such causal connections into Bayesian decision theory, a substantial technical problem arises for which there is no currently available solution that does not suffer from some damning objection or other. From a broader perspective, this then throws into question the use of decision theory as a model of human or machine planning Sticklen, J. (1989). Problem-solving architectures at the knowledge level. Journal of Experimental and Theoretical Artificial Intelligence 1:233-247. ( Cited by 19 | Google | More links ) Additional links for this entry: http://www.csa.com/partners/viewrecord.php?requester=gs=2213245CI http://www.informaworld.com/index/777585783.pdf Stone, Matthew , Agents in the real world. ( Google ) Abstract: The mid-twentieth century saw the introduction of a new general model of processes, COMPUTATION, with the work of scientists such as Turing, Chomsky, Newell and Simon.1 This model so revolutionized the intellectual world that the dominant scientific programs of the day—spearheaded by such eminent scientists as Hilbert, Bloomfield and Skinner—are today remembered as much for the way computation exposed their stark limitations as for their positive contributions.2 Ever since, the field of Artificial Intelligence (AI) has defined itself as the subfield of computer science dedicated to the understanding of intelligent entities as computational processes. Now, drawing on fifty years of results of increasing breadth and applicability, we can also characterize AI research as a concrete practice: an ENGINEER- 6.4h Robotics Beavers, Anthony F. , Between angels and animals: The question of robot ethics, or is Kantian moral agency desirable? ( Google ) Abstract: In this paper, I examine a variety of agents that appear in Kantian ethics in order to determine which would be necessary to make a robot a genuine moral agent. However, building such an agent would require that we structure into a robot’s behavioral repertoire the possibility for immoral behavior, for only then can the moral law, according to Kant, manifest itself as an ought, a prerequisite for being able to hold an agent morally accountable for its actions. Since building a moral robot requires the possibility of immoral behavior, I go on to argue that we cannot morally want robots to be genuine moral agents, but only beings that simulate moral behavior. Finally, I raise but do not answer the question that if morality requires us to want robots that are not genuine moral agents, why should we want something different in the case of human beings Breazeal, C. Brooks, Rodney (2004). Robot emotions: A functional perspective. In J. Fellous (ed.), Who Needs Emotions . Oxford University Press. ( Google ) Brooks, Rodney A. Stein, Lynn Andrea (1994). Building brains for bodies. Autonomous Robotics 1 (1):7-25. ( Cited by 281 | Google | More links ) Abstract: We describe a project to capitalize on newly available levels of computational resources in order to understand human cognition. We are building an integrated physical system including vision, sound input and output, and dextrous manipulation, all controlled by a continuously operating large scale parallel MIMD computer. The resulting system will learn to "think" by building on its bodily experiences to accomplish progressively more abstract tasks. Past experience suggests that in attempting to build such an integrated system we will have to fundamentally change the way artificial intelligence, cognitive science, linguistics, and philosophy think about the organization of intelligence. We expect to be able to better reconcile the theories that will be developed with current work in neuroscience Additional links for this entry: https://dspace.mit.edu/handle/1721.1/5948 http://www.ai.mit.edu/people/brooks/papers/brains.pdf http://people.csail.mit.edu/people/brooks/papers/brains.pdf http://www.aisland.org/cki20/media/handouts/frederic_zolnet.doc http://wexler.free.fr/library/files/brooks (1994) building brains for bodies.pdf http://www.ece.pdx.edu/~mperkows/ML_LAB/Giant_Hexapod/building-braind-brooks.pdf http://stinet.dtic.mil/oai/oai?verb=getRecord=ADA270531 http://www.csa.com/partners/viewrecord.php?requester=gs=A9432374AH http://www.springerlink.com/index/QW4834246H1K5537.pdf Brooks, Rodney (1991). Challenges for Complete Creature Architectures. In Jean-Arcady Meyer Stewart W. Wilson (eds.), From Animals to Animats: Proceedings of The First International Conference on Simulation of Adaptive Behavior (Complex Adaptive Systems) . MIT Press. ( Cited by 71 | Google | More links ) Abstract: boundaries. It is impossible to do good science without having an appreciation for the problems and concepts in the other levels of abstraction (at least in the direction from biology towards physics), but there are whole sets of tools, methods of analysis, theories and explanations within each discipline which do not cross those boundaries Additional links for this entry: http://citeseer.ist.psu.edu/brooks90challenges.html http://people.csail.mit.edu/brooks/papers/challenges.pdf http://www.cs.helsinki.fi/u/jphuttun/histo/source/challenges.pdf http://www.valentiniweb.com/Piermo/robotica/doc/Brooks/challenges.pdf http://hebb.cis.uoguelph.ca/~dastacey/Robotics/Brooks/Brookschallenges.ps http://www.ee.pdx.edu/~mperkows/ML_LAB/Giant_Hexapod/brooks-challenges.pdf http://www.ece.pdx.edu/~mperkows/ML_LAB/Giant_Hexapod/brooks-challenges.pdf Brooks, Rodney A. ; Breazeal, Cynthia ; Marjanovic, Matthew ; Scassellati, Brian Williamson, Matthew (1999). The cog project: Building a humanoid robot. Lecture Notes in Computer Science 1562:52-87. ( Cited by 302 | Google | More links ) Abstract: To explore issues of developmental structure, physical em- bodiment, integration of multiple sensory and motor systems, and social interaction, we have constructed an upper-torso humanoid robot called Cog. The robot has twenty-one degrees of freedom and a variety of sen- sory systems, including visual, auditory, vestibular, kinesthetic, and tac- tile senses. This chapter gives a background on the methodology that we have used in our investigations, highlights the research issues that have been raised during this project, and provides a summary of both the current state of the project and our long-term goals. We report on a variety of implemented visual-motor routines (smooth-pursuit track- ing, saccades, binocular vergence, and vestibular-ocular and opto-kinetic re?exes), orientation behaviors, motor control techniques, and social be- haviors (pointing to a visual target, recognizing joint attention through face and eye ?nding, imitation of head nods, and regulating interaction through expressive feedback). We further outline a number of areas for future research that will be necessary to build a complete embodied sys- tem Additional links for this entry: http://www.cs.yale.edu/homes/scaz/papers/springer-final-cog.pdf http://groups.csail.mit.edu/lbr/hrg/1998/springer-final-cog.pdf http://cs-www.cs.yale.edu/homes/scaz/papers/springer-final-cog.pdf http://www.cc.gatech.edu/classes/AY2003/cs4803b_spring/readings/brooks99cog.pdf http://www.ee.pdx.edu/~mperkows/CLASS_ROBOTICS/FEBR26-2004/Humanoids/cog-project-brooks99cog.pdf https://longway.dell11.com/03http-14777.cq.zq1.Wx6/jevrWg1G/NAe/wei/fooJ--/springer-final-cog.pdf http://books.google.com/books?hl=en=e5kkWmVDS2-6Fa5ty-n0_WWIjfE http://www.springerlink.com/index/q0ly9uyvwaddmdxj.pdf http://www.springerlink.com/index/Q0LY9UYVWADDMDXJ.pdf Bryson, Joanna J. (2006). The attentional spotlight (dennett and the cog project). Minds and Machines 16 (1):21-28. ( Google | More links ) Additional links for this entry: http://www.springerlink.com/index/B32K3HT479101W80.pdf Cardon, Alain (2006). Artificial consciousness, artificial emotions, and autonomous robots. Cognitive Processing 7 (4):245-267. ( Google | More links ) Additional links for this entry: http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation http://www.springerlink.com/index/T03M83873362716L.pdf http://www.ingentaconnect.com/content/klu/10339/2006/00000007/00000004/00000154 Chella, Antonio (2007). Towards robot conscious perception. In Antonio Chella Riccardo Manzotti (eds.), Artificial Consciousness . Imprint Academic. ( Google ) Clancey, William (1995). How situated cognition is different from situated robotics. In Luc Steels Rodney Brooks (eds.), The "Artificial Life" Route to "Artificial Intelligence": Building Situated Embodied Agents . Hillsdale, NJ: Lawrence Erlbaum Associates. ( Google ) Clark, Andy Grush, Rick (1999). Towards a cognitive robotics. Adaptive Behavior 7 (1):5-16. ( Cited by 73 | Google | More links ) Abstract: There is a definite challenge in the air regarding the pivotal notion of internal representation. This challenge is explicit in, e.g., van Gelder, 1995; Beer, 1995; Thelen & Smith, 1994; Wheeler, 1994; and elsewhere. We think it is a challenge that can be met and that (importantly) can be met by arguing from within a general framework that accepts many of the basic premises of the work (in new robotics and in dynamical systems theory) that motivates such scepticism in the first place. Our strategy will be as follows. We begin (Section 1) by offering an account (an example and something close to a definition) of what we shall term Minimal Robust Representationalism (MRR). Sections 2 & 3 address some likely worries and questions about this notion. We end (Section 4) by making explicit the conditions under which, on our account, a science (e.g., robot- ics) may claim to be addressing cognitive phenomena Additional links for this entry: http://www.isab.org/journal/adap7_1.php http://portal.acm.org/citation.cfm?id=312918 http://mind.ucsd.edu/papers/cogrob/cogrob.pdf https://www.era.lib.ed.ac.uk/handle/1842/1297 http://mind.ucsd.edu/papers/cogrob/cogrob.pdf http://www.denizyuret.com/ref/clark_andy/cogrob.ps http://www.cogs.indiana.edu/andy/tacrfinalw-Grush.pdf http://www.cogs.indiana.edu/andy/tacrfinalw-Grush.pdf http://mind.ucsd.edu/papers/cogrob/cogrobhtml/cogrob-text.html http://mind.ucsd.edu/papers/cogrob/cogrobhtml/cogrob-text.html http://www.philosophy.ed.ac.uk/staff/clark/pubs/tacrfinalw-Grush.pdf http://www.philosophy.ed.ac.uk/staff/clark/pubs/tacrfinalw-Grush.pdf http://wexler.free.fr/library/files/clark (1999) towards a cognitive robotics.pdf http://adb.sagepub.com/cgi/content/abstract/7/1/5 Dautenhahn, Kerstin ; Ogden, Bernard ; Quick, Tom Ziemke, Tom (2002). From embodied to socially embedded agents: Implications for interaction-aware robots. Cognitive Systems Research 3 (1):397-427. (Cited by 50 | Google | More links ) Additional links for this entry: http://linkinghub.elsevier.com/retrieve/pii/S1389041702000505 http://www.ingentaconnect.com/content/els/13890417/2002/00000003/00000003/art00050 Dennett, Daniel C. (ms). Cog as a thought experiment. ( Cited by 3 | Google | More links ) Abstract: In her presentation at the Monte Verità workshop, Maja Mataric showed us a videotape of her robots cruising together through the lab, and remarked, aptly: "They're flocking, but that's not what they think they're doing." This is a vivid instance of a phenomenon that lies at the heart of all the research I learned about at Monte Verità: the execution of surprisingly successful "cognitive" behaviors by systems that did not explicitly represent, and did not need to explicitly represent, what they were doing. How "high" in the intuitive scale of cognitive sophistication can such unwitting prowess reach? All the way, apparently, since I want to echo Maja's observation with one of my own: "These roboticists are doing philosophy, but that's not what they think they're doing." It is possible, then, even to do philosophy--that most intellectual of activities--without realizing that that is what you are doing. It is even possible to do it well, for this is a good, new way of addressing antique philosophical puzzles Additional links for this entry: http://ase.tufts.edu/cogstud/papers/cogthogt.htm http://cogprints.ecs.soton.ac.uk/archive/00000248/ http://cogprints.soton.ac.uk/documents/disk0/00/00/02/48/ http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:248 http://www.ingentaconnect.com/content/els/09218890/1997/00000020/00000002/art80709 http://cogprints.org/248/1/cogthogt.htm http://cogprints.org/248/0/cogthogt.htm Dennett, Daniel C. (1995). Cog: Steps toward consciousness in robots. In Thomas Metzinger (ed.), Conscious Experience . Ferdinand Schoningh. ( Cited by 3 | Google ) Elton, Matthew (1997). Robots and rights: The ethical demands of artificial agents. Ends and Means 1 (2). ( Cited by 4 | Google ) Gips, James (1994). Toward the ethical robot. In Kenneth M. Ford, C. Glymour Patrick Hayes (eds.), Android Epistemology . MIT Press. (Cited by 19 | Google | More links ) Additional links for this entry: http://portal.acm.org/citation.cfm?id=216375 http://www.roboethics.org/site/modules/mydownloads/download/references/Gips_EthicalRobot.pdf Hesslow, Germund Jirenhed, D-A. (2007). The inner world of a simple robot. Journal of Consciousness Studies 14 (7):85-96. ( Google | More links ) Abstract: The purpose of the paper is to discuss whether a particular robot can be said to have an 'inner world', something that can be taken to be a critical feature of consciousness. It has previously been argued that the mechanism underlying the appearance of an inner world in humans is an ability of our brains to simulate behaviour and perception. A robot has previously been designed in which perception can be simulated. A prima facie case can be made that this robot has an inner world in the same sense as humans. Various objections to this claim are discussed in the paper and it is concluded that the robot, although extremely simple, can easily be improved without adding any new principles, so that ascribing an inner world to it becomes intuitively reasonable Additional links for this entry: http://www.ingentaconnect.com/content/imp/jcs/2007/00000014/00000007/art00006 Holland, Owen Goodman, Russell B. (2003). Robots with internal models: A route to machine consciousness? Journal of Consciousness Studies 10 (4):77-109. ( Cited by 20 | Google | More links ) Additional links for this entry: http://www.ingentaconnect.com/content/imp/jcs/2003/00000010/F0020004/1348 http://www.ingentaconnect.com/content/imp/jcs/2003/00000010/F0020004/1342 Holland, Owen ; Knight, Rob Newcombe, Richard (2007). The role of the self process in embodied machine consciousness. In Antonio Chella Riccardo Manzotti (eds.), Artificial Consciousness . Imprint Academic. ( Google ) Ishiguro, Hiroshi (2006). Android science: Conscious and subconscious recognition. Connection Science 18 (4):319-332. ( Cited by 14 | Google | More links ) Additional links for this entry: http://www.informaworld.com/index/758709863.pdf Kitamura, T. ; Tahara, T. Asami, K. (2000). How can a robot have consciousness? Advanced Robotics 14:263-275. ( Cited by 6 | Google | More links ) Additional links for this entry: http://www.springerlink.com/index/P64723468651LH49.pdf http://www.ingentaconnect.com/content/vsp/arb/2000/00000014/00000004/art00002 Kitamura, T. (2002). What is the self of a robot? On a consciousness architecture for a mobile robot as a model of human consciousness. In Kunio Yasue, Marj Jibu Tarcisio Della Senta (eds.), No Matter, Never Mind . John Benjamins. ( Google ) Korienek, Gene Uzgalis, William L. (2002). Adaptable robots. Metaphilosophy 33 (1-2):83-97. ( Cited by 1 | Google ) Lacey, Nicola Lee, M. (2003). The epistemological foundations of artificial agents. Minds and Machines 13 (3):339-365. ( Cited by 1 | Google | More links ) Abstract:   A situated agent is one which operates within an environment. In most cases, the environment in which the agent exists will be more complex than the agent itself. This means that an agent, human or artificial, which wishes to carry out non-trivial operations in its environment must use techniques which allow an unbounded world to be represented within a cognitively bounded agent. We present a brief description of some important theories within the fields of epistemology and metaphysics. We then discuss ways in which philosophical problems of scepticism are related to the problems faced by knowledge representation. We suggest that some of the methods that philosophers have developed to address the problems of epistemology may be relevant to the problems of representing knowledge within artificial agents Additional links for this entry: http://portal.acm.org/citation.cfm?id=781081.781090 http://www.springerlink.com/content/g6g408r7457k4u21/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=5118862=1 http://www.springerlink.com/index/G6G408R7457K4U21.pdf http://www.ingentaconnect.com/content/klu/mind/2003/00000013/00000003/05118862 Menant, Christophe (2005). Information and meaning in life, humans and robots (2005). Proceedings of FIS2005 by MDPI, Basel, Switzerland . ( Google | More links ) Abstract: Information and meaning exist around us and within ourselves, and the same information can correspond to different meanings. This is true for humans and animals, and is becoming true for robots. We propose here an overview of this subject by using a systemic tool related to meaning generation that has already been published (C. Menant, Entropy 2003). The Meaning Generator System (MGS) is a system submitted to a constraint that generates a meaningful information when it receives an incident information that has a relation with the constraint. The content of the meaningful information is explicited, and its function is to trigger an action that will be used to satisfy the constraint of the system. The MGS has been introduced in the case of basic life submitted to a stay alive constraint. We propose here to see how the usage of the MGS can be extended to more complex living systems, to humans and to robots by introducing new types of constraints, and integrating the MGS into higher level systems. The application of the MGS to humans is partly based on a scenario relative to the evolution of body self-awareness toward self-consciousness that has already been presented (C. Menant, Biosemiotics 2003, and TSC 2004). The application of the MGS to robots is based on the definition of the MGS applied to robots functionality, taking into account the origins of the constraints. We conclude with a summary of this overview and with themes that can be linked to this systemic approach on meaning generation Additional links for this entry: http://www.mdpi.org/fis2005/F.45.paper.pdf http://www.mdpi.net/fis2005/F.45.paper.pdf http://cogprints.org/4531/1/F.45.paper.pdf Minsky, Marvin L. (1994). Will robots inherit the earth? Scientific American (Oct). ( Cited by 37 | Google | More links ) Abstract: Everyone wants wisdom and wealth. Nevertheless, our health often gives out before we achieve them. To lengthen our lives, and improve our minds, in the future we will need to change our our bodies and brains. To that end, we first must consider how normal Darwinian evolution brought us to where we are. Then we must imagine ways in which future replacements for worn body parts might solve most problems of failing health. We must then invent strategies to augment our brains and gain greater wisdom. Eventually we will entirely replace our brains -- using nanotechnology. Once delivered from the limitations of biology, we will be able to decide the length of our lives--with the option of immortality-- and choose among other, unimagined capabilities as well Additional links for this entry: http://www.media.mit.edu/~minsky/papers/sciam.inherit.txt http://www.media.mit.edu/~minsky/papers/sciam.inherit.html http://web.media.mit.edu/~minsky/papers/sciam.inherit.html http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation Moravec, Hans (online). Bodies, robots, minds. ( Google ) Abstract: Serious attempts to build thinking machines began after the second world war. One line of research, called Cybernetics, used electronic circuitry imitating nervous systems to make machines that learned to recognize simple patterns, and turtle-like robots that found their way to recharging plugs. A different approach, named Artificial Intelligence, harnessed the arithmetic power of post-war computers to abstract reasoning, and by the 1960s made computers prove theorems in logic and geometry, solve calculus problems and play good games of checkers. At the end of the 1960s, research groups at MIT and Stanford attached television cameras and robot arms to their computers, so "thinking" programs could begin to collect information directly from the real world Moravec, Hans (online). Robotics. Encyclopaedia Britannica Online . ( Google ) Abstract: the development of machines with motor, perceptual and cognitive skills once found only in animals and humans. The field parallels and has adopted developments from several areas, among them mechanization, automation and artificial intelligence, but adds its own gripping myth, of complete artificial mechanical human beings. Ancient images and figurines depicting animals and humans can be interpreted as steps towards this vision, as can mechanical automata from classical times on. The pace accelerated rapidly in the twentieth century with the development of electronic sensing and amplification that permitted automata to sense and react as well as merely perform. By the late twentieth century automata controlled by computers could also think and remember Moravec, Hans (online). Robots inherit human minds. ( Google ) Abstract: Our first tools, sticks and stones, were very different from ourselves. But many tools now resemble us, in function or form, and they are beginning to have minds. A loose parallel with our own evolution suggests how they may develop in future. Computerless industrial machinery exhibits the behavioral flexibility of single-celled organisms. Today's best computer-controlled robots are like the simpler invertebrates. A thousand-fold increase in computer power in this decade should make possible machines with reptile-like sensory and motor competence. Growing computer power over the next half century will allow robots that learn like mammals, model their world like primates and eventually reason like humans. Depending on your point of view, humanity will then have produced a worthy successor, or transcended inherited limitations and transformed itself into something quite new. No longer limited by the slow pace of human learning and even slower biological evolution, intelligent machinery will conduct its affairs on an ever faster, ever smaller scale, until coarse physical nature has been converted to fine-grained purposeful thought Moravec, Hans (1994). The age of robots. In Max More (ed.), Extro 1, Proceedings of the First Extropy Institute Conference on TransHumanist Thought . Extropy Institute. (Cited by 2 | Google | More links ) Abstract: _Our artifacts are getting smarter, and a loose parallel with the evolution of animal intelligence suggests one future course_ _for them. Computerless industrial machinery exhibits the behavioral flexibility of single-celled organisms. Today's best_ _computer-controlled robots are like the simpler invertebrates. A thousand-fold increase in computer power in this decade_ _should make possible machines with reptile-like sensory and motor competence. Properly configured, such robots could_ _do in the physical world what personal computers now do in the world of data--act on our behalf as literal-minded_ _slaves. Growing computer power over the next half-century will allow this reptile stage will be surpassed, in stages_ _producing robots that learn like mammals, model their world like primates and eventually reason like humans._ _Depending on your point of view, humanity will then have produced a worthy successor, or transcended inherited_ _limitations and transformed itself into something quite new. No longer limited by the slow pace of human learning and_ _even slower biological evolution, intelligent machinery will conduct its affairs on an ever faster, ever smaller scale, until_ _coarse physical nature has been converted to fine-grained purposeful thought._ Additional links for this entry: http://www.ri.cmu.edu/pubs/pub_1810_text.html http://www.frc.ri.cmu.edu/~hpm/project.archive/general.articles/1993/Robot93.html Parisi, Domenico (2007). Mental robotics. In Antonio Chella Riccardo Manzotti (eds.), Artificial Consciousness . Imprint Academic. ( Google ) Petersen, Stephen (2007). The ethics of robot servitude. Journal of Experimental and Theoretical Artificial Intelligence 19 (1):43-54. ( Google | More links ) Additional links for this entry: http://stevepetersen.net/professional/petersen-robot-servitude-slides.pdf http://stevepetersen.net/styleless/professional/petersen-robot-servitude.pdf http://journalsonline.tandf.co.uk/smpp/content~content=a770959968~db=all~jumptype=rss http://taylorandfrancis.metapress.com/index/V14W2MT8346M88N1.pdf http://www.informaworld.com/index/V14W2MT8346M88N1.pdf Schmidt, C. T. A. Kraemer, F. (2006). Robots, Dennett and the autonomous: A terminological investigation. Minds and Machines 16 (1):73-80. ( Cited by 5 | Google | More links ) Abstract: In the present enterprise we take a look at the meaning of Autonomy, how the word has been employed and some of the consequences of its use in the sciences of the artificial. Could and should robots really be autonomous entities? Over and beyond this, we use concepts from the philosophy of mind to spur on enquiry into the very essence of human autonomy. We believe our initiative, as does Dennett's life-long research, sheds light upon the problems of robot design with respect to their relation with humans Additional links for this entry: http://www.springerlink.com/content/j20034x2925vk488/fulltext.pdf http://www.springerlink.com/index/J20034X2925VK488.pdf Torrance, Steve (1994). The mentality of robots, II. Proceedings of the Aristotelian Society 68 (68):229-262. ( Google ) Young, R. A. (1994). The mentality of robots, I. Proceedings of the Aristotelian Society 68 (68):199-227. ( Google ) Ziemke, Tom (2007). What's life got to do with it? In Antonio Chella Riccardo Manzotti (eds.), Artificial Consciousness . Imprint Academic. ( Google ) 6.5 Computationalism 18 / 160 entries displayed Balogh, 1Imre ; Beakley, Brian ; Churchland, Paul ; Gorman, Michael ; Harnad, Stevan ; Mertz, David ; Pattee, H. H. ; Ramsey, William ; Ringen, John ; Schwarz, Georg ; Slator, Brian ; Strudler, Alan Wallis, Charles (1990). Responses to 'computationalism'. Social Epistemology 4 (2):155 – 199. ( Google ) Bechtel, William (1998). Dynamicists versus computationalists: Whither mechanists? Behavioral and Brain Sciences 21 (5):629-629. ( Google ) Abstract: Van Gelder's characterization of the differences between the dynamical and computational hypotheses, in terms of the contrast between change versus state and geometry versus structure, suggests that the dynamical approach is also at odds with classical mechanism. Dynamical and mechanistic approaches are in fact allies: mechanism can identify components whose properties define the variables that are related in dynamical analyses Cordeschi, Roberto ; Frixione, M. ; Cordeschi, Roberto Frixione, M. (online). Computationalism under attack. ( Google ) Abstract: in M. Marraffa, M. De Caro and F. Ferretti (eds.), Cartographies of the Mind: Philosophy and Psychology in Intersection, Springer, Berlin-Heidelberg, 2007, pp. 37-49. PDF Dyer, Michael G. (1990). Intentionality and computationalism: Minds, machines, Searle and Harnad. Journal of Experimental and Theoretical Artificial Intelligence 2:303-19. ( Cited by 23 | Annotation | Google | More links ) Reply to Searle/Harnad: systems reply, level confusions, etc. Additional links for this entry: http://www.informaworld.com/index/777957609.pdf Glennan, Stuart S. (1995). Computationalism and the problem of other minds. Philosophical Psychology 8 (4):375-88. ( Google ) Abstract: In this paper I discuss Searle's claim that the computational properties of a system could never cause a system to be conscious. In the first section of the paper I argue that Searle is correct that, even if a system both behaves in a way that is characteristic of conscious agents (like ourselves) and has a computational structure similar to those agents, one cannot be certain that that system is conscious. On the other hand, I suggest that Searle's intuition that it is “empirically absurd” that such a system could be conscious is unfounded. In the second section I show that Searle's attempt to show that a system's computational states could not possibly cause it to be conscious is based upon an erroneous distinction between computational and physical properties. On the basis of these two arguments, I conclude that, supposing that the behavior of conscious agents can be explained in terms of their computational properties, we have good reason to suppose that a system having computational properties similar to such agents is also conscious Hauser, Larry (2000). Ordinary devices: Reply to Bringsjord's Clarifying the Logic of Anti-Computationalism: Reply to Hauser . Minds and Machines 10 (1):115-117. ( Google | More links ) Abstract: What Robots Can and Can't Be (hereinafter Robots) is, as Selmer Bringsjord says "intended to be a collection of formal-arguments-that-border-on-proofs for the proposition that in all worlds, at all times, machines can't be minds" (Bringsjord, forthcoming). In his (1994) "Précis of What Robots Can and Can't Be" Bringsjord styles certain of these arguments as proceeding "repeatedly . . . through instantiations of" the "simple schema" Additional links for this entry: http://www.wutsamada.com/work/ordinary.htm http://portal.acm.org/citation.cfm?id=596715.596855 http://www.springerlink.com/content/content/h874662818vp4p8x/fulltext.pdf http://www.springerlink.com/content/h874662818vp4p8x/fulltext.pdf http://www.springerlink.com/index/H874662818VP4P8X.pdf Kazez, J. R. (1994). Computationalism and the causal role of content. Philosophical Studies 75 (3):231-60. ( Cited by 3 | Google | More links ) Additional links for this entry: http://www.springerlink.com/index/P4G1M447G1814507.pdf Laforte, Geoffrey ; Hayes, Pat Ford, Kenneth M. (1998). Why Godel's theorem cannot refute computationalism: A reply to Penrose. Artificial Intelligence 104. ( Google ) Longinotti, David (2009). Computationalism and the locality principle. Minds and Machines 19 (4):495-506. ( Google ) Abstract: Computationalism, a specie of functionalism, posits that a mental state like pain is realized by a ‘core’ computational state within a particular causal network of such states. This entails that what is realized by the core state is contingent on events remote in space and time, which puts computationalism at odds with the locality principle of physics. If computationalism is amended to respect locality, then it posits that a type of phenomenal experience is determined by a single type of computational state. But a computational state, considered by itself, is of no determinate type—it has no particular symbolic content, since it could be embedded in any of an infinite number of algorithms. Hence, if locality is respected, then the type of experience that is realized by a computational state, or whether any experience at all is realized, is under-determined by the computational nature of the state. Accordingly, Block’s absent and inverted qualia arguments against functionalism find support in the locality principle of physics. If computationalism denies locality to avoid this problem, then it cannot be considered a physicalist theory since it would entail a commitment to phenomena, like teleological causation and action-at-a-distance, that have long been rejected by modern science. The remaining theoretical alternative is to accept the locality principle for macro events and deny that formal, computational operations are sufficient to realize a phenomenal mental state Luna, Laureano Small, Christopher (2009). Intentionality and Computationalism. A Diagonal Argument. Mind and Matter 7 (1):81-90. ( Google ) Abstract: Computationalism is the claim that all possible thoughts are computations, i.e. executions of algorithms. The aim of the paper is to show that if intentionality is semantically clear, in a way defined in the paper, then computationalism must be false. Using a convenient version of the phenomenological relation of intentionality and a diagonalization device inspired by Thomson's theorem of 1962, we show there exists a thought that canno be a computation. Lyngzeidetson, Albert E. (1990). Massively parallel distributed processing and a computationalist foundation for cognitive science. British Journal for the Philosophy of Science 41 (March):121-127. ( Annotation | Google | More links ) A Connection Machine might escape the Lucas argument. Bizarre. Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/abstract/41/1/121 http://bjps.oxfordjournals.org/cgi/reprint/41/1/121 http://www.jstor.org/stable/pdfplus/688006.pdf Piccinini, Gualtiero (2009). Computationalism in the philosophy of mind. Philosophy Compass 4 (3):515-532. ( Google | More links ) Abstract: Computationalism has been the mainstream view of cognition for decades. There are periodic reports of its demise, but they are greatly exaggerated. This essay surveys some recent literature on computationalism and reaches the following conclusions. Computationalism is a family of theories about the mechanisms of cognition. The main relevant evidence for testing computational theories comes from neuroscience, though psychology and AI are relevant too. Computationalism comes in many versions, which continue to guide competing research programs in philosophy of mind as well as psychology and neuroscience. Although our understanding of computationalism has deepened in recent years, much work in this area remains to be done Additional links for this entry: http://www3.interscience.wiley.com/cgi-bin/fulltext/122301402/PDFSTART Piccinini, Gualtiero (2007). Computationalism, the church–turing thesis, and the church–turing fallacy. Synthese 154 (1):97-120. ( Google | More links ) Abstract: The Church–Turing Thesis (CTT) is often employed in arguments for computationalism. I scrutinize the most prominent of such arguments in light of recent work on CTT and argue that they are unsound. Although CTT does nothing to support computationalism, it is not irrelevant to it. By eliminating misunderstandings about the relationship between CTT and computationalism, we deepen our appreciation of computationalism as an empirical hypothesis. Additional links for this entry: http://www.springerlink.com/content/u2914611854801u1/fulltext.pdf Rescorla, Michael (2007). Church's Thesis and the Conceptual Analysis of Computability. Notre Dame Journal of Formal Logic 48 (2):253-280. ( Google | More links ) Abstract: Church's thesis asserts that a number-theoretic function is intuitively computable if and only if it is recursive. A related thesis asserts that Turing's work yields a conceptual analysis of the intuitive notion of numerical computability. I endorse Church's thesis, but I argue against the related thesis. I argue that purported conceptual analyses based upon Turing's work involve a subtle but persistent circularity. Turing machines manipulate syntactic entities. To specify which number-theoretic function a Turing machine computes, we must correlate these syntactic entities with numbers. I argue that, in providing this correlation, we must demand that the correlation itself be computable. Otherwise, the Turing machine will compute uncomputable functions. But if we presuppose the intuitive notion of a computable relation between syntactic entities and numbers, then our analysis of computability is circular. Additional links for this entry: http://www.projecteuclid.org/DPubS?verb=Display=record Rey, Georges (1994). Wittgenstein, computationalism, and qualia. In Roberto Casati, B. Smith Stephen L. White (eds.), Philosophy and the Cognitive Sciences . Holder-Pichler-Tempsky. ( Cited by 3 | Annotation | Google ) Computational functionalism about qualia is compatible with Wittgenstein's views. It makes sense of the points about "dividing through" my private objects, for example. With remarks on spectrum inversions. Robinson, William S. (1995). Brain symbols and computationalist explanation. Minds and Machines 5 (1):25-44. ( Cited by 4 | Google | More links ) Abstract:   Computationalist theories of mind require brain symbols, that is, neural events that represent kinds or instances of kinds. Standard models of computation require multiple inscriptions of symbols with the same representational content. The satisfaction of two conditions makes it easy to see how this requirement is met in computers, but we have no reason to think that these conditions are satisfied in the brain. Thus, if we wish to give computationalist explanations of human cognition, without committing ourselvesa priori to a strong and unsupported claim in neuroscience, we must first either explain how we can provide multiple brain symbols with the same content, or explain how we can abandon standard models of computation. It is argued that both of these alternatives require us to explain the execution of complex tasks that have a cognition-like structure. Circularity or regress are thus threatened, unless noncomputationalist principles can provide the required explanations. But in the latter case, we do not know that noncomputationalist principles might not bear most of the weight of explaining cognition. Four possible types of computationalist theory are discussed; none appears to provide a promising solution to the problem. Thus, despite known difficulties in noncomputationalist investigations, we have every reason to pursue the search for noncomputationalist principles in cognitive theory Additional links for this entry: http://www.springerlink.com/index/L075451262856X7G.pdf Waskan, Jonathan (forthcoming). A vehicular theory of corporeal qualia (a gift to computationalists). Philosophical Studies . ( Google ) Abstract: I have argued elsewhere that non-sentential representations that are the close kin of scale models can be, and often are, realized by computational processes. I will attempt here to weaken any resistance to this claim that happens to issue from those who favor an across-the-board computational theory of cognitive activity. I will argue that embracing the idea that certain computers harbor nonsentential models gives proponents of the computational theory of cognition the means to resolve the conspicuous disconnect between the sentential character of the data structures they posit and the nonsentential qualitative character of our perceptual experiences of corporeal (i.e., spatial, kinematic, and dynamic) properties. Along the way, I will question the viability of some externalist remedies for this disconnect, and I will explain why the computational theory put forward here falls quite clearly beyond the useful bounds of the Chinese-Room argument Wilson, Robert A. (1994). Wide computationalism. Mind 103 (411):351-72. ( Cited by 39 | Google | More links ) Additional links for this entry: http://mind.oxfordjournals.org/cgi/reprint/103/411/351 http://www.jstor.org/stable/pdfplus/2253744.pdf 6.6 Philosophy of AI, Miscellaneous Akman, Varol (2000). Introduction to the special issue on philosophical foundations of artificial intelligence. Journal of Experimental and Theoretical Artificial Intelligence 12 (3):247-250. (Cited by 2 | Google | More links ) Abstract: This is the guest editor's introduction to a JETAI special issue on philosophical foundations of AI Additional links for this entry: http://citeseer.ist.psu.edu/346061.html http://cogprints.ecs.soton.ac.uk/archive/00000961/ http://citeseer.ist.psu.edu/akman00introduction.html http://www.cs.bilkent.edu.tr/~akman/jour-papers/jetai/jetai2000.pdf http://citebase.eprints.org/cgi-bin/citations?archiveID=oai:cogprints.soton.ac.uk:961 http://www.informaworld.com/index/K7H3RFR1N065UAU2.pdf http://www.ingentaconnect.com/content/tandf/teta/2000/00000012/00000003/art00001 http://cogprints.org/961/2/intro.ps http://cogprints.org/961/0/intro.ps Alai, Mario (2004). A.I., Scientific discovery and realism. Minds and Machines 14 (1). ( Cited by 2 | Google | More links ) Abstract: Epistemologists have debated at length whether scientific discovery is a rational and logical process. If it is, according to the Artificial Intelligence hypothesis, it should be possible to write computer programs able to discover laws or theories; and if such programs were written, this would definitely prove the existence of a logic of discovery. Attempts in this direction, however, have been unsuccessful: the programs written by Simon's group, indeed, infer famous laws of physics and chemistry; but having found no new law, they cannot properly be considered discovery machines. The programs written in the Turing tradition, instead, produced new and useful empirical generalization, but no theoretical discovery, thus failing to prove the logical character of the most significant kind of discoveries. A new cognitivist and connectionist approach by Holland, Holyoak, Nisbett and Thagard, looks more promising. Reflection on their proposals helps to understand the complex character of discovery processes, the abandonment of belief in the logic of discovery by logical positivists, and the necessity of a realist interpretation of scientific research Additional links for this entry: http://www.cse.buffalo.edu/~rapaport/510/mm-alai.pdf http://www.springerlink.com/content/u078803u28817475/fulltext.pdf http://www.springerlink.com/index/U078803U28817475.pdf http://www.ingentaconnect.com/content/klu/mind/2004/00000014/00000001/05142090 Apter, Michael J. (1970). The Computer Simulation Of Behaviour. Hutchinson. ( Cited by 18 | Google ) Barbour, Ian G. (1999). Neuroscience, artificial intelligence, and human nature: Theological and philosophical reflections. In Neuroscience and the Person: Scientific Perspectives on Divine Action . Notre Dame: University Notre Dame Press. ( Cited by 7 | Google | More links ) Additional links for this entry: http://www.blackwell-synergy.com/doi/abs/10.1111/0591-2385.00222 http://www.ingentaconnect.com/content/bpl/zygo/1999/00000034/00000003/art00222 Baum, Eric B. (2004). What Is Thought? Cambridge MA: Bradford Book/MIT Press. ( Cited by 33 | Google | More links ) Additional links for this entry: http://www.reiters.com/index.cgi?ISBN=0262025485=p http://mentalhelp.net/books/books.php?type=de=2226 http://www.markus-enzenberger.de/compgo_biblio/books/what-is-thought.html http://books.google.com/books?hl=en=0oGe6jMGAotxDkTeK0Hq7OsKo5o http://books.google.com/books?hl=en=yoAV8PwnJuCYCm_o1yuHXWsro7Q http://books.google.com/books?hl=en=M13fepJ_m6oWkmKf9nvKwdUMdBs http://books.google.com/books?hl=en=LVr9z-CZ4xm_ltubPIHNCUdfzhk Beavers, Anthony F. (2002). Phenomenology and artificial intelligence. Metaphilosophy 33 (1-2):70-82. ( Cited by 6 | Google | More links ) Abstract: In CyberPhilosophy: The Intersection of Philosophy and Computing, edited by James H. Moor and Terrell Ward Bynum (Oxford, UK: Blackwell, 2002), 66-77. Also in Metaphilosophy 33.1/2 (2002): 70-82 Additional links for this entry: http://www.blackwell-synergy.com/links/doi/10.1111/1467-9973.00217/abs/ http://www.blackwell-synergy.com/doi/abs/10.1111/1467-9973.00217 http://www.ingentaconnect.com/content/bpl/meta/2002/00000033/F0020001/art00217 Bergadano, F. (1993). Machine learning and the foundations of inductive inference. Minds and Machines 3 (1):31-51. ( Google | More links ) Abstract:   The problem of valid induction could be stated as follows: are we justified in accepting a given hypothesis on the basis of observations that frequently confirm it? The present paper argues that this question is relevant for the understanding of Machine Learning, but insufficient. Recent research in inductive reasoning has prompted another, more fundamental question: there is not just one given rule to be tested, there are a large number of possible rules, and many of these are somehow confirmed by the data — how are we to restrict the space of inductive hypotheses and choose effectively some rules that will probably perform well on future examples? We analyze if and how this problem is approached in standard accounts of induction and show the difficulties that are present. Finally, we suggest that the explanation-based learning approach and related methods of knowledge intensive induction could be, if not a solution, at least a tool for solving some of these problems Additional links for this entry: http://www.springerlink.com/content/w67627184tv71rk6/fulltext.pdf http://www.springerlink.com/index/W67627184TV71RK6.pdf Boden, Margaret A. (1978). Artificial intelligence and Piagetian theory. Synthese 38 (July):389-414. ( Cited by 6 | Google | More links ) Additional links for this entry: http://www.springerlink.com/index/T68361277T087206.pdf Boden, Margaret A. (1989). Artificial Intelligence In Psychology: Interdisciplinary Essays. Cambridge: Mit Press. ( Cited by 15 | Google ) Boden, Margaret A. (1973). How artificial is artificial intelligence? British Journal for the Philosophy of Science 24 (1). ( Google ) Born, Rainer P. (ed.) (1987). Artificial Intelligence: The Case Against. St Martin's Press. ( Cited by 13 | Google ) Bostrom, Nick (1998). How long before superintelligence? International Journal of Futures Studies 2. ( Cited by 22 | Google ) Abstract: _This paper outlines the case for believing that we will have superhuman artificial intelligence_ _within the first third of the next century. It looks at different estimates of the processing power of_ _the human brain; how long it will take until computer hardware achieve a similar performance;_ _ways of creating the software through bottom-up approaches like the one used by biological_ _brains; how difficult it will be for neuroscience figure out enough about how brains work to_ _make this approach work; and how fast we can expect superintelligence to be developed once_ _there is human-level artificial intelligence._ Bostrom, Nick (online). The transhumanist FAQ. ( Cited by 14 | Google | More links ) Additional links for this entry: http://www.transhumanism.org/resources/faq.html Brooks, Rodney (2001). The relationship between matter and life. Nature 409 (6818):409-411. (Cited by 65 | Google | More links ) Abstract: Researchers in artificial intelligence (AI) Moore’s law states that computational complexity of the models is still far below that and artificial life (Alife) are interested resources for a fixed price roughly double of any living system. New experiments in evo- in understanding the properties of liv- every 18 months. From about 1975 into the lution simulate spatially isolated populations ing organisms so that they can build artificial early 1990s all the gains of Moore’s law went to investigate speciation. Over the past few systems that exhibit these properties for into the changeover from the centralized years, new directions have emerged in AI 5 , in useful purposes. AI researchers are interest- mainframe to the individual computer on attempts to implement artificial creatures in ed mostly in perception, cognition and your desk, accommodating a vastly simulated or physical environments. generation of action (Box 1), whereas Alife increased number of users. The amount of Often called the behaviour-based focuses on evolution, reproduction, computing power available to the individual approach, this new mode of thought involves morphogenesis and metabolism (Box 2). scientist did not change that much, although the connection of perception to action with Neither of these disciplines is a conventional the price came down by a factor of a little in the way of intervening representa- science; rather, they are a mixture of science thousand. But since the early 1990s, all of tional systems. Rather than relying on and engineering. Despite, or perhaps Moore’s law has gone into increasing the per- search, this approach relies on the correct because of, this hybrid structure, both disci- formance of the workstation itself. short, fast connections being present plines have been very successful and our And both AI and Alife have benefited from between sensory and motor modules. world is full of their products. this shift. Behaviour-based approaches began with Every time we use a computer we use Increased computer power has enabled insect models, but more recently they have algorithms and techniques developed by AI search-based AI to push ahead with been extended to humanoid robots 6 — researchers. Additional links for this entry: http://www.nature.com/nature/journal/v409/n6818/full/409409a0.html http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve=Citation Button, Graham ; Coulter, Jeff ; Lee, John R. E. Sharrock, Wes (1995). Computers, Minds, and Conduct. Polity Press. ( Cited by 54 | Google ) Clark, Andy (2002). Artificial intelligence. In Stephen P. Stich Ted A. Warfield (eds.), Blackwell Guide to Philosophy of Mind . Blackwell. ( Google ) Clark, Andy (2003). Artificial intelligence and the many faces of reason. In Stephen P. Stich Ted A. Warfield (eds.), The Blackwell Guide to Philosophy of Mind . Blackwell. ( Cited by 3 | Google | More links ) Abstract: wide variety of things. It covers the capacity to carry out deductive inferences, to make Additional links for this entry: http://www.philosophy.ed.ac.uk/staff/clark/pubs/AIandFacesofReason.pdf http://books.google.com/books?hl=en=a69b_8ZipUXmv8jfu6-lWPxYp4U Copeland, B. Jack (1995). Artificial Intelligence: A Philosophical Introduction. Cambridge: Blackwell. ( Cited by 77 | Google | More links ) Additional links for this entry: http://books.google.com/books?hl=en=4U5JFzAZ_uapDXPy03SDTNiW3j0 http://books.google.com/books?hl=en=RyCTIUvUlJbSuSqUcDtLDJN2IKc http://books.google.com/books?hl=en=gF3b31Zdq8wGkfR-3EzPZuAZcRE http://books.google.com/books?hl=en=QMC-cA7DK8sTMPuYT4j0Nsq8Hto Cordeschi, Roberto (2007). AI turns fifty: Revisiting its origins. Applied Artificial Intelligence 21:259-279. ( Cited by 1 | Google | More links ) Abstract: Applied Artificial Intelligence, 21, 2007, pp. 259-279 Additional links for this entry: http://www.informaworld.com/index/778482823.pdf Cordeschi, Roberto (2006). Searching in a Maze, in search of knowledge: Issues in early artificial intelligence. In O. Stock M. Schaerf (eds.), Lecture Notes In Computer Science . Springer-Verlag. ( Google | More links ) Abstract: Lecture Notes in Artificial Intelligence, vol. 4155, Springer, Berlin-Heidelberg, 2006, pp. 1-23. PDF Additional links for this entry: http://www.springerlink.com/index/k151241455035325.pdf Crosson, Frederick J. (ed.) (1967). Philosophy And Cybernetics. Notre Dame: University of Notre Dame Press. ( Cited by 9 | Google ) Culbertson, James T. (1963). The Minds Of Robots: Sense Data, Memory Images, And Behavior In Conscious Automata. Urbana: University Of Illinois Press. ( Cited by 11 | Google ) Cummins, Robert E. (ed.) (1991). Philosophy and AI. Cambridge: MIT Press. ( Cited by 6 | Google ) Dahlbom, B. (1995). Mind is artificial. In B. Dahlbom (ed.), Dennett and His Critics . Cambridge: Blackwell. ( Cited by 7 | Google ) Dreyfus, Hubert L. (1985). From socrates to expert systems: The limits and dangers of calculative rationality. In Carl Mitcham Alois Huning (eds.), Philosophy and Technology II: Information Technology and Computers in Theory and Practice . Reidel. ( Cited by 26 | Google | More links ) Abstract: Actual AI research began auspiciously around 1955 with Allen Newell and Herbert Simon's work at the RAND Corporation. Newell and Simon proved that computers could do more than calculate. They demonstrated that computers were physical symbol systems whose symbols could be made to stand for anything, including features of the real world, and whose programs could be used as rules for relating these features. In this way computers could be used to simulate certain important aspects intelligence. Thus the information-processing model of the mind was born. But, looking back over these fifty years, it seems that theoretical AI with its promise of a robot like HAL appears to be a perfect example of what Imre Lakatos has called a "degenerating research program" Additional links for this entry: http://ist-socrates.berkeley.edu/~hdreyfus/html/paper_socrates.html Drescher, Gary L. (1991). Made-Up Minds: A Constructivist Approach to Artificial Intelligence. Cambridge: MIT Press. ( Cited by 244 | Google | More links ) Additional links for this entry: http://portal.acm.org/citation.cfm?id=SERIES9797.113149 http://books.google.com/books?hl=en=_ohd54z5t1p8mkp-rhSDpR4rm94 http://books.google.com/books?hl=en=SHx5OAD7blCUzbDEiR6GolRpyWg http://books.google.com/books?hl=en=ZKziQKs5aXz52ImGfHTiJEf04Jk http://books.google.com/books?hl=en=8dzEHHgqm0WTkRm4QEOtVvsj2EI Dresher, B. Elan Hornstein, Norbert (1976). On some supposed contributions of artificial intelligence to the scientific study of language. Cognition 4 (December):321-398. ( Cited by 12 | Google ) Duch, Włodzisław (2007). What is computational intelligence and where is it going? In Wlodzislaw Duch Jacek Mandziuk (eds.), Challenges for Computational Intelligence . Springer. ( Google | More links ) Abstract: What is Computational Intelligence (CI) and what are its relations with Artificial Intelligence (AI)? A brief survey of the scope of CI journals and books with ``computational intelligence'' in their title shows that at present it is an umbrella for three core technologies (neural, fuzzy and evolutionary), their applications, and selected fashionable pattern recognition methods. At present CI has no comprehensive foundations and is more a bag of tricks than a solid branch of science. The change of focus from methods to challenging problems is advocated, with CI defined as a part of computer and engineering sciences devoted to solution of non-algoritmizable problems. In this view AI is a part of CI focused on problems related to higher cognitive functions, while the rest of the CI community works on problems related to perception and control, or lower cognitive functions. Grand challenges on both sides of this spectrum are addressed Additional links for this entry: http://www.springerlink.com/index/k275703555734407.pdf Epstein, Susan L. (1992). The role of memory and concepts in learning. Minds and Machines 2 (3). ( Cited by 10 | Google | More links ) Abstract: The extent to which concepts, memory, and planning are necessary to the simulation of intelligent behavior is a fundamental philosophical issue in Artificial Intelligence. An active and productive segement of the AI community has taken the position that multiple low-level agents, properly organized, can account for high-level behavior. Empirical research on these questions with fully operational systems has been restricted to mobile robots that do simple tasks. This paper recounts experiments with Hoyle, a system in a cerebral, rather than a physical, domain. The program learns to perform well and quickly, often outpacing its human creators at two-person, perfect information board games. Hoyle demonstrates that a surprising amount of intelligent behavior can be treated as if it were situation-determined, that often planning is unnecessary, and that the memory required to support this learning is minimal. Concepts, however, are crucial to this reactive program's ability to learn and perform Additional links for this entry: http://www.springerlink.com/content/q764210h47533476/fulltext.pdf http://www.springerlink.com/index/Q764210H47533476.pdf Fetzer, James H. (1990). Artificial Intelligence: Its Scope and Limits. Kluwer. ( Cited by 35 | Google | More links ) Additional links for this entry: http://portal.acm.org/citation.cfm?id=533526 Franchi, Stefano Guzeldere, Guven (1995). Constructions of the Mind: Artificial Intelligence and the Humanities. Stanford Humanities Review. ( Google ) Froese, Tom (2007). On the role of AI in the ongoing paradigm shift within the cognitive sciences. In M. Lungarella (ed.), 50 Years of AI . Springer-Verlag. ( Google | More links ) Abstract: This paper supports the view that the ongoing shift from orthodox to embodied-embedded cognitive science has been significantly influenced by the experimental results generated by AI research. Recently, there has also been a noticeable shift toward enactivism, a paradigm which radicalizes the embodied-embedded approach by placing autonomous agency and lived subjectivity at the heart of cognitive science. Some first steps toward a clarification of the relationship of AI to this further shift are outlined. It is concluded that the success of enactivism in establishing itself as a mainstream cognitive science research program will depend less on progress made in AI research and more on the development of a phenomenological pragmatics Additional links for this entry: http://froese.files.wordpress.com/2007/06/on-the-role-of-ai-in-the-ongoing-paradigm-shift.pdf Hall, John Storrs (forthcoming). Self-improving AI: An analysis. Minds and Machines . ( Google ) Abstract: Self-improvement was one of the aspects of AI proposed for study in the 1956 Dartmouth conference. Turing proposed a “child machine” which could be taught in the human manner to attain adult human-level intelligence. In latter days, the contention that an AI system could be built to learn and improve itself indefinitely has acquired the label of the bootstrap fallacy. Attempts in AI to implement such a system have met with consistent failure for half a century. Technological optimists, however, have maintained that a such system is possible, producing, if implemented, a feedback loop that would lead to a rapid exponential increase in intelligence. We examine the arguments for both positions and draw some conclusions Haugeland, John (1985). Artificial Intelligence: The Very Idea. Cambridge: Mit Press. ( Cited by 404 | Google | More links ) Abstract: The idea that human thinking and machine computing are radically the same provides the central theme for this marvelously lucid and witty book on... Additional links for this entry: http://portal.acm.org/citation.cfm?doid=4694 http://www.reiters.com/index.cgi?ISBN=0262580950=p http://www.cs.bilkent.edu.tr/~akman/book-revs/sigart/sigart1998.pdf Haugeland, John (ed.) (1981). Mind Design. MIT Press. ( Cited by 122 | Annotation | Google ) 12 papers on the foundations of AI and cognitive science. Haugeland, John (ed.) (1997). Mind Design II: Philosophy, Psychology, Artificial Intelligence. Cambridge: MIT Press. ( Cited by 12 | Google | More links ) Abstract: Contributors: Rodney A. Brooks, Paul M. Churchland, Andy Clark, Daniel C. Dennett, Hubert L. Dreyfus, Jerry A. Fodor, Joseph Garon, John Haugeland, Marvin... Additional links for this entry: http://portal.acm.org/citation.cfm?id=548831 http://cogprints.ecs.soton.ac.uk/archive/00000539/00/md2.ps Hayes, Patrick J. ; Ford, Kenneth M. Adams-Webber, J. R. (1994). Human reasoning about artificial intelligence. Journal of Experimental and Theoretical Artificial Intelligence 4:247-63. ( Cited by 5 | Google | More links ) Additional links for this entry: http://www.informaworld.com/index/777592360.pdf Hookway, Christopher (ed.) (1984). Minds, Machines And Evolution. Cambridge: Cambridge University Press. ( Cited by 11 | Google ) Abstract: This is a volume of original essays written by philosophers and scientists and dealing with philosophical questions arising from work in evolutionary biology and artificial intelligence. In recent years both of these areas have been the focus for attempts to provide a scientific, model of a wide range of human capacities - most prominently perhaps in sociobiology and cognitive psychology. The book therefore examines a number of issues related to the search for a 'naturalistic' or scientific account of human experience and behaviour. Some of the essays deal with the application of such models to particular behaviour, stressing the problems raised by consciousness, and the information to be derived from the differing capacities of animals and people; others consider more general questions about the logic of the explanations provided by these kinds of approach. The volume continues the informal series stemming from meetings sponsored by the Thyssen Foundation Jaki, Stanley L. (1969). Brain, Mind And Computers. Herder & Herder. ( Cited by 13 | Google ) Keeley, Brian L. (1994). Against the global replacement: On the application of the philosophy of artificial intelligence to artificial life. In C.G. Langton (ed.), Artificial Life III: Proceedings of the Workshop on Artificial Life . Reading, Mass: Addison-Wesley. ( Cited by 11 | Google ) Krellenstein, Marc F. (1987). A reply to parallel computation and the mind-body problem. Cognitive Science 11:155-7. ( Cited by 3 | Annotation | Google ) Thagard 1986 is wrong: speed and the like make no fundamental difference. With Thagard's reply: it makes a difference in practice, if not in principle. McDermott, Drew (1997). How intelligent is deep blue? New York Times (May) 14. ( Cited by 3 | Google ) Minsky, Marvin L. (1986). The Society Of Mind. Simon & Schuster. ( Cited by 2409 | Google | More links ) Additional links for this entry: http://www.reiters.com/index.cgi?ISBN=0671657135=p Moor, James H. (1998). Assessing artificial intelligence and its critics. In T.W. Bynum Moor. J. (eds.), The Digital Phoenix . Cambridge: Blackwell. ( Cited by 3 | Google ) Moody, Todd C. (1993). Philosophy and Artificial Intelligence. Prentice-Hall. ( Cited by 12 | Google ) Neumaier, Otto (1987). A Wittgensteinian view of artificial intelligence. In Artificial Intelligence . St Martin's Press. ( Cited by 1 | Google ) Pollock, John (online). Oscar: A cognitive architecture for intelligent agents. ( Google | More links ) Abstract: The “grand problem” of AI has always been to build artificial agents of human-level intelligence, capable of operating in environments of real-world complexity. OSCAR is a cognitive architecture for such agents, implemented in LISP. OSCAR is based on my extensive work in philosophy concerning both epistemology and rational decision making. This paper provides a detailed overview of OSCAR. The main conclusions are that such agents must be capablew of operating against a background of pervasive ignorance, because the real world is too complex for them to know more than a small fraction of what is true. This is handled by giving the agent the power to reason defeasibily. The OSCAR system of defeasible reasoning is sketched. It is argued that if epistemic cognition must be defeasible, planning must also be done defeasibly, and the best way to do that is to reason defeasibly about plans. A sketch is given about how this might work Additional links for this entry: http://oscarhome.soc-sci.arizona.edu/ftp/PAPERS/OSCAR architecture.pdf Pollock, John L. (1990). Philosophy and artificial intelligence. Philosophical Perspectives 4:461-498. ( Google | More links ) Additional links for this entry: http://www.jstor.org/stable/pdfplus/2214201.pdf Pollock, John L. (1999). Rational cognition in Oscar. Agent Theories . ( Cited by 7 | Google | More links ) Abstract: Stuart Russell [14] describes rational agents as --œthose that do the right thing--�. The problem of designing a rational agent then becomes the problem of figuring out what the right thing is. There are two approaches to the latter problem, depending upon the kind of agent we want to build. On the one hand, anthropomorphic agents are those that can help human beings rather directly in their intellectual endeavors. These endeavors consist of decision making and data processing. An agent that can help humans in these enterprises must make decisions and draw conclusions that are rational by human standards of rationality. Anthropomorphic agents can be contrasted with goal-oriented agents --” those that can carry out certain narrowly-defined tasks in the world. Here the objective is to get the job done, and it makes little difference how the agent achieves its design goal Additional links for this entry: http://citeseer.ist.psu.edu/246569.html http://portal.acm.org/citation.cfm?id=749598 http://oscarhome.soc-sci.arizona.edu/ftp/PAPERS/Rational Cognition in OSCAR.ps http://oscarhome.soc-sci.arizona.edu/ftp/OSCAR-web-page/PAPERS/Rational-Cognition-in-OSCAR.pdf http://books.google.com/books?hl=en=evRmQuZSv9eq2Wxij231iMjAfzA http://www.springerlink.com/index/a7r273h735877q41.pdf Pollock, John L. (2000). Rationality in philosophy and artificial intelligence. In The Proceedings of the Twentieth World Congress of Philosophy, Volume 9: Philosophy of Mind . Charlottesville: Philosophy Doc Ctr. ( Google ) Pollock, John L. (online). The Oscar project. ( Google ) Preston, Beth (1991). Anthropocentrism, and the evolution of 'intelligence'. Minds and Machines 1 (3):259-277. ( Cited by 3 | Google | More links ) Abstract:   Intuitive conceptions guide practice, but practice reciprocally reshapes intuition. The intuitive conception of intelligence in AI was originally highly anthropocentric. However, the internal dynamics of AI research have resulted in a divergence from anthropocentric concerns. In particular, the increasing emphasis on commonsense knowledge and peripheral intelligence (perception and movement) in effect constitutes an incipient reorientation of intuitions about the nature of intelligence in a non-anthropocentric direction. I argue that this conceptual shift undermines Joseph Weizenbaum's claim that the project of artificial intelligence is inherently dehumanizing Additional links for this entry: http://www.springerlink.com/index/N6J5022705T2R86X.pdf Puccetti, Roland (1974). Pattern recognition in computers and the human brain:: With special application to chess playing machines. British Journal for the Philosophy of Science 25 (2):137-154. ( Cited by 7 | Google | More links ) Abstract: 1 Matching Templates and Feature Analysers. 2 Modes of Perception in Left and Right Cerebral Hemispheres. 3 Identification and Recognition. 4 Chess Plying Machines Additional links for this entry: http://bjps.oxfordjournals.org/cgi/content/abstract/25/2/137 http://bjps.oxfordjournals.org/cgi/reprint/25/2/137 http://www.jstor.org/stable/pdfplus/686818.pdf Robinson, William S. (1992). Computers, Minds, and Robots. Temple University Press. ( Cited by 8 | Google ) Russell, S. (1991). Inductive learning by machines. Philosophical Studies 64 (October):37-64. ( Cited by 6 | Annotation | Google | More links ) A nice paper on the relationship between techniques of theory formation from machine learning and philosophical problems of induction and knowledge. Additional links for this entry: http://www.springerlink.com/index/Q5162T97787411G7.pdf Rychlak, Joseph F. (1991). Artificial Intelligence and Human Reason: A Teleological Critique. Columbia University Press. ( Cited by 13 | Google ) Schiaffonati, Viola (2003). A framework for the foundation of the philosophy of artificial intelligence. Minds and Machines 13 (4):537-552. ( Google | More links ) Abstract:   The peculiarity of the relationship between philosophy and Artificial Intelligence (AI) has been evidenced since the advent of AI. This paper aims to put the basis of an extended and well founded philosophy of AI: it delineates a multi-layered general framework to which different contributions in the field may be traced back. The core point is to underline how in the same scenario both the role of philosophy on AI and role of AI on philosophy must be considered. Moreover, this framework is revised and extended in the light of the consideration of a type of multiagent system devoted to afford the issue of scientific discovery both from a conceptual and from a practical point of view Additional links for this entry: http://www.cse.buffalo.edu/~rapaport/Papers/Papers.by.Others/mm.viola.pdf http://www.springerlink.com/content/tun11774g3434j44/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=5142084=1 http://www.springerlink.com/index/TUN11774G3434J44.pdf http://www.ingentaconnect.com/content/klu/mind/2003/00000013/00000004/05142084 Simon, Herbert A. (1995). Machine as mind. In Android Epistemology . Cambridge: MIT Press. ( Cited by 9 | Google | More links ) Additional links for this entry: http://portal.acm.org/citation.cfm?id=216350.216358 Sloman, Aaron (1978). The Computer Revolution in Philosophy: Philosophy Science and Models of Mind. Harvester. ( Cited by 87 | Annotation | Google | More links ) All about how the computer should change the way we think about the mind. Abstract: Since 1991 the author has been Professor of Artificial Intelligence and Cognitive Science in the School of Computer Science at the University of Birmingham, UK Additional links for this entry: http://www.cs.bham.ac.uk/research/projects/cogaff/crp/crp.pdf http://www.cs.bham.ac.uk/research/projects/cogaff/crp/titlepage.pdf http://www.ams.org/bull/1980-02-02/S0273-0979-1980-14750-3/home.html Sloman, Aaron (2002). The irrelevance of Turing machines to artificial intelligence. In Matthias Scheutz (ed.), Computationalism: New Directions . MIT Press. ( Cited by 18 | Google | More links ) Additional links for this entry: http://lib.org.by/_djvu/Cs_Computer science/CsAi_AI, knowledge/Scheutz M. (ed.) Computationalism (MIT, 2002)(223s).pdf#page=101 Sluckin, W. (1954). Minds And Machines. London,: Penguin,. ( Cited by 16 | Google ) Sparrow, Robert (2002). The March of the robot dogs. Ethics and Information Technology 4 (4):305-318. ( Cited by 3 | Google | More links ) Abstract: The Centre for Applied Philosophy and Public Ethics (CAPPE) was established in 2000 as a Special Research Centre in applied philosophy funded by the Australian Research Council. It has combined the complementary strengths of two existing centres specialising in applied philosophy, namely the Centre for Philosophy and Public Issues (CPPI) at the University of Melbourne and the Centre for Professional and Applied Ethics at Charles Sturt University. It operates as a unified centre with two divisions: in Melbourne at the University of Melbourne and in Canberra at Charles Sturt University. The Director of CAPPE and the head of the Canberra node is Professor Seumas Miller. Professor C.A.J. (Tony) Coady is the Deputy Director of CAPPE and the head of the Melbourne node Additional links for this entry: http://www.cs.pitt.edu/~bigrigg/cs1590/sparrow.pdf http://www.cs.cmu.edu/~social/reading/Sparrow1.pdf http://portal.acm.org/citation.cfm?id=607976.607992 http://www-2.cs.cmu.edu/~social/reading/Sparrow1.pdf http://eprints.infodiv.unimelb.edu.au/archive/00000138 http://eprints.unimelb.edu.au/archive/00000138/01/Sparrow1.pdf http://www.springerlink.com/content/lu2520h1307q6772/fulltext.pdf http://www.kluweronline.com/article.asp?PIPS=5111338=1 http://www.springerlink.com/index/LU2520H1307Q6772.pdf http://www.ingentaconnect.com/content/klu/etin/2002/00000004/00000004/05111338 Storrs Hall, J. (2006). Nano-enabled AI: Some philosophical issues. International Journal of Applied Philosophy 20 (2):247-261. ( Google ) Thagard, Paul R. (1991). Philosophical and computational models of explanation. Philosophical Studies 64 (October):87-104. ( Cited by 4 | Annotation | Google | More links ) A comparison of philosophical and AI approaches to explanation: deductive, statistical, schematic, analogical, causal, and linguistic. Additional links for this entry: http://www.springerlink.com/index/R8426372X18U8666.pdf Thagard, Paul R. (1990). Philosophy and machine learning. Canadian Journal of Philosophy 20 (2):261-76. ( Cited by 2 | Google ) Thagard, Paul R. (1986). Parallel computation and the mind-body problem. Cognitive Science 10:301-18. ( Cited by 28 | Annotation | Google | More links ) Parallelism does make a difference. Some somewhat anti-functionalist points. Additional links for this entry: http://www.leaonline.com/doi/pdfplus/10.1207/s15516709cog1003_3 http://www.cogsci.rpi.edu/CSJarchive/1986v10/i03/p0301p0318/MAIN.PDF Thórisson, Kristinn R. (2007). Integrated A.I. Systems. Minds and Machines 17 (1). ( Google | More links ) Abstract: The broad range of capabilities exhibited by humans and animals is achieved through a large set of heterogeneous, tightly integrated cognitive mechanisms. To move artificial systems closer to such general-purpose intelligence we cannot avoid replicating some subset—quite possibly a substantial portion—of this large set. Progress in this direction requires that systems integration be taken more seriously as a fundamental research problem. In this paper I make the argument that intelligence must be studied holistically. I present key issues that must be addressed in the area of integration and propose solutions for speeding up rate of progress towards more powerful, integrated A.I. systems, including (a) tools for building large, complex architectures, (b) a design methodology for building realtime A.I. systems and (c) methods for facilitating code sharing at the community level Additional links for this entry: http://www.springerlink.com/content/y04485h9j3u8x431/fulltext.pdf http://www.springerlink.com/index/Y04485H9J3U8X431.pdf http://www.ingentaconnect.com/content/klu/mind/2007/00000017/00000001/00009055 Torrance, Steven (ed.) (1984). The Mind And The Machine: Philosophical Aspects Of Artificial Intelligence. Chichester: Horwood. ( Cited by 12 | Google ) Hauser, Larry (online). Artificial intelligence. Internet Encyclopedia of Philosophy . ( Google ) van Gelder, Tim (1998). Into the deep blue yonder. Quadrant 42:33-39. ( Google ) Vinge, Vernor (online). The technological singularity. ( Cited by 43 | Google | More links ) Additional links for this entry: http://wholeearthmag.com/ArticleBin/111-3.pdf http://adsabs.harvard.edu/abs/1993vise.nasa...11V http://www.aleph.se/Trans/Global/Singularity/sing.html http://www.cs.ucsd.edu/users/goguen/misc/singularity.html http://www.kurzweilai.net/articles/art0092.html?printable=1 http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html http://www.csa.com/partners/viewrecord.php?requester=gs=N9427359AH von Neumann, John (1958). The Computer And The Brain. New Haven: Yale University Press. ( Cited by 404 | Google | More links ) Additional links for this entry: http://portal.acm.org/citation.cfm?id=SERIES11430.578873 http://books.google.com/books?hl=en=LVA9RlYQZeZ2It9LlalNsee3B78 http://books.google.com/books?hl=en=fZ-lKPYBhJvagjc0WfC7SkhZtMI http://books.google.com/books?hl=en=56GHMjh9AJa4TqJQ5IIruU8Ms1A http://books.google.com/books?hl=en=DHwS67V-SvzbAjm9NYDHA37tI74 http://books.google.com/books?hl=en=qUd4y-uRsBLh74twJozMyaVGh5o http://books.google.com/books?hl=en=ajWucxc5XPSwz6ajjtHei3f7i3I http://books.google.com/books?hl=en=uivfQKg6zUhWLgzUFPiEKaIX8yI http://books.google.com/books?hl=en=cYOMqvd1VWngQ_rqASDW0msA8-o http://books.google.com/books?hl=en=zrqTa_c5ThZ0ILmi6bcolSjcSqk http://books.google.com/books?hl=en=x6eHrPSvrSXSqDzB4FT2iiFJ-YE http://books.google.com/books?hl=en=xwPyDaNBqwNBeV8bJB_qBXeByXk http://books.google.com/books?hl=en=QHaMpf285hOXWmgcp-4jobJ-6uk Wagman, Morton (1991). Artificial Intelligence and Human Cognition. New York: Praeger. ( Cited by 7 | Google ) Warnick, Barbara (2004). Rehabilitating AI: Argument loci and the case for artificial intelligence. Argumentation 18 (2):149-170. ( Google | More links ) Additional links for this entry: http://www.springerlink.com/index/N808L6965581N152.pdf http://www.ingentaconnect.com/content/klu/argu/2004/00000018/00000002/05139657 Winograd, Terry Flores, Fernando (1987). Understanding Computers and Cognition. Addison-Wesley. ( Cited by 3155 | Google | More links ) Additional links for this entry: http://books.google.com/books?hl=en=29nUZ6v7NBR3_MW1f2iethnpq-Y http://books.google.com/books?hl=en=OVMXPUQmKwV5M0sXy_jUirMqKr8 http://books.google.com/books?hl=en=SPH_8y6W5R44eAZVF_KecsPTmFM http://books.google.com/books?hl=en=8bI0ORjPQdC0CyIgtYGGbyJ-KNM http://books.google.com/books?hl=en=CJEWYsI8caWVRQdXVOd5b7NNd4o http://books.google.com/books?hl=en=BIXmzPS0q90udE7PaQF4k2VjFt0 http://books.google.com/books?hl=en=BpZ1d0eOup2x3OzmNXXBmncf2gI http://books.google.com/books?hl=en=BGrPq-xVKWkg6CPTjGbXnOwLRzM http://books.google.com/books?hl=en=W-GRjaar6j6QRFjlhVFDGngIzPY http://books.google.com/books?hl=en=epUTB--1d3IJz9QsgxQtp2A9OYo http://books.google.com/books?hl=en=1iSm6bCFzZPx_UR0p8uKW74EKw4 http://books.google.com/books?hl=en=2GK_DpBxmYL5ydVp294DhQiIXoY http://books.google.com/books?hl=en=HgphXvbH7gfKxMVAXukzAAf05gQ http://books.google.com/books?hl=en=KZQ_QBXaLCBwwbmLJkZwbLH37-c http://books.google.com/books?hl=en=KApvVKf8mZ9A3wab70l9zlF6C5A http://books.google.com/books?hl=en=99bcvKGbsQX85xJLVbqUhjybzFA http://books.google.com/books?hl=en=qNJ48zOzeAWCzVK9IHHlHSx6qYE Yudkowsky, Eliezer (online). Creating friendly AI. ( Cited by 5 | Google ) Yudkowsky, Eliezer (online). Staring into the singularity. ( Google ) Abstract: 1: The End of History 2: The Beyondness of the Singularity 2.1: The Definition of Smartness 2.2: Perceptual Transcends 2.3: Great Big Numbers 2.4: Smarter Than We Are 3: Sooner Than You Think 4: Uploading 5: The Interim Meaning of Life 6: Getting to the Singularity 6.6a Philosophy of AI, General Works 6.6b Philosophy of AI, Misc Adam, Alison (2000). Deleting the subject: A feminist reading of epistemology in artificial intelligence. Minds and Machines 10 (2). ( Google ) Abstract:   This paper argues that AI follows classical versions of epistemology in assuming that the identity of the knowing subject is not important. In other words this serves to `delete the subject''. This disguises an implicit hierarchy of knowers involved in the representation of knowledge in AI which privileges the perspective of those who design and build the systems over alternative perspectives. The privileged position reflects Western, professional masculinity. Alternative perspectives, denied a voice, belong to less powerful groups including women. Feminist epistemology can be used to approach this from new directions, in particular, to show how women''s knowledge may be left out of consideration by AI''s focus on masculine subjects. The paper uncovers the tacitly assumed Western professional male subjects in two flagship AI systems, Cyc and Soar Kirsh, David (1995). The intelligent use of space. Artificial Intelligence 73:31-68. ( Google ) Abstract: The objective of this essay is to provide the beginning of a principled classification of some of the ways space is intelligently used. Studies of planning have typically focused on the temporal ordering of action, leaving as unaddressed questions of where to lay down instruments, ingredients, work-in-progress, and the like. But, in having a body, we are spatially located creatures: we must always be facing some direction, have only certain objects in view, be within reach of certain others. How we manage the spatial arrangement of items around us is not an afterthought: it is an integral part of the way we think, plan, and behave. The proposed classification has three main categories: spatial arrangements that simplify choice; spatial arrangements that simplify perception; and spatial dynamics that simplify internal computation. The data for such a classification is drawn from videos of cooking, assembly and packing, everyday observations in supermarkets, workshops and playrooms, and experimental studies of subjects playing Tetris, the computer game. This study, therefore, focuses on interactive processes in the medium and short term: on how agents set up their workplace for particular tasks, and how they continuously manage that workplace. Muntean, Ioan Wright, Cory D. (2007). Autonomy, allostasic mechanisms, and AI: a biomimetic perspective. Pragmatics and Cognition 15:489–513. ( Google ) Abstract: We argue that the concepts of mechanism and autonomy appear to be antagonistic when autonomy is conflated with agency. Once these concepts are disentangled, it becomes clearer how autonomy emerges from complex forms of control. Subsequently, current biomimetic strategies tend to focus on homeostatic regulatory systems; we propose that research in AI and robotics would do well to incorporate biomimetic strategies that instead invoke models of allostatic mechanisms as a way of understanding how to enhance autonomy in artificial systems. Penco, Carlo (online). Expressing the Background. Icelandic Philosophical Association (talks) . ( Google ) Silva, Porfirio U. Lima, Pedro (2007). Institutional Robotics. In F. Almeida e Costa et al (ed.), Advances in Artificial Life. ECAL 2007 . Springer-Verlag. ( Google ) terms conditions . Javascript Menu by Deluxe-Menu.com 
 Yale Computer Science has moved Please bookmark our new URL http://cpsc.yale.edu 
 UGA Mail Faculty/Staff Services UGA Libraries UGA News AI Driving Directions Graduate School Degree Programs Prospective Students MS Program Details MS Program Admissions MS Course Requirement AB in Cognitive Science People Faculty Current Students Fellows Industrial Partners Resources Theses and Oral Exam IAI Related Sites Important Links International Students Technical Support News Support IAI Dr. Don Potter Professor of Computer Science Research interests: Knowledge-based systems Genetic algorithms Expert database systems Dr. Michael Covington Adjunct Professor of Computer Science Dr. Prashant Doshi decision theory, game theory, and application to robotics Dr. Bill Hollingsworth Electronics The AI Microelectronics Lab is equipped for digital and analog circuit design, microcontroller programming, and Study at the IAI both faculty and other students. This yields massive learning opportunities in and out of the Robotics Class sensory inputs and desired goals. Motion deals with various aspects of movement. Projects are carried out Natural Language Processing Using computers to model and extend the human mind The University of Georgia has always seen cognitive science as an interdisciplinary field where computer science intersects with philosophy , psychology , linguistics , engineering and other fields.This comprises both classical artificial intelligence, which focuses on getting computers to behave intelligently, and newer approaches to cognitive computing , where the computer is seen as an extension rather than a model of the human mind. The Institute for Artificial Intelligence is an interdepartmental research and instructional unit within the Franklin College of Arts and Sciences of the University of Georgia . Strengths include logic programming, expert systems, neural nets, genetic algorithms, natural language processing and computational psycholinguistics.The Institute for Artificial Intelligence houses two degree programs, the Master of Science program in Artificial Intelligence and the bachelor's degree program in Cognitive Science. Affliliated with the Institute are over 75 people hailing from over 10 different countries. We pride ourselves on the diversity of our program student body and the ability of our program to allow for the pursuit of personal research interests. Apply to our Master's Degree Program Contact us Institute for Artificial Intelligence Boyd GSRC, Room 111 Athens GA 30602-7415 (706) 542-0358 The University of Georgia is an equal opportunity/affirmative action employer . 
 Computational Epistemology Laboratory The ( CEL ), headed by Professor of the sophy University of Waterloo See for his biography, publications, and courses. See below for information on , software , and bibliographies. Cognitive Science sites Cognitive Science at the University of Waterloo ) Cognitive Science faculty at the University of Waterloo Other interesting sites Introduction to Cognitive Science Software (LISP code in HTML format), including ACME, DECO, ECHO, IMP, and HOTCO. PI JavaECHO MacECHO MacECHO DECO DIVA THNET Bibliographies Cognitive Science bibliography. Cognitive Science glossary. Please email comments. 
