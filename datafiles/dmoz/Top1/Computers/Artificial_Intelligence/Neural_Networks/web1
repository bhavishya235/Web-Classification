====================<http://www-anw.cs.umass.edu/rlr/>====================
<html>
<head>
<title>Reinforcement Learning Repository at University of Massachusetts, Amherst</title>
</head>

<frameset cols="166,*" scrolling="auto" border=0>
     <frame src=menubar.html name="menubar">
     <frame src=rlmain.html  name="rl-main">	
</frameset>
</html>
++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://omega.albany.edu:8008/maxent.html>====================
<HEAD><!-- I am an -*- HTML-HELPER -*- file-->
<TITLE>Maximum Entropy Online Resources</TITLE>
</HEAD>
<body BGCOLOR="#FFFFFF" TEXT="#000000" LINK="#C50E0E" ALINK="#FF3300" VLINK="#000000">
<BASE HREF="http://omega.albany.edu:8008/">
<CENTER><A HREF="carlos"><IMG SRC=".icons/urhere.gif" ALT="Another service from Omega"></A></CENTER>
<center><H1>Maximum Entropy Online Resources</H1></center>
<img alt="*****" src=".icons/dot_green.gif"   align="left"  width="550"  height="10">
<br>

Workshops, tutorials, papers, software or just about anything that I can
find online related to the subject of Maximum Entropy.  <P>

If you know of a URL that should be here but it isn't, please
<A HREF="mailto:carlos@math.albany.edu">send me email at carlos@math.albany.edu</A>
<br><br>
<img alt="*****" src=".icons/dot_green.gif"   align="left"  width="550"  height="5">
<br>
<table  width="600">
<tr><td  width="50"><!-- The left margin --><br></td>
<td width="550"><!-- notice 550+50 = 600 -->
<DL>
  <dt><a href="http://www.ipp.mpg.de/maxent04/">http://www.ipp.mpg.de/maxent04/</a>
  <dd>Max-Planck-Institute Garhing/Munchen July, 25-28 2004. Email: <a href="mailto:maxent04@ipp.mpg.de">maxent04@ipp.mpg.de</a>

  <dt><a href="http://maxent23.org">MaxEnt2003: http://maxent23.org</a>
  <dd>Jackson Hole, Wyoming. August 3-8, 2003. Email: <a href="mailto:gerickson@boisestate.edu">gerickson@boisestate.edu</a>
      
  <dt><a href="http://www.uidaho.edu/LS/Stat/MAXENT/">MaxEnt2002</a>
  <dd> University of Idaho. Moscow. USA
      
  <dt><a href="http://www.jhuapl.edu/maxent2001/">MaxEnt2001</a>
  <dd> Johns Hopkins University. Baltimore. USA

  <dt><a href="http://www.supelec.fr/lss/MaxEnt2000">MaxEnt2000</a>
  <dd>Paris, France

  <dt><a href="http://www.maxent99.boisestate.edu">MaxEnt99</a>
  <dd>Boise State University, Idaho.
      <br>

  <dt><a href="http://www.ipp.mpg.de/OP/maxent98/me98.html">MaxEnt98</a>
  <dd> Max-Planck-Institut fur Plasmaphysik. Garching/Munchen/Germany.
      <a href="http://huelen.empireone.net/photos/MAXENT98/IMGALBUM/IMAGES/">
      Pictures</a>
      

  <dt><a href="maxent97.html">MaxEnt97</a>
  <dd> Joint meeting with <a href="isba">ISBA</a>

  <dt><a href="maxent96.html">MaxEnt96: Preliminary Announcement</a>
  <dd>South Africa meeting. There is a page in Zaire at
      <a href="http://gsd.is.co.za/maxent">http://gsd.is.co.za/maxent</a>
      

  <DT> <A HREF="http://planck.lanl.gov/~kmh/maxent95/">MaxEnt95</A>
  <DD> Official MaxEnt95 page at Los Alamos.

  <DT> <A HREF="http://astro.uchicago.edu/rranch/ASC/maxent95.html">V. Kashyap's report on MaxEnt95</A>
  <DD> Dr. Kashyap's personal report of his trip to MaxEnt95 but with relevant information about MaxEnt in general and some links.

  <DT> <A HREF="maxent94.html">MaxEnt94</A>
  <DD> XIII International Workshop on Maximum Entropy and Bayesian Methods. General information, abstracts, schedules, etc...
    
<DT> <A HREF="http://wol.ra.phy.cam.ac.uk/">MaxEnt/STA at Cambridge UK</A>
<DD> An idiosyncratic hybrid page of Maximum Entropy and Space Time Algebra
with general information about the Cambridge group.

<dt><a href="http://www.cs.princeton.edu/~ristad/papers/memt.html">
     MaxEnt Modeling Toolkit</a>
<dd> Software package from <a href="mailto:ristad@cs.princeton.edu">ristad@cs.princeton.edu</a>

<dt><a href="http://wwwhome.cs.utwente.nl/~terdoest/mem">Perl5 MaxEnt</a>
<dd> A Perl5 Module for Maximum Entropy Modeling by Hugo WL ter Doest. Also available from CPAN <a href="http://www.cpan.org">http://www.cpan.org</a>

<dt><a href="http://www.matzke.gmxhome.de">Manfred Matzke's site on MaxEnt. Mostly deconvolution of spectra</a>
<dd> Unfolding of Neutron and Photon Spectra.

<dt><a href="http://www.strauss.lanl.gov/outgoing/Gedanken/introtomaxent/index.html">Maxent Tutorial at Los Alamos by Gedanken.</a>
<dd>
<br>
<dt><a href="http://marco-pc.cims.nyu.edu/papers/avellaneda.abstract.html">
    Minimum-entropy calibration of asset-pricing models</a>
<dd>Entropy in <a href="http://marco-pc.cims.nyu.edu/">
    Mathematical Finance</a> by <a href="mailto:avellane@cims.nyu.edu">
    Marco Avellaneda</a>

<dt><a href="http://maxent.sourceforge.net/">SourceForge's MaxEnt page</a>
<dd> Java Toolkit for fitting MaxEnt models. Applications to Natural Language
    processing (NLP). See also <a href="http://omega.albany.edu:8008/ME-NLP.ps.gz">See also Adwait Ratnaparkhi's U. Penn 1998 Dissertation.</a>

</DL>

</td>
</tr>
</table>
<hr>
<address><a href="http://omega.albany.edu:8008/">Carlos Rodriguez &lt;carlos@math.albany.edu&gt;</a></address>
<!-- hhmts start -->
Last modified: Sat Jan  4 18:32:27 EST 2003
<!-- hhmts end -->
++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://www.neuroinf.org/>====================
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<title>The neuroinformatics site</title>
<meta name="description" content="The Neuroinformatics site." />
<meta name="keywords" content="neuroinformatics, neurobiology, informatics, workshops, courses, meetings, 
Belgium, INCF, maillists, servers" />
<link rel="stylesheet" type="text/css" href="/css/cssprint.css" media="print" />
<style type="text/css" media="all">@import "./css/cssmain.css";</style>
</head>
<body>
<p class="access"><a href="#primarycontent">Skip navigation</a></p>
<div id="containerhome">
    <div id="bannerwrap">
  	<div id="logo">
  	  <h1>The Neuroinformatics Site.org</h1>
  	</div>
  	<div id="banner">Logo</div>
  </div>

    <div id="topnav">
  	<ul>
  	  <li id="active"><a href="/index.shtml">Home</a></li>
  	  <li><a href="/logon.shtml">Login</a></li>
  	  <li><a href="/meeting/index.shtml">Meetings</a></li>
  	  <li><a href="/courses/index.shtml">Courses</a></li>
  	  <li><a href="/belgium/index.shtml">Belgium</a></li>
  	  <li><a href="/mailman/listinfo/comp-neuro">Maillist</a></li>
  	  <li><a href="/credits/index.shtml">Credits</a></li>
  	  <li><a href="/servers/index.shtml">Servers</a></li>
  	  <li><a href="/links/index.shtml">Links</a></li>
  	  <li><a href="/archive/index.shtml">Archive</a></li>
  	  <li><a href="/contact/index.shtml">Contact</a></li>
  	</ul>
  </div>


  <div id="homecontent">
  	<h1>Welcome to the neuroinf.org website.</h1>
  	
  	<p>We serve the international neuroinformatics community with information and access to courses 
  	and workshops.</p>
  	
  	<p>The <a href="http://incf.org">International Neuroinformatics Coordinating Facility</a> (INCF) 
  	is now operational.</p>
  	
  	<p>Belgium joined the INCF on 1/1/07.</p>
  	
  	<br /><br />
  	
  	<p>Number of hits counted since the start of this site on 01 Jun 2000:<br />
  	<img src="/cgi-bin/counter" alt="counter" width="150" height="34" /></p>
  	
    <br /><br />
    
    <p><b>Local time:</b><br />
    Wednesday, 26-Feb-2014 01:11:42 CET.</p>
  </div>
  	
  <div id="calendarnav">
  	<h1>Information for organizers of neuroscience meetings/courses in Belgium</h1>
  	
  	<p>The portal <a href="http://www.neuroinformatics.be" target="new">neuroinformatics.be</a> is a 
  	forum for the exchange of information for and by Belgian neuroscientists. Participants are 
  	invited to submit information on this portal.</p>
  	
  	<p>Interested? More info: 
  	<a href="http://www.neuroinformatics.be" target="new">www.neuroinformatics.be</a></p>
  </div>
  	
  	
  	<font size=1><font color=ffffff>
  	
  </font></font>
  
    <div id="homefooter">
  	<p>Page last updated on Tuesday, 11-Feb-2014 18:20:11 CET <br />
  	&copy; TNB 1998 - 2013 all rights reserved</p>
  	<p></p>
  	<p><a href="/contact/index.shtml">Contact</a>.</p>
  </div>

</div>

</body>
</html>
++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://liinwww.ira.uka.de/bibliography/Neural/index.html>====================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
<title>Bibliographies on Neural Networks</title>
<meta name="description" content="Bibliographies on Neural Networks, part of the Collection of Computer Science Bibliographies" />
<meta name="DC.DESCRIPTION" content="Bibliographies on Neural Networks, part of the Collection of Computer Science Bibliographies" />
<meta name="keywords" content="bibliographies, computer science, bibtex, RSS" />
<meta name="DC.SUBJECT" content="bibliographies, computer science, bibtex, RSS" />
<meta name="DC.CREATOR" content="Alf-Christian Achilles" />
<meta name="DC.CREATOR" content="Paul Ortyl" />
<meta name="resource-type" content="document" />
<meta name="distribution" content="global" />
<link rel="copyright" href="../Copyright.html" />
<link rel="top" href="../index.html" />
<link rel="stylesheet" href="../bibliography.css" type="text/css" />
</head>
<body>

<h1 class="page_title">The Collection of<br />Computer Science Bibliographies</h1><ul class="top_nav"><li><a href="../index.html">Collection Home</a></li>
<li><a href="../index.html">Up: The Collection of Computer Science Bibliographies</a></li>
</ul>
<h1 class="subcollectiontitle">Bibliographies on Neural Networks</h1><p>You can <a href="http://liinwww.ira.uka.de/bibliography/Contributing.html">add bibliographies and references</a> to this collection!
</p><hr />
<form method="post" action="/csbib/Neural/index" accept-charset="UTF-8">
<p class="mpsbib_form">
<label for="query">Query:</label>
<input id="query" name="query" type="text" accesskey="q" title="Enter your query.  &lt;q&gt;" style="width:70%"/> 
in <select id="field" name="field" title="Select field to be searched.">
<option value="">any</option>
<option value="au">author</option>
<option value="ti">title</option>
</select>&nbsp;<label for="field">field</label>; 
<br />
Publication <strong>year</strong>: 
in:<input name="year" type="text" size="4" accesskey="y" title="Enter publication year (4 digits). &lt;y&gt;" />, 
since:<input name="since" type="text" size="4"/>, 
before:<input name="before" type="text" size="4"/>
<em>(four digit years)</em><br />
Options: 
<select name="results" title="Records presentation style.">
	<option value="citation" selected="selected">Results as Citation</option>
	<option value="bibtex">Results in BibTeX</option>
</select>,  
<select name="maxnum" title="number of results per page">
	<option value="10">10 results per page</option>
	<option selected="selected" value="40">40 results per page</option>
	<option value="100">100 results per page</option>
	<option value="200">200 results per page</option>
</select>, 
<label for="sort">sort by</label>
<select id="sort" name="sort" title="sorting order">
	<option value="score" selected="selected">score</option>
	<option value="year">year</option>
</select>
<label for="online">online papers only</label>
<input type="checkbox" id="online" name="online" value="on"/>
<br />
<input type="submit" value="Search" style="width:50%; height:2em; margin:0.5ex;"/>
<br />
<span style="font-size:0.8em">
You may use <a href="http://lucene.apache.org/java/1_4_3/queryparsersyntax.html">Lucene syntax</a>, available fields are:
<tt>ti</tt> (title), <tt>au</tt> (author), <tt>yr</tt> (publications year).  
</span>
</p>
</form>

<script type="text/javascript">
<!-- hide script from older browsers
if (document.forms.length == 0) {
    document.writeln("<h2><a href=\"http://liinwww.ira.uka.de/bibliography/Neural/index.html\">Search these bibliographies<\/a><\/h2><p>at the <a href=\"http://liinwww.ira.uka.de/bibliography/index.html\">original site for the bibliography collection<\/a>!<\/p>");
}
//-->
</script>
<!-- subindices start -->
<table class="subcollectionindex">
<thead><tr><th>#Refs</th><th>Bibliography</th><th>Date</th></tr></thead>
<tbody>
<tr><td class="refcount">7548</td>
<td class="subcoll"><a href="SOM.LVQ.html">Bibliography on the Self-Organizing Map (SOM) and Learning Vector Quantization (LVQ)</a></td>
<td>(2005)</td>
</tr>
<tr><td class="refcount">2621</td>
<td class="subcoll"><a href="neural.6.html">Neural Network Bibliography</a></td>
<td>(1998)</td>
</tr>
<tr><td class="refcount">2558</td>
<td class="subcoll"><a href="adapt.sys.html">Bibliography of the Adaptive Systems Group of the GMD</a></td>
<td>(1994)</td>
</tr>
<tr><td class="refcount">1743</td>
<td class="subcoll"><a href="ieee-nn.html">Bibliography for the IEEE Transactions on Neural Networks</a></td>
<td>(2004)</td>
</tr>
<tr><td class="refcount">1526</td>
<td class="subcoll"><a href="soft-share.html">Bibliography of the paper &quot;Simplifying Neural networks by Soft Weight-Sharing&quot;</a></td>
<td>(1992)</td>
</tr>
<tr><td class="refcount">1344</td>
<td class="subcoll"><a href="NC.html">Bibliography of the journal &quot;Neural Computation&quot;</a></td>
<td>(2004)</td>
</tr>
<tr><td class="refcount">1179</td>
<td class="subcoll"><a href="nips.html">Bibliography for &quot;Advances in Neural Information Processing Systems&quot; (NIPS)</a></td>
<td>(1998)</td>
</tr>
<tr><td class="refcount">1080</td>
<td class="subcoll"><a href="nn.html">Bibliography for the journal &quot;Neural Networks&quot;</a></td>
<td>(2004)</td>
</tr>
<tr><td class="refcount">1041</td>
<td class="subcoll"><a href="PRNN.html">Bibliography of the book &quot;Pattern Recognition and Neural Networks&quot;</a></td>
<td>(1996)</td>
</tr>
<tr><td class="refcount">1005</td>
<td class="subcoll"><a href="neural.3.html">Bibliography on Neural Networks</a></td>
<td>(1994)</td>
</tr>
<tr><td class="refcount">642</td>
<td class="subcoll"><a href="EnsembleLearning.html">Ensemble Learning</a></td>
<td>(2007)</td>
</tr>
<tr><td class="refcount">617</td>
<td class="subcoll"><a href="Contrib.html">Bibliography on neural networks</a></td>
<td>(2012)</td>
</tr>
<tr><td class="refcount">533</td>
<td class="subcoll"><a href="neural.1.html">Bibliography on neural networks</a></td>
<td>(1994)</td>
</tr>
<tr><td class="refcount">383</td>
<td class="subcoll"><a href="Biophysics.Bochum.html">Bibliography of the Systems Biophysics Group at the University of Bochum</a></td>
<td>(2008)</td>
</tr>
<tr><td class="refcount">383</td>
<td class="subcoll"><a href="neural.genetic.html">Bibliography on genetic algorithms, neural networks and their combination</a></td>
<td>(1995)</td>
</tr>
<tr><td class="refcount">375</td>
<td class="subcoll"><a href="neurofuzzy.html">Bibliography on Neurofuzzy Systems</a></td>
<td>(1995)</td>
</tr>
<tr><td class="refcount">339</td>
<td class="subcoll"><a href="npl.html">Bibliography for the journal &quot;Neural Processing Letters&quot;</a></td>
<td>(2004)</td>
</tr>
<tr><td class="refcount">329</td>
<td class="subcoll"><a href="hildebrandt.html">Bibliography on Neural Networks</a></td>
<td>(1993)</td>
</tr>
<tr><td class="refcount">329</td>
<td class="subcoll"><a href="edna.html">Bibliography on Evolutionary Design of Neural Architectures</a></td>
<td>(1994)</td>
</tr>
<tr><td class="refcount">311</td>
<td class="subcoll"><a href="neural.5.html">Bibliography on Parallel Simulation of Neural Networks and Parallel Processing in general</a></td>
<td>(1993)</td>
</tr>
<tr><td class="refcount">308</td>
<td class="subcoll"><a href="construct.html">Bibliography on constructive algorithms for neural networks</a></td>
<td>(1998)</td>
</tr>
<tr><td class="refcount">308</td>
<td class="subcoll"><a href="rnn.html">Bibliography on Recurrent Neural Networks</a></td>
<td>(1998)</td>
</tr>
<tr><td class="refcount">250</td>
<td class="subcoll"><a href="fuzzy-nn.html">Bibliography on fuzzy logic and neural networks</a></td>
<td>(1996)</td>
</tr>
<tr><td class="refcount">203</td>
<td class="subcoll"><a href="reinforcement.learning.html">Bibliography on Reinforcement Learning</a></td>
<td>(1993)</td>
</tr>
<tr><td class="refcount">196</td>
<td class="subcoll"><a href="moe.html">Bibliography on &quot;Mixture of Experts&quot;</a></td>
<td>(1997)</td>
</tr>
<tr><td class="refcount">194</td>
<td class="subcoll"><a href="asg.uva.html">Publications of the Autonomous Systems Group at the University of Amsterdam</a></td>
<td>(2001)</td>
</tr>
<tr><td class="refcount">182</td>
<td class="subcoll"><a href="invariances.html">Bibliography on Invariances in Neural Systems</a></td>
<td>(2005)</td>
</tr>
<tr><td class="refcount">167</td>
<td class="subcoll"><a href="nlpnn.html">Bibliography on natural language processing and neural networks</a></td>
<td>(1995)</td>
</tr>
<tr><td class="refcount">167</td>
<td class="subcoll"><a href="mnpdpp.html">A Bibliography of Connectionist Models of Music</a></td>
<td>(1998)</td>
</tr>
<tr><td class="refcount">119</td>
<td class="subcoll"><a href="neural.recurrent.html">A Short Bibliography of Connectionist Systems for Temporal Behavior</a></td>
<td>(1990)</td>
</tr>
<tr><td class="refcount">115</td>
<td class="subcoll"><a href="cortical.maps.html">Bibliography on Cortical Map Formation</a></td>
<td>(2000)</td>
</tr>
<tr><td class="refcount">108</td>
<td class="subcoll"><a href="nn-mapping.html">Bibliography on the Mapping of Neural Networks</a></td>
<td>(1994)</td>
</tr>
<tr><td class="refcount">86</td>
<td class="subcoll"><a href="general-vision.html">Bibliography on Invariant Pattern Recognition with Neural Networks</a></td>
<td>(1993)</td>
</tr>
<tr><td class="refcount">79</td>
<td class="subcoll"><a href="fault.tolerant.neural.html">Bibliography of Fault Tolerance related Neural Network literature</a></td>
<td>(1993)</td>
</tr>
<tr><td class="refcount">61</td>
<td class="subcoll"><a href="art.html">Bibliography on Adaptive Resonance Theory (ART)</a></td>
<td>(2000)</td>
</tr>
<tr><td class="refcount">55</td>
<td class="subcoll"><a href="NEURALNET.html">Bibliography of &quot;Neural Networks&quot;</a></td>
<td>(1991)</td>
</tr>
<tr><td class="refcount">47</td>
<td class="subcoll"><a href="pca.html">Bibliography on Principal Component Analysis (PCA) Neural Networks</a></td>
<td>(1994)</td>
</tr>
<tr><td class="refcount">33</td>
<td class="subcoll"><a href="IDIAP-NN.html">Bibliography of the Neural Network Group at IDIAP</a></td>
<td>(1996)</td>
</tr>
<tr><td class="refcount">14</td>
<td class="subcoll"><a href="IEEETRNEURNET.html">Bibliography of &quot;IEEE Transactions on Neural Networks&quot;</a></td>
<td>(1991)</td>
</tr>
<tr><td class="refcount">2</td>
<td class="subcoll"><a href="jsnc.library.caltech.edu.html">11th Joint Symposium on Neural Computation</a></td>
<td>(2004)</td>
</tr>
</tbody>
<tbody>
<tr class="total"><td><strong>28580</strong></td><td>Total number of references in this section</td><td></td></tr>
</tbody>
</table>
<!-- subindices end -->
<ul><li><a href="others.html">Other Bibliographies on Neural Networks</a></li></ul>
<p class="footer">
<a href="http://liinwww.ira.uka.de/bibliography/Copyright.html">Copyright</a> &copy; 1995-2014 Alf-Christian Achilles, 2003-2014 Paul Ortyl, All rights reserved.
<br />
This service is brought to you by
Alf-Christian Achilles 
and <a href="http://ortyl.org">Paul Ortyl</a>
<br />
Please direct <a href="http://liinwww.ira.uka.de/bibliography/Comments.html" title="Send your comments via form.">comments</a> 
to <kbd><a href="mailto:liinwwwa@ira.uka.de" title="Send your comments via email.">liinwwwa@ira.uka.de</a></kbd>
</p>

</body>
</html>
++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://www.cs.iastate.edu/~gannadm/homepage.html>====================
<html>
<head><TITLE>Evolutionary Design of Neural Architectures </TITLE></head>
<body bgcolor=#OOOOOO text="#ccffff" LINK="#00ffff" VLINK="#ffff00" ALINK="#0077ff">
<hr>
<p>
<center>
<img src="images/gann2.gif"><br>
<img src="images/gann3.gif">
<h2><i> EVOLUTIONARY DESIGN OF NEURAL ARCHITECTURES</i> </H2>
</center>
<hr>
<h4>
Welcome to the WWW repository of resources on Evolutionary Design of 
Neural Architectures (EDNA). This repository is maintained by the 
<a href="http://www.cs.iastate.edu/~honavar/aigroup.html">Artificial 
Intelligence Research Group</A> led by 
<a href="http://www.cs.iastate.edu/~honavar/homepage.html">Vasant Honavar</A> 
in the <a href="http://www.cs.iastate.edu:80">Department of Computer 
Science</A> at <a href="http://www.cc.iastate.edu/isu-general-info.html"> 
Iowa State University</a>.  
<p>
<ul> 
<li>Evolutionary Design of Neural Architectures Resource-List<br><br>
<UL>
<LI><i>GANN Electronic Mailing List</i> <br> 
To subscribe, send email to 
<tt>majordomo@cs.iastate.edu</tt> 
with message body (not subject) <tt>subscribe gann-list</tt>. <br><br>
To unsubscribe, send email to 
<tt>majordomo@cs.iastate.edu</tt> 
with message body (not subject) <tt>unsubscribe gann-list [optional
address to be unsubscribed]</tt>. <br><br>
<LI><i>GANN List Archives</i><br>
All messages posted on the GANN Mailing List have been archived. The 
contents of the archive may be obtained by either clicking
<a href=gann-list>here</a> or sending mail to 
<tt>majordomo@cs.iastate.edu</tt> with the body <tt>index gann-list</tt>.
This returns a list of files currently archived. A specific file may
be obtained by sending a message with body <tt>get gann-list file-name</tt>
to <tt>majordomo@cs.iastate.edu</tt>.<br><br>
<LI><i>Bibliography and Guide to Literature on EDNA </i>
<ul>
<li> <a href="Bib/TR95-01.ps">Evolutionary Design of Neural Architectures - 
A Preliminary Taxonomy and Guide to Literature</A> Balakrishnan, K. and Honavar, V. (Postscript File).
<li> <a href="Bib/gann.bib">EDNA Bibliography</A> (.bib File)<br><br>
</ul>
</UL>
<li> 
<a href="http://www.cs.iastate.edu/~honavar/aigroup.html#moreailinks">Other Links of Interest
</a>
</h4>
</ul>
<img src=http://www.cs.iastate.edu/~balakris/gifs/sail_bar.gif>
<h4>
<img src="images/construction.gif">This page is under construction. Please 
send suggestions for additions or changes to: 
<a href="mailto:honavar@cs.iastate.edu">honavar@cs.iastate.edu</a> or <a href="mailto:balakris@cs.iastate.edu">balakris@cs.iastate.edu</a>.
<hr>
<i>
Thanks for visiting the GANN homepage. This page has been accessed
<!--#exec cgi="/cgi-bin/counter"-->
times since May 30th, 1996.
</i>
</h4>

++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://www.cs.toronto.edu/~carl/gp.html>====================
<html>
<head>
<title>The Gaussian Processes Web Site</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="shortcut icon" href="http://www.gaussianprocess.org/gp.ico"
type="image/x-icon">

<style type="text/css">
body {font-family: Verdana,Arial,Helvetica,sans-serif; font-size: 16px}
p.c {margin-left: 1cm; margin-right: 1cm; text-align: justify; font-size: 13px}
p.s {font-size: 13px}
</style>
</head>

<body>

<h1>The Gaussian Processes Web Site</h1>

<p align="justify">This web site aims to provide an overview of resources
concerned with probabilistic modeling, inference and learning based on Gaussian
processes.  Although Gaussian processes have a long history in the field of
statistics, they seem to have been employed extensively only in niche
areas. With the advent of kernel machines in the machine learning community,
models based on Gaussian processes have become commonplace for problems of
regression (kriging) and classification as well as a host of more specialized
applications.</p>

<p>
<table border="0" cellpadding="3" width="100%">
<tbody><tr><td align="center">
 [ <a href="#books">Books</a>
 | <a href="#events">Events</a>
 | <a href="#related">Other Web Sites</a>
 | <a href="#code">Software</a>
 | <a href="#references">Research Papers</a>
 ]
</td></tr></tbody></table>

<br><h2><a name="books">Books</a></h2>

<p><a href="http://www.gaussianprocess.org/gpml">Gaussian Processes for Machine Learning</a>, Carl Edward
Rasmussen and Chris Williams, the MIT Press, 2006, <a href="http://www.gaussianprocess.org/gpml/chapters">online version</a>.</p>

<p><a href="http://www.springer.com/sgw/cda/frontpage/0,11855,4-10129-22-2012471-0,00.html">Statistical
Interpolation of Spatial Data: Some Theory for Kriging</a>, Michael L. Stein,
Springer, 1999.</p>

<p><a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471002550.html">Statistics
for Spatial Data</a> (revised edition), Noel A. C. Cressie, Wiley, 1993</p>

<p><a href="http://www.ec-securehost.com/SIAM/CB59.html">Spline Models for
Observational Data</a>, Grace Wahba, SIAM, 1990</p>


<br><h2><a name="events">Future and Past Events</a></h2>

<p>The <a href="http://intranet.cs.man.ac.uk/ai/bark08">Bayesian Research
Kitchen</a> at The Wordsworth Hotel, Grasmere, Ambleside, Lake
District, United Kingdom 05 - 07 September 2008.</p>

<br>

<p>A <a href="http://www.nips.cc/Conferences/2006/Program/#Tutorials">tutorial</a>
entitled <a
href="http://www.nips.cc/Conferences/2006/Program/event.php?ID=4">Advances in
Gaussian Processes</a> on Dec. 4th at <a href="http://www.nips.cc">NIPS</a> <a
href="http://www.nips.cc/Conference/2006">2006</a> in VanCouver, <a href="http://www.kyb.mpg.de/~carl/gpnt06.pdf">slides</a>, <a href="http://nips.cc/Conferences/2006/Media">lecture</a>.</p>

<p>The <a href="http://www.dcs.shef.ac.uk/ml/gpip">Gaussian Processes in
Practice</a> workshop at Bletchley Park, U.K., June 12-13 2006.</p>

<p>The <a href="http://gp.kyb.tue.mpg.de/">Open Problems in Gaussian Processes
for Machine Learning</a> workshop at nips*05 in Whistler, December 10th,
2005.</p>

<p><a>The </a><a href="http://www.dcs.shef.ac.uk/ml/gprt">Gaussian Process
Round Table</a> meeting in Sheffield, June 9-10, 2005.</p>


<br><h2><a name="related">Other Web Sites of Related Interest</a></h2>

<p>The <a href="http://www.kernel-machines.org/">kernel-machines</a> web
site.</p>

<p>Wikipedia <a href="http://en.wikipedia.org/wiki/Gaussian_process">entry</a>
on Gaussian processes.</p>

<p>The <a href="http://www.ai-geostats.org/">ai-geostats</a> web site for 
spatial statistics and geostatistics.</p>

<p>The <a href="http://dsc.ijs.si/jus.kocijan/GPdyn">Bibliography of Gaussian 
Process Models in Dynamic Systems Modelling</a> web site maintained by <a href="http://dsc.ijs.si/en/people/jus_kocijan">Ju코 Kocijan</a>.</p>

<br><h2><a name="code">Software</a></h2>

<p><a href="http://www.rainsoft.de">Andreas Geiger</a> has written a
simple <a
href="http://www.rainsoft.de/projects/gausspro.html">Gaussian process
regression Java applet</a>, illustrating the behaviour of covariance functions
and hyperparameters.</p>

<table border="0" cellpadding="1"><tbody><tr><td bgcolor="#000000">
<table border="0" cellpadding="3" cellspacing="1">
<tbody><tr><td align="center" bgcolor="white"><b>package</b></td>
<td align="center" bgcolor="white"><b>title</b></td>
<td align="center" bgcolor="white"><b>author</b></td>
<td align="center" bgcolor="white"><b>implementation</b></td>
<td align="center" bgcolor="white"><b>description</b></td></tr>

<tr><td bgcolor="white"><a href="http://ida.first.fraunhofer.de/%7Eanton/software.html">bcm</a></td>
<td bgcolor="white">The Bayesian Committee Machine</td>
<td bgcolor="white"><a href="http://ida.first.fraunhofer.de/%7Eanton">Anton Schwaighofer</a></td>
<td bgcolor="white">matlab and <a href="http://www.ncrg.aston.ac.uk/netlab/index.php">NETLAB</a></td>
<td bgcolor="white"><p class="s">An extension of the Netlab implementation for GP regression. It allows large
scale regression based on the BCM approximation, see also <a href="#schwaighofer-tresp-03">
the accompanying paper</a>
</p></td>
</tr>

<tr><td bgcolor="white"><a href="http://www.cs.toronto.edu/%7Eradford/fbm.software.html">fbm</a></td>
<td bgcolor="white">Software for Flexible Bayesian Modeling</td>
<td bgcolor="white"><a href="http://www.cs.toronto.edu/%7Eradford">Radford M. Neal</a></td>
<td bgcolor="white">C for linux/unix</td>
<td align="justify" bgcolor="white"><p class="s">An extensive and well documented package implementing Markov chain Monte Carlo methods for
Bayesian inference in neural networks, Gaussian processes (regression, binary
and multi-class classification), mixture models and Dirichlet Diffusion trees.</p></td>
</tr>

<tr><td bgcolor="white"><a href="http://www.dcs.shef.ac.uk/%7Eneil/gplvm">gp-lvm</a> and <a href="http://www.dcs.shef.ac.uk/~neil/fgplvm">fgp-lvm</a></td>
<td bgcolor="white">A (fast) implementation of <a href="#lawrence-04">Gaussian Process Latent Variable Models</a></td>
<td bgcolor="white"><a href="http://www.dcs.shef.ac.uk/%7Eneil">Neil D. Lawrence</a></td>
<td bgcolor="white">matlab and C</td><td bgcolor="white">&nbsp;</td>
</tr>

<tr><td bgcolor="white"><a href="http://www.GaussianProcess.org/gpml/code/matlab">gpml</a></td>
<td bgcolor="white">Code from the Rasmussen and Williams: <a href="http://www.GaussianProcess.org/gpml">Gaussian Processes for Machine Learning</a> book.</td>
<td bgcolor="white"><a href="http://learning.eng.cam.ac.uk/carl">Carl Edward Rasmussen</a> and <a href="http://www.kyb.mpg.de/~hn">Hannes Nickisch</a></td>
<td bgcolor="white">matlab and octave</td>
<td bgcolor="white"><p class="s">
The GPML toolbox implements approximate inference algorithms for
Gaussian processes such as Expectation Propagation, the Laplace
Approximation and Variational Bayes for a wide class of likelihood
functions for both regression and classification. It comes with a big
algebra of covariance and mean functions allowing for flexible modeling.
The code is fully compatible to Octave 3.2.x. <a
  href="http://jmlr.csail.mit.edu/papers/volume11/rasmussen10a/rasmussen10a.pdf">JMLR
  paper</a> describing the toolbox.</p></td>
</tr>

<tr><td bgcolor="white"><a href="http://www.dcs.shef.ac.uk/~neil/ivmcpp/">c++-ivm</a></td>
<td bgcolor="white">Sparse approximations based on the <a href="#lawrence-seeger-herbrich-03">Informative Vector Machine</a></td>
<td bgcolor="white"><a href="http://www.dcs.shef.ac.uk/%7Eneil">Neil D. Lawrence</a></td>
<td bgcolor="white">C++</td><td bgcolor="white"><p class="s">IVM Software in C++ , also includes the null category noise model for <a href="#lawrence-jordan-05">semi-supervised learning</a>.</p></td>
</tr>

<tr><td bgcolor="white"><a href="http://www.dcs.shef.ac.uk/~neil/bfd/">BFD</a></td>
<td bgcolor="white">Bayesian Fisher's Discriminant software</td>
<td bgcolor="white"><a href="http://www.dcs.shef.ac.uk/~tpena/">Tonatiuh Pe침a
Centeno</a></td>
<td bgcolor="white">matlab</td>
<td align="justify" bgcolor="white"><p class="s">Implements a Gaussian process
interpretation of Kernel Fisher's discriminant.</p></td>
</tr>

<tr><td bgcolor="white"><a href="http://www.gatsby.ucl.ac.uk/%7chuwei/ordinalregression.html">gpor</a></td>
<td bgcolor="white">Gaussian Processes for Ordinal Regression</td>
<td bgcolor="white"><a href="http://www.gatsby.ucl.ac.uk/%7Echuwei">Wei Chu</a></td>
<td bgcolor="white">C for linux/unix</td>
<td align="justify" bgcolor="white"><p class="s">Software implementation of <a href="#chu-ghahramani-05">Gaussian Processes for Ordinal Regression</a>. Provides Laplace Approximation, Expectation Propagation and Variational Lower Bound.</p></td>
</tr>

<tr><td bgcolor="white"><a href="http://www.lce.hut.fi/research/compinf/mcmcstuff">MCMCstuff</a></td>
<td bgcolor="white">MCMC Methods for MLP and GP and Stuff</td>
<td bgcolor="white"><a href="http://www.lce.hut.fi/~ave">Aki Vehtari</a></td>
<td bgcolor="white">matlab and C</td>
<td bgcolor="white"><p class="s">A collection of matlab functions for Bayesian
inference with Markov chain Monte Carlo (MCMC) methods. The purpose of this
toolbox was to port some of the features in <a href="http://www.cs.toronto.edu/~radford/fbm.software.htm">fbm</a> to matlab for easier development for matlab users.</p></td></tr>

<tr><td bgcolor="white"><a href="http://www.kyb.tue.mpg.de/bs/people/csatol/ogp">ogp</a></td>
<td bgcolor="white">Sparse Online Gaussian Processes</td>
<td bgcolor="white"><a href="http://www.cs.ubbcluj.ro/~csatol">Lehel Csat칩</a></td>
<td bgcolor="white">matlab and <a href="http://www.ncrg.aston.ac.uk/netlab/index.php">NETLAB</a></td>
<td align="justify" bgcolor="white"><p class="s"> Approximate online learning in sparse Gaussian process models for regression (including
several non-Gaussian likelihood functions) and classification.</p></td>
</tr>

<tr><td bgcolor="white"><a href="http://cs.brown.edu/people/dang/code.shtml">sogp</a></td>
<td bgcolor="white">Sparse Online Gaussian Process C++ Library</td>
<td bgcolor="white"><a href="http://cs.brown.edu/~dang">Dan Grollman</a></td>
<td bgcolor="white">C++</td>
<td align="justify" bgcolor="white"><p class="s">
Sparse online Gaussian process C++ library based on the <a href="#csato-02">PhD thesis</a> of <a href="http://www.cs.ubbcluj.ro/~csatol">Lehel Csat칩</a></p></td>
</tr>

<tr><td bgcolor="white">spgp <a href="http://www.gatsby.ucl.ac.uk/~snelson/SPGP_dist.tgz">.tgz</a> or <a href="http://www.gatsby.ucl.ac.uk/~snelson/SPGP_dist.zip">.zip</a></td>
<td bgcolor="white">Sparse Pseudo-input Gaussian Processes</td>
<td bgcolor="white"><a href="http://research.microsoft.com/~esnelson">Ed Snelson</a></td>
<td bgcolor="white">matlab</td>
<td align="justify" bgcolor="white"><p class="s">Implements sparse GP regression as described in <a href="#snelson-ghahramani-06">Sparse Gaussian Processes using Pseudo-inputs</a> and <a href="#snelson-07">Flexible and efficient Gaussian process models for machine learning</a>. The SPGP uses gradient-based marginal likelihood optimization to find suitable basis points and kernel hyperparameters in a single joint optimization.</p></td>
</tr>

<tr><td bgcolor="white"><a href="http://www.ams.ucsc.edu/~rbgramacy/tgp.php">tgp</a></td>
<td bgcolor="white">Treed Gaussian Processes</td>
<td bgcolor="white"><a href="http://www.ams.ucsc.edu/~rbgramacy">Robert B. Gramacy</a></td>
<td bgcolor="white">C/C++ for R</td>
<td align="justify" bgcolor="white"><p class="s">Bayesian Nonparametric and
nonstationary regression by treed Gaussian processes with jumps to the limiting
linear model (LLM). Special cases also implememted include Bayesian linear
models, linear CART, stationary separable and isotropic Gaussian process
regression. Includes 1-d and 2-d plotting functions (with higher dimension
projection and slice capabilities), and tree drawing, designed for
visualization of tgp class output. See also <a href="#gramacy-07">Gramacy 2007</a></p></td>
</tr>

<tr><td bgcolor="white"><a href="http://wol.ra.phy.cam.ac.uk/mng10/GP/GP.html">Tpros</a></td>
<td bgcolor="white">Gaussian Process Regression</td>
<td bgcolor="white"><a href="http://www.inference.phy.cam.ac.uk/mackay/">David MacKay</a> and Mark Gibbs</td>
<td bgcolor="white">C</td>
<td align="justify" bgcolor="white"><p class="s"> Tpros is the Gaussian Process program written by Mark Gibbs and David
MacKay.</p></td>
</tr>

<tr><td bgcolor="white"><a href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/code/gp/Welcome.html">GP Demo</a></td>
<td bgcolor="white">Octave demonstration of Gaussian process interpolation</td>
<td bgcolor="white"><a href="http://www.inference.phy.cam.ac.uk/mackay/">David MacKay</a></td>
<td bgcolor="white">octave</td>
<td align="justify" bgcolor="white"><p class="s"> This DEMO works fine with octave-2.0 and did not work with 2.1.33.</p></td>
</tr>

<tr><td bgcolor="white"><a href="http://www.dai.ed.ac.uk/homes/ckiw/code/gpclass.tar.gz">GPClass</a></td>
<td bgcolor="white">Matlab code for Gaussian Process Classification</td>
<td bgcolor="white">
<a href="http://www.idiap.ch/~barber/">David Barber</a> and
<a href="http://www.dai.ed.ac.uk/homes/ckiw/">C. K. I. Williams</a> 
</td>
<td bgcolor="white">matlab</td>
<td align="justify" bgcolor="white"><p class="s">Implements Laplace's approximation as described in <a href="#williams-barber-98">Bayesian Classification with Gaussian Processes</a> for binary and multiclass classification.</p></td></tr>

<tr><td bgcolor="white"><a href="http://www.dcs.gla.ac.uk/people/personal/girolami/pubs_2005/VBGP">VBGP</a></td>
<td bgcolor="white">Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors</td>
<td bgcolor="white">
<a href="http://www.dcs.gla.ac.uk/~girolami">Mark Girolami</a> and
<a href="http://www.dcs.gla.ac.uk/~srogers">Simon Rogers</a> 
</td>
<td bgcolor="white">matlab</td>
<td align="justify" bgcolor="white"><p class="s">Implements a variational
approximation for Gaussian Process based multiclass classification as
described in the <a href="#girolami-rogers-06">paper</a> Variational Bayesian Multinomial Probit Regression. </td></tr>

<tr><td bgcolor="white"><a href="http://www-kd.iai.uni-bonn.de/index.php?page=software_details&id=19">pyXGPR</a></td>
<td bgcolor="white">Relational Gaussian Processes</td>
<td bgcolor="white">
<a href="http://www-kd.iai.uni-bonn.de/index.php?page=people_details&id=33">Marion Neumann</a> 
</td>
<td bgcolor="white">Python</td>
<td align="justify" bgcolor="white"><p class="s">pyXGPR is a library containing code for Gaussian Process Regression (GPR) and  relational GP (XGP) Regression.</td></tr>

<tr><td bgcolor="white"><a href="http://code.google.com/p/gaussian-process">gaussian-process</a></td>
<td bgcolor="white">Gaussian process regression</td>
<td bgcolor="white">
<a href="http://www.ams.ucsc.edu/~anand">Anand Patil</a> 
</td>
<td bgcolor="white">Python</td>
<td align="justify" bgcolor="white"><p class="s">under development</td></tr>

<tr><td bgcolor="white"><a href="http://cran.r-project.org/web/packages/gptk">gptk</a></td>
<td bgcolor="white">Gaussian Process Tool-Kit</td>
<td bgcolor="white">
<a href="http://staffwww.dcs.shef.ac.uk/people/A.Kalaitzis">Alfredo Kalaitzis</a> 
</td>
<td bgcolor="white">R</td>
<td align="justify" bgcolor="white"><p class="s">The gptk package implements a general-purpose toolkit for Gaussian process regression with an RBF covariance function. Based on a MATLAB implementation written by Neil D. Lawrence.</td></tr>

</tbody></table></td></tr>
</tbody>
</table>

<p>
Other software that way be useful for implementing Gaussian process models:
<ul>
<li>
    The <a
    href="http://www.ncrg.aston.ac.uk/netlab/index.php">NETLAB</a>
    package by <a
    href="http://www.ncrg.aston.ac.uk/People/nabneyit/Welcome.html">Ian
    Nabney</a> includes code for Gaussian process regression and many
    other useful thing, e.g. optimisers.
</li>
<li>
    See <a href="http://research.microsoft.com/~minka">Tom Minka</a>'s
    page on <a
    href="http://research.microsoft.com/~minka/software/matlab.html">accelerating
    matlab</a> and his <a
    href="http://research.microsoft.com/~minka/software/lightspeed">lightspeed</a>
    toolbox.
</li>
<li>
    <a
    href="http://www.kyb.tuebingen.mpg.de/bs/people/seeger">Matthias
    Seeger</a> shares his <a
    href="http://www.kyb.tuebingen.mpg.de/bs/people/seeger/software.html">code</a>
    for Kernel Multiple Logistic Regression, Incomplete Cholesky
    Factorization and Low-rank Updates of Cholesky Factorizations.
</li>
<li>
    See the software section of <a
    href="http://www.kernel-machines.org/">www.kernel-machines.org</a>.
</ul>



<br><h2><a name="references">Annotated Bibliography</a></h2>

<p>Below is a collection of papers relevant to learning in Gaussian process
models. The papers are ordered according to topic, with occational papers
occuring under multiple headings.

<p>
<table border="0" cellpadding="3" width="100%">
<tbody><tr><td align="center">
 [ <a href="#tut">Tutorials</a>
 | <a href="#gpr">Regression</a>
 | <a href="#class">Classification</a>
 | <a href="#cov">Covariance Functions</a>
 | <a href="#select">Model Selection</a>
 | <a href="#approx">Approximations</a>
 | <a href="#rkhs">Stats</a>
 | <a href="#cons">Learning Curves</a>
 | <a href="#rkhs">RKHS</a>
 | <a href="#rl">Reinforcement Learning</a>
 | <a href="#gplvm">GP-LVM</a>
 | <a href="#appl">Applications</a>
 | <a href="#other">Other Topics</a>
 ]
</td></tr></tbody></table>


<br>

<h3 id="tut">Tutorials</h3>

Several papers provide tutorial material suitable for a first introduction to
learning in Gaussian process models. These range from very short [<a
href="#williams-02">Williams 2002</a>] over intermediate [<a
href="#mackay-98">MacKay 1998</a>], [<a href="#williams-99">Williams 1999</a>]
to the more elaborate [<a href="#rasmussen-williams-06">Rasmussen and Williams
2006</a>]. All of these require only a minimum of prerequisites in the form of
elementary probability theory and linear algebra.


<p id="mackay-03">
D.&nbsp;J.&nbsp;C. MacKay.
<a href="http://www.inference.phy.cam.ac.uk/mackay/itila"><b>Information
  Theory, Inference and Learning Algorithms</b></a>.
Cambridge University Press, Cambridge, UK, 2003.
<a href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/534.548.pdf">chapter 45</a>.

<p class="c"><b> Comment:</b> A short introduction to GPs, emphasizing the
  relationships to paramteric models (RBF networks, neural networks,
  splines).</p>

</p>

<p id="mackay-97">
D.&nbsp;J.&nbsp;C. MacKay.
<a href="http://wol.ra.phy.cam.ac.uk/mackay/abstracts/gp.html"><b>Gaussian
  processes - a replacement for supervised neural networks?</b></a>.
Tutorial lecture notes for NIPS 1997, 1997.



</p>

<p id="mackay-98">
D.&nbsp;J.&nbsp;C. MacKay.
<a href="http://www.inference.phy.cam.ac.uk/mackay/gpB.pdf"><b>Introduction to
  Gaussian processes</b></a>.
In C.&nbsp;M. Bishop, editor, <em>Neural Networks and Machine Learning</em>, volume
  168 of <em>NATO ASI Series</em>, pages 133-165. Springer, Berlin, 1998.



</p>

<p id="press-teukolsky-vetterling-flannary-07">
W.&nbsp;H. Press, S.&nbsp;A. Teukolsky, W.&nbsp;T. Vetterling, and B.&nbsp;P. Flannary.
<a href="http://www.nr.com"><b>Numerical Recipes</b></a>.
Cambridge University Press, third edition, 2007.
Section 15.9.



</p>

<p id="rasmussen-williams-06">
C.&nbsp;E. Rasmussen and C.&nbsp;K.&nbsp;I. Williams.
<a href="http://www.gaussianprocess.org/gpml"><b>Gaussian Processes for Machine
  Learning</b></a>.
The MIT Press, Cambridge, MA, 2006.

<p class="c"><b> Comment:</b> The initial chapters contain significant amounts
  of tutorial material. The whole book, including all <a
  href="http://www.gaussianprocess.org/gpml/chapters">chapters</a> are freely
  available online.</p>

</p>

<p id="seeger-04">
M.&nbsp;Seeger.
<a href="http://www.kyb.tuebingen.mpg.de/bs/people/seeger/papers/bayesgp-tut.pdf"><b>Gaussian processes for machine learning</b></a>.
<em>International Journal of Neural Systems</em>, 14(2):69-106, 2004.<p
  class="c"><b> Abstract:</b> Gaussian processes (GPs) are natural
  generalisations of multivariate Gaussian random variables to infinite
  (countably or continuous) index sets. GPs have been applied in a large number
  of fields to a diverse range of ends, and very many deep theoretical analyses
  of various properties are available. This paper gives an introduction to
  Gaussian processes on a fairly elementary level with special emphasis on
  characteristics relevant in machine learning. It draws explicit connections
  to branches such as spline smoothing models and support vector machines in
  which similar ideas have been investigated.</p>



</p>

<p id="williams-02">
C.&nbsp;K.&nbsp;I. Williams.
<a href="http://www.dai.ed.ac.uk/homes/ckiw/postscript/hbtnn.ps.gz"><b>Gaussian
  processes</b></a>.
In M.&nbsp;A. Arbib, editor, <em>Handbook of Brain Theory and Neural Networks</em>,
  pages 466-470. The MIT Press, second edition, 2002.



</p>

<p id="williams-99">
C.&nbsp;K.&nbsp;I. Williams.
<a href="http://www.dai.ed.ac.uk/homes/ckiw/postscript/NCRG_97_012.ps.gz"><b>Prediction with Gaussian processes: From linear regression to linear
  prediction and beyond</b></a>.
In M.&nbsp;I. Jordan, editor, <em>Learning in Graphical Models</em>, pages 599-621.
  The MIT Press, Cambridge, MA, 1999.
Previously (1998) published by Kluwer Academic Press.<p class="c"><b>
  Abstract:</b> The main aim of this paper is to provide a tutorial on
  regression with Gaussian processes. We start from Bayesian linear regression,
  and show how by a change of viewpoint one can se this method as a Gaussian
  process predictor based on priors over functions, rather than on priors over
  parameters. This leads to a more general discussion of Gaussian processes in
  section 4. Section 5 deals with further issues, including hierarchical
  modelling and the setting of the parameters that control the Gaussian
  process, the covariance functions for neural network models and the use of
  Gaussian processes in classification problems.</p>



</p>


<br>

<h3 id="gpr">Regression</h3>

The simplest uses of Gaussian process models are for (the conjugate case of)
regression with Gaussian noise. See the <a href="#approx">approximation</a>
section for papers which deal specifically with sparse or fast approximation
techniques. <a href="#ohagan-78">O'Hagan 1978</a> represents an early reference
from the statistics comunity for the use of a Gaussian process as a prior over
functions, an idea which was only introduced to the machine learning community
by <a href="#williams-rasmussen-96">Williams and Rasmussen 1996</a>.


<p id="boyle-frean-05">
P.&nbsp;Boyle and M.&nbsp;Frean.
<a href="http://books.nips.cc/papers/files/nips17/NIPS2004_0225.pdf"><b>Dependent Gaussian processes</b></a>.
In L.&nbsp;K. Saul, Y.&nbsp;Weiss, and L.&nbsp;Bottou, editors, <em>Advances in Neural
  Information Processing Systems 17</em>, pages 217-224. The MIT Press,
  2005.<p class="c"><b> Abstract:</b> Gaussian processes are usually
  parameterised in terms of their covariance functions. However, this makes it
  difficult to deal with multiple outputs, because ensuring that the covariance
  matrix is positive definite is problematic. An alternative formulation is to
  treat Gaussian processes as white noise sources convolved with smoothing
  kernels, and to parameterise the kernel instead. Using this, we extend
  Gaussian processes to handle multiple, coupled outputs.</p>



</p>

<p id="goldberg-williams-bishop-98">
P.&nbsp;W. Goldberg, C.&nbsp;K.&nbsp;I. Williams, and C.&nbsp;M. Bishop.
<a href="http://books.nips.cc/papers/files/nips10/0493.pdf"><b>Regression with
  input-dependent noise: A Gaussian process treatment</b></a>.
In M.&nbsp;I. Jordan, M.&nbsp;J. Kearns, and S.&nbsp;A. Solla, editors, <em>Advances in Neural
  Information Processing Systems 10</em>. The MIT Press, Cambridge, MA,
  1998.<p class="c"><b> Abstract:</b> Gaussian processes provide natural
  non-parametric prior distributions over regression functions. In this paper
  we consider regression problems where there is noise on the output, and the
  variance of the noise depends on the inputs. If we assume that the noise is a
  smooth functon of the inputs, then it is natural to model the noise variance
  using a second Gaussian process, in addition to the Gaussian process
  governing the noise-free output value. We show that prior uncertainty about
  the parameters controlling both processes can be handled and that the
  posterior distribution of the noise rate can be sampled from using Markov
  chain Monte Carlo methods. Our results on a sythetic data set give a
  posterior noise variance that well-approximates the true variance.</p>



</p>

<p id="gramacy-07">
R.&nbsp;B. Gramacy.
<a href="http://www.jstatsoft.org/v19/i09/paper"><b>tgp: An R package for
  Bayesian nonstationary, semiparametric nonlinear regression and design by
  treed Gaussian process models</b></a>.
<em>Journal of Statistical Software</em>, 19, 2007.<p class="c"><b>
  Abstract:</b> The tgp package for R is a tool for fully Bayesian
  nonstationary, semiparametric nonlinear regression and design by treed
  Gaussian processes with jumps to the limiting linear model. Special cases
  also implemented include Bayesian linear models, linear CART, stationary
  separable and isotropic Gaussian processes. In addition to inference and
  posterior prediction, the package supports the (sequential) design of
  experiments under these models paired with several objective criteria. 1-d
  and 2-d plotting, with higher dimension projection and slice capabilities,
  and tree drawing functions (requiring maptree and combinat packages), are
  also provided for visualization of tgp objects.</p>



</p>

<p id="gramacy-lee-macready-04">
R.&nbsp;B. Gramacy, Herbert K.&nbsp;H. Lee, and William MacReady.
<a href="http://whisper.cse.ucsc.edu/~rbgramacy/papers/gra2004-02.pdf"><b>Parameter space exploration with Gaussian process trees</b></a>.
In <em>21st International Conference on Machine Learning</em>, pages 353-360.
  Omnipress and ACM Digital Library, 2004.<p class="c"><b> Abstract:</b>
  Computer experiments often require dense sweeps over input parameters to
  obtain a qualitative understanding of their response. Such sweeps can be
  prohibitively expensive, and are unnecessary in regions where the response is
  easily predicted; well-chosen designs could allow a mapping of the response
  with far fewer simulation runs. Thus, there is a need for computationally
  inexpensive surrogate models and an accompanying method for selecting small
  designs. We explore a general methodology for addressing this need that uses
  non-stationary Gaussian processes. Binary trees partition the input space to
  facilitate non-stationarity and a Bayesian interpretation provides an
  explicit measure of predictive uncertainty that can be used to guide
  sampling. Our methods are illustrated on several examples, including a
  motivating example involving computational fluid dynamics simulation of a
  NASA reentry vehicle.</p>



</p>

<p id="meeds-osindero-06">
E.&nbsp;Meeds and S.&nbsp;Osindero.
<a href="http://books.nips.cc/papers/files/nips18/NIPS2005_0798.pdf"><b>An
  alternative infinite mixture of Gaussian process experts</b></a>.
In Y.&nbsp;Weiss, B.&nbsp;Sch&ouml;lkopf, and J.&nbsp;Platt, editors, <em>Advances in Neural
  Information Processing Systems 18</em>, pages 883-890. The MIT Press,
  Cambridge, MA, 2006.<p class="c"><b> Abstract:</b> We present an infinite
  mixture model in which each component comprises a multivariate Gaussian
  distribution over an input space, and a Gaussian Process model over an output
  space. Our model is neatly able to deal with non-stationary covariance
  functions, discontinuities, multimodality and overlapping output signals. The
  work is similar to that by <a href="#rasmussen-ghahramani-02">Rasmussen and
  Ghahramani</a>; however, we use a full generative model over input and output
  space rather than just a conditional model. This allows us to deal with
  incomplete data, to perform inference over inverse functional mappings as
  well as for regression, and also leads to a more powerful and consistent
  Bayesian specification of the effective 'gating network' for the different
  experts.</p>



</p>

<p id="ohagan-78">
A.&nbsp;O'Hagan.
<b>Curve fitting and optimal design for prediction</b>.
<em>Journal of the Royal Statistical Society, Series B</em>, 40(1):1-42, 1978.



</p>

<p id="ohagan-94">
A.&nbsp;O'Hagan.
<b>Bayesian Inference</b>, volume&nbsp;2B of <em>Kendall's Advanced Theory of
  Statistics</em>.
Arnold, London, 1994.
chapter 10.48.



</p>

<p id="plate-99">
T.&nbsp;Plate.
<a href="http://www.d-reps.org/tplate/papers/bhmetrika98.ps.gz"><b>Accuracy
  versus interpretability in flexible modeling: implementing a tradeoff using
  Gaussian process models</b></a>.
<em>Behaviourmetrika</em>, 26(1):29-50, 1999.<p class="c"><b> Abstract:</b>
  One of the widely acknowledged drawbacks of flexible statistical models is
  that the fitted models are often extremely difficult to interpret. However,
  if flexible models are constrained to be additive the fitted models are much
  easier to interpret, as each input can be considered independently. The
  problem with additive models is that they cannot provide an accurate model if
  the phenomenon being modeled is not additive. This paper shows that a
  tradeoff between accuracy and additivity can be implemented easily in
  Gaussian process models, which are a type of flexible model closely related
  to feedforward neural networks. One can fit a series of Gaussian process
  models that begins with the completely flexible and are constrained to be
  progressively more additive, and thus progressively more interpretable.
  Observations of how the degree of non-additivity and the test error change as
  the models become more additive give insight into the importance of
  interactions in a particular model. Fitted models in the series can also be
  interpreted graphically with a technique for visualizing the effects of
  inputs in non-additive models that was adapted from plots for generalized
  additive models. This visualization technique shows the overall effects of
  different inputs and also shows which inputs are involved in interactions and
  how strong those interactions are.</p>



</p>

<p id="rasmussen-96">
C.&nbsp;E. Rasmussen.
<a href="http://www.kyb.mpg.de/publications/pss/ps2304.ps"><b>Evaluation of
  Gaussian Processes and other Methods for Non-linear Regression</b></a>.
PhD thesis, Department of Computer Science, University of Toronto, 1996.<p
  class="c"><b> Abstract:</b> This thesis develops two Bayesian learning
  methods relying on Gaussian processes and a rigorous statistical approach for
  evaluating such methods. In these experimental designs the sources of
  uncertainty in the estimated generalisation performances due to both
  variation in training and test sets are accounted for. The framework allows
  for estimation of generalisation performance as well as statistical tests of
  significance for pairwise comparisons. Two experimental designs are
  recommended and supported by the DELVE software environment. Two new
  non-parametric Bayesian learning methods relying on Gaussian process priors
  over functions are developed. These priors are controlled by hyperparameters
  which set the characteristic length scale for each input dimension. In the
  simplest method, these parameters are fit from the data using optimization.
  In the second, fully Bayesian method, a Markov chain Monte Carlo technique is
  used to integrate over the hyperparameters. One advantage of these Gaussian
  process methods is that the priors and hyperparameters of the trained models
  are easy to interpret. The Gaussian process methods are benchmarked against
  several other methods, on regression tasks using both real data and data
  generated from realistic simulations. The experiments show that small
  datasets are unsuitable for benchmarking purposes because the uncertainties
  in performance measurements are large. A second set of experiments provide
  strong evidence that the bagging procedure is advantageous for the
  Multivariate Adaptive Regression Splines (MARS) method. The simulated
  datasets have controlled characteristics which make them useful for
  understanding the relationship between properties of the dataset and the
  performance of different methods. The dependency of the performance on
  available computation time is also investigated. It is shown that a Bayesian
  approach to learning in multi-layer perceptron neural networks achieves
  better performance than the commonly used early stopping procedure, even for
  reasonably short amounts of computation time. The Gaussian process methods
  are shown to consistently outperform the more conventional methods.</p>



</p>

<p id="rasmussen-ghahramani-02">
C.&nbsp;E. Rasmussen and Z.&nbsp;Ghahramani.
<a href="http://books.nips.cc/papers/files/nips14/AA06.pdf"><b>Infinite
  mixtures of Gaussian process experts</b></a>.
In T.&nbsp;G. Diettrich, S.&nbsp;Becker, and Z.&nbsp;Ghahramani, editors, <em>Advances in
  Neural Information Processing Systems 14</em>. The MIT Press, 2002.<p
  class="c"><b> Abstract:</b> We present an extension to the Mixture of Experts
  (ME) model, where the individual experts are Gaussian Process (GP) regression
  models. Using a input-dependent adaptation of the Dirichlet Process, we
  implement a gating network for an infinite number of Experts. Inference in
  this model may be done efficiently using a Markov Chain relying on Gibbs
  sampling. The model allows the effective covariance function to vary with the
  inputs, and may handle large datasets - thus potentially overcoming two of
  the biggest hurdles with GP models. Simulations show the viability of this
  approach.</p>



</p>

<p id="snelson-rasmussen-ghahramani-04">
E.&nbsp;Snelson, C.&nbsp;E. Rasmussen, and Z.&nbsp;Ghahramani.
<a href="http://books.nips.cc/papers/files/nips16/NIPS2003_AA43.pdf"><b>Warped
  Gaussian processes</b></a>.
In S.&nbsp;Thrun, L.&nbsp;Saul, and B.&nbsp;Sch&ouml;lkopf, editors, <em>Advances in Neural
  Information Processing Systems 16</em>, Cambridge, MA, 2004. The MIT
  Press.<p class="c"><b> Abstract:</b> We generalise the Gaussian process (GP)
  framework for regression by learning a nonlinear transformation of the GP
  outputs. This allows for non-Gaussian processes and non-Gaussian noise. The
  learning algorithm chooses a nonlinear transformation such that transformed
  data is well-modelled by a GP. This can be seen as including a preprocessing
  transformation as an integral part of the probabilistic modelling problem,
  rather than as an ad-hoc step. We demonstrate on several real regression
  problems that learning the transformation can lead to significantly better
  performance than using a regular GP, or a GP with a fixed transformation.</p>



</p>

<p id="sollich-williams-05">
P.&nbsp;Sollich and C.&nbsp;K.&nbsp;I. Williams.
<a href="http://books.nips.cc/papers/files/nips17/NIPS2004_0362.pdf"><b>Using
  the equivalent kernel to understand Gaussian process regression</b></a>.
In L.&nbsp;K. Saul, Y.&nbsp;Weiss, and L.&nbsp;Bottou, editors, <em>Advances in Neural
  Information Processing Systems 17</em>. The MIT Press, 2005.<p
  class="c"><b> Abstract:</b> The equivalent kernel [<a
  href="#silverman-84">Silverman, 1984</a>] is a way of understanding how
  Gaussian process regression works for large sample sizes based on a continuum
  limit. In this paper we show (1) how to approximate the equivalent kernel of
  the widely-used squared exponential (or Gaussian) kernel and related kernels,
  and (2) how analysis using the equivalent kernel helps to understand the
  learning curves for Gaussian processes.</p>



</p>

<p id="vivarelli-williams-99">
F.&nbsp;Vivarelli and C.&nbsp;K.&nbsp;I Williams.
<a href="http://books.nips.cc/papers/files/nips11/0613.pdf"><b>Discovering
  hidden features with Gaussian processes regression</b></a>.
In M.&nbsp;S. Kearns, S.&nbsp;A. Solla, and D.&nbsp;A. Cohn, editors, <em>Advances in Neural
  Information Processing Systems 11</em>. The MIT Press, 1999.<p
  class="c"><b> Abstract:</b> In Gaussian process regression the covariance
  between the outputs at input locations x and x' is usually assumed to depend
  on the distance (x-x')W(x-x'), where W is a positive definite matrix. W is
  often taken to be diagonal, but if we allow W to be a general positive
  definite matrix which can be tuned on the basis of training data, then an
  eigen-analysis of W shows that we are effectively creating hidden features,
  where the dimensionality of the hidden-feature space is determined by the
  data. We demonstrate the superiority of predictions using the general matrix
  over those based on a diagonal matrix on two test problems.</p>



</p>

<p id="williams-rasmussen-96">
C.&nbsp;K.&nbsp;I. Williams and C.&nbsp;E. Rasmussen.
<a href="http://books.nips.cc/papers/files/nips08/0514.pdf"><b>Gaussian
  processes for regression</b></a>.
In D.&nbsp;S. Touretzky, M.&nbsp;C. Mozer, and M.&nbsp;E. Hasselmo, editors, <em>Advances in
  Neural Information Processing Systems 8</em>, pages 514-520. The MIT
  Press, Cambridge, MA, 1996.<p class="c"><b> Abstract:</b> The Bayesian
  analysis of neural networks is difficult because a simple prior over weights
  implies a complex prior over functions. We investigate the use of a Gaussian
  process prior over functions, which permits the predictive Bayesian analysis
  for fixed values of hyperparameters to be carried out exactly using matrix
  operations. Two methods, using optimization and averaging (via Hybrid Monte
  Carlo) over hyperparameters have been tested on a number of challenging
  problems and have produced excellent results.</p>



</p>

<p id="zhu-williams-rohwer-morciniec-98">
H.&nbsp;Zhu, C.&nbsp;K.&nbsp;I. Williams, R.&nbsp;J. Rohwer, and M.&nbsp;Morciniec.
<a href="http://www.dai.ed.ac.uk/homes/ckiw/postscript/NCRG_97_011.ps.gz"><b>Gaussian regression and optimal finite dimensional linear models</b></a>.
In C.&nbsp;M. Bishop, editor, <em>Neural Networks and Machine Learning</em>.
  Springer-Verlag, Berlin, 1998.
See also technical report NCRG/97/011, Aston University.<p class="c"><b>
  Abstract:</b> The problem of regression under Gaussian assumptions is treated
  generally. The relationship between Bayesian prediction, regularization and
  smoothing is elucidated. The ideal regression is the posterior mean and its
  computation scales as O(n<sup>3</sup>), where n is the same size. We show
  that the optimal m-dimensional linear model under a given prior is spanned by
  the first m eigenfunctions of a covaraince operator, which is a trace-class
  operator. This is an infinite dimensional analogue of principal component
  analysis. The importance of Hilbert space methods to practical statistics is
  also discussed.</p>



</p>


<br>

<h3 id="class">Classification</h3>

Exact inference in Gaussian process models for classification is not tractable.
Several approximation schemes have been suggested, including Laplace's method,
variational approximations, mean field methods, Markov chain Monte Carlo and
Expectation Propagation. See also the <a href="#approx">approximation</a>
section. Multi-class classification may be treated explicitly, or decomposed
into multiple, binary (one against the rest) problems. For introductions, see
for example <a href="#williams-barber-98">Williams and Barber 1998</a> or <a
href="#kuss-rasmussen-05">Kuss and Rasmussen 2005</a>. Bounds from the
PAC-Bayesian perspective are applied in <a href="#seeger-02">Seeger 2002</a>.


<p id="altun-hofmann-smola-04">
Y.&nbsp;Altun, T.&nbsp;Hofmann, and A.&nbsp;J. Smola.
<b>Gaussian process classification for segmenting and annotating
  sequences.</b>.
In C.&nbsp;E. Brodley, editor, <em>Proceedings of the Twenty-first International
  Conference on Machine Learning (ICML 2004)</em>. ACM, 2004.



</p>

<p id="barber-williams-97">
D.&nbsp;Barber and C.&nbsp;K.&nbsp;I. Williams.
<a href="http://books.nips.cc/papers/files/nips09/0340.pdf"><b>Gaussian
  processes for Bayesian classification via hybrid Monte Carlo</b></a>.
In M.&nbsp;C. Mozer, M.&nbsp;I. Jordan, and T.&nbsp;Petsche, editors, <em>Advances in Neural
  Information Processing Systems 9</em>, Cambridge, MA, 1997. The MIT
  Press.<p class="c"><b> Abstract:</b> The full Bayesian method for applying
  neural networks to a prediction problem is to set up the prior/hyperprior
  structure for the net and then perform the necessary integrals. However,
  these integrals are not tractable analytically, and Markov Chain Monte Carlo
  (MCMC) methods are slow, especially if the parameter space is
  high-dimensional. Using Gaussian processes we can approximate the weight
  space integral analytically, so that only a small number of hyperparameters
  need be integrated over by MCMC methods. We have applied this idea to
  classification problems, obtaining excellent results on the real-world
  problems investigated so far.</p>



</p>

<p id="choudhuri-ghosal-roy-05">
N.&nbsp;Choudhuri, S.&nbsp;Ghosal, and A.&nbsp;Roy.
<a href="http://www4.stat.ncsu.edu/~sghosal/papers.html"><b>Nonparametric
  binary regression using a Gaussian process prior</b></a>.
(unpublished), 2005.



</p>

<p id="csato-fokoue-etal-00">
L.&nbsp;Csat&oacute;, E.&nbsp;Fokou&eacute;, M.&nbsp;Opper, B.&nbsp;Schottky, and O.&nbsp;Winther.
<a href="http://books.nips.cc/papers/files/nips12/0251.pdf"><b>Efficient
  approaches to Gaussian process classification</b></a>.
In S.&nbsp;A. Solla, T.&nbsp;K. Leen, and K.-R. M&uuml;ller, editors, <em>Advances in
  Neural Information Processing Systems 12</em>, Cambridge, MA, 2000. The MIT
  Press.<p class="c"><b> Abstract:</b> We present three simple approximations
  for the calculation of the posterior mean in Gaussian Process classification.
  The first two methods are related to mean field ideas known in Statistical
  Physics. The third approach is based on Bayesan online approach which was
  motivated by recent results in the Statistical Mechanics of Neural Networks.
  We present simulation results showing: 1. that the mean field Bayesian
  evidence may be used for hyperparameter tuning and 2. that the online
  approach may achieve a low training error fast.</p>



</p>

<p id="csato-opper-01">
L.&nbsp;Csat&oacute; and M.&nbsp;Opper.
<a href="http://books.nips.cc/papers/files/nips13/CsatoOpper.pdf"><b>Sparse
  representation for Gaussian process models</b></a>.
In T.&nbsp;K. Leen, T.&nbsp;G. Dietterich, and V.&nbsp;Tresp, editors, <em>Advances in Neural
  Information Processing Systems 13</em>, Cambridge, MA, 2001. The MIT
  Press.<p class="c"><b> Abstract:</b> We develop an approach for a sparse
  representation for Gaussian Process (GP) models in order to overcome the
  limitations of GPs caused by large data sets. The method is based on a
  combination of a Bayesian online algorithm together with a sequential
  construction of a relevant subsample of the data which fully specifies the
  prediction of the model. Experimental results on toy examples and large
  real-world datasets indicate that efficiency of the approach.</p>



</p>

<p id="csato-opper-02">
L.&nbsp;Csat&oacute; and M.&nbsp;Opper.
<b>Sparse online Gaussian processes</b>.
<em>Neural Computation</em>, 14(2):641-669, 2002.<p class="c"><b>
  Abstract:</b> We develop an approach for sparse representations of gaussian
  process (GP) models (which are Bayesian types of kernel machines) in order to
  overcome their limitations for large data sets. The method is based on a
  combination of a Bayesian on-line algorithm, together with a sequential
  construction of a relevant subsample of the data that fully specifies the
  prediction of the GP model. By using an appealing parameterization and
  projection techniques in a reproducing kernel Hilbert space, recursions for
  the effective parameters and a sparse gaussian approximation of the posterior
  process are obtained. This allows for both a propagation of predictions and
  Bayesian error measures. The significance and robustness of our approach are
  demonstrated on a variety of experiments.</p>



</p>

<p id="gibbs-mackay-00">
M.&nbsp;N. Gibbs and D.&nbsp;J.&nbsp;C. MacKay.
<a href="http://www.inference.phy.cam.ac.uk/mackay/abstracts/vgc.html"><b>Variational Gaussian process classifiers</b></a>.
<em>IEEE Transactions on Neural Networks</em>, 11(6):1458-1464, 2000.<p
  class="c"><b> Abstract:</b> Gaussian processes are a promising non-linear
  interpolation tool [williams-95,<a href="#williams-rasmussen-96">Williams and
  Rasmussen 1996</a>], but it is not straightforward to solve classification
  problems with them. In this paper the variational methods of
  [jaakkola-jordan-96] are applied to Gaussian processes to produce an
  efficient Bayesian binary classifier.</p>



</p>

<p id="girolami-rogers-06">
M.&nbsp;Girolami and S.&nbsp;Rogers.
<a href="http://www.dcs.gla.ac.uk/people/personal/girolami/pubs_2005/VBGP/index.htm"><b>Variational Bayesian multinomial probit regression with Gaussian
  process priors</b></a>.
<em>Neural Computation</em>, 18(8):1790-1817, 2006.<p class="c"><b>
  Abstract:</b> It is well known in the statistics literature that augmenting
  binary and polychotomous response models with Gaussian latent variables
  enables exact Bayesian analysis via Gibbs sampling from the parameter
  posterior. By adopting such a data augmentation strategy, dispensing with
  priors over regression coefficients in favour of Gaussian Process (GP) priors
  over functions, and employing variational approximations to the full
  posterior we obtain efficient computational methods for Gaussian Process
  classification in the multi-class setting. The model augmentation with
  additional latent variables ensures full a posteriori class coupling whilst
  retaining the simple a priori independent GP covariance structure from which
  sparse approximations, such as multi-class Informative Vector Machines (IVM),
  emerge in a very natural and straightforward manner. This is the first time
  that a fully Variational Bayesian treatment for multi-class GP classification
  has been developed without having to resort to additional explicit
  approximations to the non-Gaussian likelihood term. Empirical comparisons
  with exact analysis via MCMC and Laplace approximations illustrate the
  utility of the variational approximation as a computationally economic
  alternative to full MCMC and it is shown to be more accurate than the Laplace
  approximation.</p>



</p>

<p id="kapoor-grauman-urtasun-darell-07">
A.&nbsp;Kapoor, K.&nbsp;Grauman, R.&nbsp;Urtasun, and T.&nbsp;Darell.
<a href="http://people.csail.mit.edu/rurtasun/publications/iccv_kapoor_et_al.pdf"><b>Active learning with Gaussian processes for object
  categorization</b></a>.
In <em>Proceedings of the International Conference in Cmputer Vision</em>,
  2007.<p class="c"><b> Abstract:</b> Discriminative methods for visual object
  category recognition are typically non-probabilistic, predicting class labels
  but not directly providing an estimate of uncertainty. Gaussian Processes
  (GPs) are powerful regression techniques with explicit uncertainty models; we
  show here how Gaussian Processes with covariance functions defined based on a
  Pyramid Match Kernel (PMK) can be used for probabilistic object category
  recognition. The uncertainty model provided by GPs offers confidence
  estimates at test points, and naturally allows for an active learning
  paradigm in which points are optimally selected for interactive labeling. We
  derive a novel active category learning method based on our probabilistic
  regression model, and show that a significant boost in classification
  performance is possible, especially when the amount of training data for a
  category is ultimately very small.</p>



</p>

<p id="kim-ghahramani-03">
H.-C. Kim and Z.&nbsp;Ghahramani.
<a href="http://www.gatsby.ucl.ac.uk/~zoubin/papers/ecml03.pdf"><b>The EM-EP
  algorithm for Gaussian process classification</b></a>.
In <em>Proceedings of the Workshop on Probabilistic Graphical Models for
  Classification (at ECML)</em>, 2003.<p class="c"><b> Abstract:</b> Gaussian
  process classifiers (GPCs) are fully statistical kernel classification models
  derived from Gaussian processes for regression. In GPCs, the probability of
  belonging to a certain class at an input location is monotonically related to
  the value of some latent function at that location. Starting from a prior
  over this latent function, the data are used to infer both the posterior over
  the latent function and the values of hyperparameters determining various
  aspects of the function. GPCs can also be viewed as graphical models with
  latent variables. Based on the work of [<a href="#minka-01a">Minka 2001</a>,
  <a href="#opper-winther-00">Opper and Winther 2000</a>], we present an
  approximate EM algorithm, the EM-EP algorithm for learning both the latent
  function and the hyperparameters of a GPC. The algorithm alternates the
  following steps until convergence. In the E-step, given the hyperparameters,
  a density for the latent variables defining the latent function is computed
  via the Expectation-Propagation (EP) algorithm [<a href="#minka-01a">Minka
  2001</a>, <a href="#opper-winther-00">Opper and Winther 2000</a>]. In the
  M-step, given the density for the latent values, the hyperparameters are
  selected to maximize a variational lower bound on the marginal likelihood
  (i.e. the model evidence). This algorithm is found to converge in practice
  and provides an efficient Bayesian framework for learning hyperparameters of
  the kernel. We examine the role of various different hyperparameters which
  model labeling errors, the lengthscales (i.e. relevances) of different
  features, and sharpness of the decision boundary. The added flexibility these
  provide results in signicantly improved performance. Experimental results on
  synthetic and real data sets show that the EM-EP algorithm works well, with
  GPCs giving equal or better performance than support vector machines (SVMs)
  on all data sets tested.</p>



</p>

<p id="kuss-rasmussen-05">
M.&nbsp;Kuss and C.&nbsp;E. Rasmussen.
<a href="http://www.jmlr.org/papers/volume6/kuss05a/kuss05a.pdf"><b>Assessing
  approximate inference for binary Gaussian process classification</b></a>.
<em>Journal of Machine Learning Research</em>, 6:1679-1704, 2005.<p
  class="c"><b> Abstract:</b> Gaussian process priors can be used to define
  flexible, probabilistic classification models. Unfortunately exact Bayesian
  inference is analytically intractable and various approximation techniques
  have been proposed. In this work we review and compare Laplace's method and
  Expectation Propagation for approximate Bayesian inference in the binary
  Gaussian process classification model. We present a comprehensive comparison
  of the approximations, their predictive performance and marginal likelihood
  estimates to results obtained by MCMC sampling. We explain theoretically and
  corroborate empirically the advantages of Expectation Propagation compared to
  Laplace's method.</p>



</p>

<p id="kuss-rasmussen-06">
M.&nbsp;Kuss and C.&nbsp;E. Rasmussen.
<a href="http://books.nips.cc/papers/files/nips18/NIPS2005_0163.pdf"><b>Assessing approximations for Gaussian process classification</b></a>.
In Y.&nbsp;Weiss, B.&nbsp;Sch&ouml;lkopf, and J.&nbsp;Platt, editors, <em>Advances in Neural
  Information Processing Systems 18</em>, pages 699-706, Cambridge, MA, 2006.
  The MIT Press.<p class="c"><b> Abstract:</b> Gaussian processes are
  attractive models for probabilistic classification but unfortunately exact
  inference is analytically intractable. We compare Laplace's method and
  Expectation Propagation (EP) focusing on marginal likelihood estimates and
  predictive performance. We explain theoretically and corroborate empirically
  that EP is superior to Laplace. We also compare to a sophisticated MCMC
  scheme and show that EP is surprisingly accurate.</p>



</p>

<p id="lawrence-jordan-05">
N.&nbsp;D. Lawrence and M.&nbsp;I. Jordan.
<a href="http://books.nips.cc/papers/files/nips17/NIPS2004_0257.pdf"><b>Semi-supervised learning via Gaussian processes</b></a>.
In L.&nbsp;K. Saul, Y.&nbsp;Weiss, and Bottou L, editors, <em>Advances in Neural
  Information Processing Systems 17</em>, pages 753-760, Cambridge, MA, 2005.
  The MIT Press.<p class="c"><b> Abstract:</b> We present a probabilistic
  approach to learning a Gaussian Process classifier in the presence of
  unlabeled data. Our approach involves a "null category noise model" (NCNM)
  inspired by ordered cate- gorical noise models. The noise model re ects an
  assumption that the data density is lower between the class-conditional
  densities. We illustrate our approach on a toy problem and present
  comparative results for the semi-supervised classification of handwritten
  digits.</p>



</p>

<p id="neal-98">
R.&nbsp;M. Neal.
<b>Regression and classification using Gaussian process priors</b>.
In J.&nbsp;M. Bernardo, J.&nbsp;O. Berger, A.&nbsp;P. Dawid, and A.&nbsp;F.&nbsp;M. Smith, editors,
  <em>Bayesian Statistics 6</em>, pages 475-501. Oxford University Press,
  1998.<p class="c"><b> Abstract:</b> Gaussian processes are a natural way of
  specifying prior distributions over functions of one or more input variables.
  When such a function defines the mean response in a regression model with
  Gaussian errors, inference can be done using matrix computations, which are
  feasible for datasets of up to about a thousand cases. The covariance
  function of the Gaussian process can be given a hierarchical prior, which
  allows the model to discover high-level properties of the data, such as which
  inputs are relevant to predicting the response. Inference for these
  covariance hyperparameters can be done using Markov chain sampling.
  Classification models can be defined using Gaussian processes for underlying
  latent values, which can also be sampled within the Markov chain. Gaussian
  processes are in my view the simplest and most obvious way of defining
  flexible Bayesian regression and classification models, but despite some past
  usage, they appear to have been rather neglected as a general-purpose
  technique. This may be partly due to a confusion between the properties of
  the function being modeled and the properties of the best predictor for this
  unknown function.</p>



</p>

<p id="nickisch-rasmussen-08">
H.&nbsp;Nickisch and C.&nbsp;E. Rasmussen.
<a href="http://jmlr.csail.mit.edu/papers/volume9/nickisch08a/nickisch08a.pdf"><b>Approximations for binary Gaussian process classification</b></a>.
<em>Journal of Machine Learning Research</em>, 9:2035-2078, 2008.<p
  class="c"><b> Abstract:</b> We provide a comprehensive overview of many
  recent algorithms for approximate inference in Gaussian process models for
  probabilistic binary classification. The relationships between several
  approaches are elucidated theoretically, and the properties of the different
  algorithms are corroborated by experimental results. We examine both 1) the
  quality of the predictive distributions and 2) the suitability of the
  different marginal likelihood approximations for model selection (selecting
  hyperparameters) and compare to a gold standard based on MCMC. Interestingly,
  some methods produce good predictive distributions although their marginal
  likelihood approximations are poor. Strong conclusions are drawn about the
  methods: The Expectation Propagation algorithm is almost always the method of
  choice unless the computational budget is very tight. We also extend existing
  methods in various ways, and provide unifying code implementing all
  approaches.</p>



</p>

<p id="opper-winther-00">
M.&nbsp;Opper and O.&nbsp;Winther.
<b>Gaussian processes for classification: Mean-field algorithms</b>.
<em>Neural Computation</em>, 12(11):2655-2684, 2000.<p class="c"><b>
  Abstract:</b> We derive a mean-field algorithm for binary classification with
  gaussian processes that is based on the TAP approach originally proposed in
  statistical physics of disordered systems. The theory also yields an
  approximate leave-one-out estimator for the generalization error, which is
  computed with no extra computational cost. We show that from the TAP
  approach, it is possible to derive both a simpler "naive" mean-field theory
  and support vector machines (SVMs) as limiting cases. For both mean-field
  algorithms and support vector machines, simulation results for three small
  benchmark data sets are presented. They show that one may get
  state-of-the-art performance by using the leave-one-out estimator for model
  selection and the built-in leave-one-out estimators are extremely precise
  when compared to the exact leave-one-out estimate. The second result is taken
  as strong support for the internal consistency of the mean-field
  approach.</p>



</p>

<p id="opper-winther-99">
M.&nbsp;Opper and O.&nbsp;Winther.
<a href="http://books.nips.cc/papers/files/nips11/0309.pdf"><b>Mean field
  methods for classification with Gaussian processes</b></a>.
In M.&nbsp;S. Kearns, S.&nbsp;A. Solla, and D.&nbsp;A. Cohn, editors, <em>Advances in Neural
  Information Processing Systems 11</em>, pages 309-315, Cambridge, MA, 1999.
  The MIT Press.<p class="c"><b> Abstract:</b> We discuss the application of
  TAP mean field methods known from the Statistical Mechanics of diordered
  systems to Bayesian classification models with Gaussian processes. In
  contrast to previous approaches, no knowledge about the distribution of
  inputs is needed. Simulation results for the Sonar data set are given.</p>



</p>

<p id="pena-centeno-lawrence-06">
T.&nbsp;Pe침a Centeno and N.&nbsp;D. Lawrence.
<a href="http://jmlr.csail.mit.edu/papers/volume7/centeno06a/centeno06a.pdf"><b>Optimising kernel parameters and regularisation coefficients for non-linear
  discriminant analysis</b></a>.
<em>Journal of Machine Learning Research</em>, 7:455-491, 2006.<p
  class="c"><b> Abstract:</b> In this paper we consider a novel Bayesian
  interpretation of Fisher's discriminant analysis. We relate Rayleigh's
  coefficient to a noise model that minimises a cost based on the most probable
  class centres and that abandons the 'regression to the labels' assumption
  used by other algorithms. Optimisation of the noise model yields a direction
  of discrimination equivalent to Fisher's discriminant, and with the
  incorporation of a prior we can apply Bayes' rule to infer the posterior
  distribution of the direction of discrimination. Nonetheless, we argue that
  an additional constraining distribution has to be included if sensible
  results are to be obtained. Going further, with the use of a Gaussian process
  prior we show the equivalence of our model to a regularised kernel Fisher's
  discriminant. A key advantage of our approach is the facility to determine
  kernel parameters and the regularisation coefficient through the optimisation
  of the marginal log-likelihood of the data. An added bonus of the new
  formulation is that it enables us to link the regularisation coefficient with
  the generalisation error.</p>



</p>

<p id="seeger-02">
M.&nbsp;Seeger.
<a href="http://www.jmlr.org/papers/volume3/seeger02a/seeger02a.pdf"><b>PAC-Bayesian generalisation error bounds for Gaussian process
  classification</b></a>.
<em>Journal of Machine Learning Research</em>, 3:233-269, 2002.<p
  class="c"><b> Abstract:</b> Approximate Bayesian Gaussian process (GP)
  classification techniques are powerful non-parametric learning methods,
  similar in appearance and performance to support vector machines. Based on
  simple probabilistic models, they render interpretable results and can be
  embedded in Bayesian frameworks for model selection, feature selection, etc.
  In this paper, by applying the PAC-Bayesian theorem of McAllester (1999a), we
  prove distribution-free generalisation error bounds for a wide range of
  approximate Bayesian GP classification techniques. We also provide a new and
  much simplified proof for this powerful theorem, making use of the concept of
  convex duality which is a backbone of many machine learning techniques. We
  instantiate and test our bounds for two particular GPC techniques, including
  a recent sparse method which circumvents the unfavourable scaling of standard
  GP algorithms. As is shown in experiments on a real-world task, the bounds
  can be very tight for moderate training sample sizes. To the best of our
  knowledge, these results provide the tightest known distribution-free error
  bounds for approximate Bayesian GPC methods, giving a strong
  learning-theoretical justification for the use of these techniques.</p>



</p>

<p id="seeger-jordan-04">
M.&nbsp;Seeger and M.&nbsp;I. Jordan.
<a href="http://www.kyb.tuebingen.mpg.de/bs/people/seeger/papers/ivmmulti.pdf"><b>Sparse Gaussian process classification with multiple classes</b></a>.
Technical Report TR 661, Department of Statistics, University of California
  at Berkeley, 2004.<p class="c"><b> Abstract:</b> Sparse approximations to
  Bayesian inference for nonparametric Gaussian Process models scale linearly
  in the number of training points, allowing for the application of these
  powerful kernel-based models to large datasets. We show how to generalize the
  binary classification Informative Vector Machine (IVM) to multiple classes.
  In contrast to earlier efficient approaches to kernel-based non-binary
  classification, our method is a principled approximation to Bayesian
  inference which yields valid uncertainty estimates and allows for
  hyperparameter adaption via marginal likelihood maximization. While most
  earlier proposals suggest fitting independent binary discriminants to
  heuristically chosen partitions of the data and combining these in a
  heuristic manner, our method operates jointly on the data for all classes.
  Crucially, we still achieve a linear scaling in both the number of classes
  and the number of training points.</p>



</p>

<p id="urtasun-darrell-07">
R.&nbsp;Urtasun and T.&nbsp;Darrell.
<a href="http://people.csail.mit.edu/rurtasun/publications/icml_urtasun_darrell.pdf"><b>Discriminative Gaussian process latent variable model for
  classification</b></a>.
In <em>24th International Conference on Machine Learning</em>, 2007.<p
  class="c"><b> Abstract:</b> Supervised learning is dicult with high
  dimensional input spaces and very small training sets, but accurate
  classcation may be possible if the data lie on a low-dimensional manifold.
  Gaussian Process Latent Variable Models can discover low dimensional
  manifolds given only a small number of examples, but learn a latent space
  without regard for class labels. Existing methods for discriminative manifold
  learning (e.g., LDA, GDA) do constrain the class distribution in the latent
  space, but are generally deterministic and may not generalize well with
  limited training data. We introduce a method for Gaussian Process Classcation
  using latent variable models trained with discriminative priors over the
  latent space, which can learn a discriminative latent space from a small
  training set.</p>



</p>

<p id="williams-barber-98">
C.&nbsp;K.&nbsp;I. Williams and D.&nbsp;Barber.
<a href="http://www.dai.ed.ac.uk/homes/ckiw/postscript/pami_final.ps.gz"><b>Bayesian classification with Gaussian processes</b></a>.
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>,
  20(12):1342-1351, 1998.
Accompanying <a
  href="http://www.dai.ed.ac.uk/homes/ckiw/code/gpclass.tar.gz">code</a>.<p
  class="c"><b> Abstract:</b> We consider the problem of assigning an input
  vector <b>x</b> to one of m classes by predicting P(c|<b>x</b>) for
  c=1,...,m. For a two-class problem, the probability of class 1 given <b>x</b>
  is esimated by &sigma;(y(<b>x</b>)), where &sigma;(y)=1/(1+e<sup>-y</sup>). A
  Gaussian process prior is placed on y(<b>x</b>), and is combined with the
  training data to obtain predictions for the <b>x</b> points. We provide a
  Bayesian treatment, integrating over uncertainty in y and in the parameters
  that control the Gaussian process prior; the necessary integration over y is
  carried out using Laplace's approximation. The method is generalized to
  multi-class problems (m>2) using the softmax function. We demonstrate the
  effectiveness of the method on a number of datasets.</p>



</p>


<br>

<h3 id="cov">Covariance Functions and Properties of Gaussian Processes</h3>

The properties of Gaussian processes are controlled by the (mean function and)
covariance function. Some references here describe difference covariance
functions, while others give mathematical characterizations, see eg. <a
href="#abrahamsen-97">Abrahamsen 1997</a> for a review. Some references
describe non-standard covariance functions leading to non-stationarity etc.


<p id="abrahamsen-97">
P.&nbsp;Abrahamsen.
<a href="http://publications.nr.no/917_Rapport.pdf"><b>A review of Gaussian
  random fields and correlation functions</b></a>.
Technical Report 917, Norwegian Computing Center, Oslo, 1997.



</p>

<p id="adler-81">
R.&nbsp;J. Adler.
<b>The Geometry of Random Fields</b>.
John Wiley &amp; Sons, Chichester, 1981.



</p>

<p id="doob-44">
J.&nbsp;L. Doob.
<b>The elementary Gaussian processes</b>.
<em>Annals of Mathematical Statistics</em>, 15(3):229-282, 1944.



</p>

<p id="gibbs-97">
M.&nbsp;N. Gibbs.
<a href="http://www.inference.phy.cam.ac.uk/mng10/GP/thesis.ps.gz"><b>Bayesian
  Gaussian Processes for Regression and Classification</b></a>.
PhD thesis, Department of Physics, University of Cambridge, 1997.<p
  class="c"><b> Abstract:</b> Bayesian inference offers us a powerful tool with
  which to tackle the problem of data modelling. However the performance of
  Bayesian methods is crucially dependent on being able to find good models for
  our data. The principal focus of this thesis is the development of models
  based on Gaussian process priors. Such models, which can be thought of as the
  infinite extension of several existing finite models have the flexibility to
  model complex phenomena while being mathematically simple. In thesis, I
  present a review of the theory of Gaussian processes and their covariance
  functions and demonstrate how they fit into the Bayesian framework. The
  efficient implementation of a Gaussian process is discussed with particular
  reference to approximate methods for matrix inversion based on the work of
  Skilling (1993). Several regression problems are examined. Non-stationary
  covariance functions are developed for the regression of neuron spike data
  and the use of Gaussian processes to model the potential energy surfaces of
  weakly bound molecules is discussed. Classification methods based on Gaussian
  processes are implemented using variational methods. Existing bounds
  (Jaakkola and Jordan 1996) for the sigmoid function are used to tackle binary
  problems and multi-dimensional bounds on the softmax function are presented
  for the multiple class case. The performance of the variational classifier is
  compared with that of other methods using the CRABS and PIMA datasets (Ripley
  1996) and the problem of predicting the cracking of welds based on their
  chemical composition is also investigated. The theoretical calculation of the
  density of states of crystal structures is discussed in detail. Three
  possible approaches to the problem are described based on free energy
  minimization, Gaussian processes and the theory of random matrices. Results
  from these approaches are compared with the state-of-the-art techniques
  (Pickard 1997).</p>



</p>

<p id="matheron-73">
G.&nbsp;Matheron.
<b>The intrinsic random functions and their applications</b>.
<em>Advances in Applied Probability</em>, 5:439-468, 1973.



</p>

<p id="meiring-monestiez-etal-97">
W.&nbsp;Meiring, P.&nbsp;Monestiez, P.&nbsp;D. Sampson, and Guttorp. P.
<b>Developments in the modelling of nonstationary spatial covariance structure
  for space-time monitoring data</b>.
In E.&nbsp;Y. Baafi and N.&nbsp;Schofield, editors, <em>Geostatistics Wollongong
  '96</em>, volume&nbsp;1, pages 162-173. Kluwer, 1997.



</p>

<p id="neal-96">
R.&nbsp;M. Neal.
<a href="http://www.cs.utoronto.ca/~radford/bnn.book.html"><b>Bayesian Learning
  for Neural Networks</b></a>.
Springer, New York, 1996.
chapter 2.<p class="c"><b> Abstract:</b> Artificial ``neural networks'' are now
  widely used as flexible models for regression and classification
  applications, but questions remain regarding what these models mean, and how
  they can safely be used when training data is limited. Bayesian Learning for
  Neural Networks shows that Bayesian methods allow complex neural network
  models to be used without fear of the ``overfitting'' that can occur with
  traditional neural network learning methods. Insight into the nature of these
  complex Bayesian models is provided by a theoretical investigation of the
  priors over functions that underlie them. Use of these models in practice is
  made possible using Markov chain Monte Carlo techniques. Both the theoretical
  and computational aspects of this work are of wider statistical interest, as
  they contribute to a better understanding of how Bayesian methods can be
  applied to complex problems. Presupposing only basic knowledge of probability
  and statistics, this book should be of interest to many researchers in
  Statistics, Engineering, and Artificial Intelligence. Software for Unix
  systems that implements the methods described is freely available over the
  Internet.</p>

<p class="c"><b> Comment:</b> Gaussian processes are not the main topic of this
  book but, chapter 2 entitled "Priors for Infinite Networks" contains a
  characterization of Gaussian and non-Gaussian limits of priors over functions
  generated from neural networks. See also <a href="#williams-98">Williams
  1998</a>.</p>

</p>

<p id="paciorek-schervish-04">
C.&nbsp;J. Paciorek and M.&nbsp;J. Schervish.
<a href="http://books.nips.cc/papers/files/nips16/NIPS2003_AA35.pdf"><b>Nonstationary covariance functions for Gaussian process regression</b></a>.
In S.&nbsp;Thrun, L.&nbsp;Saul, and B.&nbsp;Sch&ouml;lkopf, editors, <em>Advances in Neural
  Information Processing Systems 16</em>. The MIT Press, 2004.



</p>

<p id="sampson-guttorp-92">
P.&nbsp;D. Sampson and P.&nbsp;Guttorp.
<b>Nonparametric estimation of nonstationary spatial covariance structure</b>.
<em>Journal of the American Statistical Association</em>, 87(417):108-119,
  1992.



</p>

<p id="schmidt-ohagan-03">
A.&nbsp;M. Schmidt and A.&nbsp;O'Hagan.
<b>Bayesian inference for non-stationary spatial covariance structure via
  spatial deformations</b>.
<em>Journal of the Royal Statistical Society B</em>, 65(3):745-758, 2003.



</p>

<p id="schoenberg-38">
I.&nbsp;J. Schoenberg.
<b>Metric spaces and positive definite functions</b>.
<em>Transactions of the American Mathematical Society</em>, 44(3):522-536,
  1938.



</p>

<p id="williams-98">
C.&nbsp;K.&nbsp;I. Williams.
<a href="http://www.dai.ed.ac.uk/homes/ckiw/postscript/NCRG_97_025.ps.gz"><b>Computation with infinite neural networks</b></a>.
<em>Neural Computation</em>, 10:1203-1216, 1998.<p class="c"><b> Abstract:</b>
  For neural networks with a wide class of weight priors, it can be shown that
  in the limit of an infinite number of hidden units, the prior over functions
  tends to a Gaussian process. In this article, analytic forms are derived for
  the covariance function of the Gaussian processes corresponding to networks
  with sigmoidal and gaussian hidden units. This allows predictions to be made
  efficiently using networks with an infinite number of hidden units and shows,
  somewhat paradoxically, that it may be easier to carry out Bayesian
  prediction with infinite networks rather than finite ones.</p>



</p>

<p id="yaglom-62">
A.&nbsp;M. Yaglom.
<b>Stationary Random Functions</b>.
Prentice-Hall, Englewood Cliffs, NJ, 1962.



</p>


<br>

<h3 id="select">Model Selection</h3>


<p id="mackay-99">
D.&nbsp;J.&nbsp;C. MacKay.
<b>Comparison of approximate methods for handling hyperparameters</b>.
<em>Neural Compuration</em>, 11(5):1035-1068, 1999.



</p>

<p id="mardia-84">
K.&nbsp;V. Mardia and R.&nbsp;J. Marshall.
<b>Maximum likelihood estimation of models for residual covariance in spatial
  regression</b>.
<em>Biometrika</em>, 71(1):135-146, 1984.



</p>

<p id="qi-minka-etal-04">
Y.&nbsp;Qi, T.&nbsp;P. Minka, R.&nbsp;W. Picard, and Z.&nbsp;Ghahramani.
<b>Predictive automatic relevance determination by expectation propagation</b>.
In C.&nbsp;E. Brodley, editor, <em>Proceedings of Twenty-first International
  Conference on Machine Learning</em>, 2004.



</p>

<p id="schwaighofer-tresp-yu-05">
A.&nbsp;Schwaighofer, V.&nbsp;Tresp, and K.&nbsp;Yu.
<a href="http://books.nips.cc/papers/files/nips17/NIPS2004_0763.pdf"><b>Learning Gaussian process kernels via hierarchical bayes</b></a>.
In L.&nbsp;K. Saul, Y.&nbsp;Weiss, and L.&nbsp;Bottou, editors, <em>Advances in Neural
  Information Processing Systems 17</em>, Cambridge, MA, 2005. The MIT Press.



</p>

<p id="seeger-00">
M.&nbsp;Seeger.
<a href="http://books.nips.cc/papers/files/nips12/0603.pdf"><b>Bayesian model
  selection for support vector machines, Gaussian processes and other kernel
  classifiers</b></a>.
In S.&nbsp;A. Solla, T.&nbsp;K. Leen, and K.-R. M&uuml;ller, editors, <em>Advances in
  Neural Information Processing Systems 12</em>, pages 603-609, Cambridge, MA,
  2000. The MIT Press.



</p>

<p id="seeger-02">
M.&nbsp;Seeger.
<a href="http://www.jmlr.org/papers/volume3/seeger02a/seeger02a.pdf"><b>PAC-Bayesian generalisation error bounds for Gaussian process
  classification</b></a>.
<em>Journal of Machine Learning Research</em>, 3:233-269, 2002.<p
  class="c"><b> Abstract:</b> Approximate Bayesian Gaussian process (GP)
  classification techniques are powerful non-parametric learning methods,
  similar in appearance and performance to support vector machines. Based on
  simple probabilistic models, they render interpretable results and can be
  embedded in Bayesian frameworks for model selection, feature selection, etc.
  In this paper, by applying the PAC-Bayesian theorem of McAllester (1999a), we
  prove distribution-free generalisation error bounds for a wide range of
  approximate Bayesian GP classification techniques. We also provide a new and
  much simplified proof for this powerful theorem, making use of the concept of
  convex duality which is a backbone of many machine learning techniques. We
  instantiate and test our bounds for two particular GPC techniques, including
  a recent sparse method which circumvents the unfavourable scaling of standard
  GP algorithms. As is shown in experiments on a real-world task, the bounds
  can be very tight for moderate training sample sizes. To the best of our
  knowledge, these results provide the tightest known distribution-free error
  bounds for approximate Bayesian GPC methods, giving a strong
  learning-theoretical justification for the use of these techniques.</p>



</p>

<p id="sollich-02">
P.&nbsp;Sollich.
<b>Bayesian methods for support vector machines: Evidence and predictive class
  probabilities</b>.
<em>Machine Learning</em>, 46(1-3):21-52, 2002.<p class="c"><b> Abstract:</b>
  I describe a framework for interpreting Support Vector Machines (SVMs) as
  maximum a posteriori (MAP) solutions to inference problems with Gaussian
  Process priors. This probabilistic interpretation can provide intuitive
  guidelines for choosing a lsquogoodrsquo SVM kernel. Beyond this, it allows
  Bayesian methods to be used for tackling two of the outstanding challenges in
  SVM classification: how to tune hyperparameters\u2014the misclassification
  penalty C, and any parameters specifying the ernel\u2014and how to obtain
  predictive class probabilities rather than the conventional deterministic
  class label predictions. Hyperparameters can be set by maximizing the
  evidence; I explain how the latter can be defined and properly normalized.
  Both analytical approximations and numerical methods (Monte Carlo chaining)
  for estimating the evidence are discussed. I also compare different methods
  of estimating class probabilities, ranging from simple evaluation at the MAP
  or at the posterior average to full averaging over the posterior. A simple
  toy application illustrates the various concepts and techniques.</p>



</p>

<p id="sundararajan-keerthi-00">
S.&nbsp;Sundararajan and S.&nbsp;S. Keerthi.
<a href="http://books.nips.cc/papers/files/nips12/0631.pdf"><b>Predictive
  approaches for choosing hyperparameters in Gaussian processes,</b></a>.
In S.&nbsp;A. Solla, T.&nbsp;K. Leen, and K.-R. M&uuml;ller, editors, <em>Advances in
  Neural Information Processing Systems 12</em>, Cambridge, MA, 2000. The MIT
  Press.



</p>

<p id="sundararajan-keerthi-01">
S.&nbsp;Sundararajan and S.&nbsp;Sathiya Keerthi.
<b>Predictive approaches for choosing hyperparameters in Gaussian
  processes</b>.
<em>Neural Computation</em>, 13:1103-1118, 2001.



</p>

<p id="vivarelli-williams-99">
F.&nbsp;Vivarelli and C.&nbsp;K.&nbsp;I Williams.
<a href="http://books.nips.cc/papers/files/nips11/0613.pdf"><b>Discovering
  hidden features with Gaussian processes regression</b></a>.
In M.&nbsp;S. Kearns, S.&nbsp;A. Solla, and D.&nbsp;A. Cohn, editors, <em>Advances in Neural
  Information Processing Systems 11</em>. The MIT Press, 1999.<p
  class="c"><b> Abstract:</b> In Gaussian process regression the covariance
  between the outputs at input locations x and x' is usually assumed to depend
  on the distance (x-x')W(x-x'), where W is a positive definite matrix. W is
  often taken to be diagonal, but if we allow W to be a general positive
  definite matrix which can be tuned on the basis of training data, then an
  eigen-analysis of W shows that we are effectively creating hidden features,
  where the dimensionality of the hidden-feature space is determined by the
  data. We demonstrate the superiority of predictions using the general matrix
  over those based on a diagonal matrix on two test problems.</p>



</p>

<p id="wahba-78">
G.&nbsp;Wahba.
<b>Improper priors, spline smoothing and the problem of guarding against model
  errors in regression</b>.
<em>Journal of the Royal Statistical Society, Series B</em>, 40:364-372, 1978.



</p>


<br>

<h3 id="approx">Approximations</h3>

There are two main reasons for doing approximations in Gaussian process models.
Either because of analytical intractability such as arrises in classification
and regression with non-Gaussian noise. Or in order to gain a computational
advantage when using large datasets, by the use of <em>sparse</em>
approximations. Some methods address both issues simultaneously. The
approximation methods and approximate inference algorithms are quite diverse,
see <a href="#quinonero-candela-rasmussen-05">Qui&ntilde;onero-Candela and
Ramussen 2005</a> for a unifying framework for sparse approximations in the
Gaussian regression model.


<p id="csato-02">
L.&nbsp;Csat&oacute;.
<a href="http://www.cs.ubbcluj.ro/%7Ecsatol/publications/thesis.pdf"><b>Gaussian Processes - Iterative Sparse Approximations</b></a>.
PhD thesis, Neural Computing Research Group, Aston University, 2002.<p
  class="c"><b> Abstract:</b> In recent years there has been an increased
  interest in applying non-parametric methods to real-world problems.
  Significant research has been devoted to Gaussian processes (GPs) due to
  their increased flexibility when compared with parametric models. These
  methods use Bayesian learning, which generally leads to analytically
  intractable posteriors. This thesis proposes a two-step solution to construct
  a probabilistic approximation to the posterior. In the first step we adapt
  the Bayesian online learning to GPs: the final approximation to the posterior
  is the result of propagating the first and second moments of intermediate
  posteriors obtained by combining a new example with the previous
  approximation. The propagation of functional forms is solved by showing the
  existence of a parametrisation to posterior moments that uses combinations of
  the kernel function at the training points, transforming the Bayesian online
  learning of functions into a parametric formulation. The drawback is the
  prohibitive quadratic scaling of the number of parameters with the size of
  the data, making the method inapplicable to large datasets. The second step
  solves the problem of the exploding parameter size and makes GPs applicable
  to arbitrarily large datasets. The approximation is based on a measure of
  distance between two GPs, the KL-divergence between GPs. This second
  approximation is with a constrained GP in which only a small subset of the
  whole training dataset is used to represent the GP. This subset is called the
  Basis Vector, or BV set and the resulting GP is a sparse approximation to the
  true posterior. As this sparsity is based on the KL-minimisation, it is
  probabilistic and independent of the way the posterior approximation from the
  first step is obtained. We combine the sparse approximation with an extension
  to the Bayesian online algorithm that allows multiple iterations for each
  input and thus approximating a batch solution. The resulting sparse learning
  algorithm is a generic one: for different problems we only change the
  likelihood. The algorithm is applied to a variety of problems and we examine
  its performance both on more classical regression and classification tasks
  and to the data-assimilation and a simple density estimation problems.</p>



</p>

<p id="csato-fokoue-etal-00">
L.&nbsp;Csat&oacute;, E.&nbsp;Fokou&eacute;, M.&nbsp;Opper, B.&nbsp;Schottky, and O.&nbsp;Winther.
<a href="http://books.nips.cc/papers/files/nips12/0251.pdf"><b>Efficient
  approaches to Gaussian process classification</b></a>.
In S.&nbsp;A. Solla, T.&nbsp;K. Leen, and K.-R. M&uuml;ller, editors, <em>Advances in
  Neural Information Processing Systems 12</em>, Cambridge, MA, 2000. The MIT
  Press.<p class="c"><b> Abstract:</b> We present three simple approximations
  for the calculation of the posterior mean in Gaussian Process classification.
  The first two methods are related to mean field ideas known in Statistical
  Physics. The third approach is based on Bayesan online approach which was
  motivated by recent results in the Statistical Mechanics of Neural Networks.
  We present simulation results showing: 1. that the mean field Bayesian
  evidence may be used for hyperparameter tuning and 2. that the online
  approach may achieve a low training error fast.</p>



</p>

<p id="csato-opper-01">
L.&nbsp;Csat&oacute; and M.&nbsp;Opper.
<a href="http://books.nips.cc/papers/files/nips13/CsatoOpper.pdf"><b>Sparse
  representation for Gaussian process models</b></a>.
In T.&nbsp;K. Leen, T.&nbsp;G. Dietterich, and V.&nbsp;Tresp, editors, <em>Advances in Neural
  Information Processing Systems 13</em>, Cambridge, MA, 2001. The MIT
  Press.<p class="c"><b> Abstract:</b> We develop an approach for a sparse
  representation for Gaussian Process (GP) models in order to overcome the
  limitations of GPs caused by large data sets. The method is based on a
  combination of a Bayesian online algorithm together with a sequential
  construction of a relevant subsample of the data which fully specifies the
  prediction of the model. Experimental results on toy examples and large
  real-world datasets indicate that efficiency of the approach.</p>



</p>

<p id="csato-opper-02">
L.&nbsp;Csat&oacute; and M.&nbsp;Opper.
<b>Sparse online Gaussian processes</b>.
<em>Neural Computation</em>, 14(2):641-669, 2002.<p class="c"><b>
  Abstract:</b> We develop an approach for sparse representations of gaussian
  process (GP) models (which are Bayesian types of kernel machines) in order to
  overcome their limitations for large data sets. The method is based on a
  combination of a Bayesian on-line algorithm, together with a sequential
  construction of a relevant subsample of the data that fully specifies the
  prediction of the GP model. By using an appealing parameterization and
  projection techniques in a reproducing kernel Hilbert space, recursions for
  the effective parameters and a sparse gaussian approximation of the posterior
  process are obtained. This allows for both a propagation of predictions and
  Bayesian error measures. The significance and robustness of our approach are
  demonstrated on a variety of experiments.</p>



</p>

<p id="ferrari-trecate-opper-99">
G.&nbsp;Ferrari&nbsp;Trecate, C.&nbsp;K.&nbsp;I. Williams, and M.&nbsp;Opper.
<a href="http://books.nips.cc/papers/files/nips11/0218.pdf"><b>Finite-dimensional approximation of Gaussian processes</b></a>.
In M.&nbsp;S. Kearns, S.&nbsp;A. Solla, and D.&nbsp;A. Cohn, editors, <em>Advances in Neural
  Information Processing Systems 11</em>, pages 218-224. The MIT Press,
  1999.



</p>

<p id="gibbs-97">
M.&nbsp;N. Gibbs.
<a href="http://www.inference.phy.cam.ac.uk/mng10/GP/thesis.ps.gz"><b>Bayesian
  Gaussian Processes for Regression and Classification</b></a>.
PhD thesis, Department of Physics, University of Cambridge, 1997.<p
  class="c"><b> Abstract:</b> Bayesian inference offers us a powerful tool with
  which to tackle the problem of data modelling. However the performance of
  Bayesian methods is crucially dependent on being able to find good models for
  our data. The principal focus of this thesis is the development of models
  based on Gaussian process priors. Such models, which can be thought of as the
  infinite extension of several existing finite models have the flexibility to
  model complex phenomena while being mathematically simple. In thesis, I
  present a review of the theory of Gaussian processes and their covariance
  functions and demonstrate how they fit into the Bayesian framework. The
  efficient implementation of a Gaussian process is discussed with particular
  reference to approximate methods for matrix inversion based on the work of
  Skilling (1993). Several regression problems are examined. Non-stationary
  covariance functions are developed for the regression of neuron spike data
  and the use of Gaussian processes to model the potential energy surfaces of
  weakly bound molecules is discussed. Classification methods based on Gaussian
  processes are implemented using variational methods. Existing bounds
  (Jaakkola and Jordan 1996) for the sigmoid function are used to tackle binary
  problems and multi-dimensional bounds on the softmax function are presented
  for the multiple class case. The performance of the variational classifier is
  compared with that of other methods using the CRABS and PIMA datasets (Ripley
  1996) and the problem of predicting the cracking of welds based on their
  chemical composition is also investigated. The theoretical calculation of the
  density of states of crystal structures is discussed in detail. Three
  possible approaches to the problem are described based on free energy
  minimization, Gaussian processes and the theory of random matrices. Results
  from these approaches are compared with the state-of-the-art techniques
  (Pickard 1997).</p>



</p>

<p id="gibbs-mackay-97">
M.&nbsp;N. Gibbs and D.&nbsp;J.&nbsp;C. MacKay.
<a href="http://www.inference.phy.cam.ac.uk/mng10/GP/gpros.ps.gz"><b>Efficient
  implementation of Gaussian processes</b></a>.
Technical report, Department of Physics, Cavendish Laboratory, Cambridge
  University, 1997.<p class="c"><b> Abstract:</b> Neural networks and Bayesian
  inference provide a useful framework within which to solve regression
  problems. However their parameterisation means that the Bayesian analysis of
  neural networks can be difficult. In this paper, we investigate a method for
  regression using Gaussian Process Priors which allows exact Bayesian analysis
  using matrix manipulation for fixed values of hyperparameters. We discuss in
  detail the workings of the method and we detail a range of mathematical and
  numerical techniques that are useful in applying Gaussian Processes to
  general problems including efficient approximate matrix inversion methods
  developed by Skilling.</p>



</p>

<p id="keerthi-chu-06">
S.&nbsp;S. Keerthi and W.&nbsp;Chu.
<a href="http://books.nips.cc/papers/files/nips18/NIPS2005_0446.pdf"><b>A
  matching pursuit approach to sparse Gaussian process regression</b></a>.
In <em>Advances in Neural Information Processing Systems 18</em>, 2006.<p
  class="c"><b> Abstract:</b> In this paper we propose a new basis selection
  criterion for building sparse GP regression models that provides promising
  gains in accuracy as well as efficiency over previous methods. Our algorithm
  is much faster than that of Smola and Bartlett, while, in generalization it
  greatly outperforms the information gain approach proposed by Seeger et al,
  especially on the quality of predictive distributions.</p>



</p>

<p id="lawrence-seeger-herbrich-03">
N.&nbsp;Lawrence, M.&nbsp;Seeger, and R.&nbsp;Herbrich.
<a href="http://books.nips.cc/papers/files/nips15/AA16.pdf"><b>Fast sparse
  Gaussian process methods: The informative vector machine</b></a>.
In S.&nbsp;Becker, S.&nbsp;Thrun, and K.&nbsp;Obermayer, editors, <em>Advances in Neural
  Information Processing Systems 15</em>, pages 609-616, Cambridge, MA, 2003.
  The MIT Press.



</p>

<p id="minka-01a">
T.&nbsp;P. Minka.
<b>A Family of Algorithms for Approximate Bayesian Inference</b>.
PhD thesis, Department of Electrical Engineering and Computer Science, MIT,
  2001.



</p>

<p id="minka-01b">
T.&nbsp;P. Minka.
<b>Expectation propagation for approximate Bayesian inference</b>.
In J.&nbsp;S. Breese and D.&nbsp;Koller, editors, <em>Proceedings of the 17th Conference
  in Uncertainty in Artificial Intelligence</em>, pages 362-369. Morgan
  Kaufmann, 2001.



</p>

<p id="neal-97">
R.&nbsp;M. Neal.
<a href="http://www.cs.toronto.edu/~radford/ftp/mc-gp.pdf"><b>Monte Carlo
  implementation of Gaussian process models for Bayesian regression and
  classification</b></a>.
Technical Report 9702, Department of Statistics, University of Toronto, 1997.<p
  class="c"><b> Abstract:</b> Gaussian processes are a natural way of defining
  prior distributions over functions of one or more input variables. In a
  simple nonparametric regression problem, where such a function gives the mean
  of a Gaussian distribution for an observed response, a Gaussian process model
  can easily be implemented using matrix computations that are feasible for
  datasets of up to about a thousand cases. Hyperparameters that define the
  covariance function of the Gaussian process can be sampled using Markov chain
  methods. Regression models where the noise has a t distribution and logistic
  or probit models for classification applications can be implemented by
  sampling as well for latent values underlying the observations. Software is
  now available that implements these methods using covariance functions with
  hierarchical parameterizations. Models defined in this way can discover
  high-level properties of the data, such as which inputs are relevant to
  predicting the response.</p>



</p>

<p id="neal-98">
R.&nbsp;M. Neal.
<b>Regression and classification using Gaussian process priors</b>.
In J.&nbsp;M. Bernardo, J.&nbsp;O. Berger, A.&nbsp;P. Dawid, and A.&nbsp;F.&nbsp;M. Smith, editors,
  <em>Bayesian Statistics 6</em>, pages 475-501. Oxford University Press,
  1998.<p class="c"><b> Abstract:</b> Gaussian processes are a natural way of
  specifying prior distributions over functions of one or more input variables.
  When such a function defines the mean response in a regression model with
  Gaussian errors, inference can be done using matrix computations, which are
  feasible for datasets of up to about a thousand cases. The covariance
  function of the Gaussian process can be given a hierarchical prior, which
  allows the model to discover high-level properties of the data, such as which
  inputs are relevant to predicting the response. Inference for these
  covariance hyperparameters can be done using Markov chain sampling.
  Classification models can be defined using Gaussian processes for underlying
  latent values, which can also be sampled within the Markov chain. Gaussian
  processes are in my view the simplest and most obvious way of defining
  flexible Bayesian regression and classification models, but despite some past
  usage, they appear to have been rather neglected as a general-purpose
  technique. This may be partly due to a confusion between the properties of
  the function being modeled and the properties of the best predictor for this
  unknown function.</p>



</p>

<p id="opper-winther-00">
M.&nbsp;Opper and O.&nbsp;Winther.
<b>Gaussian processes for classification: Mean-field algorithms</b>.
<em>Neural Computation</em>, 12(11):2655-2684, 2000.<p class="c"><b>
  Abstract:</b> We derive a mean-field algorithm for binary classification with
  gaussian processes that is based on the TAP approach originally proposed in
  statistical physics of disordered systems. The theory also yields an
  approximate leave-one-out estimator for the generalization error, which is
  computed with no extra computational cost. We show that from the TAP
  approach, it is possible to derive both a simpler "naive" mean-field theory
  and support vector machines (SVMs) as limiting cases. For both mean-field
  algorithms and support vector machines, simulation results for three small
  benchmark data sets are presented. They show that one may get
  state-of-the-art performance by using the leave-one-out estimator for model
  selection and the built-in leave-one-out estimators are extremely precise
  when compared to the exact leave-one-out estimate. The second result is taken
  as strong support for the internal consistency of the mean-field
  approach.</p>



</p>

<p id="opper-winther-99">
M.&nbsp;Opper and O.&nbsp;Winther.
<a href="http://books.nips.cc/papers/files/nips11/0309.pdf"><b>Mean field
  methods for classification with Gaussian processes</b></a>.
In M.&nbsp;S. Kearns, S.&nbsp;A. Solla, and D.&nbsp;A. Cohn, editors, <em>Advances in Neural
  Information Processing Systems 11</em>, pages 309-315, Cambridge, MA, 1999.
  The MIT Press.<p class="c"><b> Abstract:</b> We discuss the application of
  TAP mean field methods known from the Statistical Mechanics of diordered
  systems to Bayesian classification models with Gaussian processes. In
  contrast to previous approaches, no knowledge about the distribution of
  inputs is needed. Simulation results for the Sonar data set are given.</p>



</p>

<p id="quinonero-candela-04">
J.&nbsp;Qui&ntilde;onero-Candela.
<b>Learning with Uncertainty-Gaussian Processes and Relevance Vector
  Machines</b>.
PhD thesis, Informatics and Mathematical Modelling, Technical Univeristy of
  Denmark, 2004.



</p>

<p id="quinonero-candela-rasmussen-05">
J.&nbsp;Qui&ntilde;onero-Candela and C.&nbsp;E. Rasmussen.
<a href="http://jmlr.csail.mit.edu/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf"><b>A unifying view of sparse approximate Gaussian process
  regression</b></a>.
<em>Journal of Machine Learning Research</em>, 6:1935-1959, 12 2005.<p
  class="c"><b> Abstract:</b> We provide a new unifying view, including all
  existing proper probabilistic sparse approximations for Gaussian process
  regression. Our approach relies on expressing the effective prior which the
  methods are using. This allows new insights to be gained, and highlights the
  relationship between existing methods. It also allows for a clear
  theoretically justified ranking of the closeness of the known approximations
  to the corresponding full GPs. Finally we point directly to designs of new
  better sparse approximations, combining the best of the existing strategies,
  within attractive computational constraints.</p>

<p class="c"><b> Comment:</b> Errata: there is a mistake in computational
  complexity for taking the derivative of the log marginal likelihood wrt all
  elements of X<sub>u</sub>, stated as O(dnm<sup>2</sup>), in line -12 on page
  1953. A more careful derivation reduces this to O(dnm+nm<sup>2</sup>).</p>

</p>

<p id="quinonero-candela-wither-03">
Joaquin Qui&ntilde;onero-Candela and Ole Winther.
<a href="http://books.nips.cc/papers/files/nips15/AA66.pdf"><b>Incremental
  Gaussian processes</b></a>.
In S.&nbsp;Becker, S.&nbsp;Thrun, and K.&nbsp;Obermayer, editors, <em>Advances in Neural
  Information Processing Systems 15</em>, Cambridge, MA, 2003. The MIT Press.



</p>

<p id="schwaighofer-tresp-03">
A.&nbsp;Schwaighofer and V.&nbsp;Tresp.
<a href="http://books.nips.cc/papers/files/nips15/AA60.pdf"><b>Transductive and
  inductive methods for approximate Gaussian process regression</b></a>.
In S.&nbsp;Becker, S.&nbsp;Thrun, and K.&nbsp;Obermayer, editors, <em>Advances in Neural
  Information Processing Systems 15</em>. The MIT Press, 2003.<p
  class="c"><b> Abstract:</b> Gaussian process regression allows a simple
  analytical treatment of exact Bayesian inference and has been found to
  provide good performance, yet scales badly with the number of training data.
  In this paper we compare several approaches towards scaling Gaussian
  processes regression to large data sets: the subset of representers method,
  the reduced rank approximation, online Gaussian processes, and the Bayesian
  committee machine. Furthermore we provide theoretical insight into some of
  our experimental results. We found that subset of representers methods can
  give good and particularly fast predictions for data sets with high and
  medium noise levels. On complex low noise data sets, the Bayesian committee
  machine achieves significantly better accuracy, yet at a higher computational
  cost.</p>



</p>

<p id="seeger-03">
M.&nbsp;Seeger.
<b>Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error
  Bounds and Sparse Approximations</b>.
PhD thesis, Institute of Adaptive and Neural Computation, University of
  Edinburgh, 2003.



</p>

<p id="seeger-williams-lawrence-03">
M.&nbsp;Seeger, C.&nbsp;K.&nbsp;I. Williams, and N.&nbsp;Lawrence.
<a href="http://www.kyb.tuebingen.mpg.de/bs/people/seeger/papers/aistats03-final.pdf"><b>Fast forward selection to speed up sparse Gaussian process
  regression</b></a>.
In C.M. Bishop and B.&nbsp;J. Frey, editors, <em>Proceedings of the Ninth
  International Workshop on Artificial Intelligence and Statistics</em>.
  Society for Artificial Intelligence and Statistics, 2003.



</p>

<p id="shen-ng-seeger-06">
Y.&nbsp;Shen, A.&nbsp;Ng, and M.&nbsp;Seeger.
<a href="http://books.nips.cc/papers/files/nips18/NIPS2005_0687.pdf"><b>Fast
  Gaussian process regression using KD-trees</b></a>.
In Y.&nbsp;Weiss, B.&nbsp;Sch&ouml;lkopf, and J.&nbsp;Platt, editors, <em>Advances in Neural
  Information Processing Systems 18</em>, pages 1227-1234. The MIT Press,
  Cambridge, MA, 2006.



</p>

<p id="smola-bartlett-01">
A.&nbsp;J. Smola and P.&nbsp;L. Bartlett.
<a href="http://books.nips.cc/papers/files/nips13/SmolaBartlett.pdf"><b>Sparse
  greedy Gaussian process regression</b></a>.
In T.&nbsp;K. Leen, T.&nbsp;G. Diettrich, and V.&nbsp;Tresp, editors, <em>Advances in Neural
  Information Processing Systems 13</em>, pages 619-625. The MIT Press,
  2001.



</p>

<p id="snelson-07">
E.&nbsp;Snelson.
<a href="http://www.gatsby.ucl.ac.uk/~snelson/thesis.pdf"><b>Flexible and
  efficient Gaussian process models for machine learning</b></a>.
PhD thesis, Gatsby Computational Neuroscience Unit, University College London,
  2007.<p class="c"><b> Abstract:</b> Gaussian process (GP) models are widely
  used to perform Bayesian nonlinear regression and classification닶asks that
  are central to many machine learning problems. A GP is nonparametric, meaning
  that the complexity of the model grows as more data points are received.
  Another attractive feature is the behaviour of the error bars. They naturally
  grow in regions away from training data where we have high uncertainty about
  the interpolating function.<br> In their standard form GPs have several
  limitations, which can be divided into two broad categories: computational
  difficulties for large data sets, and restrictive modelling assumptions for
  complex data sets. This thesis addresses various aspects of both of these
  problems.<br> The training cost for a GP has O(N<sup>3</sup>) complexity,
  where N is the number of training data points. This is due to an inversion of
  the N 칑 N covariance matrix. In this thesis we develop several new
  techniques to reduce this complexity to O(NM<sup>2</sup>), where M is a user
  chosen number much smaller than N. The sparse approximation we use is based
  on a set of M 딿seudo-inputs which are optimised together with
  hyperparameters at training time. We develop a further approximation based on
  clustering inputs that can be seen as a mixture of local and global
  approximations.<br> Standard GPs assume a uniform noise variance. We use our
  sparse approximation described above as a way of relaxing this assumption. By
  making a modification of the sparse covariance function, we can model input
  dependent noise. To handle high dimensional data sets we use supervised
  linear dimensionality reduction. As another extension of the standard GP, we
  relax the Gaussianity assumption of the process by learning a nonlinear
  transformation of the output space. All these techniques further increase the
  applicability of GPs to real complex data sets.<br> We present empirical
  comparisons of our algorithms with various competing techniques, and suggest
  problem dependent strategies to follow in practice.</p>

<p class="c"><b> Comment:</b> See also the matlab implementation <a
  href="#code">spgp</a>.</p>

</p>

<p id="snelson-ghahramani-06">
E.&nbsp;Snelson and Z&nbsp;Ghahramani.
<a href="http://books.nips.cc/papers/files/nips18/NIPS2005_0543.pdf"><b>Sparse
  Gaussian processes using pseudo-inputs</b></a>.
In Y.&nbsp;Weiss, B.&nbsp;Sch&ouml;lkopf, and J.&nbsp;Platt, editors, <em>Advances in Neural
  Information Processing Systems 18</em>, pages 1259-1266. The MIT Press,
  Cambridge, MA, 2006.<p class="c"><b> Abstract:</b> We present a new Gaussian
  process (GP) regression model whose covariance is parameterized by the the
  locations of M pseudo-input points, which we learn by a gradient based
  optimization. We take M&lt;&lt;N, where N is the number of real data points,
  and hence obtain a sparse regression method which has O(M<sup>2</sup>N)
  training cost and O(M<sup>2</sup>) prediction cost per test case. We also
  find hyperparameters of the covariance function in the same joint
  optimization. The method can be viewed as a Bayesian regression model with
  particular input dependent noise. The method turns out to be closely related
  to several other sparse GP approaches, and we discuss the relation in detail.
  We finally demonstrate its performance on some large data sets, and make a
  direct comparison to other sparse GP methods. We show that our method can
  match full GP performance with small M, i.e. very sparse solutions, and it
  significantly outperforms other approaches in this regime.</p>



</p>

<p id="tresp-00">
V.&nbsp;Tresp.
<a href="http://www.tresp.org/papers/bcm6.pdf"><b>A Bayesian committee
  machine</b></a>.
<em>Neural Computation</em>, 12(11):2719-2741, 2000.<p class="c"><b>
  Abstract:</b> The Bayesian committee machine (BCM) is a novel approach to
  combining estimators that were trained on different data sets. Although the
  BCM can be applied to the combination of any kind of estimators, the main
  foci are gaussian process regression and related systems such as
  regularization networks and smoothing splines for which the degrees of
  freedom increase with the number of training data. Somewhat surprisingly, we
  find that the performance of the BCM improves if several test points are
  queried at the same time and is optimal if the number of test points is at
  least as large as the degrees of freedom of the estimator. The BCM also
  provides a new solution for on-line learning with potential applications to
  data mining. We apply the BCM to systems with fixed basis functions and
  discuss its relationship to gaussian process regression. Finally, we show how
  the ideas behind the BCM can be applied in a non-Bayesian setting to extend
  the input-dependent combination of estimators.</p>



</p>

<p id="wainwright-sudderth-willsky-01">
M.&nbsp;J. Wainwright, E.&nbsp;B. Sudderth, and A.&nbsp;S. Willsky.
<a href="http://books.nips.cc/papers/files/nips13/WainwrightSudderthWillsky.pdf"><b>Tree-based modeling and estimation of Gaussian processes on graphs with
  cycles</b></a>.
In T.&nbsp;K. Leen, T.&nbsp;G. Dietterich, and V.&nbsp;Tresp, editors, <em>Advances in Neural
  Information Processing Systems 13</em>, Cambridge, MA, 2001. The MIT Press.



</p>

<p id="williams-rasmussen-etal-02">
C.&nbsp;K.&nbsp;I. Williams, C.&nbsp;E. Rasmussen, A.&nbsp;Schwaighofer, and V.&nbsp;Tresp.
<a href="http://www.dai.ed.ac.uk/homes/ckiw/online_pubs.html"><b>Observations
  on the Nystr&ouml;m method for Gaussian process prediction</b></a>.
Technical report, University of Edinburgh, 2002.



</p>

<p id="williams-seeger-01">
C.&nbsp;K.&nbsp;I. Williams and M.&nbsp;Seeger.
<a href="http://books.nips.cc/papers/files/nips13/WilliamsSeeger.pdf"><b>Using
  the Nystr&ouml;m method to speed up kernel machines</b></a>.
In T.&nbsp;K. Leen, T.&nbsp;G. Diettrich, and V.&nbsp;Tresp, editors, <em>Advances in Neural
  Information Processing Systems 13</em>, pages 682-688. The MIT Press,
  2001.



</p>

<p id="winther-88">
O.&nbsp;Winther.
<b>Bayesian Mean Field Algorithms for Neural Networks and Gaussian
  Processes</b>.
PhD thesis, University of Copenhagen, 1988.



</p>

<p id="wood-jiang-tanner-02">
S.&nbsp;A. Wood, W.&nbsp;Jiang, and M.&nbsp;Tanner.
<b>Bayesian mixture of splines for spatially adaptive nonparametric
  regression</b>.
<em>Biometrika</em>, 89(3):513-528, 2002.



</p>


<br>

<h3 id="stats">References from the Statistics Community</h3>

Gaussian processes have a long history in the statistics community. They have
been particularly well developed in geostatistics under the name of
<em>kriging</em>. The papers have been grouped because they are written using a
common terminology, and have slightly different focus from typical machine
learning papers,


<p id="barry-86">
D.&nbsp;Barry.
<b>Nonparametric Bayesian regression</b>.
<em>The Annals of Statistics</em>, 14(3):934-953, 1986.



</p>

<p id="blight-ott-75">
B.&nbsp;J.&nbsp;N. Blight and L.&nbsp;Ott.
<b>A Bayesian approach to model inadequacy for polynomial regression</b>.
<em>Biometrika</em>, 62(1):79-88, 1975.



</p>

<p id="cressie-93">
N.&nbsp;A.&nbsp;C. Cressie.
<b>Statistics for Spatial Data</b>.
John Wiley &amp; Sons, New York, 1993.



</p>

<p id="diggle-tawn-moyeed-98">
P.&nbsp;J. Diggle, J.&nbsp;A. Tawn, and R.&nbsp;A. Moyeed.
<b>Model-based geostatistics (with discussion)</b>.
<em>Applied Statistics</em>, 47:299-350, 1998.



</p>

<p id="handcock-stein-93">
M.&nbsp;S. Handcock and M.&nbsp;L. Stein.
<b>A Bayesian analysis of kriging</b>.
<em>Technometrics</em>, 35(4):403-410, 1993.



</p>

<p id="kent-mardia-94">
J.&nbsp;T. Kent and K.&nbsp;V. Mardia.
<b>The link between Kriging and thin-plate splines</b>.
In F.&nbsp;P. Kelly, editor, <em>Probability, Statsitics and Optimization</em>,
  pages 325-339. Wiley, 1994.



</p>

<p id="krige-51">
D.&nbsp;G. Krige.
<b>A statistical approach to some basic mine valuation problems on the
  Witwatersrand</b>.
<em>Journal of the Chemical, Metallurgical and Mining Society of South
  Africa</em>, 52(6):119-139, 1951.



</p>

<p id="laslett-94">
G.&nbsp;M. Laslett.
<b>Kriging and splines: An empirical comparison of their predictive performance
  in some applications</b>.
<em>Journal of the American Statistical Association</em>, 89(426):391-409,
  1994.



</p>

<p id="ohagan-78">
A.&nbsp;O'Hagan.
<b>Curve fitting and optimal design for prediction</b>.
<em>Journal of the Royal Statistical Society, Series B</em>, 40(1):1-42, 1978.



</p>

<p id="rue-held-05">
H.&nbsp;Rue and L.&nbsp;Held.
<a href="http://www.math.ntnu.no/~hrue/GMRF-book"><b>Gaussian Markov Random
  Fields: Theory and Applications</b></a>, volume 104 of <em>Monographs on
  Statistics and Applied Probability</em>.
Chapman &amp; Hall, London, 2005.



</p>

<p id="rue-martino-chopin-09">
H.&nbsp;Rue, S.&nbsp;Martino, and N.&nbsp;Chopin.
<a href="http://www.rss.org.uk/pdf/RueOct2008.pdf"><b>Approximate Bayesian
  inference for latent Gaussian models by using integrated nested Laplace
  approximations</b></a>.
<em>Journal of the Royal Statistical Society B</em>, 71, 2009.
PREPRINT.<p class="c"><b> Abstract:</b> Structured additive regression models
  are perhaps the most commonly used class of models in statistical
  applications. It includes, among others, (generalized) linear models,
  (generalized) additive models, smoothing spline models, state space models,
  semiparametric regression, spatial and spatiotemporal models, log-Gaussian
  Cox processes and geostatistical and geoadditive models. We consider
  approximate Bayesian inference in a popular subset of structured additive
  regression models, <em>latent Gaussian models</em>, where the latent field is
  Gaussian, controlled by a few hyperparameters and with non-Gaussian response
  variables. The posterior marginals are not available in closed form owing to
  the non-Gaussian response variables. For such models, Markov chain Monte
  Carlo methods can be implemented, but they are not without problems, in terms
  of both convergence and computational time. In some practical applications,
  the extent of these problems is such that Markov chain Monte Carlo sampling
  is simply not an appropriate tool for routine analysis.We show that, by using
  an integrated nested Laplace approximation and its simplified version, we can
  directly compute very accurate approximations to the posterior marginals. The
  main benefit of these approximations is computational: where Markov chain
  Monte Carlo algorithms need hours or days to run, our approximations provide
  more precise estimates in seconds or minutes. Another advantage with our
  approach is its generality, which makes it possible to perform Bayesian
  analysis in an automatic, streamlined way, and to compute model comparison
  criteria and various predictive measures so that models can be compared and
  the model under study can be challenged.</p>



</p>

<p id="silverman-84">
B.&nbsp;W. Silverman.
<b>Spline smoothing: The equivalent variable kernel method</b>.
<em>Annals of Statistics</em>, 12(3):898-916, 1984.



</p>

<p id="silverman-85">
B.&nbsp;W. Silverman.
<b>Some aspects of the spline smoothing approach to non-parametric regression
  curve fitting (with discussion)</b>.
<em>Journal of the Royal Statistical Society, Series B</em>, 47(1):1-52, 1985.



</p>

<p id="stein-91">
M.&nbsp;L. Stein.
<b>A kernel approximation to the kriging predictor of a spatial process</b>.
<em>Ann. Inst. Statist. Math</em>, 43(1):61-75, 1991.



</p>

<p id="stein-99">
M.&nbsp;L. Stein.
<b>Interpolation of Spatial Data</b>.
Springer, New York, 1999.



</p>

<p id="wahba-78">
G.&nbsp;Wahba.
<b>Improper priors, spline smoothing and the problem of guarding against model
  errors in regression</b>.
<em>Journal of the Royal Statistical Society, Series B</em>, 40:364-372, 1978.



</p>

<p id="yakowitz-szidarovszky-85">
S.&nbsp;J. Yakowitz and F.&nbsp;Szidarovszky.
<b>A comparison of kriging with nonparametric regression methods</b>.
<em>Journal of Multivariate Analysis</em>, 16:21-53, 1985.



</p>

<p id="young-77">
A.&nbsp;S. Young.
<b>A Bayesian approach to prediction using polynomials</b>.
<em>Biometrika</em>, 64(2):309-317, 1977.



</p>


<br>

<h3 id="cons">Consistency, Learning Curves and Bounds</h3>

The papers in this section give theoretical results on <em>learning
curves</em>, which describe the expected generalization performance as a
function of the number of training cases. Consistency addresses the question
whether the solution approaches the true data generating process in the limit
of infinitely many training examples.


<p id="choi-schervish-04">
T.&nbsp;Choi and M.&nbsp;J. Schervish.
<a href="http://www.stat.cmu.edu/tr/tr809/tr809.html"><b>Posterior consistency
  in nonparametric regression problems under Gaussian process priors</b></a>.
Technical Report 809, Department of Statistics, CMU, 2004.



</p>

<p id="kakade-seeger-foster-06">
S.&nbsp;Kakade, M.&nbsp;Seeger, and P.&nbsp;Foster.
<a href="http://books.nips.cc/papers/files/nips18/NIPS2005_0170.pdf"><b>Worst-case bounds for Gaussian process models</b></a>.
In <em>Advances in Neural Information Processing Systems 18</em>. The MIT
  Press, 2006.<p class="c"><b> Abstract:</b> We present a competitive analysis
  of some non-parametric Bayesian algorithms in a worst-case online learning
  setting, where no probabilistic assumptions about the generation of the data
  are made. We consider models which use a Gaussian process prior (over the
  space of all functions) and provide bounds on the regret (under the log loss)
  for commonly used non-parametric Bayesian algorithms-including Gaussian
  regression and logistic regression-which show how these algorithms can
  perform favorably under rather general conditions. These bounds explicitly
  handle the infinite dimensionality of these non-parametric classes in a
  natural way. We also make formal connections to the minimax and minimum
  description length (MDL) framework. Here, we show precisely how Bayesian
  Gaussian regression is a minimax strategy.</p>



</p>

<p id="malzahn-opper-02">
D.&nbsp;Malzahn and M.&nbsp;Opper.
<a href="http://books.nips.cc/papers/files/nips14/LT25.pdf"><b>A variational
  approach to learning curves</b></a>.
In T.&nbsp;G. Diettrich, S.&nbsp;Becker, and Z.&nbsp;Ghahramani, editors, <em>Advances in
  Neural Information Processing Systems 14</em>. The MIT Press, 2002.



</p>

<p id="opper-vivarelli-99">
M.&nbsp;Opper and F.&nbsp;Vivarelli.
<a href="http://books.nips.cc/papers/files/nips11/0302.pdf"><b>General bounds
  on Bayes errors for regression with Gaussian processes</b></a>.
In M.&nbsp;S. Kearns, S.&nbsp;A. Solla, and D.&nbsp;A. Cohn, editors, <em>Advances in Neural
  Information Processing Systems 11</em>, pages 302-308. The MIT Press,
  1999.



</p>

<p id="sollich-99">
P.&nbsp;Sollich.
<a href="http://www.mth.kcl.ac.uk/~psollich/papers/GaussianProcLearningCurveICANNIX.ps.gz"><b>Approximate learning curves for Gaussian processes</b></a>.
In <em>ICANN99 - Ninth International Conference on Artificial Neural
  Networks</em>, pages 437-442, London, 1999. IEE.



</p>

<p id="sollich-99b">
P.&nbsp;Sollich.
<a href="http://books.nips.cc/papers/files/nips11/0344.pdf"><b>Learning curves
  for Gaussian processes</b></a>.
In M.&nbsp;S. Kearns, S.&nbsp;A. Solla, and D.&nbsp;A. Cohn, editors, <em>Neural Information
  Processing Systems, Vol. 11</em>. The MIT Press, 1999.



</p>

<p id="sollich-halees-02">
P.&nbsp;Sollich and A.&nbsp;Halees.
<b>Learning curves for Gaussian process regression: Approximations and
  bounds</b>.
<em>Neural Computation</em>, 14:1393-1428, 2002.<p class="c"><b> Abstract:</b>
  We consider the problem of calculating learning curves (i.e., average
  generalization performance) of gaussian processes used for regression. On the
  basis of a simple expression for the generalization error, in terms of the
  eigenvalue decomposition of the covariance function, we derive a number of
  approximation schemes. We identify where these become exact and compare with
  existing bounds on learning curves; the new approximations, which can be used
  for any input space dimension, generally get substantially closer to the
  truth. We also study possible improvements to our approximations. Finally, we
  use a simple exactly solvable learning scenario to show that there are limits
  of principle on the quality of approximations and bounds expressible solely
  in terms of the eigenvalue spectrum of the covariance function.</p>



</p>

<p id="steinwart-05">
I&nbsp;Steinwart.
<b>Consistency of support vector machines and other regularized kernel
  classifiers</b>.
<em>IEEE Trans. on Information Theory</em>, 51(1):128-142, 2005.



</p>

<p id="vaart-zanten-08">
A.&nbsp;W. van&nbsp;der Vaart and J.&nbsp;H. van Zanten.
<a href="http://projecteuclid.org/euclid.aos/1211819570"><b>Rates of
  contraction of posterior distributions based on Gaussian process
  priors</b></a>.
<em>Annals of Statistics</em>, 36(3):1435-1463, 2008.<p class="c"><b>
  Abstract:</b> We derive rates of contraction of posterior distributions on
  nonparametric or semiparametric models based on Gaussian processes. The rate
  of contraction is shown to depend on the position of the true parameter
  relative to the reproducing kernel Hilbert space of the Gaussian process and
  the small ball probabilities of the Gaussian process. We determine these
  quantities for a range of examples of Gaussian priors and in several
  statistical settings. For instance, we consider the rate of contraction of
  the posterior distribution based on sampling from a smooth density model when
  the prior models the log density as a (fractionally integrated) Brownian
  motion. We also consider regression with Gaussian errors and smooth
  classification under a logistic or probit link function combined with various
  priors.</p>



</p>

<p id="williams-vivarelli-00">
C.&nbsp;K.&nbsp;I. Williams and F.&nbsp;Vivarelli.
<b>Upper and lower bounds on the learning curve for Gaussian proccesses</b>.
<em>Machine Learning</em>, 40:77-102, 2000.<p class="c"><b> Abstract:</b> In
  this paper we introduce and illustrate non-trivial upper and lower bounds on
  the learning curves for one-dimensional Guassian Processes. The analysis is
  carried out emphasising the effects induced on the bounds by the smoothness
  of the random process described by the Modified Bessel and the Squared
  Exponential covariance functions. We present an explanation of the early,
  linearly-decreasing behavior of the learning curves and the bounds as well as
  a study of the asymptotic behavior of the curves. The effects of the noise
  level and the lengthscale on the tightness of the bounds are also
  discussed.</p>



</p>


<br>

<h3 id="rkhs">Reproducing Kernel Hilbert Spaces</h3>


<p id="aronszajn-50">
N.&nbsp;Aronszajn.
<b>Theory of reproducing kernels</b>.
<em>Transactions of the American Mathematical Society</em>, 68:337-404, 1950.



</p>

<p id="genton-01">
M.&nbsp;G. Genton.
<a href="http://www.jmlr.org/papers/volume2/genton01a/genton01a.pdf"><b>Classes
  of kernels for machine learning: A statistics perspective</b></a>.
<em>Journal of Machine Learning Research</em>, 2:299-312, 2001.<p
  class="c"><b> Abstract:</b> In this paper, we present classes of kernels for
  machine learning from a statistics perspective. Indeed, kernels are positive
  definite functions and thus also covariances. After discussing key properties
  of kernels, as well as a new formula to construct kernels, we present several
  important classes of kernels: anisotropic stationary kernels, isotropic
  stationary kernels, compactly supported kernels, locally stationary kernels,
  nonstationary kernels, and separable nonstationary kernels. Compactly
  supported kernels and separable nonstationary kernels are of prime interest
  because they provide a computational reduction for kernel-based methods. We
  describe the spectral representation of the various classes of kernels and
  conclude with a discussion on the characterization of nonlinear maps that
  reduce nonstationary kernels to either stationarity or local
  stationarity.</p>



</p>

<p id="kimeldorf-wahba-70">
G.&nbsp;S. Kimeldorf and G.&nbsp;Wahba.
<b>A correspondence between Bayesian estimation on stochastic processes and
  smoothing by splines</b>.
<em>The Annals of Mathematical Statistics</em>, 41(2):495-502, 1970.



</p>

<p id="wahba-90">
G.&nbsp;Wahba.
<b>Spline Models for Observational Data</b>, volume&nbsp;59.
Society for Industrial and Applied Mathematics, Philadelphia, 1990.



</p>


<br>

<h3 id="rl">Reinforcement Learning</h3>


<p id="deisenroth-peters-rasmussen-08">
M.&nbsp;P. Deisenroth, J.&nbsp;Peters, and C.&nbsp;E. Rasmussen.
<a href="http://mlg.eng.cam.ac.uk/marc/publications/acc2008_final_pub.pdf"><b>Approximate Dynamic Programming with Gaussian Processes</b></a>.
In <em>Proceedings of the 2008 American Control Conference (ACC 2008)</em>,
  pages 4480-4485, Seattle, WA, USA, June 2008.<p class="c"><b> Abstract:</b>
  In general, it is difficult to determine an optimal closed-loop policy in
  nonlinear control problems with continuous-valued state and control domains.
  Hence, approximations are often inevitable. The standard method of
  discretizing states and controls suffers from the curse of dimensionality and
  strongly depends on the chosen temporal sampling rate. In this paper, we
  introduce Gaussian process dynamic programming (GPDP) and determine an
  approximate globally optimal closed-loop policy. In GPDP, value functions in
  the Bellman recursion of the dynamic programming algorithm are modeled using
  Gaussian processes. GPDP returns an optimal state-feedback for a finite set
  of states. Based on these outcomes, we learn a possibly discontinuous
  closed-loop policy on the entire state space by switching between two
  independently trained Gaussian processes. A binary classifier selects one
  Gaussian process to predict the optimal control signal. We show that GPDP is
  able to yield an almost optimal solution to an LQ problem using few sample
  points. Moreover, we successfully apply GPDP to the underpowered pendulum
  swing up, a complex nonlinear control problem.</p>



</p>

<p id="deisenroth-rasmussen-peters-08">
M.&nbsp;P. Deisenroth, C.&nbsp;E. Rasmussen, and J.&nbsp;Peters.
<a href="http://mlg.eng.cam.ac.uk/marc/./publications/esann2008_final_pub.pdf"><b>Model-Based Reinforcement Learning with Continuous States and
  Actions</b></a>.
In <em>Proceedings of the 16th European Symposium on Artificial Neural Networks
  (ESANN 2008)</em>, pages 19-24, Bruges, Belgium, April 2008.<p class="c"><b>
  Abstract:</b> Finding an optimal policy in a reinforcement learning (RL)
  framework with continuous state and action spaces is challenging. Approximate
  solutions are often inevitable. GPDP is an approximate dynamic programming
  algorithm based on Gaussian process (GP) models for the value functions. In
  this paper, we extend GPDP to the case of unknown transition dynamics. After
  building a GP model for the transition dynamics, we apply GPDP to this model
  and determine a continuous-valued policy in the entire state space. We apply
  the resulting controller to the underpowered pendulum swing up. Moreover, we
  compare our results on this RL task to a nearly optimal discrete DP solution
  in a fully known environment.</p>



</p>

<p id="deisenroth-rasmussen-peters-09">
M.&nbsp;P. Deisenroth, C.&nbsp;E. Rasmussen, and J.&nbsp;Peters.
<a href="http://mlg.eng.cam.ac.uk/marc/publications/neurocomputing2009_preprint.pdf"><b>Gaussian Process Dynamic Programming</b></a>.
<em>Neurocomputing</em>, 72(7-9):1508-1524, March 2009.<p class="c"><b>
  Abstract:</b> Reinforcement learning (RL) and optimal control of systems with
  continuous states and actions require approximation techniques in most
  interesting cases. In this article, we introduce Gaussian process dynamic
  programming (GPDP), an approximate value-function based RL algorithm. We
  consider both a classic optimal control problem, where problem-specific prior
  knowledge is available, and a classic RL problem, where only very general
  priors can be used. For the classic optimal control problem, GPDP models the
  unknown value functions with Gaussian processes and generalizes dynamic
  programming to continuous-valued states and actions. For the RL problem, GPDP
  starts from a given initial state and explores he state space using Bayesian
  active learning. To design a fast learner, available data has to be used
  efficiently. Hence, we propose to learn probabilistic models of the a priori
  unknown transition dynamics and the value functions on the fly. In both
  cases, we successfully apply the resulting continuous-valued controllers to
  the under-actuated pendulum swing up and analyze the performances of the
  suggested algorithms. It turns out that GPDP uses data very efficiently and
  can be applied to problems, where classic dynamic programming would be
  cumbersome.</p>



</p>

<p id="engel-05a">
Y.&nbsp;Engel.
<a href="http://www.cs.ualberta.ca/~yaki/papers/thesis.pdf"><b>Algorithms and
  Representations for Reinforcement Learning</b></a>.
PhD thesis, Hebrew University, Jerusalem, Israel, April 2005.<p class="c"><b>
  Abstract:</b> Machine Learning is a field of research aimed at constructing
  intelligent machines that gain and improve their skills by learning and
  adaptation. As such, Machine Learning research addresses several classes of
  learning problems, including for instance, supervised and unsupervised
  learning. Arguably, the most ubiquitous and realistic class of learning
  problems, faced by both living creatures and artificial agents, is known as
  Reinforcement Learning. Reinforcement Learning problems are characterized by
  a long-term interaction between the learning agent and a dynamic, unfamiliar,
  uncertain, possibly even hostile environment. Mathematically, this
  interaction is modeled as a Markov Decision Process (MDP). Probably the most
  significant contribution of this thesis is in the introduction of a new class
  of Reinforcement Learning algorithms, which leverage the power of a
  statistical set of tools known as Gaussian Processes. This new approach to
  Reinforcement Learning offers viable solutions to some of the major
  limitations of current Reinforcement Learning methods, such as the lack of
  confidence intervals for performance predictions, and the difficulty of
  appropriately reconciling exploration with exploitation. Analysis of these
  algorithms and their relationship with existing methods also provides us with
  new insights into the assumptions underlying some of the most popular
  Reinforcement Learning algorithms to date.</p>



</p>

<p id="engel-mannor-meir-03">
Y.&nbsp;Engel, S.&nbsp;Mannor, and R.&nbsp;Meir.
<a href="http://www.hpl.hp.com/conferences/icml2003/papers/306.pdf"><b>Bayes
  meets Bellman: The Gaussian process approach to temporal difference
  learning</b></a>.
In T.&nbsp;Fawcett and N.&nbsp;Mishra, editors, <em>20th International Conference on
  Machine Learning</em>. AAAI Press, 2003.<p class="c"><b> Abstract:</b> We
  present a novel Bayesian approach to the problem of value function estimation
  in con- tinuous state spaces. We derne a probabilistic generative model for
  the value function by imposing a Gaussian prior over value functions and
  assuming a Gaussian noise model. Due to the Gaussian nature of the random
  processes involved, the posterior distribution of the value function is also
  Gaussian and is therefore described entirely by its mean and covariance. We
  derive exact expressions for the posterior process moments, and utilizing an
  ecient sequential sparsfication method, we describe an on-line algorithm for
  learning them. We demonstrate the operation of the algorithm on a
  2-dimensional continuous spatial navigation domain.</p>



</p>

<p id="engel-mannor-meir-05">
Y.&nbsp;Engel, S.&nbsp;Mannor, and R.&nbsp;Meir.
<a href="http://www.machinelearning.org/proceedings/icml2005/papers/026_Reinforcement_EngelEtAl.pdf"><b>Reinforcement learning with Gaussian
  processes</b></a>.
In <em>22nd International Conference on Machine Learning</em>, pages 201-208,
  Bonn, Germany, August 2005.<p class="c"><b> Abstract:</b> Gaussian Process
  Temporal Difference (GPTD) learning offers a Bayesian solution to the policy
  evaluation problem of reinforcement learning. In this paper we extend the
  GPTD framework by addressing two pressing issues, which were not adequately
  treated in the original GPTD paper <a href="#engel-mannor-meir-03">Engel et
  al., 2003</a>. The first is the issue of stochasticity in the state
  transitions, and the second is concerned with action selection and policy
  improvement. We present a new generative model for the value function,
  deduced from its relation with the discounted return. We derive a
  corresponding on-line algorithm for learning the posterior moments of the
  value Gaussian process. We also present a SARSA based extension of GPTD,
  termed GPSARSA, that allows the selection of actions and the gradual
  improvement of policies without requiring a world-model.</p>



</p>

<p id="engel-szabo-volkinshtein-06">
Y.&nbsp;Engel, P.&nbsp;Szabo, and D.&nbsp;Volkinshtein.
<a href="http://books.nips.cc/papers/files/nips18/NIPS2005_0746.pdf"><b>Learning to control an octopus arm with Gaussian process temporal difference
  methods</b></a>.
In Y.&nbsp;Weiss, B.&nbsp;Sch&ouml;lkopf, and J.&nbsp;C. Platt, editors, <em>Advances in Neural
  Information Processing Systems 18</em>, pages 347-354, Cambridge, MA,
  U.S.A., 2006. The MIT Press.<p class="c"><b> Abstract:</b> The Octopus arm
  is a highly versatile and complex limb. How the Octopus controls such a
  hyper-redundant arm (not to mention eight of them!) is as yet unknown.
  Robotic arms based on the same mechanical principles may render present day
  robotic arms obsolete. In this paper, we tackle this control problem using an
  online reinforcement learning algorithm, based on a Bayesian approach to
  policy evaluation known as Gaussian process temporal difference (GPTD)
  learning. Our substitute for the real arm is a computer simulation of a
  2-dimensional model of an Octopus arm. Even with the simplifications inherent
  to this model, the state space we face is a high-dimensional one. We apply a
  GPTDbased algorithm to this domain, and demonstrate its operation on several
  learning tasks of varying degrees of difficulty.</p>



</p>

<p id="ghavamzadeh-engel-07">
M.&nbsp;Ghavamzadeh and Y.&nbsp;Engel.
<a href="http://books.nips.cc/papers/files/nips19/NIPS2006_0865.pdf"><b>Bayesian policy gradient algorithms</b></a>.
In B.&nbsp;Sch&ouml;lkopf, J.&nbsp;C. Platt, and T.&nbsp;Hoffman, editors, <em>Advances in
  Neural Information Processing Systems 19</em>, pages 457-464, Cambridge, MA,
  U.S.A., 2007. The MIT Press.<p class="c"><b> Abstract:</b> Policy gradient
  methods are reinforcement learning algorithms that adapt a parameterized
  policy by following a performance gradient estimate. Conventional policy
  gradient methods use Monte-Carlo techniques to estimate this gradient. Since
  Monte Carlo methods tend to have high variance, a large number of samples is
  required, resulting in slow convergence. In this paper, we propose a Bayesian
  framework that models the policy gradient as a Gaussian process. This reduces
  the number of samples needed to obtain accurate gradient estimates. Moreover,
  estimates of the natural gradient as well as a measure of the uncertainty in
  the gradient estimates are provided at little extra cost.</p>



</p>

<p id="ghavamzadeh-engel-07a">
M.&nbsp;Ghavamzadeh and Y.&nbsp;Engel.
<a href="http://www.machinelearning.org/proceedings/icml2007/papers/458.pdf"><b>Bayesian actor-critic algorithms</b></a>.
In <em>24th International Conference on Machine Learning</em>, pages 297-304,
  Corvallis, OR, U.S.A., June 2007.<p class="c"><b> Abstract:</b> We present a
  new actor-critic learning model in which a Bayesian class of non-parametric
  critics, using Gaussian process temporal difference learning is used. Such
  critics model the state-action value function as a Gaussian process, allowing
  Bayes' rule to be used in computing the posterior distribution over
  state-action value functions, conditioned on the observed data. Appropriate
  choices of the prior covariance (kernel) between state-action values and of
  the parametrization of the policy allow us to obtain closed-form expressions
  for the posterior distribution of the gradient of the average discounted
  return with respect to the policy parameters. The posterior mean, which
  serves as our estimate of the policy gradient, is used to update the policy,
  while the posterior covariance allows us to gauge the reliability of the
  update.</p>



</p>

<p id="ko-klein-fox-etal-07a">
J.&nbsp;Ko, D.&nbsp;J. Klein, D.&nbsp;Fox, and D.&nbsp;Haehnel.
<a href="http://www.cs.washington.edu/homes/fox/postscripts/gp-blimp-icra-07.pdf"><b>Gaussian Processes and Reinforcement Learning for Identification and
  Control of an Autonomous Blimp</b></a>.
In <em>Proceedings of the International Conference on Robotics and Automation
  (ICRA)</em>, pages 742-747, Rome, Italy, April 2007.<p class="c"><b>
  Abstract:</b> Blimps are a promising platform for aerial robotics and have
  been studied extensively for this purpose. Unlike other aerial vehicles,
  blimps are relatively safe and also possess the ability to loiter for long
  periods. These advantages, however, have been difficult to exploit because
  blimp dynamics are complex and inherently non-linear. The classical approach
  to system modeling represents the system as an ordinary differential equation
  (ODE) based on Newtonian principles. A more recent modeling approach is based
  on representing state transitions as a Gaussian process (GP). In this paper,
  we present a general technique for system identification that combines these
  two modeling approaches into a single formulation. This is done by training a
  Gaussian process on the residual between the non-linear model and ground
  truth training data. The result is a GP-enhanced model that provides an
  estimate of uncertainty in addition to giving better state predictions than
  either ODE or GP alone. We show how the GP-enhanced model can be used in
  conjunction with reinforcement learning to generate a blimp controller that
  is superior to those learned with ODE or GP models alone.</p>



</p>

<p id="kocijan-etal-03">
J.&nbsp;Kocijan, R.&nbsp;Murray-Smith, C.&nbsp;E. Rasmussen, and B.&nbsp;Likar.
<a href="http://www.kyb.mpg.de/publications/pdfs/pdf2283.pdf"><b>Predictive
  control with Gaussian process models</b></a>.
In B.&nbsp;Zajc and M.&nbsp;Tkal, editors, <em>Proceedings of IEEE Region 8 Eurocon 2003:
  Computer as a Tool</em>, pages 352-356, Piscataway, 2003. IEEE.<p
  class="c"><b> Abstract:</b> This paper describes model-based predictive
  control based on Gaussian processes.Gaussian process models provide a
  probabilistic non-parametric modelling approach for black-box identification
  of non-linear dynamic systems. It offers more insight in variance of obtained
  model response, as well as fewer parameters to determine than other models.
  The Gaussian processes can highlight areas of the input space where
  prediction quality is poor, due to the lack of data or its complexity, by
  indicating the higher variance around the predicted mean. This property is
  used in predictive control, where optimisation of control signal takes the
  variance information into account. The predictive control principle is
  demonstrated on a simulated example of nonlinear system.</p>



</p>

<p id="rasmussen-deisenroth-08">
C.&nbsp;E. Rasmussen and M.&nbsp;P. Deisenroth.
<a href="http://mlg.eng.cam.ac.uk/marc/./publications/ewrl2008_final_pub.pdf"><b>Probabilistic Inference for Fast Learning in Control</b></a>.
In S.&nbsp;Girgin, M.&nbsp;Loth, R.&nbsp;Munos, P.&nbsp;Preux, and D.&nbsp;Ryabko, editors, <em>Recent
  Advances in Reinforcement Learning</em>, volume 5323 of <em>Lecture Notes in
  Computer Science</em>, pages 229-242. Springer-Verlag, November 2008.<p
  class="c"><b> Abstract:</b> We provide a novel framework for very fast
  model-based reinforcement learning in continuous state and action spaces. The
  framework requires probabilistic models that explicitly characterize their
  levels of confidence. Within this framework, we use flexible, non-parametric
  models to describe the world based on previously collected experience. We
  demonstrate learning on the cart-pole problem in a setting where we provide
  very limited prior knowledge about the task. Learning progresses rapidly, and
  a good policy is found after only a hand-full of iterations.</p>



</p>

<p id="rasmussen-kuss-04">
C.&nbsp;E. Rasmussen and M.&nbsp;Kuss.
<a href="http://books.nips.cc/papers/files/nips16/NIPS2003_CN01.pdf"><b>Gaussian processes in reinforcement learning</b></a>.
In S.&nbsp;Thrun, L.&nbsp;Saul, and B.&nbsp;Sch&ouml;lkopf, editors, <em>Advances in Neural
  Information Processing Systems 16</em>. The MIT Press, Cambridge, MA,
  2004.<p class="c"><b> Abstract:</b> We exploit some useful properties of
  Gaussian process (GP) regression models for reinforcement learning in
  continuous state spaces and discrete time. We demonstrate how the GP model
  allows evaluation of the value function in closed form. The resulting policy
  iteration algorithm is demonstrated on a simple problem with a two
  dimensional state space. Further, we speculate that the intrinsic ability of
  GP models to characterise distributions of functions would allow the method
  to capture entire distributions over future values instead of merely their
  expectation, which has traditionally been the focus of much of reinforcement
  learning.</p>



</p>


<br>

<h3 id="gplvm">Gaussian Process Latent Variable Models (GP-LVM)</h3>


<p id="lawrence-04">
N.&nbsp;Lawrence.
<a href="http://books.nips.cc/papers/files/nips16/NIPS2003_AA42.pdf"><b>Gaussian process latent variable models for visualization of high dimensional
  data</b></a>.
In S.&nbsp;Thrun, L.&nbsp;Saul, and B.&nbsp;Sch&ouml;lkopf, editors, <em>Advances in Neural
  Information Processing Systems 16</em>, pages 329-336. The MIT Press,
  2004.<p class="c"><b> Abstract:</b> In this paper we introduce a new
  underlying probabilistic model for principal component analysis (PCA). Our
  formulation interprets PCA as a particular Gaussian process prior on a
  mapping from a latent space to the observed data-space. We show that if the
  prior뗩 covariance function constrains the mappings to be linear the model
  is equivalent to PCA, we then extend the model by considering less
  restrictive covariance functions which allow non-linear mappings. This more
  general Gaussian process latent variable model (GPLVM) is then evaluated as
  an approach to the visualisation of high dimensional data for three different
  data-sets. Additionally our non-linear algorithm can be further kernelised
  leading to 땂win kernel PCA in which a <em>mapping between feature
  spaces</em> occurs.</p>



</p>

<p id="lawrence-05">
N.&nbsp;Lawrence.
<a href="http://www.jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf"><b>Probabilistic non-linear principal component analysis with Gaussian process
  latent variable models</b></a>.
<em>Journal of Machine Learning Research</em>, 6:1783-1816, 2005.<p
  class="c"><b> Abstract:</b> Summarising a high dimensional data set with a
  low dimensional embedding is a standard approach for exploring its structure.
  In this paper we provide an overview of some existing techniques for
  discovering such embeddings. We then introduce a novel probabilistic
  interpretation of principal component analysis (PCA) that we term dual
  probabilistic PCA (DPPCA). The DPPCA model has the additional advantage that
  the linear mappings from the embedded space can easily be non-linearised
  through Gaussian processes. We refer to this model as a Gaussian process
  latent variable model (GP-LVM). Through analysis of the GP-LVM objective
  function, we relate the model to popular spectral techniques such as kernel
  PCA and multidimensional scaling. We then review a practical algorithm for
  GP-LVMs in the context of large data sets and develop it to also handle
  discrete valued data and missing attributes. We demonstrate the model on a
  range of real-world and artificially generated data sets.</p>



</p>

<p id="urtasun-darrell-07">
R.&nbsp;Urtasun and T.&nbsp;Darrell.
<a href="http://people.csail.mit.edu/rurtasun/publications/icml_urtasun_darrell.pdf"><b>Discriminative Gaussian process latent variable model for
  classification</b></a>.
In <em>24th International Conference on Machine Learning</em>, 2007.<p
  class="c"><b> Abstract:</b> Supervised learning is dicult with high
  dimensional input spaces and very small training sets, but accurate
  classcation may be possible if the data lie on a low-dimensional manifold.
  Gaussian Process Latent Variable Models can discover low dimensional
  manifolds given only a small number of examples, but learn a latent space
  without regard for class labels. Existing methods for discriminative manifold
  learning (e.g., LDA, GDA) do constrain the class distribution in the latent
  space, but are generally deterministic and may not generalize well with
  limited training data. We introduce a method for Gaussian Process Classcation
  using latent variable models trained with discriminative priors over the
  latent space, which can learn a discriminative latent space from a small
  training set.</p>



</p>

<p id="wang-fleet-hertzmann-06">
J.&nbsp;Wang, D.&nbsp;Fleet, and A.&nbsp;Hertzmann.
<a href="http://books.nips.cc/papers/files/nips18/NIPS2005_0548.pdf"><b>Gaussian process dynamical models</b></a>.
In Y.&nbsp;Weiss, B.&nbsp;Sch&ouml;lkopf, and J.&nbsp;Platt, editors, <em>Advances in Neural
  Information Processing Systems 18</em>, pages 1443-1450. The MIT Press,
  Cambridge, MA, 2006.<p class="c"><b> Abstract:</b> This paper introduces
  Gaussian Process Dynamical Models (GPDM) for nonlinear time series analysis.
  A GPDM comprises a low-dimensional latent space with associated dynamics, and
  a map from the latent space to an observation space. We marginalize out the
  model parameters in closed-form, using Gaussian Process (GP) priors for both
  the dynamics and the observation mappings. This results in a nonparametric
  model for dynamical systems that accounts for uncertainty in the model. We
  demonstrate the approach on human motion capture data in which each pose is
  62-dimensional. Despite the use of small data sets, the GPDM learns an
  effective representation of the nonlinear dynamics in these spaces.</p>

<p class="c"><b> Comment:</b> Web page <a
  href="http://www.dgp.toronto.edu/~jmwang/gpdm">http://www.dgp.toronto.edu/~jmwang/gpdm</a>.</p>

</p>


<br>

<h3 id="appl">Applications</h3>


<p id="archambeau-07">
C.&nbsp;Archambeau, D.&nbsp;Cornford, M.&nbsp;Opper, and J.&nbsp;Shawe-Taylor.
<a href="http://jmlr.csail.mit.edu/proceedings/papers/v1/archambeau07a/archambeau07a.pdf"><b>Gaussian process approximations of stochastic differential
  equations</b></a>.
In <em>JMLR: Workshop and Conference Proceedings</em>, 2007.<p class="c"><b>
  Abstract:</b> Stochastic differential equations arise naturally in a range of
  contexts, from financial to environmental modeling. Current solution methods
  are limited in their representation of the posterior process in the presence
  of data. In this work, we present a novel Gaussian process approximation to
  the posterior measure over paths for a general class of stochastic
  differential equations in the presence of observations. The method is applied
  to two simple problems: the Ornstein-Uhlenbeck process, of which the exact
  solution is known and can be compared to, and the double-well system, for
  which standard approaches such as the ensemble Kalman smoother fail to
  provide a satisfactory result. Experiments show that our variational
  approximation is viable and that the results are very promising as the
  variational approximate solution outperforms standard Gaussian process
  regression for non-Gaussian Markov processes.</p>



</p>

<p id="bailer-jones-irwin-etal-97">
C.&nbsp;A.&nbsp;L. Bailer-Jones, T.&nbsp;J. Sabin, D.&nbsp;J.&nbsp;C. MacKay, and P.&nbsp;J. Withers.
<a href="http://www.mpia-hd.mpg.de/homes/calj/IPMM97.pdf"><b>Prediction of
  deformed and annealed microstructures using Bayesian neural networks and
  Gaussian processes</b></a>.
In T.&nbsp;Chandra, S.&nbsp;R. Leclair, J.&nbsp;A. Meech, B.&nbsp;Verma, M.&nbsp;Smith, and
  B.&nbsp;Balachandran, editors, <em>Proceedings of the Australasia Pacific Forum on
  Intelligent Processing and Manufacturing of Materials (IPMM97)</em>,
  Brisbane, 1997. Watson Ferguson & Co.<p class="c"><b> Abstract:</b> The
  forming of metals is important in many manufacturing industries. It has long
  been known that microstructure and texture affect the properties of a
  material, but to date limited progress has been made in predicting
  microstructural development during thermomechanical forming due to the
  complexity of the relationship between microstructure and local deformation
  conditions. In this paper we investigate the utility of non-linear
  interpolation models, in particular Gaussian processes, to model the
  development of microstructure during thermomechanical processing of metals.
  We adopt a Bayesian approach which allows: (1) automatic control of the
  complexity of the non-linear model; (2) calculation of error bars describing
  the reliability of the model predictions; (3) automatic determination of the
  relevance of the various input variables. Although this method is not
  intelligent in that it does not attempt to provide a fundamental
  understanding of the underlying micromechanical deformation processes, it can
  lead to empirical relations that predict microstructure as a function of
  deformation and heat treatments. These can easily be incorporated into
  existing Finite Element forging design tools. Future work will examine the
  use of these models in reverse to guide the definition of deformation
  processes aimed at delivering the required microstructures. In order to
  thoroughly train and test a Gaussian Process or neural network model, a large
  amount of representative experimental data is required. Initial experimental
  work has focused on an Al-1%Mg alloy deformed in non-uniform cold compression
  followed by different annealing treatments to build up a set of
  microstructural data brought about by a range of processing conditions. The
  DEFORM Finite Element modelling package has been used to calculate the local
  effective strain as a function of position across the samples. This is
  correlated with measurements of grain areas to construct the data set with
  which to develop the model.</p>



</p>

<p id="chu-et-al-05">
W.&nbsp;Chu, Z.&nbsp;Ghahramani, F.&nbsp;Falciani, and D.&nbsp;L. Wild.
<a href="http://bioinformatics.oxfordjournals.org/cgi/reprint/bti526?ijkey=xoHPtTsiEzzTvfT&keytype=ref"><b>Biomarker discovery in microarray gene expression
  data with Gaussian processes</b></a>.
<em>Bioinformatics</em>, 21(16):3385-3393, 2005.<p class="c"><b> Abstract:</b>
  Motivation: In clinical practice, pathological phenotypes are often labelled
  with ordinal scales rather than binary, e.g. the Gleason grading system for
  tumor cell differentiation. However, in the literature of microarray
  analysis, these ordinal labels have been rarely treated in a principled way.
  This paper describes a gene selection algorithm based on Gaussian processes
  to discover consistent gene expression patterns associated with ordinal
  clinical phenotypes. The technique of automatic relevance determination is
  applied to represent the significance level of the genes in a Bayesian
  inference framework. Results: The usefulness of the proposed algorithm for
  ordinal labels is demonstrated by the gene expression signature associated
  with the Gleason score for prostate cancer data. Our results demonstrate how
  multi-gene markers that may be initially developed with a diagnostic or
  prognostic application in mind are also useful as an investigative tool to
  reveal associations between specific molecular and cellular events and
  features of tumor physiology. Our algorithm can also be applied to microarray
  data with binary labels with results comparable to other methods in the
  literature. Availability: The source code was written in ANSI C, which is
  accessible at <a
  href="http://www.gatsby.ucl.ac.uk/~chuwei/code/gpgenes.tar">www.gatsby.ucl.ac.uk/~chuwei/code/gpgenes.tar</a>.</p>



</p>

<p id="chu-sindwhani-06">
W.&nbsp;Chu, S.&nbsp;Sindwhani, Z.&nbsp;Ghahramani, and S.&nbsp;S. Keerthi.
<a href="http://books.nips.cc/papers/files/nips19/NIPS2006_0676.pdf"><b>Relational learning with Gaussian processes</b></a>.
In <em>Advances in Neural Information Processing Systems 18</em>, 2006.<p
  class="c"><b> Abstract:</b> Correlation between instances is often modelled
  via a kernel function using input attributes of the instances. Relational
  knowledge can further reveal additional pairwise correlations between
  variables of interest. In this paper, we develop a class of models which
  incorporates both reciprocal relational information and input attributes
  using Gaussian process techniques. This approach provides a novel
  non-parametric Bayesian framework with a data-dependent covariance function
  for supervised learning tasks. We also apply this framework to
  semi-supervised learning. Experimental results on several real world data
  sets verify the usefulness of this algorithm.</p>



</p>

<p id="cunningham-shenoy-sahani-08">
J.&nbsp;P. Cunningham, K.&nbsp;V. Shenoy, and M.&nbsp;Sahani.
<a href="http://icml2008.cs.helsinki.fi/papers/151.pdf"><b>Fast Gaussian
  process methods for point process intensity estimation</b></a>.
In Andrew McCallum and Sam Roweis, editors, <em>25th International Conference
  on Machine Learning</em>, pages 192-199. Omnipress, 2008.<p class="c"><b>
  Abstract:</b> Point processes are difficult to analyze because they provide
  only a sparse and noisy observation of the intensity function driving the
  process. Gaussian Processes offer an attractive framework within which to
  infer underlying intensity functions. The result of this inference is a
  continuous function defined across time that is typically more amenable to
  analytical efforts. However, a naive implementation will become
  computationally infeasible in any problem of reasonable size, both in memory
  and run time requirements. We demonstrate problem specific methods for a
  class of renewal processes that eliminate the memory burden and reduce the
  solve time by orders of magnitude.</p>



</p>

<p id="eichhorn-tolias-etal-04">
J.&nbsp;Eichhorn, A.&nbsp;Tolias, A.&nbsp;Zien, M.&nbsp;Kuss, C.&nbsp;E. Rasmussen, J.&nbsp;Weston,
  N.&nbsp;Logothetis, and B.&nbsp;Sch&ouml;lkopf.
<a href="http://books.nips.cc/papers/files/nips16/NIPS2003_NS16.pdf"><b>Prediction on spike data using kernel algorithms</b></a>.
In S.&nbsp;Thrun, L.&nbsp;Saul, and B.&nbsp;Sch&ouml;lkopf, editors, <em>Advances in Neural
  Information Processing Systems 16</em>, Cambridge, MA, 2004. The MIT
  Press.<p class="c"><b> Abstract:</b> We report and compare the performance of
  different learning algorithms based on data from cortical recordings. The
  task is to predict the orientation of visual stimuli from the activity of a
  population of simultaneously recorded neurons. We compare several ways of
  improving the coding of the input (i.e., the spike data) as well as of the
  output (i.e., the orientation), and report the results obtained using
  different kernel algorithms.</p>



</p>

<p id="ko-fox-08">
J.&nbsp;Ko and D.&nbsp;Fox.
<a href="http://www.cs.washington.edu/homes/fox/postscripts/gp-bayesfilter-iros-08.pdf"><b>GP-BayesFilters: Bayesian Filtering Using Gaussian Process
  Prediction and Observation Models</b></a>.
In <em>Proceedings of the 2008 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS)</em>, pages 3471-3476, Nice, France, September
  2008.<p class="c"><b> Abstract:</b> Bayesian filtering is a general framework
  for recursively estimating the state of a dynamical system. The most common
  instantiations of Bayes filters are Kalman filters (extended and unscented)
  and particle filters. Key components of each Bayes filter are probabilistic
  prediction and observation models. Recently, Gaussian processes have been
  introduced as a non-parametric technique for learning such models from
  training data. In the context of unscented Kalman filters, these models have
  been shown to provide estimates that can be superior to those achieved with
  standard, parametric models. In this paper we show how Gaussian process
  models can be integrated into other Bayes filters, namely particle filters
  and extended Kalman filters. We provide a complexity analysis of these
  filters and evaluate the alternative techniques using data collected with an
  autonomous micro-blimp.</p>



</p>

<p id="ko-klein-fox-etal-07">
J.&nbsp;Ko, D.&nbsp;J. Klein, D.&nbsp;Fox, and D.&nbsp;Haehnel.
<a href="http://www.cs.washington.edu/homes/fox/postscripts/gp-ukf-iros-07.pdf"><b>GP-UKF: Unscented Kalman Filters with Gaussian Process Prediction and
  Observation Models</b></a>.
In <em>Proceedings of the 2007 IEEE/RSJ International Conference on Intelligent
  Robots and Systems</em>, pages 1901-1907, San Diego, CA, USA, October
  2007.<p class="c"><b> Abstract:</b> This paper considers the use of
  non-parametric system models for sequential state estimation. In particular,
  motion and observation models are learned from training examples using
  Gaussian Process (GP) regression. The state estimator is an Unscented Kalman
  Filter (UKF). The resulting GP-UKF algorithm has a number of advantages over
  standard (parametric) UKFs. These include the ability to estimate the state
  of arbitrary nonlinear systems, improved tracking quality compared to a
  parametric UKF, and graceful degradation with increased model uncertainty.
  These advantages stem from the fact that GPs consider both the noise in the
  system and the uncertainty in the model. If an approximate parametric model
  is available, it can be incorporated into the GP; resulting in further
  performance improvements. In experiments, we show how the GP-UKF algorithm
  can be applied to the problem of tracking an autonomous micro-blimp.</p>



</p>

<p id="ko-klein-fox-etal-07a">
J.&nbsp;Ko, D.&nbsp;J. Klein, D.&nbsp;Fox, and D.&nbsp;Haehnel.
<a href="http://www.cs.washington.edu/homes/fox/postscripts/gp-blimp-icra-07.pdf"><b>Gaussian Processes and Reinforcement Learning for Identification and
  Control of an Autonomous Blimp</b></a>.
In <em>Proceedings of the International Conference on Robotics and Automation
  (ICRA)</em>, pages 742-747, Rome, Italy, April 2007.<p class="c"><b>
  Abstract:</b> Blimps are a promising platform for aerial robotics and have
  been studied extensively for this purpose. Unlike other aerial vehicles,
  blimps are relatively safe and also possess the ability to loiter for long
  periods. These advantages, however, have been difficult to exploit because
  blimp dynamics are complex and inherently non-linear. The classical approach
  to system modeling represents the system as an ordinary differential equation
  (ODE) based on Newtonian principles. A more recent modeling approach is based
  on representing state transitions as a Gaussian process (GP). In this paper,
  we present a general technique for system identification that combines these
  two modeling approaches into a single formulation. This is done by training a
  Gaussian process on the residual between the non-linear model and ground
  truth training data. The result is a GP-enhanced model that provides an
  estimate of uncertainty in addition to giving better state predictions than
  either ODE or GP alone. We show how the GP-enhanced model can be used in
  conjunction with reinforcement learning to generate a blimp controller that
  is superior to those learned with ODE or GP models alone.</p>



</p>

<p id="kocijan-murray-smith-etal-04">
J.&nbsp;Kocijan, R.&nbsp;Murray-Smith, C.&nbsp;E. Rasmussen, and A.&nbsp;Girard.
<a href="http://www.kyb.mpg.de/publications/pdfs/pdf2363.pdf"><b>Gasussian
  process model based predictive control</b></a>.
In <em>Proceedings of the Amarican Control Conference</em>, pages 2214-2219,
  2004.<p class="c"><b> Abstract:</b> Gaussian process models provide a
  probabilistic non-parametric modelling approach for black-box identi cation
  of non-linear dynamic systems. The Gaussian processes can highlight areas of
  the input space where prediction quality is poor, due to the lack of data or
  its complexity, by indicating the higher variance around the predicted mean.
  Gaussian process models contain noticeably less coef cients to be optimised.
  This paper illustrates possible application of Gaussian process models within
  model-based predictive control. The extra information provided within
  Gaussian process model is used in predictive control, where optimisation of
  control signal takes the variance information into account. The predictive
  control principle is demonstrated on control of pH process benchmark.</p>



</p>

<p id="laslett-94">
G.&nbsp;M. Laslett.
<b>Kriging and splines: An empirical comparison of their predictive performance
  in some applications</b>.
<em>Journal of the American Statistical Association</em>, 89(426):391-409,
  1994.



</p>

<p id="murillo-fuentes-etal-06">
J.&nbsp;J. Murillo-Fuentes, S.&nbsp;Caro, and F.&nbsp;Perez-Cruz.
<a href="http://books.nips.cc/papers/files/nips18/NIPS2005_0664.pdf"><b>Gaussian processes for multiuser detection in cdma receivers</b></a>.
In Y.&nbsp;Weiss, B.&nbsp;Sch&ouml;lkopf, and J.&nbsp;Platt, editors, <em>Advances in Neural
  Information Processing Systems 18</em>, pages 939-946. The MIT Press,
  Cambridge, MA, 2006.



</p>

<p id="ohagan-kennedy-oakley-99">
A.&nbsp;O'Hagan, M.&nbsp;C. Kennedy, and J.&nbsp;E. Oakley.
<b>Uncertainty analysis and other inference tools for complex computer
  codes</b>.
In J.&nbsp;M. Bernardo, J.&nbsp;O. Berger, A.&nbsp;P. Dawid, and A.&nbsp;F.&nbsp;M. Smith, editors,
  <em>Bayesian Statistics 6</em>, pages 503-524. Oxford University Press,
  1999.
(with discussion).



</p>

<p id="platt-burges-etal-02">
J.&nbsp;C. Platt, C.&nbsp;J.&nbsp;C. Burges, S.&nbsp;Swenson, C.&nbsp;Weare, and A.&nbsp;Zheng.
<a href="http://books.nips.cc/papers/files/nips14/AP03.pdf"><b>Learning a
  Gaussian process prior for automatically generating music
  playlists</b></a>.
In S.&nbsp;A. Solla, T.&nbsp;K. Leen, and K.-R. M&uuml;ller, editors, <em>Advances in
  Neural Information Processing Systems 12</em>, Cambridge, MA, 2000. The MIT
  Press.<p class="c"><b> Abstract:</b> This paper presents AutoDJ: a system for
  automatically generating music playlists based on one or more seed songs
  selected by a user. AutoDJ uses Gaussian Process Regression to learn a user
  preference function over songs. This function takes music metadata as inputs.
  This paper further introduces Kernel Meta-Training, which is a method of
  learning a Gaussian Process kernel from a distribution of functions that
  generates the learned function. For playlist generation, AutoDJ learns a
  kernel from a large set of albums. This learned kernel is shown to be more
  effective at predicting users' playlists than a reasonable hand-designed
  kernel.</p>



</p>

<p id="quinonero-candela-girard-etal-03">
J.&nbsp;Qui&ntilde;onero-Candela, A.&nbsp;Girard, Jan Larsen, and C.&nbsp;E. Rasmussen.
<b>Propagation of uncertainty in Bayesian kernel models - application to
  multiple-step ahead forecasting</b>.
In <em>Proceedings of the 2003 IEEE Conference on Acoustics, Speech, and Signal
  Processing (ICASSP 03)</em>, 2003.<p class="c"><b> Abstract:</b> The object
  of Bayesian modelling is the predictive distribution, which in a forecasting
  scenario enables improved estimates of forecasted values and their
  uncertainties. In this paper we focus on reliably estimating the predictive
  mean and variance of forecasted values using Bayesian kernel based models
  such as the Gaussian Process and the Relevance Vector Machine. We derive
  novel analytic expressions for the predictive mean and variance for Gaussian
  kernel shapes under the assumption of a Gaussian input distribution in the
  static case, and of a recursive Gaussian predictive density in iterative
  forecasting. The capability of the method is demonstrated for forecasting of
  time-series and compared to approximate methods.</p>



</p>

<p id="sacks-welch-etal-89">
J.&nbsp;Sacks, W.&nbsp;J. Welch, T.&nbsp;J. Mitchell, and H.&nbsp;P. Wynn.
<b>Design and analysis of computer experiments</b>.
<em>Statistical Science</em>, 4(4):409-435, 1989.



</p>

<p id="schwaighofer-grigorias-etal-04">
A.&nbsp;Schwaighofer, M.&nbsp;Grigoras, V.&nbsp;Tresp, and C&nbsp;Hoffmann.
<a href="http://books.nips.cc/papers/files/nips16/NIPS2003_AP02.pdf"><b>GPPS:
  A Gaussian process positioning system for cellular networks</b></a>.
In S.&nbsp;Thrun, L.&nbsp;Saul, and B.&nbsp;Sch&ouml;lkopf, editors, <em>Advances in Neural
  Information Processing Systems 16</em>, Cambridge, MA, 2004. The MIT
  Press.<p class="c"><b> Abstract:</b> In this article, we present a novel
  approach to solving the localization problem in cellular networks. The goal
  is to estimate a mobile users position, based on measurements of the signal
  strengths received from network base stations. Our solution works by building
  Gaussian process models for the distribution of signal strengths, as obtained
  in a series of calibration measurements. In the localization stage, the users
  position can be estimated by maximizing the likelihood of received signal
  strengths with respect to the position. We investigate the accuracy of the
  proposed approach on data obtained within a large indoor cellular
  network.</p>



</p>

<p id="shi-murray-smith-titterington-07">
Jian&nbsp;Qing Shi, B.&nbsp;Wang, Roderick Murray-Smith, and D.&nbsp;M. Titterington.
<a href="http://www.staff.ncl.ac.uk/j.q.shi/ps/gpfda.pdf"><b>Gaussian process
  functional regression modeling for batch data</b></a>.
<em>Biometrics</em>, 63:714-723, 2007.<p class="c"><b> Abstract:</b> A
  Gaussian process functional regression model is proposed for the analysis of
  batch data. Covariance structure and mean structure are considered
  simultaneously, with the covariance structure modelled by a Gaussian process
  regression model and the mean structure modelled by a functional regression
  model. The model allows the inclusion of covariates in both the covariance
  structure and the mean structure. It models the nonlinear relationship
  between a functional output variable and a set of functional and
  non-functional covariates. Several applications and simulation studies are
  reported and show that the method provides very good results for curve
  fitting and prediction.</p>



</p>

<p id="shi-wang-08">
Jian&nbsp;Qing Shi and B.&nbsp;Wang.
<a href="http://www.staff.ncl.ac.uk/j.q.shi/ps/mix-gpfr.pdf"><b>Curve
  prediction and clustering with mixtures of Gaussian process functional
  regression models</b></a>.
<em>Statistics and Computing</em>, 18:267-283, 2008.<p class="c"><b>
  Abstract:</b> <a href="#shi-murray-smith-titterington-07">Shi et al.
  (2007)</a> proposed a Gaussian process functional regression (GPFR) model to
  model functional response curves with a set of functional covariates. Two
  main problems are addressed by this method: modelling nonlinear and
  nonparametric regression relationship and modelling covariance structure and
  mean structure simultaneously. The method gives very good results for curve
  fitting and prediction but side-steps the problem of heterogeneity. In this
  paper we present a new method for modelling functional data with
  땁patially indexed data, i.e., the heterogeneity is dependent on factors
  such as region and individual patient뗩 information. For data collected
  from different sources, we assume that the data corresponding to each curve
  (or batch) follows a Gaussian process func- tional regression model as a
  lower-level model, and introduce an allocation model for the latent indicator
  variables as a higher-level model. This higher-level model is dependent on
  the information related to each batch. This method takes ad- vantage of both
  GPFR and mixture models and therefore improves the accuracy of predictions.
  The mixture model has also been used for curve clustering, but focusing on
  the problem of clustering functional relationships between response curve and
  covariates. The model is examined on simulated data and real data.</p>



</p>

<p id="sinz-quinonero-candela-etal-04">
F.&nbsp;Sinz, J.&nbsp;Qui&ntilde;onero-Candela, G.&nbsp;H. Bakir, C.&nbsp;E. Rasmussen, and M.&nbsp;O.
  Franz.
<a href="http://www.kyb.mpg.de/publications/pdfs/pdf2644.pdf"><b>Learning depth
  from stereo</b></a>.
In C.&nbsp;E. Rasmussen, H.&nbsp;H. Buelthoff, M.&nbsp;A. Giese, and B.&nbsp;Sch&ouml;lkopf,
  editors, <em>Pattern Recognition, Proc. 26th DAGM Symposium</em>, LNCS 3175,
  pages 245-252. Springer, Berlin, 2004.<p class="c"><b> Abstract:</b> We
  compare two approaches to the problem of estimating the depth of a point in
  space from observing its image position in two different cameras: 1. The
  classical photogrammetric approach explicitly models the two cameras and
  estimates their intrinsic and extrinsic parameters using a tedious
  calibration procedure; 2. A generic machine learning approach where the
  mapping from image to spatial coordinates is directly approximated by a
  Gaussian Process regression. Our results show that the generic learning
  approach, in addition to simplifying the procedure of calibration, can lead
  to higher depth accuracies than classical calibration although no specific
  domain knowledge is used.</p>



</p>

<p id="welch-buck-etal-92">
W.&nbsp;J. Welch, R.&nbsp;J. Buck, J.&nbsp;Sacks, H.&nbsp;P. Wynn, T.&nbsp;J. Mitchell, and M.&nbsp;D.
  Morris.
<b>Screening, predicting, and computer experiments</b>.
<em>Technometrics</em>, 34:15-25, 1992.

<p class="c"><b> Comment:</b> Application of noise free Gaussian Process to
  screening (input selection) and prediction in computer experiments. The
  covariance function is $C(x, x'|&theta;, p)\propto\exp
  (-&theta;_j|x_j-x_j'|^p_j)$; variable selection is done by lumping some
  hyperparameters and letting others vary freely. Training by maximum
  likelihood using linesearches and the simplex algorithm.</p>

</p>


<br>

<h3 id="other">Other Topics</h3>

This section contains a very diverse collection of other uses of inference
in Gaussian processes, which don't fit well in any of the above categories.


<p id="adams-stegle-08">
R.&nbsp;P. Adams and O.&nbsp;Stegle.
<a href="http://www.inference.phy.cam.ac.uk/rpa23/papers/adams-stegle-2008a.pdf"><b>Gaussian process product models for nonparametric
  nonstationarity</b></a>.
In <em>25th International Conference on Machine Learning</em>, 2008.<p
  class="c"><b> Abstract:</b> Stationarity is often an unrealistic prior
  assumption for Gaussian process regression. One solution is to predefine an
  explicit nonstationary covariance function, but such covariance functions can
  be difficult to specify and require detailed prior knowledge of the
  nonstationarity. We propose the Gaussian process product model (GPPM) which
  models data as the pointwise product of two latent Gaussian processes to
  nonparametrically infer nonstationary variations of amplitude. This approach
  differs from other nonparametric approaches to covariance function inference
  in that it operates on the outputs rather than the inputs, resulting in a
  significant reduction in computational cost and required data for inference.
  We present an approximate inference scheme using Expectation Propagation.
  This variational approximation yields convenient GP hyperparameter selection
  and compact approximate predictive distributions.</p>



</p>

<p id="chu-ghahramani-05">
W.&nbsp;Chu and Z.&nbsp;Ghahramani.
<a href="http://www.jmlr.org/papers/volume6/chu05a/chu05a.pdf"><b>Gaussian
  processes for ordinal regression</b></a>.
<em>Journal of Machine Learning Research</em>, 6:1019-1041, 2005.<p
  class="c"><b> Abstract:</b> We present a probabilistic kernel approach to
  ordinal regression based on Gaussian processes. A threshold model that
  generalizes the probit function is used as the likelihood function for
  ordinal variables. Two inference techniques, based on the Laplace
  approximation and the expectation propagation algorithm respectively, are
  derived for hyperparameter learning and model selection. We compare these two
  Gaussian process approaches with a previous ordinal regression method based
  on support vector machines on some benchmark and real-world data sets,
  including applications of ordinal regression to collaborative filtering and
  gene expression analysis. Experimental results on these data sets verify the
  usefulness of our approach.</p>



</p>

<p id="chu-ghahramani-05b">
W.&nbsp;Chu and Z.&nbsp;Ghahramani.
<a href="http://www.gatsby.ucl.ac.uk/~zoubin/papers/icml05chuwei-pl.pdf"><b>Preference learning with Gaussian processe</b></a>.
In <em>22nd International Conference on Machine Learning</em>, 2005.<p
  class="c"><b> Abstract:</b> In this paper, we propose a probabilistic kernel
  approach to preference learning based on Gaussian processes. A new likelihood
  function is proposed to capture the preference relations in the Bayesian
  framework. The generalized formulation is also applicable to tackle many
  multiclass problems. The overall approach has the advantages of Bayesian
  methods for model selection and probabilistic prediction. Experimental
  results compared against the constraint classification approach on several
  benchmark datasets verify the usefulness of this algorithm.</p>



</p>

<p id="der-lee-06">
R.&nbsp;Der and D.&nbsp;Lee.
<a href="http://books.nips.cc/papers/files/nips18/NIPS2005_0605.pdf"><b>Beyond
  Gaussian processes: On the distributions of infinite networks</b></a>.
In Y.&nbsp;Weiss, B.&nbsp;Sch&ouml;lkopf, and J.&nbsp;Platt, editors, <em>Advances in Neural
  Information Processing Systems 18</em>, pages 275-282. The MIT Press,
  Cambridge, MA, 2006.<p class="c"><b> Abstract:</b> A general analysis of the
  limiting distribution of neural network functions is performed, with emphasis
  on non-Gaussian limits. We show that with i.i.d. symmetric stable output
  weights, and more generally with weights distributed from the normal domain
  of attraction of a stable variable, that the neural functions converge in
  distribution to stable processes. Conditions are also investigated under
  which Gaussian limits do occur when the weights are independent but not
  identically distributed. Some particularly tractable classes of stable
  distributions are examined, and the possibility of learning with such
  processes.</p>



</p>

<p id="friedman-nachman-00">
N.&nbsp;Friedman and I.&nbsp;Nachman.
<a href="https://eprints.kfupm.edu.sa/42431/1/42431.pdf"><b>Gaussian process
  networks</b></a>.
In <em>Proceedings of the Sixteenth Conference on Uncertainty in Artificial
  Intelligence (UAI</em>, pages 211-219. Morgan Kaufmann, 2000.<p
  class="c"><b> Abstract:</b> In this paper we address the problem of learning
  the structure of a Bayesian network in domains with continuous variables.
  This task requires a procedure for comparing different candidate structures.
  In the Bayesian framework, this is done by evaluating the marginal likelihood
  of the data given a candidate structure. This term can be computed in
  closed-form for standard parametric families (e.g., Gaussians), and can be
  approximated, at some computational cost, for some semi-parametric families
  (e.g., mixtures of Gaussians) We present a new family of continuous variable
  probabilistic networks that are based on Gaussian Process priors. These
  priors are semiparametric in nature and can learn almost arbitrary noisy
  functional relations. Using these priors, we can directly compute marginal
  likelihoods for structure learning. The resulting method can discover a wide
  range of functional dependencies in multivariate data. We develop the
  Bayesian score of Gaussian Process Networks and describe how to learn them
  from data. We present empirical results on artificial data as well as on
  real-life domains with non-linear dependencies.</p>



</p>

<p id="girard-rasmussen-etal-03">
A.&nbsp;Girard, C.&nbsp;Rasmussen, J.Qui&ntilde;onero-Candela, and R.&nbsp;Murray-Smith.
<a href="http://books.nips.cc/papers/files/nips15/AA06.pdf"><b>Multiple-step
  ahead prediction for non linear dynamic systems - a Gaussian process
  treatment wih propagation of the uncertainty</b></a>.
In S.&nbsp;Becker, S.&nbsp;Thrun, and K.&nbsp;Obermayer, editors, <em>Advances in Neural
  Information Processing Systems 15</em>, Cambridge, MA, 2003. The MIT
  Press.<p class="c"><b> Abstract:</b> We consider the problem of multi-step
  ahead prediction in time series analysis using the non-parametric Gaussian
  process model. k-step ahead forecasting of a discrete-time non-linear dynamic
  system can be performed by doing repeated one-step ahead predictions. For a
  state-space model of the form y_t = f(y_t-1,...,y_t-L), the prediction of
  y at time t + k is based on the point estimates of the previous outputs. In
  this paper, we show how, using an analytical Gaussian approximation, we can
  formally incorporate the uncertainty about intermediate regressor values,
  thus updating the uncertainty on the current prediction</p>



</p>

<p id="graepel-03">
T.&nbsp;Graepel.
<a href="http://www.research.microsoft.com/~thoreg/papers/graepel03.pdf"><b>Solving noisy linear operator equations by Gaussian processes: Application to
  ordinary and partial differential equations</b></a>.
In <em>20th International Conference on Machine Learning</em>, 2003.<p
  class="c"><b> Abstract:</b> We formulate the problem of solving stochastic
  linear operator equations in a Bayesian Gaussian process (GP) framework. The
  solution is obtained in the spirit of a collocation method based on noisy
  evaluations of the target function at randomly drawn or deliberately chosen
  points. Prior knowledge about the solution is encoded in terms of the
  covariance kernel of the GP. As in GP regression, analytical expressions for
  the mean and variance of the estimated target function are obtained from
  which the solution to the operator equation follows by a manipulation of the
  kernel. Linear initial and boundary value constraints can be enforced by
  embedding the non-parametric model in a form that automatically satisfies the
  boundary conditions. The method is illustrated on a noisy linear first-order
  ordinary differential equation with initial condition and on a noisy
  second-order partial differential equation with Dirichlet boundary
  conditions.</p>



</p>

<p id="jackson-davy-doucet-fitzgerald-07">
E.&nbsp;Jackson, M.&nbsp;Davy, A.&nbsp;Doucet, and W.&nbsp;Fitzgerald.
<a href="http://www-lagis.univ-lille1.fr/~davy/papers/Jackson_ICASSP_2007.pdf"><b>Bayesian unsupervised signal classification by Dirichlet process mixtures
  of Gaussian processes</b></a>.
In <em>International Conference on Acoustics, Speech and Signal Processing
  2008</em>, volume&nbsp;3, pages 1077-1080, Honolulu, USA, 2007. IEEE.
<a href="http://www-lagis.univ-lille1.fr/~davy/code/dpmgp.tar.gz">matlab
  code</a>.<p class="c"><b> Abstract:</b> This paper presents a Bayesian
  technique aimed at classifying signals without prior training (clustering).
  The approach consists of modelling the observed signals, known only through a
  finite set of samples corrupted by noise, as Gaussian processes. As in many
  other Bayesian clustering approaches, the clusters are defined thanks to a
  mixture model. In order to estimate the number of clusters, we assume a
  priori a countably infinite number of clusters, thanks to a Dirichlet process
  model over the Gaussian processes parameters. Computations are performed
  thanks to a dedicated Monte Carlo Markov Chain algorithm, and results
  involving real signals (mRNA expression profiles) are presented.</p>



</p>

<p id="ko-fox-08">
J.&nbsp;Ko and D.&nbsp;Fox.
<a href="http://www.cs.washington.edu/homes/fox/postscripts/gp-bayesfilter-iros-08.pdf"><b>GP-BayesFilters: Bayesian Filtering Using Gaussian Process
  Prediction and Observation Models</b></a>.
In <em>Proceedings of the 2008 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS)</em>, pages 3471-3476, Nice, France, September
  2008.<p class="c"><b> Abstract:</b> Bayesian filtering is a general framework
  for recursively estimating the state of a dynamical system. The most common
  instantiations of Bayes filters are Kalman filters (extended and unscented)
  and particle filters. Key components of each Bayes filter are probabilistic
  prediction and observation models. Recently, Gaussian processes have been
  introduced as a non-parametric technique for learning such models from
  training data. In the context of unscented Kalman filters, these models have
  been shown to provide estimates that can be superior to those achieved with
  standard, parametric models. In this paper we show how Gaussian process
  models can be integrated into other Bayes filters, namely particle filters
  and extended Kalman filters. We provide a complexity analysis of these
  filters and evaluate the alternative techniques using data collected with an
  autonomous micro-blimp.</p>



</p>

<p id="ko-klein-fox-etal-07">
J.&nbsp;Ko, D.&nbsp;J. Klein, D.&nbsp;Fox, and D.&nbsp;Haehnel.
<a href="http://www.cs.washington.edu/homes/fox/postscripts/gp-ukf-iros-07.pdf"><b>GP-UKF: Unscented Kalman Filters with Gaussian Process Prediction and
  Observation Models</b></a>.
In <em>Proceedings of the 2007 IEEE/RSJ International Conference on Intelligent
  Robots and Systems</em>, pages 1901-1907, San Diego, CA, USA, October
  2007.<p class="c"><b> Abstract:</b> This paper considers the use of
  non-parametric system models for sequential state estimation. In particular,
  motion and observation models are learned from training examples using
  Gaussian Process (GP) regression. The state estimator is an Unscented Kalman
  Filter (UKF). The resulting GP-UKF algorithm has a number of advantages over
  standard (parametric) UKFs. These include the ability to estimate the state
  of arbitrary nonlinear systems, improved tracking quality compared to a
  parametric UKF, and graceful degradation with increased model uncertainty.
  These advantages stem from the fact that GPs consider both the noise in the
  system and the uncertainty in the model. If an approximate parametric model
  is available, it can be incorporated into the GP; resulting in further
  performance improvements. In experiments, we show how the GP-UKF algorithm
  can be applied to the problem of tracking an autonomous micro-blimp.</p>



</p>

<p id="krause-singh-guestrin-08">
A.&nbsp;Krause, A.&nbsp;Singh, and Guestrin C.
<a href="http://www.jmlr.org/papers/volume9/krause08a/krause08a.pdf"><b>Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms
  and Empirical Studies</b></a>.
<em>Journal of Machine Learning Research</em>, 9:235-284, February 2008.<p
  class="c"><b> Abstract:</b> When monitoring spatial phenomena, which can
  often be modeled as Gaussian processes (GPs), choosing sensor locations is a
  fundamental task. There are several common strategies to address this task,
  for example, geometry or disk models, placing sensors at the points of
  highest entropy (variance) in the GP model, and A-, D-, or E-optimal design.
  In this paper, we tackle the combinatorial optimization problem of maximizing
  the mutual information between the chosen locations and the locations which
  are not selected. We prove that the problem of finding the configuration that
  maximizes mutual information is NP-complete. To address this issue, we
  describe a polynomial-time approximation that is within (1-1/e) of the
  optimum by exploiting the submodularity of mutual information. We also show
  how submodularity can be used to obtain online bounds, and design branch and
  bound search procedures. We then extend our algorithm to exploit lazy
  evaluations and local structure in the GP, yielding significant speedups. We
  also extend our approach to find placements which are robust against node
  failures and uncertainties in the model. These extensions are again
  associated with rigorous theoretical approximation guarantees, exploiting the
  submodularity of the objective function. We demonstrate the advantages of our
  approach towards optimizing mutual information in a very extensive empirical
  study on two real-world data sets.</p>



</p>

<p id="murray-smith-girard-01">
R.&nbsp;Murray-Smith and A.&nbsp;Girard.
<a href="http://www.dcs.gla.ac.uk/~rod/publications/MurGir01.pdf"><b>Gaussian
  process priors with ARMA noise models</b></a>.
In <em>Irish Signals and Systems Conference</em>, pages 147-153, Maynooth,
  2001.



</p>

<p id="ohagan-91">
A.&nbsp;O'Hagan.
<b>Bayes-Hermite quadrature</b>.
<em>Journal of Statistical Planning and Inference</em>, 29:245-260, 1991.



</p>

<p id="rasmussen-03">
C.&nbsp;E. Rasmussen.
<a href="http://www.kyb.mpg.de/publications/pdfs/pdf2080.pdf"><b>Gaussian
  processes to speed up hybrid Monte Carlo for expensive Bayesian
  integrals</b></a>.
In J.&nbsp;M. Bernardo, M.&nbsp;J. Bayarri, J.&nbsp;O. Berger, A.&nbsp;P. Dawid, D.&nbsp;Heckerman,
  A.&nbsp;F.&nbsp;M. Smith, and M.&nbsp;West, editors, <em>Bayesian Statistics 7</em>, pages
  651-659. Oxford University Press, 2003.<p class="c"><b> Abstract:</b> Hybrid
  Monte Carlo (HMC) is often the method of choice for computing Bayesian
  integrals that are not analytically tractable. However the success of this
  method may require a very large number of evaluations of the (un-normalized)
  posterior and its partial derivatives. In situations where the posterior is
  computationally costly to evaluate, this may lead to an unacceptable
  computational load for HMC. I propose to use a Gaussian Process model of the
  (log of the) posterior for most of the computations required by HMC. Within
  this scheme only occasional evaluation of the actual posterior is required to
  guarantee that the samples generated have exactly the desired distribution,
  even if the GP model is somewhat inaccurate. The method is demonstrated on a
  10 dimensional problem, where 200 evaluations suffice for the generation of
  100 roughly independent points from the posterior. Thus, the proposed scheme
  allows Bayesian treatment of models with posteriors that are computationally
  demanding, such as models involving computer simulation.</p>



</p>

<p id="rasmussen-ghahramani-02">
C.&nbsp;E. Rasmussen and Z.&nbsp;Ghahramani.
<a href="http://books.nips.cc/papers/files/nips14/AA06.pdf"><b>Infinite
  mixtures of Gaussian process experts</b></a>.
In T.&nbsp;G. Diettrich, S.&nbsp;Becker, and Z.&nbsp;Ghahramani, editors, <em>Advances in
  Neural Information Processing Systems 14</em>. The MIT Press, 2002.<p
  class="c"><b> Abstract:</b> We present an extension to the Mixture of Experts
  (ME) model, where the individual experts are Gaussian Process (GP) regression
  models. Using a input-dependent adaptation of the Dirichlet Process, we
  implement a gating network for an infinite number of Experts. Inference in
  this model may be done efficiently using a Markov Chain relying on Gibbs
  sampling. The model allows the effective covariance function to vary with the
  inputs, and may handle large datasets - thus potentially overcoming two of
  the biggest hurdles with GP models. Simulations show the viability of this
  approach.</p>



</p>

<p id="rasmussen-ghahramani-03">
C.&nbsp;E. Rasmussen and Z.&nbsp;Ghahramani.
<a href="http://books.nips.cc/papers/files/nips15/AA01.pdf"><b>Bayesian Monte
  Carlo</b></a>.
In S.&nbsp;Becker, S.&nbsp;Thrun, and K.&nbsp;Obermayer, editors, <em>Advances in Neural
  Information Processing Systems 15</em>, Cambridge, MA, 2003. The MIT
  Press.<p class="c"><b> Abstract:</b> We investigate Bayesian alternatives to
  classical Monte Carlo methods for evaluating integrals. Bayesian Monte Carlo
  (BMC) allows the incorporation of prior knowledge, such as smoothness of the
  integrand, into the estimation. In a simple problem we show that this
  outperforms any classical importance sampling method. We also attempt more
  challenging multidimensional integrals involved in computing marginal
  likelihoods of statistical models (a.k.a. partition functions and model
  evidences). We find that Bayesian Monte Carlo outperformed Annealed
  Importance Sampling, although for very high dimensional problems or problems
  with massive multimodality BMC may be less adequate. One advantage of the
  Bayesian approach to Monte Carlo is that samples can be drawn from any
  distribution. This allows for the possibility of active design of sample
  points so as to maximise information gain.</p>



</p>

<p id="seeger-06">
M.&nbsp;Seeger.
<a href="http://www.kyb.mpg.de/bs/people/seeger/papers/gpbp.pdf"><b>Predicting
  Structured Data</b></a>, chapter Gaussian Process Belief Propagation, pages
  301-318.
The MIT Press, 2006.<p class="c"><b> Abstract:</b> The framework of graphical
  models is a cornerstone of applied Statistics, allowing for an intuitive
  graphical specification of the main features of a model, and providing a
  basis for general Bayesian inference computations though belief propagation
  (BP). In the latter, messages are passed between marginal beliefs of groups
  of variables. In parametric models, where all variables are of fixed finite
  dimension, these beliefs and messages can be represented easily in tables or
  parameters of exponential families, and BP techniques are widely used in this
  case. In this paper, we are interested in nonparametric models, where belief
  representations do not have a finite dimension, but grow with the dataset
  size. In the presence of several dependent domain variables, each of which is
  represented as a nonparametric random field, we aim for a synthesis of BP and
  nonparametric approximate inference techniques. We highlight the difficulties
  in exercising this venture and suggest possible techniques for remedies. We
  demonstrate our program using the example of semiparametric latent factor
  models [15], which can be used to model conditional dependencies between
  multiple responses.</p>



</p>

<p id="solak-murray-smith-etal-02">
E.&nbsp;Solak, R.&nbsp;Murray-Smith, W.&nbsp;E. Leithead, D.&nbsp;Leith, and C.&nbsp;E. Rasmussen.
<a href="http://books.nips.cc/papers/files/nips15/AA70.pdf"><b>Derivative
  observations in Gaussian process models of dynamic systems</b></a>.
In S.&nbsp;Thrun Becker, S. and K.&nbsp;Obermayer, editors, <em>Advances in Neural
  Information Processing Systems 15</em>, pages 1033-1040. The MIT Press,
  2003.<p class="c"><b> Abstract:</b> Gaussian processes provide an approach to
  nonparametric modelling which allows a straightforward combination of
  function and derivative observations in an empirical model. This is of
  particular importance in identification of nonlinear dynamic systems from
  experimental data. 1) It allows us to combine derivative information, and
  associated uncertainty with normal function observations into the learning
  and inference process. This derivative information can be in the form of
  priors specified by an expert or identified from perturbation data close to
  equilibrium. 2) It allows a seamless fusion of multiple local linear models
  in a consistent manner, inferring consistent models and ensuring that
  integrability constraints are met. 3) It improves dramatically the
  computational efficiency of Gaussian process models for dynamic system
  identification, by summarising large quantities of near-equilibrium data by a
  handful of linearisations, reducing the training set size - traditionally a
  problem for Gaussian process models.</p>



</p>

<p id="suykens-etal-02">
J.&nbsp;A.&nbsp;K. Suykens, T.&nbsp;Van Gentel, J.&nbsp;De Brabanter, B.&nbsp;De Moor, and
  J.&nbsp;Vandewalle.
<a href="http://www.esat.kuleuven.ac.be/sista/lssvmlab/book.html"><b>Least
  Squares Support Vector Machines</b></a>.
World Scientific Pub. Co., Singapore, 2002.

<p class="c"><b> Comment:</b> This (curriously named) book is essentially about
  Gaussian process models, although the connection is mentioned only in passing
  (eg in sec 3.6.3). Regression is done with a Gaussian process and
  classification is achieved by regressing on the labels
  ("least-squares-classification" or "label-regression") using a Gaussian
  process. The failure of viewing the model as a GP leads to a non-standard
  treatment of the probabilistic aspects: eg lack of simple standard predictive
  variance for regression (see eq 4.69) and a treament of probabilistic
  classification involving modelling the density of inputs in feature space,
  see sec 4.1.3.</p>

</p>

<p id="williams-felderhof-01">
C.&nbsp;K.&nbsp;I. Williams and S.&nbsp;N. Felderhof.
<b>Products and sums of tree-structured Gaussian processes</b>.
In <em>Proceedings of the Fourth International ICSC Symposium on Soft
  Computing</em>. ICSC-NAISO Academic Press, 2001.



</p>


<hr>

<table width="100%"><tr><td>
This page is maintained by <a href="http://learning.eng.cam.ac.uk/carl">Carl
Edward Rasmussen</a> and most recently updated on Febriary 23rd, 2011.<br>
Please send suggestions for corrections or additions via email.</td>
<td align="right">
<a href="http://t.extreme-dm.com/?login=gaussian"
target="_top"><img src="http://t1.extreme-dm.com/i.gif"
name="EXim" border="0" height="38" width="41"
alt="eXTReMe Tracker"></img></a>
<script type="text/javascript" language="javascript1.2"><!--
EXs=screen;EXw=EXs.width;navigator.appName!="Netscape"?
EXb=EXs.colorDepth:EXb=EXs.pixelDepth;//-->
</script><script type="text/javascript"><!--
var EXlogin='gaussian' // Login
var EXvsrv='s9' // VServer
navigator.javaEnabled()==1?EXjv="y":EXjv="n";
EXd=document;EXw?"":EXw="na";EXb?"":EXb="na";
EXd.write("<img src=http://e0.extreme-dm.com",
"/"+EXvsrv+".g?login="+EXlogin+"&amp;",
"jv="+EXjv+"&amp;j=y&amp;srw="+EXw+"&amp;srb="+EXb+"&amp;",
"l="+escape(EXd.referrer)+" height=1 width=1>");//-->
</script><noscript><img height="1" width="1" alt=""
src="http://e0.extreme-dm.com/s9.g?login=gaussian&amp;j=n&amp;jv=n"/>
</noscript>
</td></tr></table>


</body></html>
++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://en.wikipedia.org/wiki/Neural_network>====================
<!DOCTYPE html>
<html lang="en" dir="ltr" class="client-nojs">
<head>
<meta charset="UTF-8" />
<title>Artificial neural network - Wikipedia, the free encyclopedia</title>
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />
<meta name="generator" content="MediaWiki 1.23wmf14" />
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Artificial_neural_network&amp;action=edit" />
<link rel="edit" title="Edit this page" href="/w/index.php?title=Artificial_neural_network&amp;action=edit" />
<link rel="apple-touch-icon" href="//bits.wikimedia.org/apple-touch/wikipedia.png" />
<link rel="shortcut icon" href="//bits.wikimedia.org/favicon/wikipedia.ico" />
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd" />
<link rel="copyright" href="//creativecommons.org/licenses/by-sa/3.0/" />
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
<link rel="canonical" href="http://en.wikipedia.org/wiki/Artificial_neural_network" />
<link rel="stylesheet" href="//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&amp;lang=en&amp;modules=ext.gadget.DRN-wizard%2CReferenceTooltips%2Ccharinsert%2Cteahouse%7Cext.rtlcite%2Cwikihiero%7Cext.uls.nojs%7Cext.visualEditor.viewPageTarget.noscript%7Cmediawiki.legacy.commonPrint%2Cshared%7Cskins.common.interface%7Cskins.vector.styles&amp;only=styles&amp;skin=vector&amp;*" />
<meta name="ResourceLoaderDynamicStyles" content="" />
<link rel="stylesheet" href="//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&amp;lang=en&amp;modules=site&amp;only=styles&amp;skin=vector&amp;*" />
<style>a:lang(ar),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none}
/* cache key: enwiki:resourceloader:filter:minify-css:7:3904d24a08aa08f6a68dc338f9be277e */</style>

<script src="//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector&amp;*"></script>
<script>if(window.mw){
mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Artificial_neural_network","wgTitle":"Artificial neural network","wgCurRevisionId":595510034,"wgRevisionId":595510034,"wgArticleId":21523,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Pages containing cite templates with deprecated parameters","Use dmy dates from June 2013","All articles with unsourced statements","Articles with unsourced statements from August 2011","Articles with unsourced statements from March 2012","Articles needing expert attention with no reason or talk parameter","Articles needing expert attention from November 2008","All articles needing expert attention","Technology articles needing expert attention","Miscellaneous articles needing expert attention","Articles with unsourced statements from August 2012","Articles with Open Directory Project links","Computational statistics","Neural networks","Classification algorithms","Computational neuroscience","Knowledge representation"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Artificial_neural_network","wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgRedirectedFrom":"Neural_network","wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"hidesig":true,"templateEditor":false,"templates":false,"preview":false,"previewDialog":false,"publish":false,"toc":false},"wgArticleFeedbackv5Permissions":{"aft-reader":false,"aft-member":false,"aft-editor":false,"aft-monitor":false,"aft-administrator":false,"aft-oversighter":false,"aft-noone":false},"wgBetaFeaturesFeatures":[],"wgVisualEditor":{"isPageWatched":false,"magnifyClipIconURL":"//bits.wikimedia.org/static-1.23wmf14/skins/common/images/magnify-clip.png","pageLanguageCode":"en","pageLanguageDir":"ltr","svgMaxSize":2048},"wikilove-recipient":"","wikilove-anon":0,"wgGuidedTourHelpGuiderUrl":"Help:Guided tours/guider","wgULSAcceptLanguageList":[],"wgULSCurrentAutonym":"English","wgFlaggedRevsParams":{"tags":{"status":{"levels":1,"quality":2,"pristine":3}}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","Geo":{"city":"","country":""},"wgNoticeProject":"wikipedia","aftv5Article":{"id":21523,"title":"Artificial neural network","namespace":0,"categories":["All articles needing expert attention","All articles with unsourced statements","Articles needing expert attention from November 2008","Articles needing expert attention with no reason or talk parameter","Articles with Open Directory Project links","Articles with unsourced statements from August 2011","Articles with unsourced statements from August 2012","Articles with unsourced statements from March 2012","Classification algorithms","Computational neuroscience","Computational statistics","Knowledge representation","Miscellaneous articles needing expert attention","Neural networks","Pages containing cite templates with deprecated parameters","Technology articles needing expert attention","Use dmy dates from June 2013"],"permissionLevel":false},"wgWikibaseItemId":"Q192776"});
}</script><script>if(window.mw){
mw.loader.implement("user.options",function(){mw.user.options.set({"ccmeonemails":0,"cols":80,"date":"default","diffonly":0,"disablemail":0,"editfont":"default","editondblclick":0,"editsectiononrightclick":0,"enotifminoredits":0,"enotifrevealaddr":0,"enotifusertalkpages":1,"enotifwatchlistpages":0,"extendwatchlist":0,"fancysig":0,"forceeditsummary":0,"gender":"unknown","hideminor":0,"hidepatrolled":0,"imagesize":2,"math":0,"minordefault":0,"newpageshidepatrolled":0,"nickname":"","noconvertlink":0,"norollbackdiff":0,"numberheadings":0,"previewonfirst":0,"previewontop":1,"rcdays":7,"rclimit":50,"rememberpassword":0,"rows":25,"showhiddencats":false,"shownumberswatching":1,"showtoolbar":1,"skin":"vector","stubthreshold":0,"thumbsize":4,"underline":2,"uselivepreview":0,"usenewrc":0,"vector-simplesearch":1,"watchcreations":1,"watchdefault":0,"watchdeletion":0,"watchlistdays":3,"watchlisthideanons":0,"watchlisthidebots":0,"watchlisthideliu":0,"watchlisthideminor":0,"watchlisthideown":0,
"watchlisthidepatrolled":0,"watchmoves":0,"wllimit":250,"useeditwarning":1,"prefershttps":1,"flaggedrevssimpleui":1,"flaggedrevsstable":0,"flaggedrevseditdiffs":true,"flaggedrevsviewdiffs":false,"usebetatoolbar":1,"usebetatoolbar-cgd":1,"aftv5-last-filter":null,"visualeditor-enable":0,"visualeditor-enable-experimental":0,"visualeditor-enable-mwmath":0,"visualeditor-betatempdisable":0,"wikilove-enabled":1,"echo-subscriptions-web-page-review":true,"echo-subscriptions-email-page-review":false,"ep_showtoplink":false,"ep_bulkdelorgs":false,"ep_bulkdelcourses":true,"ep_showdyk":true,"echo-subscriptions-web-education-program":true,"echo-subscriptions-email-education-program":false,"echo-notify-show-link":true,"echo-show-alert":true,"echo-email-frequency":0,"echo-email-format":"html","echo-subscriptions-email-system":true,"echo-subscriptions-web-system":true,"echo-subscriptions-email-other":false,"echo-subscriptions-web-other":true,"echo-subscriptions-email-edit-user-talk":false,
"echo-subscriptions-web-edit-user-talk":true,"echo-subscriptions-email-reverted":false,"echo-subscriptions-web-reverted":true,"echo-subscriptions-email-article-linked":false,"echo-subscriptions-web-article-linked":false,"echo-subscriptions-email-mention":false,"echo-subscriptions-web-mention":true,"echo-subscriptions-web-edit-thank":true,"echo-subscriptions-email-edit-thank":false,"echo-subscriptions-web-flow-discussion":true,"echo-subscriptions-email-flow-discussion":false,"gettingstarted-task-toolbar-show-intro":true,"uls-preferences":"","language":"en","variant-gan":"gan","variant-iu":"iu","variant-kk":"kk","variant-ku":"ku","variant-shi":"shi","variant-sr":"sr","variant-tg":"tg","variant-uz":"uz","variant-zh":"zh","searchNs0":true,"searchNs1":false,"searchNs2":false,"searchNs3":false,"searchNs4":false,"searchNs5":false,"searchNs6":false,"searchNs7":false,"searchNs8":false,"searchNs9":false,"searchNs10":false,"searchNs11":false,"searchNs12":false,"searchNs13":false,"searchNs14":
false,"searchNs15":false,"searchNs100":false,"searchNs101":false,"searchNs108":false,"searchNs109":false,"searchNs118":false,"searchNs119":false,"searchNs446":false,"searchNs447":false,"searchNs710":false,"searchNs711":false,"searchNs828":false,"searchNs829":false,"gadget-teahouse":1,"gadget-ReferenceTooltips":1,"gadget-DRN-wizard":1,"gadget-charinsert":1,"gadget-mySandbox":1,"variant":"en"});},{},{});mw.loader.implement("user.tokens",function(){mw.user.tokens.set({"editToken":"+\\","patrolToken":false,"watchToken":false});},{},{});
/* cache key: enwiki:resourceloader:filter:minify-js:7:5e4ad49afa9f26a7ed4e93dac6d2e189 */
}</script>
<script>if(window.mw){
mw.loader.load(["mediawiki.page.startup","mediawiki.legacy.wikibits","mediawiki.legacy.ajax","ext.centralauth.centralautologin","ext.visualEditor.viewPageTarget.init","ext.uls.init","ext.uls.interface","wikibase.client.init","ext.centralNotice.bannerController","skins.vector.js"]);
}</script>
<script src="//bits.wikimedia.org/geoiplookup"></script><link rel="dns-prefetch" href="//meta.wikimedia.org" /><!--[if lt IE 7]><style type="text/css">body{behavior:url("/w/static-1.23wmf14/skins/vector/csshover.min.htc")}</style><![endif]--></head>
<body class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Artificial_neural_network skin-vector action-view vector-animateLayout">
		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>
			<div id="mw-js-message" style="display:none;"></div>
						<div id="siteNotice"><!-- CentralNotice --></div>
						<h1 id="firstHeading" class="firstHeading" lang="en"><span dir="auto">Artificial neural network</span></h1>
			<div id="bodyContent">
								<div id="siteSub">From Wikipedia, the free encyclopedia</div>
								<div id="contentSub">먝(Redirected from <a href="/w/index.php?title=Neural_network&amp;redirect=no" title="Neural network">Neural network</a>)</div>
												<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="#mw-navigation">navigation</a>, 					<a href="#p-search">search</a>
				</div>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="dablink">"Neural network" redirects here. For networks of living neurons, see <a href="/wiki/Biological_neural_network" title="Biological neural network">Biological neural network</a>.  For the journal, see <a href="/wiki/Neural_Networks_(journal)" title="Neural Networks (journal)">Neural Networks (journal)</a>.</div>
<table class="vertical-navbox nowraplinks" cellspacing="5" cellpadding="0" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%;">
<tr>
<th class="" style=";padding:0.2em 0.4em 0.2em;;font-size:145%;line-height:1.2em;"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br />
<a href="/wiki/Data_mining" title="Data mining">data mining</a></th>
</tr>
<tr>
<td class="" style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="/wiki/File:Linear-svm-scatterplot.svg" class="image" title="Scatterplot featuring a linear support vector machine's decision boundary (dashed line)"><img alt="Scatterplot featuring a linear support vector machine's decision boundary (dashed line)" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Linear-svm-scatterplot.svg/220px-Linear-svm-scatterplot.svg.png" width="220" height="165" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Linear-svm-scatterplot.svg/330px-Linear-svm-scatterplot.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/46/Linear-svm-scatterplot.svg/440px-Linear-svm-scatterplot.svg.png 2x" /></a></td>
</tr>
<tr>
<th class="" style="padding:0.1em;;border-top:1px solid #aaa;;">Problems</th>
</tr>
<tr>
<td class="" style="padding:0 0.1em 0.4em;;">
<div class="hlist">
<ul>
<li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th class="" style="padding:0.1em;;border-top:1px solid #aaa;;">
<div style="padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br />
<span style="font-weight:normal;"><small style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160; <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</small></span></div>
</th>
</tr>
<tr>
<td class="" style="padding:0 0.1em 0.4em;;">
<div class="hlist">
<ul>
<li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a> (<a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a>, <a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a>, <a href="/wiki/Random_forest" title="Random forest">Random forest</a>)</li>
<li><a href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification" class="mw-redirect"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><strong class="selflink">Neural networks</strong></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th class="" style="padding:0.1em;;border-top:1px solid #aaa;;"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></th>
</tr>
<tr>
<td class="" style="padding:0 0.1em 0.4em;;">
<div class="hlist">
<ul>
<li><a href="/wiki/BIRCH_(data_clustering)" title="BIRCH (data clustering)">BIRCH</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation-maximization_algorithm" title="Expectation-maximization algorithm" class="mw-redirect">Expectation-maximization (EM)</a></li>
<li><br />
<a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th class="" style="padding:0.1em;;border-top:1px solid #aaa;;"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></th>
</tr>
<tr>
<td class="" style="padding:0 0.1em 0.4em;;">
<div class="hlist">
<ul>
<li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation_analysis" title="Canonical correlation analysis" class="mw-redirect">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/T-Distributed_Stochastic_Neighbor_Embedding" title="T-Distributed Stochastic Neighbor Embedding">t-SNE</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th class="" style="padding:0.1em;;border-top:1px solid #aaa;;"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></th>
</tr>
<tr>
<td class="" style="padding:0 0.1em 0.4em;;">
<div class="hlist">
<ul>
<li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a> (<a href="/wiki/Conditional_random_field" title="Conditional random field">CRF</a>, <a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">HMM</a>)</li>
</ul>
</div>
</td>
</tr>
<tr>
<th class="" style="padding:0.1em;;border-top:1px solid #aaa;;"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></th>
</tr>
<tr>
<td class="" style="padding:0 0.1em 0.4em;;">
<div class="hlist">
<ul>
<li><a href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification" class="mw-redirect"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th class="" style="padding:0.1em;;border-top:1px solid #aaa;;"><strong class="selflink">ANN</strong></th>
</tr>
<tr>
<td class="" style="padding:0 0.1em 0.4em;;">
<div class="hlist">
<ul>
<li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">Feedforward neural network</a></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Deep_Learning" title="Deep Learning" class="mw-redirect">Deep Learning</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th class="" style="padding:0.1em;;border-top:1px solid #aaa;;">Theory</th>
</tr>
<tr>
<td class="" style="padding:0 0.1em 0.4em;;">
<div class="hlist">
<ul>
<li><a href="/wiki/Bias-variance_dilemma" title="Bias-variance dilemma">Bias-variance dilemma</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik뉶hervonenkis theory">VC theory</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<td class="plainlist" style="padding:0.3em 0.4em 0.3em;font-weight:bold;border-top:1px solid #aaa;border-bottom:1px solid #aaa;">
<ul>
<li><a href="/wiki/File:Internet_map_1024.jpg" class="image"><img alt="Portal icon" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Internet_map_1024.jpg/16px-Internet_map_1024.jpg" width="16" height="16" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Internet_map_1024.jpg/24px-Internet_map_1024.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Internet_map_1024.jpg/32px-Internet_map_1024.jpg 2x" /></a> <a href="/wiki/Portal:Computer_science" title="Portal:Computer science">Computer science portal</a></li>
<li><a href="/wiki/File:Fisher_iris_versicolor_sepalwidth.svg" class="image"><img alt="Portal icon" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/16px-Fisher_iris_versicolor_sepalwidth.svg.png" width="16" height="11" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/24px-Fisher_iris_versicolor_sepalwidth.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/32px-Fisher_iris_versicolor_sepalwidth.svg.png 2x" /></a> <a href="/wiki/Portal:Statistics" title="Portal:Statistics">Statistics portal</a></li>
</ul>
</td>
</tr>
<tr>
<td style="text-align:right;font-size:115%;">
<div class="plainlinks hlist navbar mini" style="">
<ul>
<li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><span title="View this template" style="">v</span></a></li>
<li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><span title="Discuss this template" style="">t</span></a></li>
<li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><span title="Edit this template" style="">e</span></a></li>
</ul>
</div>
</td>
</tr>
</table>
<div class="thumb tright">
<div class="thumbinner" style="width:302px;"><a href="/wiki/File:Colored_neural_network.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png" width="300" height="361" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/450px-Colored_neural_network.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/600px-Colored_neural_network.svg.png 2x" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Colored_neural_network.svg" class="internal" title="Enlarge"><img src="//bits.wikimedia.org/static-1.23wmf14/skins/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
An artificial neural network is an interconnected group of nodes, akin to the vast network of <a href="/wiki/Neuron" title="Neuron">neurons</a> in a <a href="/wiki/Brain" title="Brain">brain</a>. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one neuron to the input of another.</div>
</div>
</div>
<p>In <a href="/wiki/Computer_science" title="Computer science">computer science</a> and related fields, <b>artificial neural networks</b> are computational <a href="/wiki/Statistical_model" title="Statistical model">models</a> inspired by animals' <a href="/wiki/Central_nervous_system" title="Central nervous system">central nervous systems</a> (in particular the <a href="/wiki/Brain" title="Brain">brain</a>) that are capable of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>. They are usually presented as systems of interconnected "<a href="/wiki/Artificial_neuron" title="Artificial neuron">neurons</a>" that can compute values from inputs by feeding information through the network.</p>
<p>For example, in a neural network for <a href="/wiki/Handwriting_recognition" title="Handwriting recognition">handwriting recognition</a>, a set of input neurons may be activated by the pixels of an input image representing a letter or digit. The activations of these neurons are then passed on, weighted and transformed by some function determined by the network's designer, to other neurons, etc., until finally an output neuron is activated that determines which character was read.</p>
<p>Like other machine learning methods, neural networks have been used to solve a wide variety of tasks that are hard to solve using ordinary rule-based programming, including <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> and <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>.</p>
<p></p>
<div id="toc" class="toc">
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Background"><span class="tocnumber">1</span> <span class="toctext">Background</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#History"><span class="tocnumber">2</span> <span class="toctext">History</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Recent_improvements"><span class="tocnumber">2.1</span> <span class="toctext">Recent improvements</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Successes_in_pattern_recognition_contests_since_2009"><span class="tocnumber">2.2</span> <span class="toctext">Successes in pattern recognition contests since 2009</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-5"><a href="#Models"><span class="tocnumber">3</span> <span class="toctext">Models</span></a>
<ul>
<li class="toclevel-2 tocsection-6"><a href="#Network_function"><span class="tocnumber">3.1</span> <span class="toctext">Network function</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Learning"><span class="tocnumber">3.2</span> <span class="toctext">Learning</span></a>
<ul>
<li class="toclevel-3 tocsection-8"><a href="#Choosing_a_cost_function"><span class="tocnumber">3.2.1</span> <span class="toctext">Choosing a cost function</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-9"><a href="#Learning_paradigms"><span class="tocnumber">3.3</span> <span class="toctext">Learning paradigms</span></a>
<ul>
<li class="toclevel-3 tocsection-10"><a href="#Supervised_learning"><span class="tocnumber">3.3.1</span> <span class="toctext">Supervised learning</span></a></li>
<li class="toclevel-3 tocsection-11"><a href="#Unsupervised_learning"><span class="tocnumber">3.3.2</span> <span class="toctext">Unsupervised learning</span></a></li>
<li class="toclevel-3 tocsection-12"><a href="#Reinforcement_learning"><span class="tocnumber">3.3.3</span> <span class="toctext">Reinforcement learning</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-13"><a href="#Learning_algorithms"><span class="tocnumber">3.4</span> <span class="toctext">Learning algorithms</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-14"><a href="#Employing_artificial_neural_networks"><span class="tocnumber">4</span> <span class="toctext">Employing artificial neural networks</span></a></li>
<li class="toclevel-1 tocsection-15"><a href="#Applications"><span class="tocnumber">5</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2 tocsection-16"><a href="#Real-life_applications"><span class="tocnumber">5.1</span> <span class="toctext">Real-life applications</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="#Neural_networks_and_neuroscience"><span class="tocnumber">5.2</span> <span class="toctext">Neural networks and neuroscience</span></a>
<ul>
<li class="toclevel-3 tocsection-18"><a href="#Types_of_models"><span class="tocnumber">5.2.1</span> <span class="toctext">Types of models</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-19"><a href="#Neural_network_software"><span class="tocnumber">6</span> <span class="toctext">Neural network software</span></a></li>
<li class="toclevel-1 tocsection-20"><a href="#Types_of_artificial_neural_networks"><span class="tocnumber">7</span> <span class="toctext">Types of artificial neural networks</span></a></li>
<li class="toclevel-1 tocsection-21"><a href="#Theoretical_properties"><span class="tocnumber">8</span> <span class="toctext">Theoretical properties</span></a>
<ul>
<li class="toclevel-2 tocsection-22"><a href="#Computational_power"><span class="tocnumber">8.1</span> <span class="toctext">Computational power</span></a></li>
<li class="toclevel-2 tocsection-23"><a href="#Capacity"><span class="tocnumber">8.2</span> <span class="toctext">Capacity</span></a></li>
<li class="toclevel-2 tocsection-24"><a href="#Convergence"><span class="tocnumber">8.3</span> <span class="toctext">Convergence</span></a></li>
<li class="toclevel-2 tocsection-25"><a href="#Generalization_and_statistics"><span class="tocnumber">8.4</span> <span class="toctext">Generalization and statistics</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="#Dynamic_properties"><span class="tocnumber">8.5</span> <span class="toctext">Dynamic properties</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-27"><a href="#Criticism"><span class="tocnumber">9</span> <span class="toctext">Criticism</span></a></li>
<li class="toclevel-1 tocsection-28"><a href="#Gallery"><span class="tocnumber">10</span> <span class="toctext">Gallery</span></a></li>
<li class="toclevel-1 tocsection-29"><a href="#See_also"><span class="tocnumber">11</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-30"><a href="#References"><span class="tocnumber">12</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-31"><a href="#Bibliography"><span class="tocnumber">13</span> <span class="toctext">Bibliography</span></a></li>
<li class="toclevel-1 tocsection-32"><a href="#External_links"><span class="tocnumber">14</span> <span class="toctext">External links</span></a></li>
</ul>
</div>
<p></p>
<h2><span class="mw-headline" id="Background">Background</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=1" title="Edit section: Background">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The inspiration for the neural networks came from examination of <a href="/wiki/Central_nervous_system" title="Central nervous system">central nervous systems</a>. In an artificial neural network, simple artificial <a href="/wiki/Node_(neural_networks)" title="Node (neural networks)" class="mw-redirect">nodes</a>, called "<a href="/wiki/Artificial_neuron" title="Artificial neuron">neurons</a>", "neurodes", "processing elements" or "units", are connected together to form a network which mimics a biological neural network.</p>
<p>There is no single formal definition of what an artificial neural network is. Commonly, a class of statistical models may be called "neural" if they</p>
<ol>
<li>consist of sets of <a href="/wiki/Adaptive_systems" title="Adaptive systems" class="mw-redirect">adaptive</a> weights, i.e. numerical parameters that are tuned by a learning <a href="/wiki/Algorithm" title="Algorithm">algorithm</a>, and</li>
<li>are capable of <a href="/wiki/Approximation_theory" title="Approximation theory">approximating</a> non-linear functions of their inputs.</li>
</ol>
<p>The adaptive weights are conceptually connection strengths between neurons, which are activated during training and prediction.</p>
<p>Neural networks are also similar to biological neural networks in performing functions collectively and in parallel by the units, rather than there being a clear delineation of subtasks to which various units are assigned. The term "neural network" usually refers to models employed in <a href="/wiki/Statistics" title="Statistics">statistics</a>, <a href="/wiki/Cognitive_psychology" title="Cognitive psychology">cognitive psychology</a> and <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>. Neural network models which emulate the central nervous system are part of <a href="/wiki/Theoretical_neuroscience" title="Theoretical neuroscience" class="mw-redirect">theoretical neuroscience</a> and <a href="/wiki/Computational_neuroscience" title="Computational neuroscience">computational neuroscience</a>.</p>
<p>In modern <a href="/wiki/Neural_network_software" title="Neural network software">software implementations</a> of artificial neural networks, the approach inspired by biology has been largely abandoned for a more practical approach based on statistics and signal processing. In some of these systems, neural networks or parts of neural networks (like artificial neurons) form components in larger systems that combine both adaptive and non-adaptive elements. While the more general approach of such systems is more suitable for real-world problem solving, it has little to do with the traditional artificial intelligence connectionist models. What they do have in common, however, is the principle of non-linear, distributed, parallel and local processing and adaptation. Historically, the use of neural networks models marked a paradigm shift in the late eighties from high-level (symbolic) <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>, characterized by <a href="/wiki/Expert_system" title="Expert system">expert systems</a> with knowledge embodied in <i>if-then</i> rules, to low-level (sub-symbolic) <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, characterized by knowledge embodied in the parameters of a <a href="/wiki/Cognitive_Model#Dynamical_systems" title="Cognitive Model" class="mw-redirect">dynamical system</a>.</p>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=2" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><a href="/wiki/Warren_McCulloch" title="Warren McCulloch" class="mw-redirect">Warren McCulloch</a> and <a href="/wiki/Walter_Pitts" title="Walter Pitts">Walter Pitts</a><sup id="cite_ref-1" class="reference"><a href="#cite_note-1"><span>[</span>1<span>]</span></a></sup> (1943) created a computational model for neural networks based on <a href="/wiki/Mathematics" title="Mathematics">mathematics</a> and algorithms. They called this model <a href="/w/index.php?title=Threshold_logic&amp;action=edit&amp;redlink=1" class="new" title="Threshold logic (page does not exist)">threshold logic</a>. The model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.</p>
<p>In the late 1940s psychologist <a href="/wiki/Donald_Hebb" title="Donald Hebb" class="mw-redirect">Donald Hebb</a><sup id="cite_ref-2" class="reference"><a href="#cite_note-2"><span>[</span>2<span>]</span></a></sup> created a hypothesis of learning based on the mechanism of neural plasticity that is now known as <a href="/wiki/Hebbian_learning" title="Hebbian learning" class="mw-redirect">Hebbian learning</a>. Hebbian learning is considered to be a 'typical' <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a> rule and its later variants were early models for <a href="/wiki/Long_term_potentiation" title="Long term potentiation" class="mw-redirect">long term potentiation</a>. These ideas started being applied to computational models in 1948 with <a href="/wiki/Unorganized_machine" title="Unorganized machine">Turing's B-type machines</a>.</p>
<p>Farley and Clark<sup id="cite_ref-3" class="reference"><a href="#cite_note-3"><span>[</span>3<span>]</span></a></sup> (1954) first used computational machines, then called calculators, to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda<sup id="cite_ref-4" class="reference"><a href="#cite_note-4"><span>[</span>4<span>]</span></a></sup> (1956).</p>
<p><a href="/wiki/Frank_Rosenblatt" title="Frank Rosenblatt">Frank Rosenblatt</a><sup id="cite_ref-5" class="reference"><a href="#cite_note-5"><span>[</span>5<span>]</span></a></sup> (1958) created the <a href="/wiki/Perceptron" title="Perceptron">perceptron</a>, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the <a href="/wiki/Exclusive-or" title="Exclusive-or" class="mw-redirect">exclusive-or</a> circuit, a circuit whose mathematical computation could not be processed until after the <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a> algorithm was created by <a href="/wiki/Paul_Werbos" title="Paul Werbos">Paul Werbos</a><sup id="cite_ref-Werbos_1975_6-0" class="reference"><a href="#cite_note-Werbos_1975-6"><span>[</span>6<span>]</span></a></sup> (1975).</p>
<p>Neural network research stagnated after the publication of machine learning research by <a href="/wiki/Marvin_Minsky" title="Marvin Minsky">Marvin Minsky</a> and <a href="/wiki/Seymour_Papert" title="Seymour Papert">Seymour Papert</a><sup id="cite_ref-7" class="reference"><a href="#cite_note-7"><span>[</span>7<span>]</span></a></sup> (1969). They discovered two key issues with the computational machines that processed neural networks. The first issue was that single-layer neural networks were incapable of processing the exclusive-or circuit. The second significant issue was that computers were not sophisticated enough to effectively handle the long run time required by large neural networks. Neural network research slowed until computers achieved greater processing power. Also key later advances was the <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a> algorithm which effectively solved the exclusive-or problem (Werbos 1975).<sup id="cite_ref-Werbos_1975_6-1" class="reference"><a href="#cite_note-Werbos_1975-6"><span>[</span>6<span>]</span></a></sup></p>
<p>The <a href="/wiki/Connectionism" title="Connectionism">parallel distributed processing</a> of the mid-1980s became popular under the name <a href="/wiki/Connectionism" title="Connectionism">connectionism</a>. The text by <a href="/wiki/David_E._Rumelhart" title="David E. Rumelhart" class="mw-redirect">David E. Rumelhart</a> and <a href="/wiki/James_McClelland_(psychologist)" title="James McClelland (psychologist)">James McClelland</a><sup id="cite_ref-8" class="reference"><a href="#cite_note-8"><span>[</span>8<span>]</span></a></sup> (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes.</p>
<p>Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of <a href="/w/index.php?title=Neural_processing&amp;action=edit&amp;redlink=1" class="new" title="Neural processing (page does not exist)">neural processing</a> in the brain, even though the relation between this model and brain biological architecture is debated, as it is not clear to what degree artificial neural networks mirror brain function.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9"><span>[</span>9<span>]</span></a></sup></p>
<p>In the 1990s, neural networks were overtaken in popularity in machine learning by <a href="/wiki/Support_vector_machine" title="Support vector machine">support vector machines</a> and other, much simpler methods such as <a href="/wiki/Linear_classifier" title="Linear classifier">linear classifiers</a>. Renewed interest in neural nets was sparked in the 2000s by the advent of <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>.</p>
<h3><span class="mw-headline" id="Recent_improvements">Recent improvements</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=3" title="Edit section: Recent improvements">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Biophysics" title="Biophysics">Biophysical</a> models, such as <a href="/wiki/BCM_theory" title="BCM theory">BCM theory</a>, have been important in understanding mechanisms for <a href="/wiki/Synaptic_plasticity" title="Synaptic plasticity">synaptic plasticity</a>, and have had applications in both computer science and neuroscience. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for <a href="/wiki/Radial_basis_networks" title="Radial basis networks" class="mw-redirect">radial basis networks</a> and <a href="/wiki/Neural_backpropagation" title="Neural backpropagation">neural backpropagation</a> as mechanisms for processing data.</p>
<p>Computational devices have been created in CMOS, for both biophysical simulation and <a href="/wiki/Neuromorphic_computing" title="Neuromorphic computing" class="mw-redirect">neuromorphic computing</a>. More recent efforts show promise for creating <a href="/wiki/Nanodevice" title="Nanodevice" class="mw-redirect">nanodevices</a><sup id="cite_ref-10" class="reference"><a href="#cite_note-10"><span>[</span>10<span>]</span></a></sup> for very large scale <a href="/wiki/Principal_component" title="Principal component" class="mw-redirect">principal components</a> analyses and <a href="/wiki/Convolution" title="Convolution">convolution</a>. If successful, these efforts could usher in a new era of <a href="/wiki/Neural_computing" title="Neural computing" class="mw-redirect">neural computing</a><sup id="cite_ref-11" class="reference"><a href="#cite_note-11"><span>[</span>11<span>]</span></a></sup> that is a step beyond digital computing, because it depends on <a href="/wiki/Learning" title="Learning">learning</a> rather than <a href="/wiki/Programming_language" title="Programming language">programming</a> and because it is fundamentally <a href="/wiki/Analog_signal" title="Analog signal">analog</a> rather than <a href="/wiki/Digital_data" title="Digital data">digital</a> even though the first instantiations may in fact be with CMOS digital devices.</p>
<p>Between 2009 and 2012, the <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural networks</a> and deep feedforward neural networks developed in the research group of <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="J칲rgen Schmidhuber">J칲rgen Schmidhuber</a> at the <a href="/wiki/IDSIA" title="IDSIA" class="mw-redirect">Swiss AI Lab IDSIA</a> have won eight international competitions in <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>.<sup id="cite_ref-12" class="reference"><a href="#cite_note-12"><span>[</span>12<span>]</span></a></sup> For example, multi-dimensional <a href="/wiki/Long_short_term_memory" title="Long short term memory">long short term memory</a> (LSTM)<sup id="cite_ref-13" class="reference"><a href="#cite_note-13"><span>[</span>13<span>]</span></a></sup><sup id="cite_ref-14" class="reference"><a href="#cite_note-14"><span>[</span>14<span>]</span></a></sup> won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.</p>
<p>Variants of the <a href="/wiki/Back-propagation" title="Back-propagation" class="mw-redirect">back-propagation</a> algorithm as well as unsupervised methods by <a href="/wiki/Geoff_Hinton" title="Geoff Hinton" class="mw-redirect">Geoff Hinton</a> and colleagues at the <a href="/wiki/University_of_Toronto" title="University of Toronto">University of Toronto</a><sup id="cite_ref-15" class="reference"><a href="#cite_note-15"><span>[</span>15<span>]</span></a></sup><sup id="cite_ref-16" class="reference"><a href="#cite_note-16"><span>[</span>16<span>]</span></a></sup> can be used to train deep, highly nonlinear neural architectures similar to the 1980 <a href="/wiki/Neocognitron" title="Neocognitron">Neocognitron</a> by <a href="/wiki/Kunihiko_Fukushima" title="Kunihiko Fukushima" class="mw-redirect">Kunihiko Fukushima</a>,<sup id="cite_ref-K._Fukushima._Neocognitron_1980_17-0" class="reference"><a href="#cite_note-K._Fukushima._Neocognitron_1980-17"><span>[</span>17<span>]</span></a></sup> and the "standard architecture of vision",<sup id="cite_ref-M_Riesenhuber.2C_1999_18-0" class="reference"><a href="#cite_note-M_Riesenhuber.2C_1999-18"><span>[</span>18<span>]</span></a></sup> inspired by the simple and complex cells identified by <a href="/wiki/David_H._Hubel" title="David H. Hubel">David H. Hubel</a> and <a href="/wiki/Torsten_Wiesel" title="Torsten Wiesel">Torsten Wiesel</a> in the primary <a href="/wiki/Visual_cortex" title="Visual cortex">visual cortex</a>.</p>
<p><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a> feedforward networks, such as <a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">convolutional neural networks</a>, alternate <a href="/wiki/Convolution" title="Convolution">convolutional</a> layers and max-pooling layers, topped by several pure <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> layers. Fast <a href="/wiki/GPU" title="GPU" class="mw-redirect">GPU</a>-based implementations of this approach have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition<sup id="cite_ref-19" class="reference"><a href="#cite_note-19"><span>[</span>19<span>]</span></a></sup> and the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge.<sup id="cite_ref-D._Ciresan.2C_A._Giusti_2012_20-0" class="reference"><a href="#cite_note-D._Ciresan.2C_A._Giusti_2012-20"><span>[</span>20<span>]</span></a></sup> Such neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance<sup id="cite_ref-C._Ciresan.2C_U._Meier_2012_21-0" class="reference"><a href="#cite_note-C._Ciresan.2C_U._Meier_2012-21"><span>[</span>21<span>]</span></a></sup> on benchmarks such as traffic sign recognition (IJCNN 2012), or the <a href="/wiki/MNIST_dataset" title="MNIST dataset" class="mw-redirect">MNIST handwritten digits problem</a> of <a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a> and colleagues at <a href="/wiki/NYU" title="NYU" class="mw-redirect">NYU</a>.</p>
<h3><span class="mw-headline" id="Successes_in_pattern_recognition_contests_since_2009">Successes in pattern recognition contests since 2009</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=4" title="Edit section: Successes in pattern recognition contests since 2009">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Between 2009 and 2012, the <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural networks</a> and deep feedforward neural networks developed in the research group of <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="J칲rgen Schmidhuber">J칲rgen Schmidhuber</a> at the <a href="/wiki/IDSIA" title="IDSIA" class="mw-redirect">Swiss AI Lab IDSIA</a> have won eight international competitions in <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>.<sup id="cite_ref-22" class="reference"><a href="#cite_note-22"><span>[</span>22<span>]</span></a></sup> For example, the bi-directional and <a href="/wiki/Multi-dimensional" title="Multi-dimensional" class="mw-redirect">multi-dimensional</a> <a href="/wiki/Long_short_term_memory" title="Long short term memory">long short term memory</a> (LSTM)<sup id="cite_ref-23" class="reference"><a href="#cite_note-23"><span>[</span>23<span>]</span></a></sup><sup id="cite_ref-24" class="reference"><a href="#cite_note-24"><span>[</span>24<span>]</span></a></sup> of Alex Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned. Fast <a href="/wiki/GPU" title="GPU" class="mw-redirect">GPU</a>-based implementations of this approach by Dan Ciresan and colleagues at <a href="/wiki/IDSIA" title="IDSIA" class="mw-redirect">IDSIA</a> have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition,<sup id="cite_ref-25" class="reference"><a href="#cite_note-25"><span>[</span>25<span>]</span></a></sup> the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge,<sup id="cite_ref-D._Ciresan.2C_A._Giusti_2012_20-1" class="reference"><a href="#cite_note-D._Ciresan.2C_A._Giusti_2012-20"><span>[</span>20<span>]</span></a></sup> and others. Their neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance<sup id="cite_ref-C._Ciresan.2C_U._Meier_2012_21-1" class="reference"><a href="#cite_note-C._Ciresan.2C_U._Meier_2012-21"><span>[</span>21<span>]</span></a></sup> on important benchmarks such as traffic sign recognition (IJCNN 2012), or the <a href="/wiki/MNIST_database" title="MNIST database">MNIST handwritten digits problem</a> of <a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a> at <a href="/wiki/NYU" title="NYU" class="mw-redirect">NYU</a>. Deep, highly nonlinear neural architectures similar to the 1980 <a href="/wiki/Neocognitron" title="Neocognitron">neocognitron</a> by <a href="/wiki/Kunihiko_Fukushima" title="Kunihiko Fukushima" class="mw-redirect">Kunihiko Fukushima</a><sup id="cite_ref-K._Fukushima._Neocognitron_1980_17-1" class="reference"><a href="#cite_note-K._Fukushima._Neocognitron_1980-17"><span>[</span>17<span>]</span></a></sup> and the "standard architecture of vision"<sup id="cite_ref-M_Riesenhuber.2C_1999_18-1" class="reference"><a href="#cite_note-M_Riesenhuber.2C_1999-18"><span>[</span>18<span>]</span></a></sup> can also be pre-trained by unsupervised methods<sup id="cite_ref-26" class="reference"><a href="#cite_note-26"><span>[</span>26<span>]</span></a></sup><sup id="cite_ref-27" class="reference"><a href="#cite_note-27"><span>[</span>27<span>]</span></a></sup> of <a href="/wiki/Geoff_Hinton" title="Geoff Hinton" class="mw-redirect">Geoff Hinton</a>'s lab at <a href="/wiki/University_of_Toronto" title="University of Toronto">University of Toronto</a>. A team from this lab won a 2012 contest sponsored by <a href="/wiki/Merck_%26_Co." title="Merck &amp; Co.">Merck</a> to design software to help find molecules that might lead to new drugs.<sup id="cite_ref-28" class="reference"><a href="#cite_note-28"><span>[</span>28<span>]</span></a></sup></p>
<h2><span class="mw-headline" id="Models">Models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=5" title="Edit section: Models">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Neural network models in artificial intelligence are usually referred to as artificial neural networks (ANNs); these are essentially simple mathematical models defining a function <img class="tex" alt="\scriptstyle  f&#160;: X \rightarrow Y " src="//upload.wikimedia.org/math/2/a/f/2af59bc191a1b0f7e4d9429d4014995d.png" /> or a distribution over <img class="tex" alt="\scriptstyle X" src="//upload.wikimedia.org/math/5/1/c/51cea10940d0755e9c5b34dff3c328fd.png" /> or both <img class="tex" alt="\scriptstyle X" src="//upload.wikimedia.org/math/5/1/c/51cea10940d0755e9c5b34dff3c328fd.png" /> and <img class="tex" alt="\scriptstyle Y" src="//upload.wikimedia.org/math/f/6/2/f622e012a22e65b1660aaff8a2fcbf21.png" />, but sometimes models are also intimately associated with a particular learning algorithm or learning rule. A common use of the phrase ANN model really means the definition of a <i>class</i> of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity).</p>
<h3><span class="mw-headline" id="Network_function">Network function</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=6" title="Edit section: Network function">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="rellink boilerplate seealso">See also: <a href="/wiki/Graphical_models" title="Graphical models" class="mw-redirect">Graphical models</a></div>
<p>The word <i>network</i> in the term 'artificial neural network' refers to the inter늒onnections between the neurons in the different layers of each system. An example system has three layers. The first layer has input neurons which send data via synapses to the second layer of neurons, and then via more synapses to the third layer of output neurons. More complex systems will have more layers of neurons with some having increased layers of input neurons and output neurons. The synapses store parameters called "weights" that manipulate the data in the calculations.</p>
<p>An ANN is typically defined by three types of parameters:</p>
<ol>
<li>The interconnection pattern between the different layers of neurons</li>
<li>The learning process for updating the weights of the interconnections</li>
<li>The activation function that converts a neuron's weighted input to its output activation.</li>
</ol>
<p>Mathematically, a neuron's network function <img class="tex" alt="\scriptstyle f(x)" src="//upload.wikimedia.org/math/e/6/d/e6dd6df2c92712c6b5d407fa452f1fd5.png" /> is defined as a composition of other functions <img class="tex" alt="\scriptstyle g_i(x)" src="//upload.wikimedia.org/math/9/4/2/9424c5af5ec4c5a093b33b123a24301c.png" />, which can further be defined as a composition of other functions. This can be conveniently represented as a network structure, with arrows depicting the dependencies between variables. A widely used type of composition is the <i>nonlinear weighted sum</i>, where <img class="tex" alt="\scriptstyle f (x) = K \left(\sum_i w_i g_i(x)\right) " src="//upload.wikimedia.org/math/6/c/0/6c0f522f813b3b21914013a9ab4894f8.png" />, where <img class="tex" alt="\scriptstyle K" src="//upload.wikimedia.org/math/5/8/a/58a0d43f50180ea2c46c506e745f0e8d.png" /> (commonly referred to as the <a href="/wiki/Activation_function" title="Activation function">activation function</a><sup id="cite_ref-29" class="reference"><a href="#cite_note-29"><span>[</span>29<span>]</span></a></sup>) is some predefined function, such as the <a href="/wiki/Hyperbolic_tangent" title="Hyperbolic tangent" class="mw-redirect">hyperbolic tangent</a>. It will be convenient for the following to refer to a collection of functions <img class="tex" alt="\scriptstyle g_i" src="//upload.wikimedia.org/math/a/c/f/acfe8f43673ae57644ff499d3775769b.png" /> as simply a vector <img class="tex" alt="\scriptstyle g = (g_1, g_2, \ldots, g_n)" src="//upload.wikimedia.org/math/9/a/3/9a31da9ddd5ffbb67d441602edf34b80.png" />.</p>
<div class="thumb tright">
<div class="thumbinner" style="width:152px;"><a href="/wiki/File:Ann_dependency_(graph).svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Ann_dependency_%28graph%29.svg/150px-Ann_dependency_%28graph%29.svg.png" width="150" height="107" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Ann_dependency_%28graph%29.svg/225px-Ann_dependency_%28graph%29.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Ann_dependency_%28graph%29.svg/300px-Ann_dependency_%28graph%29.svg.png 2x" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Ann_dependency_(graph).svg" class="internal" title="Enlarge"><img src="//bits.wikimedia.org/static-1.23wmf14/skins/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
ANN dependency graph</div>
</div>
</div>
<p>This figure depicts such a decomposition of <img class="tex" alt="\scriptstyle f" src="//upload.wikimedia.org/math/d/5/2/d52558b155faf73f2ed588accb0539bd.png" />, with dependencies between variables indicated by arrows. These can be interpreted in two ways.</p>
<p>The first view is the functional view: the input <img class="tex" alt="\scriptstyle x" src="//upload.wikimedia.org/math/0/a/2/0a23b9f6eb1e2996867bc4b08879589d.png" /> is transformed into a 3-dimensional vector <img class="tex" alt="\scriptstyle h" src="//upload.wikimedia.org/math/c/c/f/ccff27d04fdb4f47241396d2e4deb952.png" />, which is then transformed into a 2-dimensional vector <img class="tex" alt="\scriptstyle g" src="//upload.wikimedia.org/math/6/c/6/6c6efbd2ea6fe06a26191b4ca5aff835.png" />, which is finally transformed into <img class="tex" alt="\scriptstyle f" src="//upload.wikimedia.org/math/d/5/2/d52558b155faf73f2ed588accb0539bd.png" />. This view is most commonly encountered in the context of <a href="/wiki/Mathematical_optimization" title="Mathematical optimization">optimization</a>.</p>
<p>The second view is the probabilistic view: the <a href="/wiki/Random_variable" title="Random variable">random variable</a> <img class="tex" alt="\scriptstyle F = f(G) " src="//upload.wikimedia.org/math/d/1/7/d179d433af97bacc19fd47722ab13593.png" /> depends upon the random variable <img class="tex" alt="\scriptstyle G = g(H)" src="//upload.wikimedia.org/math/f/a/4/fa44817e8b4a3c009752995249a7a62e.png" />, which depends upon <img class="tex" alt="\scriptstyle H=h(X)" src="//upload.wikimedia.org/math/1/3/b/13b3083b8f2fb1c7b7167c7eb4dac524.png" />, which depends upon the random variable <img class="tex" alt="\scriptstyle X" src="//upload.wikimedia.org/math/5/1/c/51cea10940d0755e9c5b34dff3c328fd.png" />. This view is most commonly encountered in the context of <a href="/wiki/Graphical_models" title="Graphical models" class="mw-redirect">graphical models</a>.</p>
<p>The two views are largely equivalent. In either case, for this particular network architecture, the components of individual layers are independent of each other (e.g., the components of <img class="tex" alt="\scriptstyle g" src="//upload.wikimedia.org/math/6/c/6/6c6efbd2ea6fe06a26191b4ca5aff835.png" /> are independent of each other given their input <img class="tex" alt="\scriptstyle h" src="//upload.wikimedia.org/math/c/c/f/ccff27d04fdb4f47241396d2e4deb952.png" />). This naturally enables a degree of parallelism in the implementation.</p>
<div class="thumb tright">
<div class="thumbinner" style="width:122px;"><a href="/wiki/File:Recurrent_ann_dependency_graph.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/7/79/Recurrent_ann_dependency_graph.png/120px-Recurrent_ann_dependency_graph.png" width="120" height="134" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/79/Recurrent_ann_dependency_graph.png/180px-Recurrent_ann_dependency_graph.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/79/Recurrent_ann_dependency_graph.png/240px-Recurrent_ann_dependency_graph.png 2x" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Recurrent_ann_dependency_graph.png" class="internal" title="Enlarge"><img src="//bits.wikimedia.org/static-1.23wmf14/skins/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
Two separate depictions of the recurrent ANN dependency graph</div>
</div>
</div>
<p>Networks such as the previous one are commonly called <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward</a>, because their graph is a <a href="/wiki/Directed_acyclic_graph" title="Directed acyclic graph">directed acyclic graph</a>. Networks with <a href="/wiki/Path_(graph_theory)" title="Path (graph theory)">cycles</a> are commonly called <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent</a>. Such networks are commonly depicted in the manner shown at the top of the figure, where <img class="tex" alt="\scriptstyle f" src="//upload.wikimedia.org/math/d/5/2/d52558b155faf73f2ed588accb0539bd.png" /> is shown as being dependent upon itself. However, an implied temporal dependence is not shown.</p>
<h3><span class="mw-headline" id="Learning">Learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=7" title="Edit section: Learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>What has attracted the most interest in neural networks is the possibility of <i>learning</i>. Given a specific <i>task</i> to solve, and a <i>class</i> of functions <img class="tex" alt="\scriptstyle F" src="//upload.wikimedia.org/math/0/5/a/05a4eacbd53ee615aa8bce3f4c4b2256.png" />, learning means using a set of <i>observations</i> to find <img class="tex" alt="\scriptstyle  f^{*} \in F" src="//upload.wikimedia.org/math/1/b/4/1b4fccbbee89092f70a5291eda2b347b.png" /> which solves the task in some <i>optimal</i> sense.</p>
<p>This entails defining a cost function <img class="tex" alt="\scriptstyle C&#160;: F \rightarrow \mathbb{R}" src="//upload.wikimedia.org/math/6/a/b/6abd4cff1dde429832b7ea90ca9e5c99.png" /> such that, for the optimal solution <img class="tex" alt="\scriptstyle f^*" src="//upload.wikimedia.org/math/3/2/8/328048ec4405e260dc102252ec461c50.png" />, <img class="tex" alt="\scriptstyle C(f^*) \leq C(f)" src="//upload.wikimedia.org/math/9/4/a/94a36a7263c2730dc11ec23c08d68362.png" /> <img class="tex" alt="\scriptstyle \forall f \in F" src="//upload.wikimedia.org/math/a/d/7/ad7de9ab80d7f6b58fbeda0144a30d60.png" />  i.e., no solution has a cost less than the cost of the optimal solution (see <a href="/wiki/Mathematical_optimization" title="Mathematical optimization">Mathematical optimization</a>).</p>
<p>The cost function <img class="tex" alt="\scriptstyle C" src="//upload.wikimedia.org/math/f/c/7/fc783207375c0abb99cfb79841b1708d.png" /> is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. Learning algorithms search through the solution space to find a function that has the smallest possible cost.</p>
<p>For applications where the solution is dependent on some data, the cost must necessarily be a <i>function of the observations</i>, otherwise we would not be modelling anything related to the data. It is frequently defined as a <a href="/wiki/Statistic" title="Statistic">statistic</a> to which only approximations can be made. As a simple example, consider the problem of finding the model <img class="tex" alt="\scriptstyle f" src="//upload.wikimedia.org/math/d/5/2/d52558b155faf73f2ed588accb0539bd.png" />, which minimizes <img class="tex" alt="\scriptstyle C=E\left[(f(x) - y)^2\right]" src="//upload.wikimedia.org/math/d/e/3/de3e13d7da78fefd7905e4acdb26deca.png" />, for data pairs <img class="tex" alt="\scriptstyle (x,y)" src="//upload.wikimedia.org/math/6/e/2/6e2d2ca441ecf5abacb1390c05c042b5.png" /> drawn from some distribution <img class="tex" alt="\scriptstyle \mathcal{D}" src="//upload.wikimedia.org/math/9/1/9/919abdd99b9e7ad0163720639aea82b6.png" />. In practical situations we would only have <img class="tex" alt="\scriptstyle N" src="//upload.wikimedia.org/math/3/6/9/369aada125ca0266d9cbc465345311f0.png" /> samples from <img class="tex" alt="\scriptstyle \mathcal{D}" src="//upload.wikimedia.org/math/9/1/9/919abdd99b9e7ad0163720639aea82b6.png" /> and thus, for the above example, we would only minimize <img class="tex" alt="\scriptstyle \hat{C}=\frac{1}{N}\sum_{i=1}^N (f(x_i)-y_i)^2" src="//upload.wikimedia.org/math/f/0/3/f0315f667fff8229d45691e71735c373.png" />. Thus, the cost is minimized over a sample of the data rather than the entire data set.</p>
<p>When <img class="tex" alt="\scriptstyle N \rightarrow \infty" src="//upload.wikimedia.org/math/4/3/9/43985d2fd996e084c2ba6637a4ea6123.png" /> some form of <a href="/wiki/Online_machine_learning" title="Online machine learning">online machine learning</a> must be used, where the cost is partially minimized as each new example is seen. While online machine learning is often used when <img class="tex" alt="\scriptstyle \mathcal{D}" src="//upload.wikimedia.org/math/9/1/9/919abdd99b9e7ad0163720639aea82b6.png" /> is fixed, it is most useful in the case where the distribution changes slowly over time. In neural network methods, some form of online machine learning is frequently used for finite datasets.</p>
<div class="rellink boilerplate seealso">See also: <a href="/wiki/Mathematical_optimization" title="Mathematical optimization">Mathematical optimization</a>, <a href="/wiki/Estimation_theory" title="Estimation theory">Estimation theory</a>,&#160;and <a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></div>
<h4><span class="mw-headline" id="Choosing_a_cost_function">Choosing a cost function</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=8" title="Edit section: Choosing a cost function">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>While it is possible to define some arbitrary <a href="/wiki/Ad_hoc" title="Ad hoc">ad hoc</a> cost function, frequently a particular cost will be used, either because it has desirable properties (such as <a href="/wiki/Convex_function" title="Convex function">convexity</a>) or because it arises naturally from a particular formulation of the problem (e.g., in a probabilistic formulation the posterior probability of the model can be used as an inverse cost). Ultimately, the cost function will depend on the desired task. An overview of the three (3) main categories of learning tasks is provided below:</p>
<h3><span class="mw-headline" id="Learning_paradigms">Learning paradigms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=9" title="Edit section: Learning paradigms">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>There are three major learning paradigm, each corresponding to a particular abstract learning task. These are <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a>, <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a> and <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a>.</p>
<h4><span class="mw-headline" id="Supervised_learning">Supervised learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=10" title="Edit section: Supervised learning">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>In <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a>, we are given a set of example pairs <img class="tex" alt="\scriptstyle (x, y), x \in X, y \in Y" src="//upload.wikimedia.org/math/2/c/6/2c6d557e8cc23286322b393941e75b9e.png" /> and the aim is to find a function <img class="tex" alt="\scriptstyle  f&#160;: X \rightarrow Y " src="//upload.wikimedia.org/math/2/a/f/2af59bc191a1b0f7e4d9429d4014995d.png" /> in the allowed class of functions that matches the examples. In other words, we wish to <i>infer</i> the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain.</p>
<p>A commonly used cost is the <a href="/wiki/Mean-squared_error" title="Mean-squared error" class="mw-redirect">mean-squared error</a>, which tries to minimize the average squared error between the network's output, f(x), and the target value y over all the example pairs. When one tries to minimize this cost using <a href="/wiki/Gradient_descent" title="Gradient descent">gradient descent</a> for the class of neural networks called <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptrons</a>, one obtains the common and well-known <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation algorithm</a> for training neural networks.</p>
<p>Tasks that fall within the paradigm of supervised learning are <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> (also known as classification) and <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a> (also known as function approximation). The supervised learning paradigm is also applicable to sequential data (e.g., for speech and gesture recognition). This can be thought of as learning with a "teacher," in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.</p>
<h4><span class="mw-headline" id="Unsupervised_learning">Unsupervised learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=11" title="Edit section: Unsupervised learning">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>In <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a>, some data <img class="tex" alt="\scriptstyle x" src="//upload.wikimedia.org/math/0/a/2/0a23b9f6eb1e2996867bc4b08879589d.png" /> is given and the cost function to be minimized, that can be any function of the data <img class="tex" alt="\scriptstyle x" src="//upload.wikimedia.org/math/0/a/2/0a23b9f6eb1e2996867bc4b08879589d.png" /> and the network's output, <img class="tex" alt="\scriptstyle f" src="//upload.wikimedia.org/math/d/5/2/d52558b155faf73f2ed588accb0539bd.png" />.</p>
<p>The cost function is dependent on the task (what we are trying to model) and our <i>a priori</i> assumptions (the implicit properties of our model, its parameters and the observed variables).</p>
<p>As a trivial example, consider the model <img class="tex" alt="\scriptstyle f(x) = a" src="//upload.wikimedia.org/math/3/7/e/37e295db003bb1dab11c7b87750f1c2d.png" /> where <img class="tex" alt="\scriptstyle a" src="//upload.wikimedia.org/math/d/a/7/da78e62859bac6cf6e227212010761f9.png" /> is a constant and the cost <img class="tex" alt="\scriptstyle C=E[(x - f(x))^2]" src="//upload.wikimedia.org/math/7/b/d/7bdfe3a462e428e140cde7d2d15eb817.png" />. Minimizing this cost will give us a value of <img class="tex" alt="\scriptstyle a" src="//upload.wikimedia.org/math/d/a/7/da78e62859bac6cf6e227212010761f9.png" /> that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the <a href="/wiki/Mutual_information" title="Mutual information">mutual information</a> between <img class="tex" alt="\scriptstyle x" src="//upload.wikimedia.org/math/0/a/2/0a23b9f6eb1e2996867bc4b08879589d.png" /> and <img class="tex" alt="\scriptstyle f(x)" src="//upload.wikimedia.org/math/e/6/d/e6dd6df2c92712c6b5d407fa452f1fd5.png" />, whereas in statistical modeling, it could be related to the <a href="/wiki/Posterior_probability" title="Posterior probability">posterior probability</a> of the model given the data. (Note that in both of those examples those quantities would be maximized rather than minimized).</p>
<p>Tasks that fall within the paradigm of unsupervised learning are in general <a href="/wiki/Approximation" title="Approximation">estimation</a> problems; the applications include <a href="/wiki/Data_clustering" title="Data clustering" class="mw-redirect">clustering</a>, the estimation of <a href="/wiki/Statistical_distributions" title="Statistical distributions" class="mw-redirect">statistical distributions</a>, <a href="/wiki/Data_compression" title="Data compression">compression</a> and <a href="/wiki/Bayesian_spam_filtering" title="Bayesian spam filtering">filtering</a>.</p>
<h4><span class="mw-headline" id="Reinforcement_learning">Reinforcement learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=12" title="Edit section: Reinforcement learning">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>In <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a>, data <img class="tex" alt="\scriptstyle x" src="//upload.wikimedia.org/math/0/a/2/0a23b9f6eb1e2996867bc4b08879589d.png" /> are usually not given, but generated by an agent's interactions with the environment. At each point in time <img class="tex" alt="\scriptstyle t" src="//upload.wikimedia.org/math/f/5/7/f57665e10ff8abeb3c3ff7a9d118d398.png" />, the agent performs an action <img class="tex" alt="\scriptstyle y_t" src="//upload.wikimedia.org/math/4/c/3/4c3b48eaeecc60ce867e2045e5991e33.png" /> and the environment generates an observation <img class="tex" alt="\scriptstyle x_t" src="//upload.wikimedia.org/math/9/5/b/95b97803f1527885224f9de8c2d5402c.png" /> and an instantaneous cost <img class="tex" alt="\scriptstyle c_t" src="//upload.wikimedia.org/math/d/0/f/d0f76b7b85fd8899a6ee7a32cb94c922.png" />, according to some (usually unknown) dynamics. The aim is to discover a <i>policy</i> for selecting actions that minimizes some measure of a long-term cost; i.e., the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated.</p>
<p>More formally the environment is modelled as a <a href="/wiki/Markov_decision_process" title="Markov decision process">Markov decision process</a> (MDP) with states <img class="tex" alt="\scriptstyle {s_1,...,s_n}\in S " src="//upload.wikimedia.org/math/e/7/b/e7bf598d05c075826967e2aed096c9c4.png" /> and actions <img class="tex" alt="\scriptstyle {a_1,...,a_m} \in A" src="//upload.wikimedia.org/math/2/c/4/2c47140af2abf3f3b66fa9de539c01d5.png" /> with the following probability distributions: the instantaneous cost distribution <img class="tex" alt="\scriptstyle P(c_t|s_t)" src="//upload.wikimedia.org/math/8/6/5/865e3fccd004bff75bccddfb9f569552.png" />, the observation distribution <img class="tex" alt="\scriptstyle P(x_t|s_t)" src="//upload.wikimedia.org/math/d/2/c/d2c5e12165a279652cf7fcf37598bbe6.png" /> and the transition <img class="tex" alt="\scriptstyle P(s_{t+1}|s_t, a_t)" src="//upload.wikimedia.org/math/b/0/5/b058f6822b8469e41829402167ea32bd.png" />, while a policy is defined as conditional distribution over actions given the observations. Taken together, the two then define a <a href="/wiki/Markov_chain" title="Markov chain">Markov chain</a> (MC). The aim is to discover the policy that minimizes the cost; i.e., the MC for which the cost is minimal.</p>
<p>ANNs are frequently used in reinforcement learning as part of the overall algorithm.<sup id="cite_ref-30" class="reference"><a href="#cite_note-30"><span>[</span>30<span>]</span></a></sup><sup id="cite_ref-31" class="reference"><a href="#cite_note-31"><span>[</span>31<span>]</span></a></sup> <a href="/wiki/Dynamic_programming" title="Dynamic programming">Dynamic programming</a> has been coupled with ANNs (Neuro dynamic programming) by <a href="/wiki/Dimitri_Bertsekas" title="Dimitri Bertsekas">Bertsekas</a> and Tsitsiklis<sup id="cite_ref-32" class="reference"><a href="#cite_note-32"><span>[</span>32<span>]</span></a></sup> and applied to multi-dimensional nonlinear problems such as those involved in <a href="/wiki/Vehicle_routing" title="Vehicle routing" class="mw-redirect">vehicle routing</a>,<sup id="cite_ref-33" class="reference"><a href="#cite_note-33"><span>[</span>33<span>]</span></a></sup> <a href="/wiki/Natural_resource_management" title="Natural resource management">natural resources management</a><sup id="cite_ref-34" class="reference"><a href="#cite_note-34"><span>[</span>34<span>]</span></a></sup><sup id="cite_ref-35" class="reference"><a href="#cite_note-35"><span>[</span>35<span>]</span></a></sup> or <a href="/wiki/Medicine" title="Medicine">medicine</a><sup id="cite_ref-36" class="reference"><a href="#cite_note-36"><span>[</span>36<span>]</span></a></sup> because of the ability of ANNs to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems.</p>
<p>Tasks that fall within the paradigm of reinforcement learning are control problems, <a href="/wiki/Game" title="Game">games</a> and other <a href="/w/index.php?title=Sequential_decision_making&amp;action=edit&amp;redlink=1" class="new" title="Sequential decision making (page does not exist)">sequential decision making</a> tasks.</p>
<div class="rellink boilerplate seealso">See also: <a href="/wiki/Dynamic_programming" title="Dynamic programming">dynamic programming</a>&#160;and <a href="/wiki/Stochastic_control" title="Stochastic control">stochastic control</a></div>
<h3><span class="mw-headline" id="Learning_algorithms">Learning algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=13" title="Edit section: Learning algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Training a neural network model essentially means selecting one model from the set of allowed models (or, in a <a href="/wiki/Bayesian_probability" title="Bayesian probability">Bayesian</a> framework, determining a distribution over the set of allowed models) that minimizes the cost criterion. There are numerous algorithms available for training neural network models; most of them can be viewed as a straightforward application of <a href="/wiki/Mathematical_optimization" title="Mathematical optimization">optimization</a> theory and <a href="/wiki/Statistical_estimation" title="Statistical estimation" class="mw-redirect">statistical estimation</a>.</p>
<p>Most of the algorithms used in training artificial neural networks employ some form of <a href="/wiki/Gradient_descent" title="Gradient descent">gradient descent</a>. This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a <a href="/wiki/Gradient-related" title="Gradient-related">gradient-related</a> direction.</p>
<p><a href="/wiki/Evolutionary_methods" title="Evolutionary methods" class="mw-redirect">Evolutionary methods</a>,<sup id="cite_ref-37" class="reference"><a href="#cite_note-37"><span>[</span>37<span>]</span></a></sup> <a href="/wiki/Gene_expression_programming" title="Gene expression programming">gene expression programming</a>,<sup id="cite_ref-38" class="reference"><a href="#cite_note-38"><span>[</span>38<span>]</span></a></sup> <a href="/wiki/Simulated_annealing" title="Simulated annealing">simulated annealing</a>,<sup id="cite_ref-39" class="reference"><a href="#cite_note-39"><span>[</span>39<span>]</span></a></sup> <a href="/wiki/Expectation-maximization" title="Expectation-maximization" class="mw-redirect">expectation-maximization</a>, <a href="/wiki/Non-parametric_methods" title="Non-parametric methods" class="mw-redirect">non-parametric methods</a> and <a href="/wiki/Particle_swarm_optimization" title="Particle swarm optimization">particle swarm optimization</a><sup id="cite_ref-40" class="reference"><a href="#cite_note-40"><span>[</span>40<span>]</span></a></sup> are some commonly used methods for training neural networks.</p>
<div class="rellink boilerplate seealso">See also: <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a></div>
<h2><span class="mw-headline" id="Employing_artificial_neural_networks">Employing artificial neural networks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=14" title="Edit section: Employing artificial neural networks">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Perhaps the greatest advantage of ANNs is their ability to be used as an arbitrary function approximation mechanism that 'learns' from observed data. However, using them is not so straightforward, and a relatively good understanding of the underlying theory is essential.</p>
<ul>
<li>Choice of model: This will depend on the data representation and the application. Overly complex models tend to lead to problems with learning.</li>
<li>Learning algorithm: There are numerous trade-offs between learning algorithms. Almost any algorithm will work well with the <i>correct <a href="/wiki/Hyperparameter" title="Hyperparameter">hyperparameters</a></i> for training on a particular fixed data set. However, selecting and tuning an algorithm for training on unseen data requires a significant amount of experimentation.</li>
<li>Robustness: If the model, cost function and learning algorithm are selected appropriately the resulting ANN can be extremely robust.</li>
</ul>
<p>With the correct implementation, ANNs can be used naturally in <a href="/wiki/Online_algorithm" title="Online algorithm">online learning</a> and large data set applications. Their simple implementation and the existence of mostly local dependencies exhibited in the structure allows for fast, parallel implementations in hardware.</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=15" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations. This is particularly useful in applications where the complexity of the data or task makes the design of such a function by hand impractical.</p>
<h3><span class="mw-headline" id="Real-life_applications">Real-life applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=16" title="Edit section: Real-life applications">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The tasks artificial neural networks are applied to tend to fall within the following broad categories:</p>
<ul>
<li><a href="/wiki/Function_approximation" title="Function approximation">Function approximation</a>, or <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a>, including <a href="/wiki/Time_series_prediction" title="Time series prediction" class="mw-redirect">time series prediction</a>, <a href="/wiki/Fitness_approximation" title="Fitness approximation">fitness approximation</a> and modeling.</li>
<li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a>, including <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern</a> and sequence recognition, <a href="/wiki/Novelty_detection" title="Novelty detection">novelty detection</a> and sequential decision making.</li>
<li><a href="/wiki/Data_processing" title="Data processing">Data processing</a>, including filtering, clustering, <a href="/wiki/Blind_source_separation" title="Blind source separation" class="mw-redirect">blind source separation</a> and compression.</li>
<li><a href="/wiki/Robotics" title="Robotics">Robotics</a>, including directing manipulators, <a href="/wiki/Prosthesis" title="Prosthesis">prosthesis</a>.</li>
<li><a href="/wiki/Control_engineering" title="Control engineering">Control</a>, including <a href="/wiki/Computer_numerical_control" title="Computer numerical control" class="mw-redirect">Computer numerical control</a>.</li>
</ul>
<p>Application areas include the system identification and control (vehicle control, process control, <a href="/wiki/Natural_resource" title="Natural resource">natural resources</a> management), quantum chemistry,<sup id="cite_ref-Balabin_2009_41-0" class="reference"><a href="#cite_note-Balabin_2009-41"><span>[</span>41<span>]</span></a></sup> game-playing and decision making (backgammon, chess, <a href="/wiki/Poker" title="Poker">poker</a>), pattern recognition (radar systems, face identification, object recognition and more), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications (e.g. <a href="/wiki/Algorithmic_trading" title="Algorithmic trading">automated trading systems</a>), <a href="/wiki/Data_mining" title="Data mining">data mining</a> (or knowledge discovery in databases, "KDD"), visualization and <a href="/wiki/E-mail_spam" title="E-mail spam" class="mw-redirect">e-mail spam</a> filtering.</p>
<p>Artificial neural networks have also been used to diagnose several cancers. An ANN based hybrid lung cancer detection system named HLND improves the accuracy of diagnosis and the speed of lung cancer radiology.<sup id="cite_ref-42" class="reference"><a href="#cite_note-42"><span>[</span>42<span>]</span></a></sup> These networks have also been used to diagnose prostate cancer. The diagnoses can be used to make specific models taken from a large group of patients compared to information of one given patient. The models do not depend on assumptions about correlations of different variables. Colorectal cancer has also been predicted using the neural networks. Neural networks could predict the outcome for a patient with colorectal cancer with more accuracy than the current clinical methods. After training, the networks could predict multiple patient outcomes from unrelated institutions.<sup id="cite_ref-43" class="reference"><a href="#cite_note-43"><span>[</span>43<span>]</span></a></sup></p>
<h3><span class="mw-headline" id="Neural_networks_and_neuroscience">Neural networks and neuroscience</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=17" title="Edit section: Neural networks and neuroscience">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Theoretical and <a href="/wiki/Computational_neuroscience" title="Computational neuroscience">computational neuroscience</a> is the field concerned with the theoretical analysis and the computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behavior, the field is closely related to cognitive and behavioral modeling.</p>
<p>The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (<a href="/wiki/Biological_neural_network" title="Biological neural network">biological neural network</a> models) and theory (statistical learning theory and <a href="/wiki/Information_theory" title="Information theory">information theory</a>).</p>
<h4><span class="mw-headline" id="Types_of_models">Types of models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=18" title="Edit section: Types of models">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Many models are used in the field, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of <a href="/wiki/Biological_neuron_models" title="Biological neuron models" class="mw-redirect">individual neurons</a>, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.</p>
<h2><span class="mw-headline" id="Neural_network_software">Neural network software</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=19" title="Edit section: Neural network software">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="rellink relarticle mainarticle">Main article: <a href="/wiki/Neural_network_software" title="Neural network software">Neural network software</a></div>
<p><b>Neural network software</b> is used to <a href="/wiki/Simulation" title="Simulation">simulate</a>, <a href="/wiki/Research" title="Research">research</a>, develop and apply artificial neural networks, <a href="/wiki/Biological_neural_network" title="Biological neural network">biological neural networks</a> and, in some cases, a wider array of <a href="/wiki/Adaptive_system" title="Adaptive system">adaptive systems</a>.</p>
<h2><span class="mw-headline" id="Types_of_artificial_neural_networks">Types of artificial neural networks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=20" title="Edit section: Types of artificial neural networks">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="rellink relarticle mainarticle">Main article: <a href="/wiki/Types_of_artificial_neural_networks" title="Types of artificial neural networks">Types of artificial neural networks</a></div>
<p>Artificial neural network types vary from those with only one or two layers of single direction logic, to complicated multi늘nput many directional feedback loops and layers. On the whole, these systems use algorithms in their programming to determine control and organization of their functions. Some may be as simple as a one-neuron layer with an input and an output, and others can mimic complex systems such as <a href="/wiki/List_of_artificial_intelligence_projects#Software_libraries" title="List of artificial intelligence projects">dANN</a>, which can mimic chromosomal DNA through sizes at the cellular level, into artificial organisms and simulate reproduction, mutation and population sizes.<sup id="cite_ref-44" class="reference"><a href="#cite_note-44"><span>[</span>44<span>]</span></a></sup></p>
<p>Most systems use "weights" to change the parameters of the throughput and the varying connections to the neurons. Artificial neural networks can be autonomous and learn by input from outside "teachers" or even self-teaching from written-in rules.</p>
<h2><span class="mw-headline" id="Theoretical_properties">Theoretical properties</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=21" title="Edit section: Theoretical properties">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Computational_power">Computational power</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=22" title="Edit section: Computational power">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <a href="/wiki/Multi-layer_perceptron" title="Multi-layer perceptron" class="mw-redirect">multi-layer perceptron</a> (MLP) is a universal function approximator, as proven by the <a href="/wiki/Cybenko_theorem" title="Cybenko theorem" class="mw-redirect">Cybenko theorem</a>. However, the proof is not constructive regarding the number of neurons required or the settings of the weights.</p>
<p>Work by <a href="/wiki/Hava_Siegelmann" title="Hava Siegelmann">Hava Siegelmann</a> and <a href="/wiki/Eduardo_D._Sontag" title="Eduardo D. Sontag">Eduardo D. Sontag</a> has provided a proof that a specific recurrent architecture with rational valued weights (as opposed to full precision <a href="/wiki/Real_number" title="Real number">real number</a>-valued weights) has the full power of a <a href="/wiki/Universal_Turing_Machine" title="Universal Turing Machine" class="mw-redirect">Universal Turing Machine</a><sup id="cite_ref-45" class="reference"><a href="#cite_note-45"><span>[</span>45<span>]</span></a></sup> using a finite number of neurons and standard linear connections. They have further shown that the use of irrational values for weights results in a machine with <a href="/wiki/Hypercomputation" title="Hypercomputation">super-Turing</a> power.<sup class="Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (August 2011)">citation needed</span></a></i>]</sup></p>
<h3><span class="mw-headline" id="Capacity">Capacity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=23" title="Edit section: Capacity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Artificial neural network models have a property called 'capacity', which roughly corresponds to their ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.</p>
<h3><span class="mw-headline" id="Convergence">Convergence</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=24" title="Edit section: Convergence">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Nothing can be said in general about convergence since it depends on a number of factors. Firstly, there may exist many local minima. This depends on the cost function and the model. Secondly, the optimization method used might not be guaranteed to converge when far away from a local minimum. Thirdly, for a very large amount of data or parameters, some methods become impractical. In general, it has been found that theoretical guarantees regarding convergence are an unreliable guide to practical application.<sup class="Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (March 2012)">citation needed</span></a></i>]</sup></p>
<h3><span class="mw-headline" id="Generalization_and_statistics">Generalization and statistics</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=25" title="Edit section: Generalization and statistics">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In applications where the goal is to create a system that generalizes well in unseen examples, the problem of over-training has emerged. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. There are two schools of thought for avoiding this problem: The first is to use <a href="/wiki/Cross-validation_(statistics)" title="Cross-validation (statistics)">cross-validation</a> and similar techniques to check for the presence of overtraining and optimally select hyperparameters such as to minimize the generalization error. The second is to use some form of <i><a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">regularization</a></i>. This is a concept that emerges naturally in a probabilistic (Bayesian) framework, where the regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.</p>
<div class="thumb tright">
<div class="thumbinner" style="width:202px;"><a href="/wiki/File:Synapse_deployment.jpg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/2/22/Synapse_deployment.jpg/200px-Synapse_deployment.jpg" width="200" height="154" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/en/thumb/2/22/Synapse_deployment.jpg/300px-Synapse_deployment.jpg 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/2/22/Synapse_deployment.jpg/400px-Synapse_deployment.jpg 2x" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Synapse_deployment.jpg" class="internal" title="Enlarge"><img src="//bits.wikimedia.org/static-1.23wmf14/skins/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
Confidence analysis of a neural network</div>
</div>
</div>
<p>Supervised neural networks that use an <a href="/wiki/Mean_squared_error" title="Mean squared error">MSE</a> cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the <a href="/wiki/Confidence_interval" title="Confidence interval">confidence interval</a> of the output of the network, assuming a <a href="/wiki/Normal_distribution" title="Normal distribution">normal distribution</a>. A confidence analysis made this way is statistically valid as long as the output <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> stays the same and the network is not modified.</p>
<p>By assigning a <a href="/wiki/Softmax_activation_function" title="Softmax activation function" class="mw-redirect">softmax activation function</a>, a generalization of the <a href="/wiki/Logistic_function" title="Logistic function">logistic function</a>, on the output layer of the neural network (or a softmax component in a component-based neural network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is very useful in classification as it gives a certainty measure on classifications.</p>
<p>The softmax activation function is:</p>
<dl>
<dd><img class="tex" alt="y_i=\frac{e^{x_i}}{\sum_{j=1}^c e^{x_j}}" src="//upload.wikimedia.org/math/0/2/a/02a0d787371e595719f83f8431ee898a.png" /></dd>
</dl>
<h3><span class="mw-headline" id="Dynamic_properties">Dynamic properties</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=26" title="Edit section: Dynamic properties">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<table class="metadata plainlinks ambox ambox-content" role="presentation">
<tr>
<td class="mbox-image">
<div style="width:52px;"><img alt="" src="//upload.wikimedia.org/wikipedia/en/f/f4/Ambox_content.png" width="40" height="40" /></div>
</td>
<td class="mbox-text"><span class="mbox-text-span">This article <b>needs attention from an expert in Technology</b>. <span class="hide-when-compact">Please add a <i>reason</i> or a <i>talk</i> parameter to this template to explain the issue with the article. <a href="/wiki/Wikipedia:WikiProject_Technology" title="Wikipedia:WikiProject Technology">WikiProject Technology</a> (or its <a href="/wiki/Wikipedia:P" title="Wikipedia:P" class="mw-redirect">Portal</a>) may be able to help recruit an expert.</span> <small><i>(November 2008)</i></small></span></td>
</tr>
</table>
<p>Various techniques originally developed for studying disordered magnetic systems (i.e., the <a href="/wiki/Spin_glass" title="Spin glass">spin glass</a>) have been successfully applied to simple neural network architectures, such as the <a href="/wiki/Hopfield_network" title="Hopfield network">Hopfield network</a>. Influential work by E. Gardner and B. Derrida has revealed many interesting properties about <a href="/wiki/Perceptron" title="Perceptron">perceptrons</a> with real-valued synaptic weights, while later work by W. Krauth and M. Mezard has extended these principles to binary-valued synapses.</p>
<h2><span class="mw-headline" id="Criticism">Criticism</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=27" title="Edit section: Criticism">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Dean Pomerleau, in his research presented in the paper "Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving," uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns  it should not learn to always turn right). These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches.</p>
<p><a href="/wiki/A._K._Dewdney" title="A. K. Dewdney" class="mw-redirect">A. K. Dewdney</a>, a former <i><a href="/wiki/Scientific_American" title="Scientific American">Scientific American</a></i> columnist, wrote in 1997, "Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool." (Dewdney, p.&#160;82)</p>
<p>Arguments for Dewdney's position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a <a href="/wiki/Graph_(mathematics)" title="Graph (mathematics)">graph</a> of neurons, simulating even a most simplified form on <a href="/wiki/Von_Neumann" title="Von Neumann" class="mw-redirect">Von Neumann</a> technology may compel a neural network designer to fill many millions of <a href="/wiki/Database" title="Database">database</a> rows for its connections  which can consume vast amounts of computer <a href="/wiki/Random-access_memory" title="Random-access memory">memory</a> and <a href="/wiki/Hard_drive" title="Hard drive" class="mw-redirect">hard disk</a> space. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons  which must often be matched with incredible amounts of <a href="/wiki/CPU" title="CPU" class="mw-redirect">CPU</a> processing power and time. While neural networks often yield <i>effective</i> programs, they too often do so at the cost of <i>efficiency</i> (they tend to consume considerable amounts of time and money).</p>
<p>Arguments against Dewdney's position are that neural nets have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraft<sup id="cite_ref-46" class="reference"><a href="#cite_note-46"><span>[</span>46<span>]</span></a></sup> to detecting credit card fraud .<sup class="Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (August 2012)">citation needed</span></a></i>]</sup></p>
<p>Technology writer <a href="/w/index.php?title=Roger_Bridgman&amp;action=edit&amp;redlink=1" class="new" title="Roger Bridgman (page does not exist)">Roger Bridgman</a> commented on Dewdney's statements about neural nets:</p>
<blockquote>
<p>Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be "an opaque, unreadable table...valueless as a scientific resource".</p>
<p>In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.<sup id="cite_ref-47" class="reference"><a href="#cite_note-47"><span>[</span>47<span>]</span></a></sup></p>
</blockquote>
<p>In response to this kind of criticism, one should note that although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles which allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.<sup id="cite_ref-48" class="reference"><a href="#cite_note-48"><span>[</span>48<span>]</span></a></sup></p>
<p>Some other criticisms came from believers of hybrid models (combining neural networks and symbolic approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990).</p>
<h2><span class="mw-headline" id="Gallery">Gallery</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=28" title="Edit section: Gallery">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul class="gallery mw-gallery-traditional">
<li class="gallerybox" style="width: 255px">
<div style="width: 255px">
<div class="thumb" style="width: 250px;">
<div style="margin:15px auto;"><a href="/wiki/File:Single_layer_ann.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/b/be/Single_layer_ann.svg/172px-Single_layer_ann.svg.png" width="172" height="120" /></a></div>
</div>
<div class="gallerytext">
<p>A single-layer feedforward artificial neural network. Arrows originating from <img class="tex" alt="\scriptstyle x_2" src="//upload.wikimedia.org/math/f/f/5/ff533a5595d6ff01fdee6c5e9fe509b6.png" /> are omitted for clarity. There are p inputs to this network and q outputs. In this system, the value of the qth output, <img class="tex" alt="\scriptstyle y_q" src="//upload.wikimedia.org/math/b/3/b/b3bdad074e7cc32c88c3a860b7c888f7.png" /> would be calculated as <img class="tex" alt="\scriptstyle y_q = \sum(x_i*w_{iq}) " src="//upload.wikimedia.org/math/b/e/7/be75ed40052d6db37830d93838cdcd89.png" /></p>
</div>
</div>
</li>
<li class="gallerybox" style="width: 255px">
<div style="width: 255px">
<div class="thumb" style="width: 250px;">
<div style="margin:15px auto;"><a href="/wiki/File:Two_layer_ann.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/7/7f/Two_layer_ann.svg/94px-Two_layer_ann.svg.png" width="94" height="120" /></a></div>
</div>
<div class="gallerytext">
<p>A two-layer feedforward artificial neural network.</p>
</div>
</div>
</li>
<li class="gallerybox" style="width: 255px">
<div style="width: 255px">
<div class="thumb" style="width: 250px;">
<div style="margin:15px auto;"><a href="/wiki/File:Artificial_neural_network.svg" class="image"><img alt="Artificial neural network.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/134px-Artificial_neural_network.svg.png" width="134" height="120" /></a></div>
</div>
<div class="gallerytext"></div>
</div>
</li>
<li class="gallerybox" style="width: 255px">
<div style="width: 255px">
<div class="thumb" style="width: 250px;">
<div style="margin:15px auto;"><a href="/wiki/File:Ann_dependency_(graph).svg" class="image"><img alt="Ann dependency (graph).svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Ann_dependency_%28graph%29.svg/168px-Ann_dependency_%28graph%29.svg.png" width="168" height="120" /></a></div>
</div>
<div class="gallerytext"></div>
</div>
</li>
</ul>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=29" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="div-col columns column-count column-count-3" style="-moz-column-count: 3; -webkit-column-count: 3; column-count: 3;">
<ul>
<li><a href="/wiki/20Q" title="20Q">20Q</a></li>
<li><a href="/wiki/ADALINE" title="ADALINE">ADALINE</a></li>
<li><a href="/wiki/Adaptive_resonance_theory" title="Adaptive resonance theory">Adaptive resonance theory</a></li>
<li><a href="/wiki/Artificial_life" title="Artificial life">Artificial life</a></li>
<li><a href="/wiki/Associative_Memory_Base" title="Associative Memory Base" class="mw-redirect">Associative memory</a></li>
<li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Backpropagation" title="Backpropagation">Backpropagation</a></li>
<li><a href="/wiki/BEAM_robotics" title="BEAM robotics">BEAM robotics</a></li>
<li><a href="/wiki/Biological_cybernetics" title="Biological cybernetics" class="mw-redirect">Biological cybernetics</a></li>
<li><a href="/wiki/Biologically_inspired_computing" title="Biologically inspired computing" class="mw-redirect">Biologically inspired computing</a></li>
<li><a href="/wiki/Blue_brain" title="Blue brain" class="mw-redirect">Blue brain</a></li>
<li><a href="/wiki/Cerebellar_Model_Articulation_Controller" title="Cerebellar Model Articulation Controller">Cerebellar Model Articulation Controller</a></li>
<li><a href="/wiki/Cognitive_architecture" title="Cognitive architecture">Cognitive architecture</a></li>
<li><a href="/wiki/Cognitive_science" title="Cognitive science">Cognitive science</a></li>
<li><a href="/wiki/Connectionist_expert_system" title="Connectionist expert system">Connectionist expert system</a></li>
<li><a href="/wiki/Connectomics" title="Connectomics">Connectomics</a></li>
<li><a href="/wiki/Cultured_neuronal_networks" title="Cultured neuronal networks" class="mw-redirect">Cultured neuronal networks</a></li>
<li><a href="/wiki/Digital_morphogenesis" title="Digital morphogenesis">Digital morphogenesis</a></li>
<li><a href="/wiki/Encog" title="Encog">Encog</a></li>
<li><a href="/wiki/Fuzzy_logic" title="Fuzzy logic">Fuzzy logic</a></li>
<li><a href="/wiki/Gene_expression_programming" title="Gene expression programming">Gene expression programming</a></li>
<li><a href="/wiki/Genetic_algorithm" title="Genetic algorithm">Genetic algorithm</a></li>
<li><a href="/wiki/Group_method_of_data_handling" title="Group method of data handling">Group method of data handling</a></li>
<li><a href="/wiki/Habituation" title="Habituation">Habituation</a></li>
<li><a href="/wiki/In_Situ_Adaptive_Tabulation" title="In Situ Adaptive Tabulation" class="mw-redirect">In Situ Adaptive Tabulation</a></li>
<li><a href="/wiki/Memristor" title="Memristor">Memristor</a></li>
<li><a href="/wiki/Multilinear_subspace_learning" title="Multilinear subspace learning">Multilinear subspace learning</a></li>
<li><a href="/wiki/Neuroevolution" title="Neuroevolution">Neuroevolution</a></li>
<li><a href="/wiki/Neural_gas" title="Neural gas">Neural gas</a></li>
<li><a href="/wiki/Neural_network_software" title="Neural network software">Neural network software</a></li>
<li><a href="/wiki/Neuroscience" title="Neuroscience">Neuroscience</a></li>
<li><a href="/wiki/Ni1000" title="Ni1000">Ni1000</a> chip</li>
<li><a href="/wiki/Nonlinear_system_identification" title="Nonlinear system identification">Nonlinear system identification</a></li>
<li><a href="/wiki/Optical_neural_network" title="Optical neural network">Optical neural network</a></li>
<li><a href="/wiki/Parallel_Constraint_Satisfaction_Processes" title="Parallel Constraint Satisfaction Processes" class="mw-redirect">Parallel Constraint Satisfaction Processes</a></li>
<li><a href="/wiki/Parallel_distributed_processing" title="Parallel distributed processing" class="mw-redirect">Parallel distributed processing</a></li>
<li><a href="/wiki/Radial_basis_function_network" title="Radial basis function network">Radial basis function network</a></li>
<li><a href="/wiki/Recurrent_neural_networks" title="Recurrent neural networks" class="mw-redirect">Recurrent neural networks</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">Self-organizing map</a></li>
<li><a href="/wiki/Systolic_array" title="Systolic array">Systolic array</a></li>
<li><a href="/wiki/Tensor_product_network" title="Tensor product network">Tensor product network</a></li>
<li><a href="/wiki/Time_delay_neural_network" title="Time delay neural network">Time delay neural network</a> (TDNN)</li>
</ul>
</div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=30" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-count references-column-count-2" style="-moz-column-count: 2; -webkit-column-count: 2; column-count: 2; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><span class="citation journal">McCulloch, Warren; Walter Pitts (1943). "A Logical Calculus of Ideas Immanent in Nervous Activity". <i>Bulletin of Mathematical Biophysics</i> <b>5</b> (4): 115133. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2FBF02478259">10.1007/BF02478259</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=A+Logical+Calculus+of+Ideas+Immanent+in+Nervous+Activity&amp;rft.aufirst=Warren&amp;rft.aulast=McCulloch&amp;rft.au=McCulloch%2C+Warren&amp;rft.date=1943&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1007%2FBF02478259&amp;rft.issue=4&amp;rft.jtitle=Bulletin+of+Mathematical+Biophysics&amp;rft.pages=115%E2%80%93133&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=5" class="Z3988"><span style="display:none;">&#160;</span></span> <span style="display:none;font-size:100%" class="error citation-comment">Cite uses deprecated parameters (<a href="/wiki/Help:CS1_errors#deprecated_params" title="Help:CS1 errors">help</a>)</span></span></li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><span class="citation book">Hebb, Donald (1949). <i>The Organization of Behavior</i>. New York: Wiley.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.aufirst=Donald&amp;rft.au=Hebb%2C+Donald&amp;rft.aulast=Hebb&amp;rft.btitle=The+Organization+of+Behavior&amp;rft.date=1949&amp;rft.genre=book&amp;rft.place=New+York&amp;rft.pub=Wiley&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><span class="citation journal">Farley, B; W.A. Clark (1954). "Simulation of Self-Organizing Systems by Digital Computer". <i>IRE Transactions on Information Theory</i> <b>4</b> (4): 7684. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1109%2FTIT.1954.1057468">10.1109/TIT.1954.1057468</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Simulation+of+Self-Organizing+Systems+by+Digital+Computer&amp;rft.au=Farley%2C+B&amp;rft.aufirst=B&amp;rft.aulast=Farley&amp;rft.date=1954&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1954.1057468&amp;rft.issue=4&amp;rft.jtitle=IRE+Transactions+on+Information+Theory&amp;rft.pages=76%E2%80%9384&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=4" class="Z3988"><span style="display:none;">&#160;</span></span> <span style="display:none;font-size:100%" class="error citation-comment">Cite uses deprecated parameters (<a href="/wiki/Help:CS1_errors#deprecated_params" title="Help:CS1 errors">help</a>)</span></span></li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><span class="citation journal">Rochester, N.; J.H. Holland, L.H. Habit, and W.L. Duda (1956). "Tests on a cell assembly theory of the action of the brain, using a large digital computer". <i>IRE Transactions on Information Theory</i> <b>2</b> (3): 8093. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1109%2FTIT.1956.1056810">10.1109/TIT.1956.1056810</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Tests+on+a+cell+assembly+theory+of+the+action+of+the+brain%2C+using+a+large+digital+computer&amp;rft.aufirst=N.&amp;rft.aulast=Rochester&amp;rft.au=Rochester%2C+N.&amp;rft.date=1956&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1956.1056810&amp;rft.issue=3&amp;rft.jtitle=IRE+Transactions+on+Information+Theory&amp;rft.pages=80%E2%80%9393&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=2" class="Z3988"><span style="display:none;">&#160;</span></span> <span style="display:none;font-size:100%" class="error citation-comment">Cite uses deprecated parameters (<a href="/wiki/Help:CS1_errors#deprecated_params" title="Help:CS1 errors">help</a>)</span></span></li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><span class="citation journal">Rosenblatt, F. (1958). "The Perceptron: A Probalistic Model For Information Storage And Organization In The Brain". <i>Psychological Review</i> <b>65</b> (6): 386408. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1037%2Fh0042519">10.1037/h0042519</a>. <a href="/wiki/PubMed_Identifier" title="PubMed Identifier" class="mw-redirect">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/13602029">13602029</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=The+Perceptron%3A+A+Probalistic+Model+For+Information+Storage+And+Organization+In+The+Brain&amp;rft.aufirst=F.&amp;rft.aulast=Rosenblatt&amp;rft.au=Rosenblatt%2C+F.&amp;rft.date=1958&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1037%2Fh0042519&amp;rft_id=info%3Apmid%2F13602029&amp;rft.issue=6&amp;rft.jtitle=Psychological+Review&amp;rft.pages=386%E2%80%93408&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=65" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-Werbos_1975-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-Werbos_1975_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Werbos_1975_6-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><span class="citation book">Werbos, P.J. (1975). <i>Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences</i>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.aufirst=P.J.&amp;rft.aulast=Werbos&amp;rft.au=Werbos%2C+P.J.&amp;rft.btitle=Beyond+Regression%3A+New+Tools+for+Prediction+and+Analysis+in+the+Behavioral+Sciences&amp;rft.date=1975&amp;rft.genre=book&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><span class="citation book">Minsky, M.; S. Papert (1969). <i>An Introduction to Computational Geometry</i>. MIT Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-262-63022-2" title="Special:BookSources/0-262-63022-2">0-262-63022-2</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.aufirst=M.&amp;rft.aulast=Minsky&amp;rft.au=Minsky%2C+M.&amp;rft.btitle=An+Introduction+to+Computational+Geometry&amp;rft.date=1969&amp;rft.genre=book&amp;rft.isbn=0-262-63022-2&amp;rft.pub=MIT+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span> <span style="display:none;font-size:100%" class="error citation-comment">Cite uses deprecated parameters (<a href="/wiki/Help:CS1_errors#deprecated_params" title="Help:CS1 errors">help</a>)</span></span></li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><span class="citation book">Rumelhart, D.E; James McClelland (1986). <i>Parallel Distributed Processing: Explorations in the Microstructure of Cognition</i>. Cambridge: MIT Press.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.aufirst=D.E&amp;rft.aulast=Rumelhart&amp;rft.au=Rumelhart%2C+D.E&amp;rft.btitle=Parallel+Distributed+Processing%3A+Explorations+in+the+Microstructure+of+Cognition&amp;rft.date=1986&amp;rft.genre=book&amp;rft.place=Cambridge&amp;rft.pub=MIT+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span> <span style="display:none;font-size:100%" class="error citation-comment">Cite uses deprecated parameters (<a href="/wiki/Help:CS1_errors#deprecated_params" title="Help:CS1 errors">help</a>)</span></span></li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><span class="citation web">Russell, Ingrid. <a rel="nofollow" class="external text" href="http://uhaweb.hartford.edu/compsci/neural-networks-definition.html">"Neural Networks Module"</a><span class="reference-accessdate">. Retrieved 2012</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.aufirst=Ingrid&amp;rft.aulast=Russell&amp;rft.au=Russell%2C+Ingrid&amp;rft.btitle=Neural+Networks+Module&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fuhaweb.hartford.edu%2Fcompsci%2Fneural-networks-definition.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text">Yang, J. J.; Pickett, M. D.; Li, X. M.; Ohlberg, D. A. A.; Stewart, D. R.; Williams, R. S. Nat. Nanotechnol. 2008, 3, 429433.</span></li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text">Strukov, D. B.; Snider, G. S.; Stewart, D. R.; Williams, R. S. <i>Nature</i> 2008, 453, 8083.</span></li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions">http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions</a> 2012 Kurzweil AI Interview with <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="J칲rgen Schmidhuber">J칲rgen Schmidhuber</a> on the eight competitions won by his Deep Learning team 20092012</span></li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text">Graves, Alex; and Schmidhuber, J칲rgen; <i>Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</i>, in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), <i>Advances in Neural Information Processing Systems 22 (NIPS'22), December 7th10th, 2009, Vancouver, BC</i>, Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545552</span></li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text">A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, J. Schmidhuber. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, 2009.</span></li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="http://www.scholarpedia.org/article/Deep_belief_networks">http://www.scholarpedia.org/article/Deep_belief_networks</a> /</span></li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><span class="citation journal"><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Hinton, G. E.</a>; Osindero, S.; Teh, Y. (2006). <a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">"A fast learning algorithm for deep belief nets"</a>. <i><a href="/wiki/Neural_Computation" title="Neural Computation" class="mw-redirect">Neural Computation</a></i> <b>18</b> (7): 15271554. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1162%2Fneco.2006.18.7.1527">10.1162/neco.2006.18.7.1527</a>. <a href="/wiki/PubMed_Identifier" title="PubMed Identifier" class="mw-redirect">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/16764513">16764513</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=A+fast+learning+algorithm+for+deep+belief+nets&amp;rft.aufirst=G.+E.&amp;rft.au=Hinton%2C+G.+E.&amp;rft.aulast=Hinton&amp;rft.au=Osindero%2C+S.&amp;rft.au=Teh%2C+Y.&amp;rft.date=2006&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.cs.toronto.edu%2F~hinton%2Fabsps%2Ffastnc.pdf&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.2006.18.7.1527&amp;rft_id=info%3Apmid%2F16764513&amp;rft.issue=7&amp;rft.jtitle=Neural+Computation&amp;rft.pages=1527%E2%80%931554&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=18" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-K._Fukushima._Neocognitron_1980-17"><span class="mw-cite-backlink">^ <a href="#cite_ref-K._Fukushima._Neocognitron_1980_17-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-K._Fukushima._Neocognitron_1980_17-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><span class="citation journal">Fukushima, K. (1980). "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position". <i>Biological Cybernetics</i> <b>36</b> (4): 93202. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2FBF00344251">10.1007/BF00344251</a>. <a href="/wiki/PubMed_Identifier" title="PubMed Identifier" class="mw-redirect">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/7370364">7370364</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Neocognitron%3A+A+self-organizing+neural+network+model+for+a+mechanism+of+pattern+recognition+unaffected+by+shift+in+position&amp;rft.au=Fukushima%2C+K.&amp;rft.aulast=Fukushima%2C+K.&amp;rft.date=1980&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1007%2FBF00344251&amp;rft_id=info%3Apmid%2F7370364&amp;rft.issue=4&amp;rft.jtitle=Biological+Cybernetics&amp;rft.pages=93%E2%80%93202&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=36" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-M_Riesenhuber.2C_1999-18"><span class="mw-cite-backlink">^ <a href="#cite_ref-M_Riesenhuber.2C_1999_18-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-M_Riesenhuber.2C_1999_18-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">M Riesenhuber, <a href="/wiki/Tomaso_Poggio" title="Tomaso Poggio">T Poggio</a>. Hierarchical models of object recognition in cortex. Nature neuroscience, 1999.</span></li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text">D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber. Multi-Column Deep Neural Network for Traffic Sign Classification. Neural Networks, 2012.</span></li>
<li id="cite_note-D._Ciresan.2C_A._Giusti_2012-20"><span class="mw-cite-backlink">^ <a href="#cite_ref-D._Ciresan.2C_A._Giusti_2012_20-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-D._Ciresan.2C_A._Giusti_2012_20-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">D. Ciresan, A. Giusti, L. Gambardella, J. Schmidhuber. Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images. In Advances in Neural Information Processing Systems (NIPS 2012), Lake Tahoe, 2012.</span></li>
<li id="cite_note-C._Ciresan.2C_U._Meier_2012-21"><span class="mw-cite-backlink">^ <a href="#cite_ref-C._Ciresan.2C_U._Meier_2012_21-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-C._Ciresan.2C_U._Meier_2012_21-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">D. C. Ciresan, U. Meier, <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="J칲rgen Schmidhuber">J. Schmidhuber</a>. Multi-column Deep Neural Networks for Image Classification. IEEE Conf. on Computer Vision and Pattern Recognition CVPR 2012.</span></li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions">2012 Kurzweil AI Interview</a> with <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="J칲rgen Schmidhuber">J칲rgen Schmidhuber</a> on the eight competitions won by his Deep Learning team 20092012</span></li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text">Graves, Alex; and Schmidhuber, J칲rgen; <i>Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</i>, in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), <i>Advances in Neural Information Processing Systems 22 (NIPS'22), 710 December 2009, Vancouver, BC</i>, Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545552.</span></li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text">A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="J칲rgen Schmidhuber">J. Schmidhuber</a>. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, 2009.</span></li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text">D. C. Ciresan, U. Meier, J. Masci, <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="J칲rgen Schmidhuber">J. Schmidhuber</a>. Multi-Column Deep Neural Network for Traffic Sign Classification. Neural Networks, 2012.</span></li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.scholarpedia.org/article/Deep_belief_networks">Deep belief networks</a> at Scholarpedia.</span></li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><span class="citation journal"><a href="/wiki/Geoff_Hinton" title="Geoff Hinton" class="mw-redirect">Hinton, G. E.</a>; Osindero, S.; Teh, Y. W. (2006). <a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">"A Fast Learning Algorithm for Deep Belief Nets"</a>. <i><a href="/wiki/Neural_Computation" title="Neural Computation" class="mw-redirect">Neural Computation</a></i> <b>18</b> (7): 15271554. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1162%2Fneco.2006.18.7.1527">10.1162/neco.2006.18.7.1527</a>. <a href="/wiki/PubMed_Identifier" title="PubMed Identifier" class="mw-redirect">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/16764513">16764513</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=A+Fast+Learning+Algorithm+for+Deep+Belief+Nets&amp;rft.aufirst=G.+E.&amp;rft.au=Hinton%2C+G.+E.&amp;rft.aulast=Hinton&amp;rft.au=Osindero%2C+S.&amp;rft.au=Teh%2C+Y.+W.&amp;rft.date=2006&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.cs.toronto.edu%2F~hinton%2Fabsps%2Ffastnc.pdf&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.2006.18.7.1527&amp;rft_id=info%3Apmid%2F16764513&amp;rft.issue=7&amp;rft.jtitle=Neural+Computation&amp;rft.pages=1527%E2%80%931554&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=18" class="Z3988"><span style="display:none;">&#160;</span></span> <span class="plainlinks noprint" style="font-size:smaller"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Cite_doi/10.1162.2Fneco.2006.18.7.1527&amp;action=edit&amp;editintro=Template:Cite_doi/editintro2">edit</a></span></span></li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><span class="citation news">John Markoff (November 23, 2012). <a rel="nofollow" class="external text" href="http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html">"Scientists See Promise in Deep-Learning Programs"</a>. <i>New York Times</i>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Scientists+See+Promise+in+Deep-Learning+Programs&amp;rft.au=John+Markoff&amp;rft.aulast=John+Markoff&amp;rft.date=November+23%2C+2012&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.nytimes.com%2F2012%2F11%2F24%2Fscience%2Fscientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html&amp;rft.jtitle=New+York+Times&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text"><span class="citation web"><a rel="nofollow" class="external text" href="http://www.cse.unsw.edu.au/~billw/mldict.html#activnfn">"The Machine Learning Dictionary"</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.btitle=The+Machine+Learning+Dictionary&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.cse.unsw.edu.au%2F~billw%2Fmldict.html%23activnfn&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text"><span class="citation conference">Dominic, S., Das, R., Whitley, D., Anderson, C. (July 1991). <a rel="nofollow" class="external text" href="http://dx.doi.org/10.1109/IJCNN.1991.155315">"Genetic reinforcement learning for neural networks"</a>. <i>IJCNN-91-Seattle International Joint Conference on Neural Networks</i>. IJCNN-91-Seattle International Joint Conference on Neural Networks. Seattle, Washington, USA: IEEE. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1109%2FIJCNN.1991.155315">10.1109/IJCNN.1991.155315</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-7803-0164-1" title="Special:BookSources/0-7803-0164-1">0-7803-0164-1</a><span class="reference-accessdate">. Retrieved 29 July 2012</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=IJCNN-91-Seattle+International+Joint+Conference+on+Neural+Networks&amp;rft.au=Dominic%2C+S.%2C+Das%2C+R.%2C+Whitley%2C+D.%2C+Anderson%2C+C.&amp;rft.aulast=Dominic%2C+S.%2C+Das%2C+R.%2C+Whitley%2C+D.%2C+Anderson%2C+C.&amp;rft.btitle=Genetic+reinforcement+learning+for+neural+networks&amp;rft.date=July+1991&amp;rft.genre=bookitem&amp;rft_id=http%3A%2F%2Fdx.doi.org%2F10.1109%2FIJCNN.1991.155315&amp;rft_id=info%3Adoi%2F10.1109%2FIJCNN.1991.155315&amp;rft.isbn=0-7803-0164-1&amp;rft.place=Seattle%2C+Washington%2C+USA&amp;rft.pub=IEEE&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text"><span class="citation journal">Hoskins, J.C.; Himmelblau, D.M. (1992). "Process control via artificial neural networks and reinforcement learning". <i>Computers &amp; Chemical Engineering</i> <b>16</b> (4): 241251. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1016%2F0098-1354%2892%2980045-B">10.1016/0098-1354(92)80045-B</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Process+control+via+artificial+neural+networks+and+reinforcement+learning&amp;rft.aufirst=J.C.&amp;rft.au=Hoskins%2C+J.C.&amp;rft.aulast=Hoskins&amp;rft.date=1992&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1016%2F0098-1354%2892%2980045-B&amp;rft.issue=4&amp;rft.jtitle=Computers+%26+Chemical+Engineering&amp;rft.pages=241%E2%80%93251&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=16" class="Z3988"><span style="display:none;">&#160;</span></span> <span style="display:none;font-size:100%" class="error citation-comment">Cite uses deprecated parameters (<a href="/wiki/Help:CS1_errors#deprecated_params" title="Help:CS1 errors">help</a>)</span></span></li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><span class="citation book">Bertsekas, D.P., Tsitsiklis, J.N. (1996). <i>Neuro-dynamic programming</i>. Athena Scientific. p.&#160;512. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/1-886529-10-8" title="Special:BookSources/1-886529-10-8">1-886529-10-8</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.au=Bertsekas%2C+D.P.%2C+Tsitsiklis%2C+J.N.&amp;rft.aulast=Bertsekas%2C+D.P.%2C+Tsitsiklis%2C+J.N.&amp;rft.btitle=Neuro-dynamic+programming&amp;rft.date=1996&amp;rft.genre=book&amp;rft.isbn=1-886529-10-8&amp;rft.pages=512&amp;rft.pub=Athena+Scientific&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><span class="citation journal">Secomandi, Nicola (2000). "Comparing neuro-dynamic programming algorithms for the vehicle routing problem with stochastic demands". <i>Computers &amp; Operations Research</i> <b>27</b> (1112): 12011225. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1016%2FS0305-0548%2899%2900146-X">10.1016/S0305-0548(99)00146-X</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Comparing+neuro-dynamic+programming+algorithms+for+the+vehicle+routing+problem+with+stochastic+demands&amp;rft.aufirst=Nicola&amp;rft.aulast=Secomandi&amp;rft.au=Secomandi%2C+Nicola&amp;rft.date=2000&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1016%2FS0305-0548%2899%2900146-X&amp;rft.issue=11%E2%80%9312&amp;rft.jtitle=Computers+%26+Operations+Research&amp;rft.pages=1201%E2%80%931225&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=27" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-34">^</a></b></span> <span class="reference-text"><span class="citation conference">de Rigo, D., Rizzoli, A. E., Soncini-Sessa, R., Weber, E., Zenesi, P. (2001). <a rel="nofollow" class="external text" href="https://zenodo.org/record/7482/files/de_Rigo_etal_MODSIM2001_activelink_authorcopy.pdf">"Neuro-dynamic programming for the efficient management of reservoir networks"</a>. <i>Proceedings of MODSIM 2001, International Congress on Modelling and Simulation</i>. <a rel="nofollow" class="external text" href="http://www.mssanz.org.au/MODSIM01/MODSIM01.htm">MODSIM 2001, International Congress on Modelling and Simulation</a>. Canberra, Australia: Modelling and Simulation Society of Australia and New Zealand. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.5281%2Fzenodo.7481">10.5281/zenodo.7481</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-867405252" title="Special:BookSources/0-867405252">0-867405252</a><span class="reference-accessdate">. Retrieved 29 July 2012</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Proceedings+of+MODSIM+2001%2C+International+Congress+on+Modelling+and+Simulation&amp;rft.au=de+Rigo%2C+D.%2C+Rizzoli%2C+A.+E.%2C+Soncini-Sessa%2C+R.%2C+Weber%2C+E.%2C+Zenesi%2C+P.&amp;rft.aulast=de+Rigo%2C+D.%2C+Rizzoli%2C+A.+E.%2C+Soncini-Sessa%2C+R.%2C+Weber%2C+E.%2C+Zenesi%2C+P.&amp;rft.btitle=Neuro-dynamic+programming+for+the+efficient+management+of+reservoir+networks&amp;rft.date=2001&amp;rft.genre=bookitem&amp;rft_id=https%3A%2F%2Fzenodo.org%2Frecord%2F7482%2Ffiles%2Fde_Rigo_etal_MODSIM2001_activelink_authorcopy.pdf&amp;rft_id=info%3Adoi%2F10.5281%2Fzenodo.7481&amp;rft.isbn=0-867405252&amp;rft.place=Canberra%2C+Australia&amp;rft.pub=Modelling+and+Simulation+Society+of+Australia+and+New+Zealand&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35">^</a></b></span> <span class="reference-text"><span class="citation conference">Damas, M., Salmeron, M., Diaz, A., Ortega, J., Prieto, A., Olivares, G. (2000). <a rel="nofollow" class="external text" href="http://dx.doi.org/10.1109/CEC.2000.870269">"Genetic algorithms and neuro-dynamic programming: application to water supply networks"</a>. <i>Proceedings of 2000 Congress on Evolutionary Computation</i>. 2000 Congress on Evolutionary Computation. La Jolla, California, USA: IEEE. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1109%2FCEC.2000.870269">10.1109/CEC.2000.870269</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-7803-6375-2" title="Special:BookSources/0-7803-6375-2">0-7803-6375-2</a><span class="reference-accessdate">. Retrieved 29 July 2012</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Proceedings+of+2000+Congress+on+Evolutionary+Computation&amp;rft.au=Damas%2C+M.%2C+Salmeron%2C+M.%2C+Diaz%2C+A.%2C+Ortega%2C+J.%2C+Prieto%2C+A.%2C+Olivares%2C+G.&amp;rft.aulast=Damas%2C+M.%2C+Salmeron%2C+M.%2C+Diaz%2C+A.%2C+Ortega%2C+J.%2C+Prieto%2C+A.%2C+Olivares%2C+G.&amp;rft.btitle=Genetic+algorithms+and+neuro-dynamic+programming%3A+application+to+water+supply+networks&amp;rft.date=2000&amp;rft.genre=bookitem&amp;rft_id=http%3A%2F%2Fdx.doi.org%2F10.1109%2FCEC.2000.870269&amp;rft_id=info%3Adoi%2F10.1109%2FCEC.2000.870269&amp;rft.isbn=0-7803-6375-2&amp;rft.place=La+Jolla%2C+California%2C+USA&amp;rft.pub=IEEE&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text"><span class="citation journal">Deng, Geng; Ferris, M.C. (2008). "Neuro-dynamic programming for fractionated radiotherapy planning". <i>Springer Optimization and Its Applications</i> <b>12</b>: 4770. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2F978-0-387-73299-2_3">10.1007/978-0-387-73299-2_3</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Neuro-dynamic+programming+for+fractionated+radiotherapy+planning&amp;rft.au=Deng%2C+Geng&amp;rft.aufirst=Geng&amp;rft.aulast=Deng&amp;rft.date=2008&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1007%2F978-0-387-73299-2_3&amp;rft.jtitle=Springer+Optimization+and+Its+Applications&amp;rft.pages=47%E2%80%9370&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=12" class="Z3988"><span style="display:none;">&#160;</span></span> <span style="display:none;font-size:100%" class="error citation-comment">Cite uses deprecated parameters (<a href="/wiki/Help:CS1_errors#deprecated_params" title="Help:CS1 errors">help</a>)</span></span></li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text"><span class="citation conference">de Rigo, D., Castelletti, A., Rizzoli, A.E., Soncini-Sessa, R., Weber, E. (January 2005). <a rel="nofollow" class="external text" href="http://www.nt.ntnu.no/users/skoge/prost/proceedings/ifac2005/Papers/Paper4269.html">"A selective improvement technique for fastening Neuro-Dynamic Programming in Water Resources Network Management"</a>. In Pavel Z칤tek. <i>Proceedings of the 16th IFAC World Congress  IFAC-PapersOnLine</i>. <a rel="nofollow" class="external text" href="http://www.nt.ntnu.no/users/skoge/prost/proceedings/ifac2005/Index.html">16th IFAC World Congress</a> <b>16</b>. Prague, Czech Republic: IFAC. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.3182%2F20050703-6-CZ-1902.02172">10.3182/20050703-6-CZ-1902.02172</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-902661-75-3" title="Special:BookSources/978-3-902661-75-3">978-3-902661-75-3</a><span class="reference-accessdate">. Retrieved 30 December 2011</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Proceedings+of+the+16th+IFAC+World+Congress+%E2%80%93+IFAC-PapersOnLine&amp;rft.au=de+Rigo%2C+D.%2C+Castelletti%2C+A.%2C+Rizzoli%2C+A.E.%2C+Soncini-Sessa%2C+R.%2C+Weber%2C+E.&amp;rft.aulast=de+Rigo%2C+D.%2C+Castelletti%2C+A.%2C+Rizzoli%2C+A.E.%2C+Soncini-Sessa%2C+R.%2C+Weber%2C+E.&amp;rft.btitle=A+selective+improvement+technique+for+fastening+Neuro-Dynamic+Programming+in+Water+Resources+Network+Management&amp;rft.date=January+2005&amp;rft.genre=bookitem&amp;rft_id=http%3A%2F%2Fwww.nt.ntnu.no%2Fusers%2Fskoge%2Fprost%2Fproceedings%2Fifac2005%2FPapers%2FPaper4269.html&amp;rft_id=info%3Adoi%2F10.3182%2F20050703-6-CZ-1902.02172&amp;rft.isbn=978-3-902661-75-3&amp;rft.place=Prague%2C+Czech+Republic&amp;rft.pub=IFAC&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.volume=16" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text"><span class="citation web">Ferreira, C. (2006). <a rel="nofollow" class="external text" href="http://www.gene-expression-programming.com/webpapers/Ferreira-ASCT2006.pdf">"Designing Neural Networks Using Gene Expression Programming"</a>. In A. Abraham, B. de Baets, M. K칬ppen, and B. Nickolay, eds., Applied Soft Computing Technologies: The Challenge of Complexity, pages 517536, Springer-Verlag.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.au=Ferreira%2C+C.&amp;rft.aufirst=C.&amp;rft.aulast=Ferreira&amp;rft.btitle=Designing+Neural+Networks+Using+Gene+Expression+Programming&amp;rft.date=2006&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.gene-expression-programming.com%2Fwebpapers%2FFerreira-ASCT2006.pdf&amp;rft.pub=In+A.+Abraham%2C+B.+de+Baets%2C+M.+K%C3%B6ppen%2C+and+B.+Nickolay%2C+eds.%2C+Applied+Soft+Computing+Technologies%3A+The+Challenge+of+Complexity%2C+pages+517%E2%80%93536%2C+Springer-Verlag&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39">^</a></b></span> <span class="reference-text"><span class="citation conference">Da, Y., Xiurun, G. (July 2005). T. Villmann, ed. "An improved PSO-based ANN with simulated annealing technique". <a rel="nofollow" class="external text" href="http://www.dice.ucl.ac.be/esann/proceedings/electronicproceedings.htm">New Aspects in Neurocomputing: 11th European Symposium on Artificial Neural Networks</a>. Elsevier. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1016%2Fj.neucom.2004.07.002">10.1016/j.neucom.2004.07.002</a><span class="reference-accessdate">. Retrieved 30 December 2011</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.au=Da%2C+Y.%2C+Xiurun%2C+G.&amp;rft.aulast=Da%2C+Y.%2C+Xiurun%2C+G.&amp;rft.btitle=An+improved+PSO-based+ANN+with+simulated+annealing+technique&amp;rft.date=July+2005&amp;rft.genre=book&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2004.07.002&amp;rft.pub=Elsevier&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text"><span class="citation conference">Wu, J., Chen, E. (May 2009). Wang, H., Shen, Y., Huang, T., Zeng, Z., ed. "A Novel Nonparametric Regression Ensemble for Rainfall Forecasting Using Particle Swarm Optimization Technique Coupled with Artificial Neural Network". <a rel="nofollow" class="external text" href="http://www2.mae.cuhk.edu.hk/~isnn2009/">6th International Symposium on Neural Networks, ISNN 2009</a>. Springer. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2F978-3-642-01513-7_6">10.1007/978-3-642-01513-7_6</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-642-01215-0" title="Special:BookSources/978-3-642-01215-0">978-3-642-01215-0</a><span class="reference-accessdate">. Retrieved 1 January 2012</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.aulast=Wu%2C+J.%2C+Chen%2C+E.&amp;rft.au=Wu%2C+J.%2C+Chen%2C+E.&amp;rft.btitle=A+Novel+Nonparametric+Regression+Ensemble+for+Rainfall+Forecasting+Using+Particle+Swarm+Optimization+Technique+Coupled+with+Artificial+Neural+Network&amp;rft.date=May+2009&amp;rft.genre=book&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-01513-7_6&amp;rft.isbn=978-3-642-01215-0&amp;rft.pub=Springer&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-Balabin_2009-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-Balabin_2009_41-0">^</a></b></span> <span class="reference-text"><span class="citation journal">Roman M. Balabin, Ekaterina I. Lomakina (2009). "Neural network approach to quantum-chemistry data: Accurate prediction of density functional theory energies". <i><a href="/wiki/J._Chem._Phys." title="J. Chem. Phys." class="mw-redirect">J. Chem. Phys.</a></i> <b>131</b> (7): 074104. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1063%2F1.3206326">10.1063/1.3206326</a>. <a href="/wiki/PubMed_Identifier" title="PubMed Identifier" class="mw-redirect">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/19708729">19708729</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Neural+network+approach+to+quantum-chemistry+data%3A+Accurate+prediction+of+density+functional+theory+energies&amp;rft.aulast=Roman+M.+Balabin%2C++Ekaterina+I.+Lomakina&amp;rft.au=Roman+M.+Balabin%2C++Ekaterina+I.+Lomakina&amp;rft.date=2009&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1063%2F1.3206326&amp;rft_id=info%3Apmid%2F19708729&amp;rft.issue=7&amp;rft.jtitle=J.+Chem.+Phys.&amp;rft.pages=074104&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=131" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-42">^</a></b></span> <span class="reference-text"><span class="citation web">Ganesan, N. <a rel="nofollow" class="external text" href="http://www.ijcaonline.org/journal/number26/pxc387783.pdf">"Application of Neural Networks in Diagnosing Cancer Disease Using Demographic Data"</a>. International Journal of Computer Applications.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.aufirst=N&amp;rft.au=Ganesan%2C+N&amp;rft.aulast=Ganesan&amp;rft.btitle=Application+of+Neural+Networks+in+Diagnosing+Cancer+Disease+Using+Demographic+Data&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.ijcaonline.org%2Fjournal%2Fnumber26%2Fpxc387783.pdf&amp;rft.pub=International+Journal+of+Computer+Applications&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text"><span class="citation web">Bottaci, Leonardo. <a rel="nofollow" class="external text" href="http://www.lcc.uma.es/~jja/recidiva/042.pdf">"Artificial Neural Networks Applied to Outcome Prediction for Colorectal Cancer Patients in Separate Institutions"</a>. The Lancet.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.au=Bottaci%2C+Leonardo&amp;rft.aufirst=Leonardo&amp;rft.aulast=Bottaci&amp;rft.btitle=Artificial+Neural+Networks+Applied+to+Outcome+Prediction+for+Colorectal+Cancer+Patients+in+Separate+Institutions&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.lcc.uma.es%2F~jja%2Frecidiva%2F042.pdf&amp;rft.pub=The+Lancet&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text"><span class="citation web"><a rel="nofollow" class="external text" href="http://wiki.syncleus.com/index.php/DANN:Genetic_Wavelets">"DANN:Genetic Wavelets"</a>. dANN project. <a rel="nofollow" class="external text" href="http://web.archive.org/web/20100821112612/http://wiki.syncleus.com/index.php/DANN:Genetic_Wavelets">Archived</a> from the original on 21 August 2010<span class="reference-accessdate">. Retrieved 12 July 2010</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.btitle=DANN%3AGenetic+Wavelets&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwiki.syncleus.com%2Findex.php%2FDANN%3AGenetic_Wavelets&amp;rft.pub=dANN+project&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text"><span class="citation journal">Siegelmann, H.T.; Sontag, E.D. (1991). <a rel="nofollow" class="external text" href="http://www.math.rutgers.edu/~sontag/FTP_DIR/aml-turing.pdf">"Turing computability with neural nets"</a>. <i>Appl. Math. Lett.</i> <b>4</b> (6): 7780. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1016%2F0893-9659%2891%2990080-F">10.1016/0893-9659(91)90080-F</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Turing+computability+with+neural+nets&amp;rft.aufirst=H.T.&amp;rft.aulast=Siegelmann&amp;rft.au=Siegelmann%2C+H.T.&amp;rft.au=Sontag%2C+E.D.&amp;rft.date=1991&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.math.rutgers.edu%2F~sontag%2FFTP_DIR%2Faml-turing.pdf&amp;rft_id=info%3Adoi%2F10.1016%2F0893-9659%2891%2990080-F&amp;rft.issue=6&amp;rft.jtitle=Appl.+Math.+Lett.&amp;rft.pages=77%E2%80%9380&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=4" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.nasa.gov/centers/dryden/news/NewsReleases/2003/03-49.html">NASA - Dryden Flight Research Center - News Room: News Releases: NASA NEURAL NETWORK PROJECT PASSES MILESTONE</a>. Nasa.gov. Retrieved on 2013-11-20.</span></li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://members.fortunecity.com/templarseries/popper.html">Roger Bridgman's defence of neural networks</a></span></li>
<li id="cite_note-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-48">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/4">http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/4</a></span></li>
</ol>
</div>
<h2><span class="mw-headline" id="Bibliography">Bibliography</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=31" title="Edit section: Bibliography">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="references-small">
<ul>
<li><span class="citation journal">Bhadeshia H. K. D. H. (1999). <a rel="nofollow" class="external text" href="http://www.msm.cam.ac.uk/phase-trans/abstracts/neural.review.pdf">"Neural Networks in Materials Science"</a>. <i>ISIJ International</i> <b>39</b> (10): 966979. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.2355%2Fisijinternational.39.966">10.2355/isijinternational.39.966</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Neural+Networks+in+Materials+Science&amp;rft.au=Bhadeshia+H.+K.+D.+H.&amp;rft.aulast=Bhadeshia+H.+K.+D.+H.&amp;rft.date=1999&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.msm.cam.ac.uk%2Fphase-trans%2Fabstracts%2Fneural.review.pdf&amp;rft_id=info%3Adoi%2F10.2355%2Fisijinternational.39.966&amp;rft.issue=10&amp;rft.jtitle=ISIJ+International&amp;rft.pages=966%E2%80%93979&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=39" class="Z3988"><span style="display:none;">&#160;</span></span></li>
<li>Bishop, C.M. (1995) <i>Neural Networks for Pattern Recognition</i>, Oxford: Oxford University Press. <a href="/wiki/Special:BookSources/0198538499" class="internal mw-magiclink-isbn">ISBN 0-19-853849-9</a> (hardback) or <a href="/wiki/Special:BookSources/0198538642" class="internal mw-magiclink-isbn">ISBN 0-19-853864-2</a> (paperback)</li>
<li>Cybenko, G.V. (1989). Approximation by Superpositions of a Sigmoidal function, <i><a href="/wiki/Mathematics_of_Control,_Signals,_and_Systems" title="Mathematics of Control, Signals, and Systems">Mathematics of Control, Signals, and Systems</a></i>, Vol. 2 pp.&#160;303314. <a rel="nofollow" class="external text" href="http://actcomm.dartmouth.edu/gvc/papers/approx_by_superposition.pdf">electronic version</a></li>
<li>Duda, R.O., Hart, P.E., Stork, D.G. (2001) <i>Pattern classification (2nd edition)</i>, Wiley, <a href="/wiki/Special:BookSources/0471056693" class="internal mw-magiclink-isbn">ISBN 0-471-05669-3</a></li>
<li><span class="citation journal">Egmont-Petersen, M., de Ridder, D., Handels, H. (2002). "Image processing with neural networks  a review". <i>Pattern Recognition</i> <b>35</b> (10): 22792301. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1016%2FS0031-3203%2801%2900178-9">10.1016/S0031-3203(01)00178-9</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AArtificial+neural+network&amp;rft.atitle=Image+processing+with+neural+networks+%E2%80%93+a+review&amp;rft.au=Egmont-Petersen%2C+M.%2C+de+Ridder%2C+D.%2C+Handels%2C+H.&amp;rft.aulast=Egmont-Petersen%2C+M.%2C+de+Ridder%2C+D.%2C+Handels%2C+H.&amp;rft.date=2002&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1016%2FS0031-3203%2801%2900178-9&amp;rft.issue=10&amp;rft.jtitle=Pattern+Recognition&amp;rft.pages=2279%E2%80%932301&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=35" class="Z3988"><span style="display:none;">&#160;</span></span></li>
<li>Gurney, K. (1997) <i>An Introduction to Neural Networks</i> London: Routledge. <a href="/wiki/Special:BookSources/1857286731" class="internal mw-magiclink-isbn">ISBN 1-85728-673-1</a> (hardback) or <a href="/wiki/Special:BookSources/1857285034" class="internal mw-magiclink-isbn">ISBN 1-85728-503-4</a> (paperback)</li>
<li>Haykin, S. (1999) <i>Neural Networks: A Comprehensive Foundation</i>, Prentice Hall, <a href="/wiki/Special:BookSources/0132733501" class="internal mw-magiclink-isbn">ISBN 0-13-273350-1</a></li>
<li>Fahlman, S, Lebiere, C (1991). <i>The Cascade-Correlation Learning Architecture</i>, created for <a href="/wiki/National_Science_Foundation" title="National Science Foundation">National Science Foundation</a>, Contract Number EET-8716324, and <a href="/wiki/Defense_Advanced_Research_Projects_Agency" title="Defense Advanced Research Projects Agency" class="mw-redirect">Defense Advanced Research Projects Agency</a> (DOD), ARPA Order No. 4976 under Contract F33615-87-C-1499. <a rel="nofollow" class="external text" href="http://www.cs.iastate.edu/~honavar/fahlman.pdf">electronic version</a></li>
<li>Hertz, J., Palmer, R.G., Krogh. A.S. (1990) <i>Introduction to the theory of neural computation</i>, Perseus Books. <a href="/wiki/Special:BookSources/0201515601" class="internal mw-magiclink-isbn">ISBN 0-201-51560-1</a></li>
<li>Lawrence, Jeanette (1994) <i>Introduction to Neural Networks</i>, California Scientific Software Press. <a href="/wiki/Special:BookSources/1883157005" class="internal mw-magiclink-isbn">ISBN 1-883157-00-5</a></li>
<li>Masters, Timothy (1994) <i>Signal and Image Processing with Neural Networks</i>, John Wiley &amp; Sons, Inc. <a href="/wiki/Special:BookSources/0471049638" class="internal mw-magiclink-isbn">ISBN 0-471-04963-8</a></li>
<li><a href="/wiki/Brian_D._Ripley" title="Brian D. Ripley">Ripley, Brian D</a>. (1996) <i>Pattern Recognition and Neural Networks</i>, Cambridge</li>
<li>Siegelmann, H.T. and <a href="/wiki/Eduardo_D._Sontag" title="Eduardo D. Sontag">Sontag, E.D.</a> (1994). Analog computation via neural networks, <i>Theoretical Computer Science</i>, v. 131, no. 2, pp.&#160;331360. <a rel="nofollow" class="external text" href="http://www.math.rutgers.edu/~sontag/FTP_DIR/nets-real.pdf">electronic version</a></li>
<li>Sergios Theodoridis, Konstantinos Koutroumbas (2009) "Pattern Recognition", 4th Edition, Academic Press, <a href="/wiki/Special:BookSources/9781597492720" class="internal mw-magiclink-isbn">ISBN 978-1-59749-272-0</a>.</li>
<li>Smith, Murray (1993) <i>Neural Networks for Statistical Modeling</i>, Van Nostrand Reinhold, <a href="/wiki/Special:BookSources/0442013108" class="internal mw-magiclink-isbn">ISBN 0-442-01310-8</a></li>
<li>Wasserman, Philip (1993) <i>Advanced Methods in Neural Computing</i>, Van Nostrand Reinhold, <a href="/wiki/Special:BookSources/0442004613" class="internal mw-magiclink-isbn">ISBN 0-442-00461-3</a></li>
<li><i>Computational Intelligence: A Methodological Introduction</i> by Kruse, Borgelt, Klawonn, Moewes, Steinbrecher, Held, 2013, Springer, <a href="/wiki/Special:BookSources/9781447150121" class="internal mw-magiclink-isbn">ISBN 9781447150121</a></li>
<li><i>Neuro-Fuzzy-Systeme</i> (3rd edition) by Borgelt, Klawonn, Kruse, Nauck, 2003, Vieweg, <a href="/wiki/Special:BookSources/9783528252656" class="internal mw-magiclink-isbn">ISBN 9783528252656</a></li>
</ul>
</div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit&amp;section=32" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<table class="metadata mbox-small plainlinks" style="border:1px solid #aaa; background-color:#f9f9f9;">
<tr>
<td class="mbox-image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Wikibooks-logo-en-noslogan.svg/40px-Wikibooks-logo-en-noslogan.svg.png" width="40" height="40" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Wikibooks-logo-en-noslogan.svg/60px-Wikibooks-logo-en-noslogan.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/df/Wikibooks-logo-en-noslogan.svg/80px-Wikibooks-logo-en-noslogan.svg.png 2x" /></td>
<td class="mbox-text plainlist" style="">Wikibooks has a book on the topic of: <i><b><a href="//en.wikibooks.org/wiki/Artificial_Neural_Networks" class="extiw" title="wikibooks:Artificial Neural Networks">Artificial Neural Networks</a></b></i></td>
</tr>
</table>
<ul>
<li><a rel="nofollow" class="external text" href="http://www.dmoz.org/Computers/Artificial_Intelligence/Neural_Networks">Neural Networks</a> on the <a href="/wiki/Open_Directory_Project" title="Open Directory Project">Open Directory Project</a></li>
</ul>


<!-- 
NewPP limit report
Parsed by mw1014
CPU time usage: 2.060 seconds
Real time usage: 17.373 seconds
Preprocessor visited node count: 4540/1000000
Preprocessor generated node count: 23618/1500000
Post난xpand include size: 108514/2048000 bytes
Template argument size: 15442/2048000 bytes
Highest expansion depth: 16/40
Expensive parser function count: 9/500
Lua time usage: 0.199/10.000 seconds
Lua memory usage: 3.38 MB/50 MB
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:21523-0!*!0!!en!4!*!math=0 and timestamp 20140223012756
 -->
<noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>								<div class="printfooter">
				Retrieved from "<a href="http://en.wikipedia.org/w/index.php?title=Artificial_neural_network&amp;oldid=595510034">http://en.wikipedia.org/w/index.php?title=Artificial_neural_network&amp;oldid=595510034</a>"				</div>
												<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Computational_statistics" title="Category:Computational statistics">Computational statistics</a></li><li><a href="/wiki/Category:Neural_networks" title="Category:Neural networks">Neural networks</a></li><li><a href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></li><li><a href="/wiki/Category:Computational_neuroscience" title="Category:Computational neuroscience">Computational neuroscience</a></li><li><a href="/wiki/Category:Knowledge_representation" title="Category:Knowledge representation">Knowledge representation</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Pages_containing_cite_templates_with_deprecated_parameters" title="Category:Pages containing cite templates with deprecated parameters">Pages containing cite templates with deprecated parameters</a></li><li><a href="/wiki/Category:Use_dmy_dates_from_June_2013" title="Category:Use dmy dates from June 2013">Use dmy dates from June 2013</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_August_2011" title="Category:Articles with unsourced statements from August 2011">Articles with unsourced statements from August 2011</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_March_2012" title="Category:Articles with unsourced statements from March 2012">Articles with unsourced statements from March 2012</a></li><li><a href="/wiki/Category:Articles_needing_expert_attention_with_no_reason_or_talk_parameter" title="Category:Articles needing expert attention with no reason or talk parameter">Articles needing expert attention with no reason or talk parameter</a></li><li><a href="/wiki/Category:Articles_needing_expert_attention_from_November_2008" title="Category:Articles needing expert attention from November 2008">Articles needing expert attention from November 2008</a></li><li><a href="/wiki/Category:All_articles_needing_expert_attention" title="Category:All articles needing expert attention">All articles needing expert attention</a></li><li><a href="/wiki/Category:Technology_articles_needing_expert_attention" title="Category:Technology articles needing expert attention">Technology articles needing expert attention</a></li><li><a href="/wiki/Category:Miscellaneous_articles_needing_expert_attention" title="Category:Miscellaneous articles needing expert attention">Miscellaneous articles needing expert attention</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_August_2012" title="Category:Articles with unsourced statements from August 2012">Articles with unsourced statements from August 2012</a></li><li><a href="/wiki/Category:Articles_with_Open_Directory_Project_links" title="Category:Articles with Open Directory Project links">Articles with Open Directory Project links</a></li></ul></div></div>												<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
				<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
	<h3 id="p-personal-label">Personal tools</h3>
	<ul>
<li id="pt-createaccount"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Artificial+neural+network&amp;type=signup">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Artificial+neural+network" title="You're encouraged to log in; however, it's not mandatory. [o]" accesskey="o">Log in</a></li>	</ul>
</div>
				<div id="left-navigation">
					<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
	<h3 id="p-namespaces-label">Namespaces</h3>
	<ul>
					<li  id="ca-nstab-main" class="selected"><span><a href="/wiki/Artificial_neural_network"  title="View the content page [c]" accesskey="c">Article</a></span></li>
					<li  id="ca-talk"><span><a href="/wiki/Talk:Artificial_neural_network"  title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li>
			</ul>
</div>
<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
	<h3 id="mw-vector-current-variant">
		</h3>
	<h3 id="p-variants-label"><span>Variants</span><a href="#"></a></h3>
	<div class="menu">
		<ul>
					</ul>
	</div>
</div>
				</div>
				<div id="right-navigation">
					<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
	<h3 id="p-views-label">Views</h3>
	<ul>
					<li id="ca-view" class="selected"><span><a href="/wiki/Artificial_neural_network" >Read</a></span></li>
					<li id="ca-edit"><span><a href="/w/index.php?title=Artificial_neural_network&amp;action=edit"  title="You can edit this page. &#10;Please review your changes before saving. [e]" accesskey="e">Edit</a></span></li>
					<li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Artificial_neural_network&amp;action=history"  title="Past versions of this page [h]" accesskey="h">View history</a></span></li>
			</ul>
</div>
<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
	<h3 id="p-cactions-label"><span>Actions</span><a href="#"></a></h3>
	<div class="menu">
		<ul>
					</ul>
	</div>
</div>
<div id="p-search" role="search">
	<h3><label for="searchInput">Search</label></h3>
	<form action="/w/index.php" id="searchform">
					<div id="simpleSearch">
					<input type="search" name="search" placeholder="Search" title="Search Wikipedia [f]" accesskey="f" id="searchInput" /><input type="hidden" value="Special:Search" name="title" /><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton" /><input type="submit" name="go" value="Go" title="Go to a page with this exact name if one exists" id="searchButton" class="searchButton" />		</div>
	</form>
</div>
				</div>
			</div>
			<div id="mw-panel">
					<div id="p-logo" role="banner"><a style="background-image: url(//upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page"  title="Visit the main page"></a></div>
				<div class="portal" role="navigation" id='p-navigation' aria-labelledby='p-navigation-label'>
	<h3 id='p-navigation-label'>Navigation</h3>
	<div class="body">
		<ul>
			<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
			<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
			<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content  the best of Wikipedia">Featured content</a></li>
			<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
			<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			<li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li>
			<li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikimedia Shop">Wikimedia Shop</a></li>
		</ul>
	</div>
</div>
<div class="portal" role="navigation" id='p-interaction' aria-labelledby='p-interaction-label'>
	<h3 id='p-interaction-label'>Interaction</h3>
	<div class="body">
		<ul>
			<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
			<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
			<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
			<li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact page</a></li>
		</ul>
	</div>
</div>
<div class="portal" role="navigation" id='p-tb' aria-labelledby='p-tb-label'>
	<h3 id='p-tb-label'>Tools</h3>
	<div class="body">
		<ul>
			<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Artificial_neural_network" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
			<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Artificial_neural_network" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
			<li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li>
			<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li>
			<li id="t-permalink"><a href="/w/index.php?title=Artificial_neural_network&amp;oldid=595510034" title="Permanent link to this revision of the page">Permanent link</a></li>
			<li id="t-info"><a href="/w/index.php?title=Artificial_neural_network&amp;action=info">Page information</a></li>
			<li id="t-wikibase"><a href="//www.wikidata.org/wiki/Q192776" title="Link to connected data repository item [g]" accesskey="g">Data item</a></li>
<li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Artificial_neural_network&amp;id=595510034" title="Information on how to cite this page">Cite this page</a></li>		</ul>
	</div>
</div>
<div class="portal" role="navigation" id='p-coll-print_export' aria-labelledby='p-coll-print_export-label'>
	<h3 id='p-coll-print_export-label'>Print/export</h3>
	<div class="body">
		<ul>
			<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Artificial+neural+network">Create a book</a></li>
			<li id="coll-download-as-rl"><a href="/w/index.php?title=Special:Book&amp;bookcmd=render_article&amp;arttitle=Artificial+neural+network&amp;oldid=595510034&amp;writer=rl">Download as PDF</a></li>
			<li id="t-print"><a href="/w/index.php?title=Artificial_neural_network&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>
		</ul>
	</div>
</div>
<div class="portal" role="navigation" id='p-lang' aria-labelledby='p-lang-label'>
	<h3 id='p-lang-label'>Languages</h3>
	<div class="body">
		<ul>
			<li class="interlanguage-link interwiki-ar"><a href="//ar.wikipedia.org/wiki/%D8%B4%D8%A8%D9%83%D8%A7%D8%AA_%D8%B9%D8%B5%D8%A8%D9%88%D9%86%D9%8A%D8%A9_%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A%D8%A9" title="얮뻋 좢왖뻋걫궺 왖좣궺  Arabic" lang="ar" hreflang="ar">좢쐊뻋궺</a></li>
			<li class="interlanguage-link interwiki-az"><a href="//az.wikipedia.org/wiki/S%C3%BCni_Neyron_%C5%9E%C9%99b%C9%99k%C9%99l%C9%99r" title="S칲ni Neyron 뤨뗙톛k톛l톛r  Azerbaijani" lang="az" hreflang="az">Az톛rbaycanca</a></li>
			<li class="interlanguage-link interwiki-bs"><a href="//bs.wikipedia.org/wiki/Vje%C5%A1ta%C4%8Dke_neuronske_mre%C5%BEe" title="Vje코ta캜ke neuronske mre쬰  Bosnian" lang="bs" hreflang="bs">Bosanski</a></li>
			<li class="interlanguage-link interwiki-ca"><a href="//ca.wikipedia.org/wiki/Xarxa_neuronal_artificial" title="Xarxa neuronal artificial  Catalan" lang="ca" hreflang="ca">Catal</a></li>
			<li class="interlanguage-link interwiki-de"><a href="//de.wikipedia.org/wiki/K%C3%BCnstliches_neuronales_Netz" title="K칲nstliches neuronales Netz  German" lang="de" hreflang="de">Deutsch</a></li>
			<li class="interlanguage-link interwiki-es"><a href="//es.wikipedia.org/wiki/Red_neuronal_artificial" title="Red neuronal artificial  Spanish" lang="es" hreflang="es">Espa침ol</a></li>
			<li class="interlanguage-link interwiki-eo"><a href="//eo.wikipedia.org/wiki/Artefarita_ne%C5%ADra_reto" title="Artefarita ne콠ra reto  Esperanto" lang="eo" hreflang="eo">Esperanto</a></li>
			<li class="interlanguage-link interwiki-fa"><a href="//fa.wikipedia.org/wiki/%D8%B4%D8%A8%DA%A9%D9%87_%D8%B9%D8%B5%D8%A8%DB%8C_%D9%85%D8%B5%D9%86%D9%88%D8%B9%DB%8C" title="얮뻌뽳 좢왖뻍 왗걪좥  Persian" lang="fa" hreflang="fa">쐊</a></li>
			<li class="interlanguage-link interwiki-fr"><a href="//fr.wikipedia.org/wiki/R%C3%A9seau_de_neurones_artificiels" title="R칠seau de neurones artificiels  French" lang="fr" hreflang="fr">Fran칞ais</a></li>
			<li class="interlanguage-link interwiki-ga"><a href="//ga.wikipedia.org/wiki/L%C3%ADonra_n%C3%A9ar%C3%B3gach" title="L칤onra n칠ar칩gach  Irish" lang="ga" hreflang="ga">Gaeilge</a></li>
			<li class="interlanguage-link interwiki-ko"><a href="//ko.wikipedia.org/wiki/%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D" title="述젉씻멁쒛勢  Korean" lang="ko" hreflang="ko">涯럵옰諄</a></li>
			<li class="interlanguage-link interwiki-hy"><a href="//hy.wikipedia.org/wiki/%D4%B1%D6%80%D5%B0%D5%A5%D5%BD%D5%BF%D5%A1%D5%AF%D5%A1%D5%B6_%D5%B6%D5%A5%D5%B5%D6%80%D5%B8%D5%B6%D5%A1%D5%B5%D5%AB%D5%B6_%D6%81%D5%A1%D5%B6%D6%81" title="쐈썟봣쫫뫗뫗 웻봣왔쟷웻뫗왓 뫗웼  Armenian" lang="hy" hreflang="hy">뫗왓봤봣</a></li>
			<li class="interlanguage-link interwiki-hi"><a href="//hi.wikipedia.org/wiki/%E0%A4%95%E0%A5%83%E0%A4%A4%E0%A5%8D%E0%A4%B0%E0%A4%BF%E0%A4%AE_%E0%A4%A4%E0%A4%82%E0%A4%A4%E0%A5%8D%E0%A4%B0%E0%A4%BF%E0%A4%95%E0%A4%BE_%E0%A4%A8%E0%A5%87%E0%A4%9F%E0%A4%B5%E0%A4%B0%E0%A5%8D%E0%A4%95" title="胛鉀胛鉀胛胛胛 胛胛胛鉀胛胛胛胛 胛鉀胛胛胛鉀胛  Hindi" lang="hi" hreflang="hi">胛胛胛鉀胛鉀</a></li>
			<li class="interlanguage-link interwiki-hr"><a href="//hr.wikipedia.org/wiki/Umjetna_neuronska_mre%C5%BEa" title="Umjetna neuronska mre쬬  Croatian" lang="hr" hreflang="hr">Hrvatski</a></li>
			<li class="interlanguage-link interwiki-id"><a href="//id.wikipedia.org/wiki/Jaringan_saraf_tiruan" title="Jaringan saraf tiruan  Indonesian" lang="id" hreflang="id">Bahasa Indonesia</a></li>
			<li class="interlanguage-link interwiki-is"><a href="//is.wikipedia.org/wiki/Gervitauganet" title="Gervitauganet  Icelandic" lang="is" hreflang="is">칈slenska</a></li>
			<li class="interlanguage-link interwiki-he"><a href="//he.wikipedia.org/wiki/%D7%A8%D7%A9%D7%AA_%D7%A2%D7%A6%D7%91%D7%99%D7%AA_%D7%9E%D7%9C%D7%90%D7%9B%D7%95%D7%AA%D7%99%D7%AA" title="뻉뽱 뮁뷍놩뙁 뤵럥넁돵쀙뙁  Hebrew" lang="he" hreflang="he">뮁놩뻉뙁</a></li>
			<li class="interlanguage-link interwiki-ka"><a href="//ka.wikipedia.org/wiki/%E1%83%AE%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%9C%E1%83%A3%E1%83%A0%E1%83%98_%E1%83%9C%E1%83%94%E1%83%98%E1%83%A0%E1%83%9D%E1%83%9C%E1%83%A3%E1%83%9A%E1%83%98_%E1%83%A5%E1%83%A1%E1%83%94%E1%83%9A%E1%83%98" title="쉬더뛰뢔돼러머 러더떠머뢔러뛰 봬뫠더뛰  Georgian" lang="ka" hreflang="ka">봬너머뛰</a></li>
			<li class="interlanguage-link interwiki-la"><a href="//la.wikipedia.org/wiki/Artificiale_neuronum_rete" title="Artificiale neuronum rete  Latin" lang="la" hreflang="la">Latina</a></li>
			<li class="interlanguage-link interwiki-lt"><a href="//lt.wikipedia.org/wiki/Dirbtinis_neuroninis_tinklas" title="Dirbtinis neuroninis tinklas  Lithuanian" lang="lt" hreflang="lt">Lietuvi콥</a></li>
			<li class="interlanguage-link interwiki-mk"><a href="//mk.wikipedia.org/wiki/%D0%92%D0%B5%D1%88%D1%82%D0%B0%D1%87%D0%BA%D0%B0_%D0%BD%D0%B5%D0%B2%D1%80%D0%BE%D0%BD%D1%81%D0%BA%D0%B0_%D0%BC%D1%80%D0%B5%D0%B6%D0%B0" title="뉋왐걤햟혢햨햟 햫햣쒬쮏쫨햨햟 햪햣햤햟  Macedonian" lang="mk" hreflang="mk">햎햟햨햣햢쮏쫨햨햦</a></li>
			<li class="interlanguage-link interwiki-mg"><a href="//mg.wikipedia.org/wiki/Tambajotra_ner%C3%B4nina" title="Tambajotra ner칪nina  Malagasy" lang="mg" hreflang="mg">Malagasy</a></li>
			<li class="interlanguage-link interwiki-ml"><a href="//ml.wikipedia.org/wiki/%E0%B4%95%E0%B5%83%E0%B4%A4%E0%B5%8D%E0%B4%B0%E0%B4%BF%E0%B4%AE_%E0%B4%A8%E0%B4%BE%E0%B4%A1%E0%B5%80%E0%B4%B5%E0%B5%8D%E0%B4%AF%E0%B5%82%E0%B4%B9%E0%B4%82" title="絳綱絳綱絳絳絳 絳絳絳綱絳綱絳綱絳絳  Malayalam" lang="ml" hreflang="ml">絳絳絳絳絳絳</a></li>
			<li class="interlanguage-link interwiki-ja"><a href="//ja.wikipedia.org/wiki/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF" title="봮쩎뽺꽦걲쩎  Japanese" lang="ja" hreflang="ja">了봱랿妨</a></li>
			<li class="interlanguage-link interwiki-pt"><a href="//pt.wikipedia.org/wiki/Rede_neural" title="Rede neural  Portuguese" lang="pt" hreflang="pt">Portugu칡s</a></li>
			<li class="interlanguage-link interwiki-ro"><a href="//ro.wikipedia.org/wiki/Re%C8%9Bea_neural%C4%83_artificial%C4%83" title="Re탵ea neural캒 artificial캒  Romanian" lang="ro" hreflang="ro">Rom칙n캒</a></li>
			<li class="interlanguage-link interwiki-ru"><a href="//ru.wikipedia.org/wiki/%D0%98%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C" title="햊혜햨혞혜혜혝쒫왏쫧쫧썜 햫햣햧쮏쫧쫧썜 혜햣혝혧  Russian" lang="ru" hreflang="ru">먬혜혜햨햦햧</a></li>
			<li class="interlanguage-link interwiki-ta"><a href="//ta.wikipedia.org/wiki/%E0%AE%9A%E0%AF%86%E0%AE%AF%E0%AE%B1%E0%AF%8D%E0%AE%95%E0%AF%88_%E0%AE%A8%E0%AE%B0%E0%AE%AE%E0%AF%8D%E0%AE%AA%E0%AE%A3%E0%AF%81%E0%AE%AA%E0%AF%8D_%E0%AE%AA%E0%AE%BF%E0%AE%A3%E0%AF%88%E0%AE%AF%E0%AE%AE%E0%AF%8D" title="彊慷彊彊慷彊慷 彊彊彊慷彊彊慷彊慷 彊彊彊慷彊彊慷  Tamil" lang="ta" hreflang="ta">彊彊彊彊慷</a></li>
			<li class="interlanguage-link interwiki-th"><a href="//th.wikipedia.org/wiki/%E0%B9%82%E0%B8%84%E0%B8%A3%E0%B8%87%E0%B8%82%E0%B9%88%E0%B8%B2%E0%B8%A2%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B8%AA%E0%B8%B2%E0%B8%97%E0%B9%80%E0%B8%97%E0%B8%B5%E0%B8%A2%E0%B8%A1" title="薑舡舡舡舡薑舡舡舡舡舡舡舡舡薑舡舡舡舡  Thai" lang="th" hreflang="th">薑舡舡</a></li>
			<li class="interlanguage-link interwiki-uk"><a href="//uk.wikipedia.org/wiki/%D0%A8%D1%82%D1%83%D1%87%D0%BD%D0%B0_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6%D0%B0" title="햗혝혞혢햫햟 햫햣햧쮏쫧쫧 햪햣햣햤햟  Ukrainian" lang="uk" hreflang="uk">햒햨햟혱햫혜혧햨햟</a></li>
			<li class="interlanguage-link interwiki-ur"><a href="//ur.wikipedia.org/wiki/%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%DB%8C_%D8%B9%D8%B5%D8%A8%DB%8C_%D8%AC%D8%A7%D9%84%DA%A9%D8%A7%D8%B1" title="왖좥 좢왖뻍 섖뽲  Urdu" lang="ur" hreflang="ur">쐊</a></li>
			<li class="interlanguage-link interwiki-vi"><a href="//vi.wikipedia.org/wiki/M%E1%BA%A1ng_n%C6%A1-ron_nh%C3%A2n_t%E1%BA%A1o" title="M故멽g n쿼-ron nh칙n t故멾  Vietnamese" lang="vi" hreflang="vi">Ti故쯡g Vi敲t</a></li>
			<li class="interlanguage-link interwiki-zh"><a href="//zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" title="啖쥔인明륂즲母놶즾  Chinese" lang="zh" hreflang="zh">疸쇉둖</a></li>
			<li class="uls-p-lang-dummy"><a href="#"></a></li>
			<li class="wbc-editpage"><a href="//www.wikidata.org/wiki/Q192776#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></li>
		</ul>
	</div>
</div>
			</div>
		</div>
		<div id="footer" role="contentinfo">
							<ul id="footer-info">
											<li id="footer-info-lastmod"> This page was last modified on 14 February 2014 at 22:25.<br /></li>
											<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy.</a> <br/>
Wikipedia춽 is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
									</ul>
							<ul id="footer-places">
											<li id="footer-places-privacy"><a href="//wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
											<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
											<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
											<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
											<li id="footer-places-developers"><a class="external" href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
											<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/wiki/Artificial_neural_network" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
									</ul>
										<ul id="footer-icons" class="noprint">
					<li id="footer-copyrightico">
						<a href="//wikimediafoundation.org/"><img src="//bits.wikimedia.org/images/wikimedia-button.png" width="88" height="31" alt="Wikimedia Foundation"/></a>
					</li>
					<li id="footer-poweredbyico">
						<a href="//www.mediawiki.org/"><img src="//bits.wikimedia.org/static-1.23wmf14/skins/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" width="88" height="31" /></a>
					</li>
				</ul>
						<div style="clear:both"></div>
		</div>
		<script>/*<![CDATA[*/window.jQuery && jQuery.ready();/*]]>*/</script><script>if(window.mw){
mw.loader.state({"site":"loading","user":"ready","user.groups":"ready"});
}</script>
<script>if(window.mw){
mw.loader.load(["ext.cite","mobile.desktop","mediawiki.action.view.postEdit","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.DRN-wizard","ext.gadget.charinsert","mw.MwEmbedSupport.style","ext.articleFeedbackv5.startup","ext.wikimediaShopLink.core","ext.navigationTiming","schema.UniversalLanguageSelector","ext.uls.eventlogger","ext.uls.interlanguage","skins.vector.collapsibleNav"],null,true);
}</script>
<script src="//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&amp;lang=en&amp;modules=site&amp;only=scripts&amp;skin=vector&amp;*"></script>
<!-- Served by mw1081 in 0.290 secs. -->
	</body>
</html>
++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://www.brain.riken.jp/labs/mns/geczy/Links-E.html>====================
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Frameset//EN">

<HTML>

<HEAD>
<TITLE>Peter GECZY's Neural Network Links</TITLE>
<meta http-equiv="pics-label" content='(pics-1.1 "http://www.icra.org/ratingsv02.html" l gen false for "http://www.mns.brain.riken.go.jp/~geczy/Links-E.html" r (cz 1 lz 1 nz 1 oz 1 vz 1) "http://www.rsac.org/ratingsv01.html" l gen false for "http://www.mns.brain.riken.go.jp/~geczy/Links-E.html" r (n 0 s 0 v 0 l 0))'>
<meta NAME="Author"        CONTENT="Peter GECZY">
<meta NAME="Keywords"      CONTENT="Software, Jourlans, Books, Databases, Lists, Archives, Societies, Newsgroups, Centers, Resources, Research, Links, Projects, Brain Science Institute, BSI, Neurosciences Research Program, NRP, RIKEN, Theoretical Neurobiology, Experimental Neurobiology, Neurobiology, Neurology, brain, human brain, Research Institute, RI, Scientific Lectures, Seminars, Neural Networks, NN, scientists, researcher, Peter Geczy, Computer Science, CS, Optimization, Statistics, Artificial Intelligence, AI, Model, neuron">
<meta NAME="Distribution"  CONTENT="global">
<meta NAME="Description"   CONTENT="Neural Network Resources page contains extensive information and numerous links to Software, Journals, Books, Societies, Databases, Newsgroups, Archives, E-Lists, etc. Resources are well organized, thoroughly catalogued and presented in easy-to-access manner with user friendly graphical interface."></head>
<meta NAME="Page-Type"     CONTENT="Scientific">
<meta NAME="Audience"      CONTENT="All">
<meta NAME="Language"      CONTENT="EN">
<meta NAME="Robots"        CONTENT="all">
<meta NAME="Revisit-After" CONTENT="30 days">
</HEAD>

<frameset cols="130,*" frameborder="no" framespacing="0">
  <frame name="NNLinkPanel" src="NNLinks/NNL_Panel.html" scrolling="No" frameborder="No" noresize>
  <frame name="NNLinkPage" src="NNLinks/NN_Centers.html" scrolling="Auto" frameborder="No" noresize>
</frameset>

<BODY BGCOLOR="white" TEXT="black" LINK="blue" VLINK="violet" BACKGROUND=""></BODY>

</HTML>
++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://rumelhartprize.org/>====================
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head profile="http://gmpg.org/xfn/11">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>The David E. Rumelhart Prize  For Contributions to the Theoretical Foundations of Human Cognition</title>
<meta name="robots" content="noodp, noydir" />
<meta name="description" content="For Contributions to the Theoretical Foundations of Human Cognition" />
<link rel="stylesheet" href="http://rumelhartprize.org/wp-content/themes/thesis_181/style.css" type="text/css" media="screen, projection" />
<link rel="stylesheet" href="http://rumelhartprize.org/wp-content/themes/thesis_181/custom/layout.css" type="text/css" media="screen, projection" />
<!--[if lte IE 8]><link rel="stylesheet" href="http://rumelhartprize.org/wp-content/themes/thesis_181/lib/css/ie.css" type="text/css" media="screen, projection" /><![endif]-->
<link rel="stylesheet" href="http://rumelhartprize.org/wp-content/themes/thesis_181/custom/custom.css" type="text/css" media="screen, projection" />
<link rel="canonical" href="http://rumelhartprize.org/" />
<link rel="alternate" type="application/rss+xml" title="The David E. Rumelhart Prize RSS Feed" href="http://rumelhartprize.org/?feed=rss2" />
<link rel="pingback" href="http://rumelhartprize.org/xmlrpc.php" />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://rumelhartprize.org/xmlrpc.php?rsd" />
<link rel="alternate" type="application/rss+xml" title="The David E. Rumelhart Prize &raquo; Home Comments Feed" href="http://rumelhartprize.org/?feed=rss2&amp;page_id=5" />
<link rel='stylesheet' id='jquery-plugins-slider-style-css'  href='http://rumelhartprize.org/wp-content/plugins/jj-nextgen-jquery-slider/stylesheets/nivo-slider.css?ver=3.1.2' type='text/css' media='all' />
<link rel='stylesheet' id='NextGEN-css'  href='http://rumelhartprize.org/wp-content/plugins/nextgen-gallery/css/ngg_shadow2.css?ver=1.0.0' type='text/css' media='screen' />
<link rel='stylesheet' id='shutter-css'  href='http://rumelhartprize.org/wp-content/plugins/nextgen-gallery/shutter/shutter-reloaded.css?ver=1.3.2' type='text/css' media='screen' />
<script type='text/javascript' src='http://rumelhartprize.org/wp-includes/js/l10n.js?ver=20101110'></script>
<script type='text/javascript' src='http://rumelhartprize.org/wp-includes/js/jquery/jquery.js?ver=1.4.4'></script>
<script type='text/javascript' src='http://rumelhartprize.org/wp-content/plugins/jj-nextgen-jquery-slider/script/jquery.nivo.slider.pack.js?ver=2.4'></script>
<script type='text/javascript' src='http://rumelhartprize.org/wp-content/plugins/jj-nextgen-jquery-slider/script/jquery.jj_ngg_shuffle.js?ver=3.1.2'></script>
<script type='text/javascript' src='http://rumelhartprize.org/wp-content/plugins/jj-nextgen-jquery-slider/script/jjnggutils.js?ver=3.1.2'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var shutterSettings = {
	msgLoading: "L O A D I N G",
	msgClose: "Click to Close",
	imageCount: "1"
};
/* ]]> */
</script>
<script type='text/javascript' src='http://rumelhartprize.org/wp-content/plugins/nextgen-gallery/shutter/shutter-reloaded.js?ver=1.3.2'></script>
<script type='text/javascript' src='http://rumelhartprize.org/wp-content/plugins/nextgen-gallery/js/jquery.cycle.all.min.js?ver=2.88'></script>
<script type='text/javascript' src='http://rumelhartprize.org/wp-content/plugins/nextgen-gallery/js/ngg.slideshow.min.js?ver=1.05'></script>

<meta name='NextGEN' content='1.8.0' />
</head>
<body class="custom home">
<div id="container">
<div id="page">
	<div id="header">
		<p id="logo"><a href="http://rumelhartprize.org">The David E. Rumelhart Prize</a></p>
		<h1 id="tagline">For Contributions to the Theoretical Foundations of Human Cognition</h1>
	</div>
<ul class="menu">
<li class="tab tab-1 current"><a href="http://rumelhartprize.org/" title="Home">Home</a></li>
<li class="tab tab-2"><a href="http://rumelhartprize.org/?page_id=10" title="About Rumelhart">About Rumelhart</a></li>
<li class="tab tab-3"><a href="http://rumelhartprize.org/?page_id=12" title="Recipients">Recipients</a></li>
<li class="tab tab-4"><a href="http://rumelhartprize.org/?page_id=18" title="Prize Selection">Prize Selection</a></li>
<li class="tab tab-5"><a href="http://rumelhartprize.org/?page_id=260" title="The Medal">The Medal</a></li>
<li class="tab tab-6"><a href="http://rumelhartprize.org/?page_id=28" title="Symposia">Symposia</a></li>
</ul>
	<div id="content_box">
		<div id="content">

			<div class="post_box top" id="post-5">
				<div class="headline_area">
					<h2>Home</h2>
				</div>
				<div class="format_text">
<p>The David E. Rumelhart Prize is awarded annually to an individual or collaborative team making a significant contemporary contribution to the theoretical foundations of human cognition. Contributions may be formal in nature: mathematical modeling of human cognitive processes, formal analysis of language and other products of human cognitive activity, and computational analyses of human cognition using symbolic or non-symbolic frameworks all fall within the scope of the award.</p>
<p>The David E. Rumelhart Prize is funded by the Robert J. Glushko and Pamela Samuelson Foundation. Robert J. Glushko received a Ph.D. in Cognitive Psychology from the University of California, San Diego in 1979 under Rumelhart&#8217;s supervision. He is an Adjunct Full Professor at the School of Information (I-School) at the University of California, Berkeley.</p>
<p>The prize consists of a hand-crafted, custom <a href="index.php?page_id=260">bronze medal</a>, a certificate, a citation of the awardee&#8217;s contribution, and a monetary award of $100,000.</p>
<h3 style="text-align: center;"><strong>The 2014 David E. Rumelhart Prize Recipient</strong></h3>
<p>&nbsp;</p>
<p style="text-align: left;"><a href="http://rumelhartprize.org/wp-content/uploads/2013/07/RayJackendoff160.jpg"><img class="alignright" title="Ray Jackendoff" src="http://rumelhartprize.org/wp-content/uploads/2013/07/RayJackendoff160.jpg" alt="" width="160" height="240" /></a>The recipient of the fourteenth David E. Rumelhart Prize is <a title="Ray Jackendoff" href="http://rumelhartprize.org/?page_id=466">Ray Jackendoff</a>, one of the world뗩 leading figures in the cognitive science of language. He has developed a theory of language that articulates the contribution of each level of linguistic representation and their interaction, while also elucidating how language relates to other cognitive systems. Dr. Jackendoff is Seth Merrin Professor of Philosophy and Co-Director of the Center for Cognitive Studies at Tufts University. After receiving his BA in Mathematics from Swarthmore College in 1965, he completed his PhD in Linguistics from MIT in 1969, under the supervision of Noam Chomsky. He then joined the faculty at Brandeis University, where he remained until 2005, until taking up his current position at Tufts. Jackendoff is a Fellow of the American Academy of Arts and Sciences, of the American Association for the Advancement of Science, of the Linguistic Society of America, and of the Cognitive Science Society.<strong> </strong></p>
<p style="text-align: left;">&nbsp;</p>
<p>&nbsp;</p>
				</div>
			</div>
		</div>

		<div id="sidebars">
			<div id="sidebar_1" class="sidebar">
				<ul class="sidebar_list">
<li class="widget widget_text" id="text-4"><h3>David E. Rumelhart</h3>			<div class="textwidget"><img src="wp-content/uploads/2011/07/davidrumelhart1.jpg" /></div>
		</li><li class="widget jj-nexgen-jquery_slider" id="jj-nexgen-jquery_slider-3">
<ul class="ul_jj_slider">
    <li class="li_jj_slider">
<h3>Prize Recipients</h3>
<style type="text/css">
  div#slider { width: 160px !important;height: 240px !important; }
  div#slider_container .nivo_slider .nivo-controlNav { width: 160px !important; }
</style>
<div id="slider_container" class="nivo_slider_container">
  <div id="slider" class="nivo_slider">
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/lindasmith160.jpg" title="Linda Smith, 2013"  border="0" class="nivo_image" />
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/1peterdayan.jpg" title="Peter Dayan, 2012"  border="0" class="nivo_image" />
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/2pearl.gif" title="Judea Pearl, 2011"  border="0" class="nivo_image" />
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/3jlm.gif" title="James McLelland, 2010"  border="0" class="nivo_image" />
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/carey2.gif" title="Susan Carey, 2009"  border="0" class="nivo_image" />
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/shimon.gif" title="Shimon Ullman, 2008"  border="0" class="nivo_image" />
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/jeffelman.gif" title="Jeff Elman, 2007"  border="0" class="nivo_image" />
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/7rogershepard.jpg" title="Roger Shepard, 2006"  border="0" class="nivo_image" />
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/8paulsmolensky.jpg" title="Paul Smolensky, 2005"  border="0" class="nivo_image" />
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/9johnanderson.jpg" title="John Anderson, 2004"  border="0" class="nivo_image" />
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/10aravindjoshi.jpg" title="Aravind Joshi, 2003"  border="0" class="nivo_image" />
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/11richardmshiffrin.jpg" title="Richard Shiffrin, 2002"  border="0" class="nivo_image" />
    <img src="http://rumelhartprize.org/wp-content/gallery/prizewinners_1/12geoffreyehinton.jpg" title="Geoffrey Hinton, 2001"  border="0" class="nivo_image" />
  </div>
</div>
<script type="text/javascript">
  jQuery(window).load(function() {
    jQuery('div#slider').nivoSlider({effect: 'fade',controlNav: false,pauseOnHover: true});
  });
</script>

    </li>
  </ul>
</li>				</ul>
			</div>
		</div>
	</div>
	<div id="footer">
   <p><a href="http://creativecommons.org/licenses/by-nc/3.0/"><img src="http://groups.ischool.berkeley.edu/rumelhartprize/Rumelhart/wp-content/uploads/2012/07/cclicense.png" alt="creative commons license"/></a><br><font size = 1>This work is licensed under a <a href="http://creativecommons.org/licenses/by-nc/3.0/">Creative Commons<br> Attribution-NonCommercial 3.0 Unported License</a></font><br/></p>
	</div>
</div>
</div>
<!--[if lte IE 8]>
<div id="ie_clear"></div>
<![endif]-->
</body>
</html>++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://faculty.washington.edu/~wcalvin/bk9>====================
<META HTTP-EQUIV=REFRESH CONTENT="0; URL=http://www.williamcalvin.com/bk9/index.htm">

++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://www.inference.phy.cam.ac.uk/mackay/itprnn/book.html>====================
 <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><HEAD><META NAME="GENERATOR" CONTENT="Generated by website.p, free software from TradeNotAid.com - dedicated to making the web useful for poor countries">
<link rel="shortcut icon" href="/favicon.ico">
<Title>
David MacKay: Information Theory, Pattern Recognition and Neural Networks: The Book 
</Title>
<STYLE TYPE="TEXT/CSS"><!--
H4,P,TD,TH,UL,DD,DT,DL,OL { font-family:  'Lucida Grande', Verdana, Geneva, Lucida, Arial, Helvetica, sans-serif}
P.indented {margin-left:0.5cm; margin-right:0.5cm}
BODY,DIV { font-family:  'Lucida Grande', Verdana, Geneva, Lucida, Arial, Helvetica, sans-serif;}
TT,PRE { font-family: Courier, monospace;}
SUP,SUB,SMALL { font-size: 90%}
.smaller { font-size: 80%}
SPAN.header { font-family: times, roman; font-size: 200%}
H1 { font-family:  'Lucida Grande', Verdana, Geneva, Lucida, arial, helvetica, sans serif; font-size: 180%}
H2 { font-family:  'Lucida Grande', Verdana, Geneva, Lucida, arial, helvetica, sans serif; font-size: 135%}
H3 { font-family:  'Lucida Grande', Verdana, Geneva, Lucida, arial, helvetica, sans serif; font-size: 120%}
abbr, acronym, .help {
  border-bottom: 1px dotted #333;
  cursor: help;
}
.talklist_date{ background-color: #dddddd; font-weight: bold; }
.talklist_header{ font-weight: bold; }
.talklist_entry{ }
.talklist_block{ clear: right; }
.talklist_block_small{ font-size: 0.8em; }
.talklist_date_small{ background-color: #dddddd; font-weight: bold; }
.talklist_header_small{ font-weight: bold; }
--></STYLE></HEAD>
<BODY BGCOLOR="#ffffee"  TEXT="#000000" LINK="#0000C8" ALINK="#C80000" VLINK="#C80000">
<!-- This is the master table that everything sits inside -->
<TABLE BORDER=0 CELLSPACING=0 CELLPADDING=0>
<TR valign=top ><!--This row is the parent organization head bar -->
<td colspan="2"  bgcolor=#beffd4> <center> 
<TABLE BORDER=0 CELLSPACING=0 CELLPADDING=4><tr><td  >
<center> <b><A HREF="http://www.inference.phy.cam.ac.uk/mackay/">David MacKay</a></b> </center> 
</td>
</tr></table>
</center> </td>
</tr> 
<tr bgcolor=#006666 ><td colspan=3 ><img src="/images/pixel.gif" width="1" height="4" alt="."></td></tr>
<TR valign=top><!--This row contains the first bit of the navigation column (left) and the body of the page (right) -->
<!--begin sidebar-->
<TD height="400" VALIGN=TOP WIDTH="150" BGCOLOR="#ffff67" >
<TABLE BORDER=0 CELLSPACING=0 CELLPADDING=4><tr>
<td VALIGN=TOP WIDTH="150" >
<font face="Helvetica">
<br><div> <center><a HRef="./" ><font size=3><b>Information Theory, Pattern Recognition and Neural Networks</b></font></a></center></div>
<br><div> <center><b><img src="/logo.gif" width=121 height=125></b></center></div>
<br><div> <table width="100%" cellpadding=0 cellspacing=0 border=0>
<tr valign=top><td colspan=2 width=-2 nowrap><b><a hRef=prereq.html >Prerequisites</a></b>&nbsp;</td></tr>
<tr valign=top><td colspan=2 width=-2 nowrap><b><a hRef=SummaryTop.html >Summary</a></b>&nbsp;</td></tr>
<tr valign=top><td colspan=2 width=-2 nowrap><b><a hRef=Videos.shtml >Videos</a></b>&nbsp;</td></tr>
<tr valign=top><td colspan=2 width=-2 nowrap><b><a hRef=Slides.shtml >Slides 2012</a></b>&nbsp;</td></tr>
<tr valign=top><td colspan=2 width=-2 nowrap><b><a hRef=Supervisions.html >Supervisions</a></b>&nbsp;</td></tr>
<tr valign=top><td colspan=2 width=-2 nowrap bgcolor="#ffffee" ><b><em style="color:black"><a hRef=book.html >The Book</a></em>&nbsp;<b>&#171;</b></b>&nbsp;</td></tr>
<tr valign=top><td width=7 >&#183;</td><td width=-9 nowrap><a hRef=p11.html >Errors</a>&nbsp;</td></tr>
<tr valign=top><td colspan=2 width=-2 nowrap><b><a hRef=software.html >Software</a></b>&nbsp;</td></tr>
</table>
</div>
<br><div> </div>
<br><div> <b><a href="http://www.inference.phy.cam.ac.uk/is/faq.html">Any questions?</a></b></div>
<br><div> <FORM METHOD=GET ACTION="http://web-search.cam.ac.uk/query.html">
Search :
<INPUT NAME="qp" TYPE="hidden" VALUE="site:www.inference.phy.cam.ac.uk">
<INPUT NAME="qt" TYPE="text" SIZE=10 VALUE="">
</FORM>
</div>
</font>
</td></tr></table>
</td>
<!--end sidebar-->
<td  bgcolor=#006666 width=1 rowspan=1><img src="/images/pixel.gif" width="1" height="1" alt="."></td>
<TD rowspan=1 VALIGN=TOP WIDTH="740" BGCOLOR="#ffffee" >
<TABLE BORDER=0 CELLSPACING=0 CELLPADDING=6><tr><td VALIGN=TOP WIDTH="740" >
<font face="Helvetica">
<a name="notrealbook"><h3>Information Theory, Inference, and Learning Algorithms</h3></a>
  (Hardback, 640 pages, Published September 2003)
<hr noshade size=1>
<h3>Order your copy</h3>
<p><b>Price: &pound;35.00 / $60.00</b> from |<a href=http://uk.cambridge.org/catalogue/catalogue.asp?isbn=0521642981>CUP <b>UK</b></a>/<a href=http://www.cup.org/titles/catalogue.asp?isbn=0521642981><b>USA</b></a>| |<A HREF="http://www.amazon.co.uk/exec/obidos/ASIN/0521642981/davidmackay0f-21">amazon.co.uk</A>/<A HREF="http://www.amazon.com/exec/obidos/redirect?tag=davidmackayca-20&path=tg/detail/-/0521642981/qid%3D1057850920/sr%3D1-4">.com</A>/<A HREF="http://www.amazon.ca/exec/obidos/ASIN/0521642981/davidmackay-20">.ca</A>/<a href=http://www.amazon.co.jp/exec/obidos/ASIN/0521642981/qid%3D/250-6186089-9581851>.co.jp</a>| | <a href="http://search.barnesandnoble.com/Information-Theory-Inference-and-Learning-Algorithms/MacKay-David-J-C-MacKay-David-J-C/e/9780521642989/?itm=1&afsrc=1&lkid=J28491940&pubid=K122413&byo=1">Barnes &amp; Noble <b>USA</b></A>  | <a href=http://www.booksprice.com/compare.do?inputData=0521642981&searchType=isbn>booksprice</a><a href=http://www.booksprice.com/compare.do?searchType=compare&inputData=0521642981>.</a> | <a href=http://www.fetchbook.info/compare.do?search=0521642981>fetchbook.info</a> | <a href=http://www.allbookstores.com/book/compare/0521642981>allbookstores</a> | <a href=http://www.biggerbooks.com/bk_detail.asp?isbn=0521642981>biggerbooks</a> | <a href=http://www.allbookstores.com/book/buy/Blackwells/0521642981>blackwells</a> | <a href=http://www.directtextbook.com/prices.php?q=0521642981&qtitle=&qauthor=&qkeywords=&qisbn=0521642981&redirected=yes>directtextbook</a> | <a href=http://www.kalahari.net/BK/product.asp?toolbar=&sku=24741221&format=detail>kalahari.net (South Africa)</a> | <b><a href=http://www.inference.phy.cam.ac.uk/mackay/itila/SAsiaEd.html>Special Paperback edition for South Asia</a>.| </p>
<hr noshade size=1>
<h3>Download the book too</h3>
<p>
You can browse <b>and search</b> the book on <a
href=http://books.google.com/books?ie=UTF-8&hl=en&id=AKuMj4PN_EMC&dq=0521642981&pg=PP1&printsec=0&lpg=PP1&sig=4uCBC0N7cFAaTQ-GMB-NpFR7Ess>Google
books</a>.
</p>
<p>You may download 
<a name="realbook"><b>The book in one file</b></a>
  (640 pages):
  </p>
<table bgcolor="#ddffee" cellpadding=4 cellspacing=4>
     <tr><td width=20%></td>
     <td colspan=2 bgcolor="#aaeedd"><b>U.K.</b> <img border=0 src="http://www.inference.phy.cam.ac.uk/images/flags/great-britain.gif" alt="english" width="32" height="22"></td>
     <td colspan=2 bgcolor="#aaeedd"><b>Canada</b> <img src="http://www.inference.phy.cam.ac.uk/images/flags/canada.gif"  width="40" height="20" alt="canada" border=0></td>
<td colspan=2 bgcolor="#aaeedd"><b>South Africa</b> <img src="http://www.inference.phy.cam.ac.uk/images/flags/southafrican_small.gif"  width="40" height="20" alt="south africa" border=0></td>
     </tr>
     <tr><td bgcolor="#aaeedd">
<b>PDF (A4)</b>
 </td>
<td colspan=2>
 <a href=http://www.inference.phy.cam.ac.uk/itprnn/book.pdf>pdf</a> (9M)  (fourth printing, March 2005)
 </td>
 <td colspan=2>
  <a href="http://www.cs.toronto.edu/~mackay/itprnn/book.pdf">pdf</a>
</td>
 <td colspan=2>
  <a href="http://www.aims.ac.za/~mackay/itprnn/book.pdf">pdf</a>
</td>
</tr>
     <tr><td bgcolor="#aaeedd">
<b>Postscript (A4)</b>
 </td>
 <td colspan=2>
 <a href=http://www.inference.phy.cam.ac.uk/itprnn/book.ps.gz>postscript</a>  (fourth printing, March 2005) (5M)
</td>
 <td colspan=2>
  <a href="http://www.cs.toronto.edu/~mackay/itprnn/book.ps.gz">postscript</a>
</td>
 <td colspan=2>
  <a href="http://www.aims.ac.za/~mackay/itprnn/book.ps.gz">postscript</a>
</td>
</tr>
     <tr><td></td><td colspan=4>
</td>
</tr>
     <tr><td colspan=1 bgcolor="#aaeedd"><b>DJVU</b></td>
 <td colspan=2> <a href=http://www.inference.phy.cam.ac.uk/mackay/book.djvu>djvu file</a> (6M) (2nd printing)
 </td>
     <td colspan=2> <a href="http://www.cs.toronto.edu/~mackay/book.djvu">djvu file</a></td>
     <td colspan=2> <a href="http://www.aims.ac.za/~mackay/book.djvu">djvu file</a></td>
     </tr>
     <tr><td></td>
<td colspan=4 align=center>(<a href=http://www.djvuzone.org/>djvu information</a> | <a href=http://djvu.sourceforge.net/>Download djView)</a></td>
</tr>

     <tr><td colspan=1 bgcolor="#aaeedd"><b>Just the words</b></td><td colspan=2>
  <a href=http://www.inference.phy.cam.ac.uk/mackay/itprnn/booktext.txt>(latex)</a> [provided for convenient searching] (2.4M)
  </td>
  <td colspan=2>
 (<a href="http://www.cs.toronto.edu/~mackay/itprnn/booktext.txt">latex</a>)
</td>
  <td colspan=2>
 (<a href="http://www.aims.ac.za/~mackay/itprnn/booktext.txt">latex</a>)
</td></tr>

     <tr><td valign=top rowspan=2 colspan=1 bgcolor="#aaeedd"><B style="color:black;background-color:#ffff66">Just the figures</b> <B style="color:black;background-color:#ffff66">NEW</b></td><td colspan=1>
      <b>All in one file</b>
  <a href=http://www.inference.phy.cam.ac.uk/mackay/itprnn/figures.ps.gz>(postscript)</a> [provided for use of teachers] (2M)<br>
  </td>
  <td>
  <a href=http://www.inference.phy.cam.ac.uk/mackay/itprnn/figures.pdf>(pdf)</a>  (5M) </td>
  <td colspan=1>
 (<a href="http://www.cs.toronto.edu/~mackay/itprnn/figures.ps.gz">ps.gz</a>)
</td>
  <td colspan=1>
 (<a href="http://www.cs.toronto.edu/~mackay/itprnn/figures.pdf">pdf</a>)
</td>
  <td colspan=2>

</td></tr>
<tr><td colspan=2>
  <b><a href=http://www.inference.phy.cam.ac.uk/mackay/itprnn/individual/>In individual eps files</a></b>
</td>
<td colspan=2></td>
<td colspan=2></td>
</tr>
     <tr bgcolor=""#ffffee" "><td colspan=1 bgcolor="#aaeedd"><b>Individual chapters
</b></td><td colspan=2>
     <a href=http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/>postscript and pdf available from this page</a>
</td><td colspan=2>
 <a href="http://www.cs.toronto.edu/~mackay/itprnn/ps/">mirror</a>

</td><td colspan=2>
 <a href="http://www.aims.ac.za/~mackay/itprnn/ps/">mirror</a>

</td>
</tr>


</table>
<p>
<a name="book"><a name="notes"></a></a>
<a name="printingnotes">Notes</a>:
<ul>
<li> Version 6.0 was released Thu 26/6/03; the book is  finished.
 You are welcome to view the book on-screen. Version 6.0 was used for the first printing,
  published by C.U.P. September 2003.
</li>
<li>
 Version 6.6 was released Mon 22/12/03; it will be used for the second printing,
  to be released January 2004.
<small>   In this second printing,
 a small number of typographical errors were corrected,
 and the design of the book was altered slightly.
 Page-numbering generally remains unchanged,
 except in chapters 1, 6, and 28,
 where
 a few paragraphs, figures, and equations have
 moved around.
 All equation, section, and exercise numbers are unchanged.
</small>
</li>
<li>Version 7.0 is the third printing (November 2004). 
 Its only differences from the 2nd printing are a number of corrections,
 and the renaming of the chapter `Correlated Random Variables' to `Dependent Random Variables'.
</li>
     </ul>
<dl>
<!-- see the file tmp-->
</dl>
</p>
<p>
 <b>Copyright issues:</b> The book is copyright (c) Cambridge University Press.
  It has been available in bookstores since September 2003. The cover price in 2003 was  30 pounds (UK)
   and $50 (USA); in 2006, 35 pounds and $60 (USA). 
 </p><p>
 Now the book is published, these files <b>will remain viewable</b> on
 this website.  The same copyright rules will apply to the online copy of the
 book as apply to normal books. [e.g., copying the whole book onto paper is
 not permitted.]
<p>
<b>History:</b><br>
Draft 1.1.1 - March 14 1997.<br>
Draft 1.2.1 - April  4 1997.<br>
Draft 1.2.3 - April  9 1997.<br>
Draft 1.2.4 - April 10 1997. Margins altered so as to print better
                                 on Northamerican paper<br>
Draft 1.3.0 - December 23 1997.<br>
Draft 1.9.0 - Feb 1 1999.<br>
Draft 2.0.0 - Jan 6 2000. New page layout.<br>
Draft 2.2.0 - Dec 23 2000. Fresh draft.<br>
Draft 3.1415 - Jan 12 2003. Nearly finished.<br>
Draft 4.0 - April 15 2003. Chapter sequence finalized.<br>
Draft 4.1 - April 18 2003. Adding new frontmatter (Preface etc)to book. Corrected printing of letter version.<br>
Version 6.0 - Thu 26 June 2003. Final version.<br>
Version 6.6 - Mon 22 December 2003. Second printing.<br>
<ul>
<li> Here is my method for converting to two-up under linux:<br>
     <pre>
pstops '4:0L&#64;.67(20cm,1cm)+1L@.67(20cm,15cm),3R@.67(1cm,15.25cm)\
+2R&#64;.67(1cm,29.25cm)' $*.ps $*.dps </pre>
</li></ul>


</td></tr><tr><td>
<hr size=2 noshade><div align=right><A HREF="http://www.inference.phy.cam.ac.uk/mackay/">David J.C. MacKay</a></div><div align=left>Site last modified Mon Oct 29 12:10:05 GMT 2012
</div>
   </font>
</td></tr></table>
</td>
</tr>
 </table>
</body></html>
++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://www.alyuda.com/>====================
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<script language="javascript">
<!--
	with (navigator) if (appName.indexOf('Microsoft')!=-1 && appVersion.indexOf('Mac')==-1) document.write(''+
	'<scr'+'ipt language="VBScript">\nOn error resume next\n'+
	'ssfli = False\n' +
	'For i = 10 To 1 Step -1\n' +
	'If NOT ssfli Then\n' +
		'ssfli = IsObject(CreateObject("ShockwaveFlash.ShockwaveFlash." & i))\n' +
		'If ssfli Then\n' +
			'ssflv = CStr(i)\n' +
		'End If\n'+
	'End If\n'+	
	'Next\n' +
	'</scr'+'ipt>');
//-->
</script><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>

<meta name="google-site-verification" content="LhY4upQg2b8FeL2ThrzbGutR7S5Cl0-73UKC-BuwQ_I" />

<title>Neural Network Software, Forecasting Software, Neural Networks System | Alyuda</title>
<meta name="description" content="Neural network software, neural network system for forecasting, stock market prediction, stock pattern recognition, trading, ANN program design and simulation solution.">
<meta name="keywords" content="neural network software, forecasting software, stock forecasting, neural net tool, pattern recognition, neural networks, Excel, trading, add-in, data mining, modeling, forecast, prediction, neural network system">
<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
<meta name="revisit-after" content="5days"> 
<META NAME="AUTHOR" CONTENT="Alyuda Research, LLC">
<META NAME="COPYRIGHT" CONTENT="Copyright (c) 2002-2005 by Alyuda Research, LLC">

<meta http-equiv="Content-Type" content="text/html; charset=windows-1251">
<script language="JavaScript" type="text/JavaScript" src="init.js"></script>
<script language="JavaScript" type="text/JavaScript" src="js/jquery.js"></script>
<script language="JavaScript" type="text/JavaScript" src="js/superfish.js"></script>

<link href="css/style.css" rel="stylesheet" type="text/css">
<link href="css/superfish.css" rel="stylesheet" type="text/css">

<LINK REL="SHORTCUT ICON" href="alyuda.ico">

</head>
<body>
<div id="wrapper">
 <!-- header -->
 <meta http-equiv="Content-Type" content="text/html; charset=windows-1251" />
<style type="text/css">
img {
	behavior:	url("pngbehavior.htc");
}
</style>
<script type="text/javascript">

		// initialise plugins
		jQuery(function(){
			jQuery('ul.sf-menu').superfish();
		});

</script>
<div id="header">
<div class="hlbg"><a href="http://www.alyuda.com/"><img src="image/header/logo_alyuda.png" width="173" height="61" border="0" /></a>
</div>

<div class="hmm">
     <ul class="sf-menu">
			<li style="width: 7.2em; height:38px">
				<span class="hablack">HOME</span>			</li>
			<li style="width: 17.5em; height:38px; background-image: url(image/arrow-menu.png); background-repeat: no-repeat; background-position: 178px 17px">
				<span class="point"><a href="neural-network-software.htm" class="ha" onclick="return false;">PRODUCTS &amp; SOLUTIONS</a></span>
				<ul>
					<li>
						<a href="" class="point">By Business Domain</a>
                        <ul>
							<li><a href="products/business_domains/for_active_traders.htm">Active Traders</a></li>
                            <li><a href="products/business_domains/for_security_brokers.htm">Brokers</a></li>
                            <!--<li><a href="products/business_domains/for_banks.htm">Banks</a></li>-->
                            <li><a href="products/business_domains/for_energy_generating_and_distribution_companies.htm">Energy</a></li>
                            <li><a href="products/business_domains/for_retail_companies.htm">Retail</a></li>
                            <li><a href="products/business_domains/for_healthcare_service_providers.htm">Healthcare</a></li>
                            <li><a href="products/business_domains/for_insurance_companies.htm">Insurance</a></li>
                            <li><a href="products/business_domains/for_scientists_and_rd_labs.htm">Science</a></li>
						</ul>
					</li>
					<li>
						<a href="" class="point">By Solution</a>
						<ul>
							<li><a href="neural-networks-software.htm">NeuroIntelligence</a></li>
                            <li><a href="forecasting-excel-software-with-neural-network.htm">Forecaster XL</a></li>
                            <li><a href="neural-networks-library.htm">NeuroFusion</a></li>
                            <li><a href="products/tradecision/trading_software.htm">Tradecision</a></li>
                            <!--<li><a href="http://www.plug-n-score.com/" target="_blank">Plug&amp;Score</a></li>-->
                            <li><a href="forecasting-software.htm">Forecaster</a></li>
                            <li><a href="neural-network-system.htm">NeuroDienst</a></li>
                            <li><a href="products/brokerbooster/brokerbooster-overview.htm">BrokerBOOSTER</a></li>
                            <li><a href="products/ibrokerage/overview.htm">iBrokerAge</a></li>
						</ul>
					</li>
					<li><a href="clients.htm">Customers</a></li>
                    <li><a href="testimonials.htm">Testimonials</a></li>
				</ul>

			</li>
			<li style="width: 9.5em; height:38px; background-image: url(image/arrow-menu.png); background-repeat: no-repeat; background-position: 79px 17px">
				<span class="point"><a href="services.htm" class="ha" onclick="return false;">SERVICES</a></span>
				<ul>
					<li><a href="services.htm">Overview</a></li>
                    <li><a href="services_it_services.htm">Outsourced Product Development</a></li>
                    <li><a href="services_expertise.htm">Expertise</a></li>
                    <li><a href="services_experience.htm">Experience</a></li>
                    <li><a href="services_business_models.htm">Business Models</a></li>
                    <li><a href="services_product_delivery.htm">Product Delivery</a></li>
				</ul>
			</li>
			<li style="width: 9.2em; height:38px"><a href="purchase.htm" class="ha">PURCHASE</a></li>
            <li style="width: 9em; height:38px; background-image: url(image/arrow-menu.png); background-repeat: no-repeat; background-position: 76px 17px">
				<span class="point"><a href="freestandardsupport.htm" class="ha" onclick="return false;">SUPPORT</a></span>
                <ul>
                    <li><a href="freestandardsupport.htm">Support Centre</a></li>
                    <li><a href="priorityannualsupport.htm">Priority Annual Support</a></li>
                    <li><a href="support.htm">Support Request</a></li>
                    <li><a href="request.htm">Request a Feature</a></li>
                </ul>
			</li>
            <li style="width: 8.7em; height:38px; background-image: url(image/arrow-menu.png); background-repeat: no-repeat; background-position: 78px 17px">
				<span class="point"><a href="companyinfo.htm" class="ha" onclick="return false;">COMPANY</a></span>
				<ul>
					<li><a href="companyinfo.htm">Company Info</a></li>
                    <li><a href="mission.htm">Mission</a></li>
                    <li><a href="consulting-services.htm">Consulting</a></li>
                    <li><a href="neuralnetworks.htm">Technology</a></li>
                    <li><a href="research.htm">Research</a></li>
                    <li><a href="contact.htm">Contact Us</a></li>
                </ul>
			</li>	
		</ul>
 </div>
</div>




 <!-- /header -->
  <div id="container">
   <!--content-->
   <div class="cnt">
    <div style="padding:20px 0 10px 0" align="center">
<img src="image/vis.jpg" />
    </div>
    <div style="padding:10px 0 20px 0">
     <table width="100%" border="0" cellpadding="0" cellspacing="0" style="margin-top:10px; margin-bottom:10px" class="text">
		    <tr>
			 <td rowspan="4" valign="top" width="100%">

<div style="border:solid 1px #FF9900">
<div style="font-size:14px; padding:5px 10px; background-color:#FF9900"><strong>Highlights</strong></div>
<div style="padding:10px">
<div style="padding:0 0 8px 0">
<strong><a href="forecasting-excel-software-with-neural-network.htm">Forecasting Excel add-in</a></strong>
</div>
New version of the Forecaster XL forecasting software for Office 2013 is immediately available for trial.
<div style="height:2px; margin:8px 0 8px 0" class="dtsh"></div>
<div style="padding:0 0 8px 0">
<strong><a href="http://www.plug-n-score.com/" target="_blank">Credit Scoring Software</a></strong>
</div>
Easy-to-use credit scoring system, specially designed 
for mid- and small-size banks and financial institutions.
   </div>
</div>

<div style="border:solid 1px #bfbfbf; margin-top:8px">
<div style="background-color:#2d2a4f"><h2 style="font-size:14px; padding:5px 10px; color:#ffffff">Solutions by Industry</h2></div>
<div style="padding:10px">
Learn how high-end forecasting, trading and neural network analytics are increasing profitability, insight and decision-making ability in your industry:
<ul style="margin-bottom:0; padding-bottom:0">
<!-- <li><a href="products/business_domains/for_banks.htm">Banking</a></li> -->
<li><a href="products/business_domains/for_security_brokers.htm">Brokerage</a></li>
<li><a href="products/business_domains/for_energy_generating_and_distribution_companies.htm">Energy</a></li>
<li><a href="products/business_domains/for_healthcare_service_providers.htm">Healthcare</a></li>
<li><a href="products/business_domains/for_insurance_companies.htm">Insurance</a></li>
<li><a href="products/business_domains/for_retail_companies.htm">Retail</a></li>
<li><a href="products/business_domains/for_scientists_and_rd_labs.htm">Science</a></li>
</ul>
</div>
</div>

<div style="border:solid 1px #bfbfbf; margin-top:8px">
<div style="background-color:#9b9aab"><h2 style="font-size:14px; padding:5px 10px; color:#ffffff">About Alyuda Research</h2></div>
<div style="padding:10px">
Alyuda Research is the leader in making the latest neural networks and data analytics technology incredibly easy-to-use and perfectly tailored for niche applications.
<br><br>
A unique mix of exclusive business expertise and cutting-edge software development experience is powering Alyuda's ability to help you improve business results and ensure success in highly competitive marketplaces.
<div style="text-align:right"><a href="services.htm">More about Alyuda.</a></div>
</ul>
</div>
</div>
		 
             </td>
             <td rowspan="4"><div style="width:55px"></div></td>
			 <td width="290" valign="top">
              <table width="100%" border="0" cellpadding="0" cellspacing="0" class="text">
			   <tr>
			    <td>
                 <h1 style="font-family:Arial Narrow; font-size:24px; font-weight:normal; color:#ff9900; padding:15px 0 20px 0">Neural Network Software</h1>
                 <table width="100%" border="0" cellspacing="0" cellpadding="0">
                  <tr>
                   <td><img src="image/pict_i_nn.jpg" alt="Neural Network Software"></td>
                   <td style="padding:0 0 0 8px">
<strong style="font-size:15px; line-height:15px; color:#ff9900">For researchers, data mining experts and predictive analysts</strong>
                   </td>
                  </tr> 
                 </table>
                
                </td>
			   </tr>
			   <tr>
			    <td height="165" valign="top" style="padding:10px 0 0 0">
High-end professional neural network software system to get the maximum predictive power from artificial neural network technology. Alyuda's neural network software is successfully used by thousands of experts to solve tough data mining problems, empower pattern recognition and predictive modeling, build classifiers and neural net simulators, design trading systems and forecasting solutions.
<br>
<br>
<strong>Featured products</strong>:
<ul>
<li>
<strong>Neural networks software</strong> -
<div style="text-align:right"><a href="neural-networks-software.htm">Alyuda NeuroIntelligence</a></div>
</li>
</ul>
				</td>
			   </tr>
			  </table>
			 </td>
			 <td rowspan="2"><div style="width:55px"></div></td>
			 <td width="290" valign="top">
<table width="100%" border="0" cellpadding="0" cellspacing="0" class="text">
			   <tr>
			    <td>
                <h2 style="font-family:Arial Narrow; font-size:24px; font-weight:normal; color:#ff9900; padding:15px 0 20px 0">Forecasting Software</h2>
                <table width="100%" border="0" cellspacing="0" cellpadding="0" class="text">
                  <tr>
                   <td><img src="image/pict_i_f.jpg" alt="Forecasting Software"></td>
                  <td width="100%" style="padding:0 0 0 8px">
<strong style="font-size:15px; line-height:15px; color:#ff9900">
For forecasters, <br />business analysts <br />and managers</strong>
                   </td>
                  </tr> 
                 </table>
                
                </td>
			   </tr>
			   <tr>
			    <td height="165" valign="top" style="padding:10px 0 0 0">
Based on proprietary self-constructive neural networks, Alyuda forecasting software provides reliable forecasts even when the input data is noisy, full of non-linear dependencies or incomplete. <a href="forecasting-excel-software-with-neural-network.htm"><strong>Alyuda forecasting software</strong></a> makes it easy to start with neural nets as it automatically designs, trains and tests neural network forecasting models using the latest advances in artificial neural networks. 
<br>
<br>
<strong>Featured solutions</strong>:
<ul>
<li>
<strong>Forecasting Excel add-in</strong> - 
<div style="text-align:right"><a href="forecasting-excel-software-with-neural-network.htm">Alyuda Forecaster XL</a></div>
</li>
</ul>
				</td>
			   </tr>
			  </table>
              </td>

			</tr>
			<tr>
			 <td valign="top" style="padding:20px 0 0 0">
<table width="100%" border="0" cellpadding="0" cellspacing="0">
			   <tr>
			    <td>
                <h2 style="font-family:Arial Narrow; font-size:24px; font-weight:normal; color:#ff9900; padding:0 0 20px 0">Trading Software</h2>
                <table width="290" border="0" cellspacing="0" cellpadding="0">
                  <tr>
                   <td><img src="image/pict_i_t.jpg" alt="Trading Software"></td>
                   <td width="100%" style="padding:0 0 0 8px">
<strong style="font-size:15px; line-height:15px; color:#ff9900">For active stock, <br />forex and futures <br>traders</strong>
                   </td>
                  </tr> 
                 </table>
                 
                </td>
			   </tr>
			   <tr>
			    <td valign="top" style="padding:10px 0 0 0">
Alyuda offers feature-rich professional <a href="http://www.tradecision.com/" target="_blank"><strong>trading software</strong></a> based on proven technical analysis techniques and state-of-the-art stock pattern recognition systems. With Tradecision, you can create trading strategies, predictive neural network models, stock forecasting and stock market analysis techniques, money management approaches, custom indicators, and studies to minimize your risks and maximize profits. 
<br>
<br>
<strong>Featured product</strong>:
<ul>
<li>
<strong>Technical analysis software and neural network trading software</strong> - 
<div style="text-align:right"><a href="products/tradecision/trading_software.htm">Alyuda Tradecision</a></div>
</li>
</ul>				</td>
			   </tr>
			  </table>			 
			  </td>
			 <td valign="top" style="padding:20px 0 0 0">
<table width="100%" border="0" cellpadding="0" cellspacing="0">
			   <tr>
			    <td>
                <h2 style="font-family:Arial Narrow; font-size:24px; font-weight:normal; color:#ff9900; padding:0 0 20px 0">Credit Scoring System</h2>
                <table width="290" border="0" cellspacing="0" cellpadding="0" class="text">
                  <tr>
                   <td><img src="image/pict_i_css.jpg" alt="Credit Scoring Software"></td>
                   <td width="100%" style="padding:0 0 0 8px">
<strong style="font-size:15px; line-height:15px; color:#ff9900">For risk managers, retail credit analysts and scorecard modelers</strong>
                   </td>
                  </tr> 
                 </table>
                
                </td>
			   </tr>
			   <tr>
			    <td height="190" valign="top" style="padding:5px">
Using state-of-the-art scoring technologies Alyuda developed analytical core for premium-quality complete <a href="http://www.plug-n-score.com/" target="_blank"><strong>credit scoring systems</strong></a> for consumer and SME lending that can be effectively used for Application, Collection and Behavioral scoring as well as Fraud Detection in retail banking. These systems are powered with the easiest and fastest scorecard development solution on the market, as well as with a unique scoring software that is the easiest for integration with any existing IT infrastructure. 
<br>
<br>
<strong>Featured partner product</strong>:
<ul>
<li><strong>Credit scoring system</strong> - <a href="http://www.plug-n-score.com/" target="_blank">Plug&amp;Score</a></li>
</ul>

				</td>
			   </tr>
			  </table>
			 </td>
			</tr>
            <!--
            
            <tr>
             <td colspan="4" style="height:5px"></td>
            </tr>
            
            <tr>
             <td colspan="3" style="padding:30px 0 0 0">
              <table width="100%" border="0" cellspacing="0" cellpadding="0">
               <tr>
                <td align="center" style="border:solid 1px #dedee7; padding:10px; margin:30px 0 0 0">
For customer acquisition, credit risk mitigation, customer management and fraud prevention solutions<br /> we recommend Scorto Corp.
<div style="padding-top:10px"> 
<a href="http://www.scorto.com/" target="_blank">Decision Management</a> | <a href="http://www.scorto.com/loan-origination-software.htm" target="_blank">Loan Origination Software</a> | <a href="http://www.scorto.com/customer-management.htm" target="_blank">Customer Management</a> | <a href="http://www.scorto.com/debt-collection-software.htm" target="_blank">Debt Collection Software</a><br /><a href="http://www.scorto.com/fraud-detection-software.htm" target="_blank">Fraud Detection Systems</a>
</div>
                 </td>
                </tr>
               </table>             
             </td>
            </tr>
            -->
		   </table>
    </div>
   </div>
   <!--/content-->
  </div>
    <div id="footer"><div class="fbg">
<div class="fleft">
<div class="flpad">
 &copy; 2001-2014 <a href="http://www.alyuda.com" class="lineless"> 
 Alyuda Research, LLC</a>. All rights reserved.<br>
 Use of this website signifies your agreement to the <a href="terms.htm">Terms of Use</a>.
</div>
</div>
<div class="fright">
<div class="frpad">       
 <a href="privacy.htm">Privacy Policy</a> | 
 <a href="contact.htm">Contact us</a> |
 <a href="sitemap.htm">Site map</a>
</div>
</div>
</div>

<div class="hrequest">

<div class="increq">
<div style="padding:5px 0 0 0; cursor:pointer; float:left" 
onclick='document.getElementById("div_question").className="show2"'><a href="javascript:void(0);" onclick='document.getElementById("div_question").className="show2"'>Ask Us a Question</a></div>
<div style="float:left; padding:5px 0 0 0">&nbsp;&nbsp;|&nbsp;&nbsp;</div>
</div>

<div id="div_question" class="hide">
<script language="javascript1.2">

function valid() {
var subm;
		if (document.form.num.value==document.form.num2.value) { 
		subm=true;
		} else {
		alert("Type the characters you see in the picture below!");
		document.form.num.focus();
		subm=false;
		}
return subm;
}
</script>
<form id="form_id" name="form" method="post" action="send_question.php" onSubmit="return valid()">
<input name="num2" type="hidden" value="9304">
  <table width="100%" border="0" cellpadding="4" cellspacing="4">
    <tbody>
    <tr>
     <td colspan="2">
     <div style="float:left"><strong>Ask Us a Question</strong></div>
     <div>
<img src="image/button_close.gif" alt="Close window" onclick='document.getElementById("div_question").className="hide"' style="cursor: pointer; display: block; float: right" align="right" border="0">
</div>     
     </td>
    </tr>
    <tr><td height="25" colspan="2" align="left">
    <font color="#999999">*Indicates a required field</font>
    </td></tr>
    <tr><td align="right">Name*:</td><td width="80%">
    <input name="name" type="text" class="form" id="name" style="width: 100%;">
    </td></tr>
   <tr><td align="right">E-mail*:</td>
   <td>
   <input name="email" type="text" class="form" id="email" style="width: 100%;">
   </td>
  </tr>
   
    <tr>
    <td align="right">Company*:</td>
    <td>
    <input name="company" type="text" class="form" id="company" style="width: 100%;">
    </td>
    </tr>
    
    <tr>
     <td></td>
     <td height="30" valign="bottom">Please enter your message here*:</td>
    </tr>
	<tr><td valign="top" align="right"></td>
	<td>
    <textarea name="message" id="message" class="form" cols="25" rows="4" wrap="virtual" style="width: 100%;"></textarea>
    
    </td></tr>
    
    <tr>
    <td></td>
    <td valign="top">Type the characters 
                    you see in the picture:&nbsp;<img src="pic.php?r=9304" align="absmiddle" style="border:1px Solid #efefef"> 
                    <input size=5 name="num" class="form">
                  </td>
                </tr>
    
    <tr>
    <td></td>
    <td style="padding-top: 3px; padding-right: 40px;" valign="top" nowrap="nowrap">
	 <input type="submit" name="btn_Download" value="Submit" class="form1" />&nbsp;&nbsp;

     <input type="reset" name="btn_CLear" value="  Clear  " class="form1" />
	</td>
    </tr>
  </tbody></table>
</form>	
</div>
<!--
//-->
<div class="increq2">
<div style="padding:5px 0 0 0; cursor:pointer;" onclick='document.getElementById("div_quote").className="show2"'><a href="javascript:void(0);" onclick='document.getElementById("div_quote").className="show2"'>Request a Quote</a></div>
</div>


<div id="div_quote" class="hide">
<script language="javascript1.2">

function valid1() {
var subm;
                if (document.FormLoadDemo.num.value==document.FormLoadDemo.num2.value) {
                subm=true;
                } else {
                alert("Type the characters you see in the picture below!");
                document.FormLoadDemo.num.focus();
                subm=false;
                }
return subm;
}
</script>
<form id="FormLoadDemo" name="FormLoadDemo" method="post" action="send_request_n.php" onSubmit="return valid1()">
<input name="num2" type="hidden" value="9304">
  <table width="100%" border="0" cellpadding="4" cellspacing="4">
    <tbody>
    <tr>
     <td colspan="2">
     <div style="float:left"><strong>Request a Quote</strong></div>
<div>
<img src="image/button_close.gif" alt="Close window" onclick='document.getElementById("div_quote").className="hide"' style="cursor: pointer; display: block; float: right" align="right" border="0">
</div>    
     </td>
    </tr>
    <tr><td height="25" colspan="2" align="left"><font color="#999999">*Indicates a required field</font></td></tr>
    <tr><td align="right">Name*:</td><td width="80%">
    <input name="name" type="text" class="form" id="name" style="width: 100%;">
    </td></tr>
   <tr><td align="right">E-mail*:</td>
   <td>
   <input name="email" type="text" class="form" id="email" style="width: 100%;">
   </td>
  </tr>
   
    <tr>
    <td align="right">Company*:</td>
    <td>
    <input name="company" type="text" class="form" id="company" style="width: 100%;">
    </td>
    </tr>
    
    <tr>
     <td></td>
     <td><br />Select the item you are interested in:</td>
    </tr>
    
    <tr>
     <td></td>
     <td>
                      <select name="domain" class="form" width="15" style="width:60%;">
                        <option value="" selected>-- By Business Domain --</option>
                        <option value="Active Traders">Active Traders</option>
                        <option value="Brokers">Brokers</option>
                        <option value="Financial Institutions">Financial Institutions</option>
                        <option value="Energy">Energy</option>
                        <option value="Healthcare">Healthcare</option>
                        <option value="Insurance">Insurance</option>
                        <option value="Science">Science</option>
                      </select>
                      <br />
                      <select name="solution" class="form" width="15" style="margin-top:5px; width:60%;">
                        <option value="" selected>-- By Solution --</option>
                        <option value="BrokerBooster">BrokerBooster</option>
                        <option value="NeuroIntelligence ">NeuroIntelligence</option>
                        <option value="NeuroDienst">NeuroDienst</option>
                        <option value="Tradecision">Tradecision</option>
                        <option value="Plug&Score">Plug&amp;Score</option>
                        <option value="Forecaster XL">Forecaster XL</option>
                        <option value="Forecaster">Forecaster </option>
                        <option value="NeuroFusion">NeuroFusion</option>
                      </select>
     </td>
    </tr>
    
    <tr>
     <td></td>
     <td height="30" valign="bottom">Please enter your message here*:</td>
    </tr>
	<tr><td valign="top" align="right"></td>
	<td>
    <textarea name="message" id="message" class="form" cols="25" rows="4" wrap="virtual" style="width: 100%;"></textarea>
    
    </td>
    </tr>
   <tr>
    <td></td>
    <td valign="top">Type the characters
                    you see in the picture:&nbsp;<img src="pic.php?r=9304" align="absmiddle" style="border:1px Solid #efefef">
                    <input size=5 name="num" class="form">
    </td>
    </tr>
 
    <tr>
    <td></td>
    <td style="padding-top: 3px; padding-right: 40px;" valign="top" nowrap="nowrap">
	 <input type="submit" name="btn_Download" value="Submit" class="form1" />&nbsp;&nbsp;

     <input type="reset" name="btn_CLear" value="  Clear  " class="form1" />
	</td></tr>
  </tbody>
  </table>
</form>	
</div>
</div></div>
</div>
</body>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-5493935-5']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</html>
++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://www.nd.com/>====================
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="description" content="The development tool of choice among neural network researchers and application developers is NeuroSolutions.
NeuroSolutions' icon-based graphical user interface provides the most powerful and flexible development environment available on the market today.">

<meta name="keywords" content="neural networks software network artificial intelligence AIs">

<title>
NeuroDimension - Neural Network Software,  Neural Net Software, Neural Networks, Neural Nets, NeuroSolutions, TradingSolutions, Trader68, OptiGen Library, Neural Network Course

</title>

<link rel="stylesheet" href="/css/nd_style.css" type="text/css" media="screen" />

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.6.1/jquery.js" type="text/javascript"></script>

<!-- VideoBox Begin -->
<link rel="stylesheet" href="/css/prettyPhoto.css" type="text/css" media="screen" title="prettyPhoto main stylesheet" charset="utf-8" />
<script src="/js/jquery.prettyPhoto.js" type="text/javascript" charset="utf-8"></script>
<script type="text/javascript" charset="utf-8">
	$(document).ready(function(){
		$("a[rel^='prettyPhoto']").prettyPhoto();
	});
</script>
<!-- VideoBox End -->

<!-- Dropdown Menu Begin -->
<script type="text/javascript">
var timeout         = 500;
var closetimer		= 0;
var ddmenuitem      = 0;

function jsddm_open()
{	jsddm_canceltimer();
	jsddm_close();
	ddmenuitem = $(this).find('ul').eq(0).css('visibility', 'visible');}

function jsddm_close()
{	if(ddmenuitem) ddmenuitem.css('visibility', 'hidden');}

function jsddm_timer()
{	closetimer = window.setTimeout(jsddm_close, timeout);}

function jsddm_canceltimer()
{	if(closetimer)
	{	window.clearTimeout(closetimer);
		closetimer = null;}}

$(document).ready(function()
{	$('#jsddm-content > li').bind('mouseover', jsddm_open);
	$('#jsddm-content > li').bind('mouseout',  jsddm_timer);});

document.onclick = jsddm_close;
</script>
<!-- Dropdown Menu End -->

<!-- News Ticker Begin -->
<script src="/js/jquery.ticker.js" type="text/javascript"></script>
<script type="text/javascript">
    $(function () {
        $('#js-news').ticker({
            speed: 0.10,
            fadeInSpeed: 600,
            titleText: '',
            controls: false,
        });
    });
</script>

<!-- News Ticker End -->

<link rel="stylesheet" type="text/css" href="/css/jquery.lightbox-0.5.css" media="screen" />
<script type="text/javascript" src="/js/jquery.lightbox-0.5.js"></script>
<script type="text/javascript">
$(function() {
	$('a.lightbox').lightBox(); // Select all links with lightbox class
	});
</script>


</head>

<body>
	<div class="container">

		<!--Start Top ssi -->

			<table cellspacing="0" cellpadding="0" id="menu-bar">
				<tr>
			<!-- Logo -->
				<td colspan="2"><a href="http://www.nd.com"><img src="http://www.nd.com/_new/images/logo.jpg" alt="logo" style="display:block;"></a></td>
				<td align="right" valign="top">
					<table cellspacing="0" cellpadding="0" width="155">
					<tr>
					<td align="right">
					<div class="contact">
					<b><a href="http://www.nd.com/contact.html">Contact NeuroDimension</a></b><p>
					<a href="http://www.nd.com"><img src="http://www.neurosolutions.com/images/ND_48x48.png" align="left"></a>
					<img src="http://www.nd.com/images/phone.jpg" align="right"><br>
					<a href="mailto:info@nd.com"><img src="/images/email_icon.png" alt="Email Us"></a>
					<a href="http://www.facebook.com/NeuroDimension"><img src="/images/facebook-icon.png" alt="Find us on Facebook"></a>
					<a href="https://plus.google.com/110184004013389341724" rel="publisher"><img src="/images/google_plus.png"></a>
					<a href="http://www.twitter.com/neurodimension"><img src="/images/twitter_icon.gif" alt="Find us on Twitter"></a>
					<a href="skype:neurodimension?call"><img src="/images/skype-icon-16.png" alt="Contact Sales via Skype"></a>
					</div>
					</td>
					</tr>
					</table>
				</td>
				</tr>
				<tr>
			<!-- Menu Bar -->
				<td class="home-menu-option" id="home-menu-option-link"><a href="/index.html">Home</a></td>
				<td class="menu-option"><ul id="jsddm-content">
					<li><a href="http://www.neurosolutions.com/products/">Products</a>
							<ul>
								<li><a href="http://www.neurosolutions.com">NeuroSolutions <img src="http://www.nd.com/_new/images/new_page_icon_white.gif"></a></li>
								<li><a href="http://www.tradingsolutions.com">TradingSolutions <img src="http://www.nd.com/_new/images/new_page_icon_white.gif"></a></li>
								<li><a href="http://www.trader68.com">Trader68 <img src="http://www.nd.com/_new/images/new_page_icon_white.gif"></a></li>
								<li><a href="http://www.neurosolutions.com/products/course/">Neural Network Course <img src="http://www.nd.com/_new/images/new_page_icon_white.gif"></a></li>
								<li><a href="http://www.nd.com/genetic/">OptiGen Library</a></li>
								<li><a href="http://www.neurosolutions.com/products/nsbook/">Interactive Book <img src="http://www.nd.com/_new/images/new_page_icon_white.gif"></a></li>
							</ul>
					</li>
					<li><a href="/resources/partners.html">Services</a>
							<ul>
								<li><a href="http://www.nd.com/resources/partners.html">Neural Network Consulting</a></li>
								<li><a href="http://www.tradingsolutions.com/services/">Financial System Development</a></li>
								<li><a href="http://www.tradingsolutions.com/services/custom.html">Custom Software</a></li>
							</ul>
					</li>
					<li><a href="http://www.nd.com/apps/">Applications</a>
							<ul>
								<li><a href="http://www.nd.com/apps/">Main Page</a></li>
								<li><a href="http://www.nd.com/apps/industry/business">Business</a></li>
								<li><a href="http://www.nd.com/apps/industry/finance">Finance & Trading</a></li>
								<li><a href="http://www.nd.com/apps/industry/medical">Medical</a></li>
								<li><a href="http://www.nd.com/apps/industry/science">Science</a></li>
								<!-- <li><a href="http://www.nd.com/apps/control.html">Manufacturing</a></li> -->
								<li><a href="http://www.nd.com/apps/industry/sports">Sports</a></li>
							</ul>
					</li>
					<li><a href="http://www.neurosolutions.com/resources/">Resources</a>
						<ul>
							<li><a href="http://www.neurosolutions.com/products/ns/whatisNN.html">Intro to Neural Networks</a></li>
							<li><a href="http://www.nd.com/genetic/whatisga.html">Intro to Genetic Algorithms</a></li>
							<li><a href="http://www.neurosolutions.com/resources/interviews.html">Customer Interviews</a></li>
							<li><a href="http://www.nd.com/customers.html">Customer List</a></li>
							<li><a href="http://www.nd.com/history.html">Company History</a></li>
						</ul>
					</li>
					<li><a href="http://www.neurosolutions.com/support/">Support</a>
						<ul>
							<li><a href="http://www.nd.com/contact.html">Contact Us</a></li>
						</ul>
					</li>
					<li><a href="#">Order</a>
						<ul>
							<li><a href="https://ssl10.pair.com/neurodim/"><img src="http://www.nd.com/_new/images/secure-lock-icon.png"> Secure Online Order</a></li>
							<li><a href="http://www.neurosolutions.com/order/">Order Information</a></li>
							<li><a href="http://www.nd.com/advisor/">Product Advisor</a></li>
							<li><a href="http://www.neurosolutions.com/pricing.html">Product Pricing</a></li>
							<li><a href="http://www.neurosolutions.com/products/ns/levelcompare.html">Level Summary</a></li>
						</ul>
					</li>
					</ul>

				</td>
				<td class="luc-menu-option" id="luc-menu-option"><img src="http://www.nd.com/_new/images/secure-lock-icon.png"> <a href="http://www.neurodimension.com/customercenter/licensedusers.asp" target="_blank">Licensed User Center</a></td>
				</tr>
				</table>

	<!-- End Top ssi -->



	<!-- Content Begin -->

	<!-- Top Content Begin -->

		<table id="main-content" width="100%" cellspacing="0" cellpading="0">
			<tr>
			<td id="main-content">
			<h3>Neural Networks and Intelligent Software Solutions</h3>
			<a href="http://www.neurosolutions.com/products/ns/whatisNN.html">Neural networks</a>
   			are an exciting form of artificial intelligence which mimic the learning process of the brain.  Our neural network software products are among the most powerful and flexible on the market today, yet their intuitive graphical user interfaces make
			them incredibly easy to use.
			<table border="0" cellspacing="0" cellpadding="0">
				<tr>
				<td width="50%" valign="top">
				<ul class="green-arrow">
				<li id="main-bullets">Perform <b>cluster analysis, sales forecasting, sports predictions, medical classification, and much more</b> with <a href="http://www.neurosolutions.com">NeuroSolutions</a>, including versions for <a href="http://neurosolutions.com/products/nsexcel/">Excel</a> and <a href="http://neurosolutions.com/products/nsmatlab/">MATLAB</a>.</li>
				<li id="main-bullets">Perform <b>price prediction</b> and <b>advanced stock, FOREX and Futures trading analysis</b> with <a href="http://www.tradingsolutions.com/">TradingSolutions</a> and automatically trade real-time signals with <a href="http://www.trader68.com/">Trader68</a>.</li>
				<li id="main-bullets">Utilize advanced optimization techniques including <b>Genetic Optimization</b> and <b>Attribute Searching</b> in <a href="http://www.neurosolutions.com">NeuroSolutions</a>, powered by our <a href="/genetic/">OptiGen Library</a>.</li>
				</ul>
				</td>

				<td width="50%" valign="top">
				<ul class="green-arrow">
				<li id="main-bullets">Leverage over <b>20 years of industry experience</b> with our advanced <a href="/resources/partners.html">consulting services</a>, <a href="http://www.neurosolutions.com/products/course/">Neural Network Courses</a>, and <a href="http://www.neurosolutions.com/products/nsbook/">Interactive Book</a>.</li>
				<li id="main-bullets">Harness the massive processing power of <b>NVIDIA<sup>TM</sup> CUDA<sup>TM</sup></b> for speed improvements of up to <b>hundreds of times faster</b> in <a href="http://www.neurosolutions.com/">NeuroSolutions</a>!</li>
				<li id="main-bullets"><b>Find out more</b> about <a href="http://www.neurosolutions.com/products/ns/whatisNN.html">Neural Networks</a> and <a href="/apps/">Neural Network Applications</a> or visit the <a href="/advisor/">Product Advisor</a> to determine which products best meet your needs.</li>
				</ul>
				</td>
				</tr>
			</table>
			</td>

			<td id="main-content" align="right">
			<img src="/images/rside-header.jpg" style="display:block;">
			<table border="0" cellspacing="0" cellpadding="0">
				<tr>
				<td colspan="2"><a href="http://www.nd.com/advisor"><img src="/images/rside-pa.jpg" target="_blank" style="display:block;"></a>
				</td>
				</tr>

				<tr>
				<td><a href="http://www.neurosolutions.com"><img src="/images/rside-ns.jpg" target="_blank" style="display:block;"></a>
				</td>
				<td><a href="/resources/partners.html"><img src="/images/rside-con.jpg" target="_blank" style="display:block;"></a>
				</td>
				</tr>

				<tr>
				<td><a href="http://www.tradingsolutions.com/home.html"><img src="/images/rside-ts.jpg" target="_blank" style="display:block;"></a>
				</td>
				<td><a href="http://www.trader68.com/"><img src="/images/rside-t68.jpg" target="_blank" style="display:block;"></a>
				</td>
				</tr>
			</table>
			<p>


	<!-- Top Content End -->

	<!-- Lower Content Begin -->
			<tr>
			<td colspan="2">
			<table border="0" cellspacing="0" cellpadding="0" width="100%">
				<tr>
				<td colspan="2" id="home-column-news">
						<!-- News SSI Begin -->
					<table border="0" cellspacing="0" cellpadding="0" width="100%">
						<tr>
						<td valign="top"><img src="/images/lside-news-bg.png"></td>
						<td align="left" valign="center">
							<div id="ticker-wrapper" class="no-js">
							    <ul id="js-news" class="js-hidden">
							        <li class="news-item"><b>New Product:</b> NeuroSolutions Accelerator add-on for supercomputer-like speeds!  <a href="http://www.neurosolutions.com/products/cuda/">Click here to find out more!</a></li>
							        <li class="news-item">New <a href="http://www.neurosolutions.com/products/nsmatlab/">NeuroSolutions for MATLAB</a> 4 now supporting R2013a, 64-bit and more!</li>
							        <li class="news-item">Check out <a href="http://www.neurosolutions.com/resources/whatsnew60.html">What's New in NeuroSolutions 6.31</a> for complete details on the latest release of NeuroSolutions.</li>
							        <li class="news-item">New NeuroSolutions Tip Box: <a href="http://www.neurosolutions.com/resources/videotour.html">Click here to find out more!</a></li>
							    </ul>
							</div>
						</td>
						<td valign="top"><img src="/images/rside-news-bg.png"></td>
						</tr>
					</table>
	<!-- News SSI End -->


					<p>
				</td>
				</tr>
				<tr>
				<td id="home-column-left">
				<img src="/images/ns-icon.jpg" align="right" id="img-padding">
				<h3><a href="http://www.neurosolutions.com/">NeuroSolutions</a></h3>
				This powerful and flexible <a href="http://www.neurosolutions.com/">neural network software</a> is the perfect tool for solving your data modeling problems.  <a href="http://www.neurosolutions.com/download.html">Download a FREE evaluation</a> copy today, try it out with your own data and see first hand why it is the most widely used neural network software.
				</td>
				<td id="home-column-right">
				<img src="/images/consulting-icon.jpg" align="right" id="img-padding">
				<h3><a href="/resources/partners.html">Consulting Services</a></h3>
				Our <a href="/resources/partners.html">consulting services</a> offer advantages unmatched by any other neural network company. If your application needs artificial
				intelligence and neural network technology, let NeuroDimension engineers create a solution for you.
				</td>
				</tr>

				<tr>
				<td id="home-column-left">
				<img src="/images/ts-icon.jpg" align="right" id="img-padding">
				<h3><a href="http://www.tradingsolutions.com/">TradingSolutions</a></h3>
				Advanced <a href="http://www.tradingsolutions.com/">trading software</a> that combines neural network and genetic algorithm technologies with traditional technical analysis to create a highly effective tool for financial modeling. <a href="http://www.tradingsolutions.com/downloads/eval.html">Download a FREE evaluation copy</a> today and see for yourself.
				</td>
				<td id="home-column-right">
				<img src="http://trader68.com/images/trader68-logo.jpg" align="right" id="img-padding">
				<h3><a href="http://www.trader68.com">Trader68</a></h3>
				Free <a href="http://www.trader68.com">auto-trading software</a> for TradingSolutions and Collective2 users. Trader68 allows you to auto trade your real-time signals live -- as soon as they are generated.
				</td>
				</tr>

				<tr>
				<td id="home-column-left">
				<img src="/images/oglib-icon.jpg" align="right" id="img-padding">
				<h3><a href="/genetic">OptiGen Library</a></h3>
				The <a href="/genetic">OptiGen Library</a> is a flexible Software Development Kit (SDK) that allows programmers to easily use genetic algorithms to solve their optimization problems. They can also be used to embed
				genetic optimization into a custom application which can then be distributed free of charge.
				</td>
				<td id="home-column-right">
				<img src="/images/course-icon.jpg" align="right" id="img-padding">
				<h3><a href="http://www.neurosolutions.com/products/course/">Neural Network Course</a></h3>
				The <a href="http://www.neurosolutions.com/products/course/">Neural Network Course</a>  is the perfect way to learn about neural networks and NeuroSolutions. Find out how to utilize the latest technology to solve your data modeling problems through live instruction and interactive examples.

				</td>
				</tr>

	<!-- Facebook Begin -->
				<tr>
				<td colspan="2">
				<p>
				<script src="http://connect.facebook.net/en_US/all.js#xfbml=1"></script><fb:like show_faces="false" width="900" action="recommend" font="arial"></fb:like>
				</td>
				</tr>
	<!-- Facebook End -->
			</table>


	<!-- Content End -->
				</td>
			</tr>
			</table>
			<div id="clear>"</div>

	<!-- Footer Begin -->
			<img src="http://www.neurosolutions.com//images/main-content-lower.png" style="display:block;">
			<div id="footer">
			<table id="footer" cellspacing="0" cellpading="0">
			<tr>
			<td>
			 2014 <a href="http://www.nd.com">NeuroDimension, Inc.</a> All trademarks and copyrights are the property of their respective owners.<br>
			<a href="http://www.nd.com/privacy.html" target="_blank">Privacy Policy</a> |
			<a href="http://www.nd.com/resellers.html" target="_blank">Become an Authorized Reseller</a>
			<p>
			<a href="http://www.nd.com" target="_blank">ND.com</a> |
			<a href="http://www.neurosolutions.com" target="_blank">NeuroSolutions.com</a> |
			<a href="http://www.trader68.com" target="_blank">Trader68.com</a> |
			<a href="http://www.tradingsolutions.com/" target="_blank">TradingSolutions.com</a>
			</td>
			<td>
			<a href="http://www.nd.com" target="_blank"><img src="http://www.neurosolutions.com//images/nd-logo.gif" align="right"></a>
			</td>
			</tr>
			</table>
			</div>
			<div id="clear>"</div>
			<img src="http://www.neurosolutions.com//images/main-content-bottom.png" style="display:block;">


<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-21994956-3']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

	</div>

</body>
</html>++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://www.bio-comp.com/>====================
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>BioComp Systems</title>
<link href="css/style.css" rel="stylesheet" type="text/css" />
<link href="css/layout.css" rel="stylesheet" type="text/css" />
<script src="maxheight.js" type="text/javascript"></script>
<script type="text/javascript" src="./js/jquery-1.2.6.js"></script>
<script type="text/javascript">
$(document).ready(function() {
    $('a.flippy').click(function() {
        var heading = $(this).html().substr(1);
        var content = $(this).parent('p').next('div.slider');
        if ($(content).is(':hidden')) {
            $(this).html('&#x25BC;' + heading);
            $(content).slideDown('fast');
        } else {
            $(this).html('&#x25BA;' + heading);
            $(content).slideUp('fast');
        }
        return false;
    });
});
</script>
<style type="text/css">
<!--
.style1 {font-size: small}
.style3 {font-weight: bold}
.style10 {font-size: medium}
.style7 {color: #FF0000}
.style9 {font-size: x-small}
-->
</style>

</head>

<body id="index" onload="new ElementMaxHeight();">
	<div id="header_tall">
		<div id="main">
			<!--header -->
			<div id="header">
				<div class="h_logo">
						<div class="left">
						<img alt="" src="images/logo.jpg" /><br />
					</div>
					    <div class="clear"></div>
				</div>
				<div id="menu">
					<div class="rightbg">
						<div class="leftbg">
							<div class="padding">
								<ul>
									<li><span>Home</span></li>
									<li><a href="markets/index.html">Markets</a></li>
									<li><a href="products/index.html">Products</a></li>
									<li><a href="clients/index.html">Clients</a></li>
									<li><a href="services/index.html">Services</a></li>
									<li class="last"><a href="company/index.html">Contact Us</a></li>
								</ul>
								<br class="clear" />
							</div>
						</div>
					</div>
				</div>
				<div class="content"><br />
				    <span class="style3"><br />
				  </span>
			      <div class="text style1">
						<p>&nbsp;</p>
						<p>&nbsp;</p>
						<p>&nbsp;</p>
						<p>&nbsp;</p>
						<p>&nbsp;</p>
						<p>&nbsp;</p>
						<p class="style10">Understand, Predict and Optimize Your Operations... with Unheard of Returns on Investment.</p>
						<p class="style10">&nbsp;</p>
						<p class="style10"><a href="clients/benefits/100ROI.html"><img src="images/btn_learn_more.png" alt="" width="100" height="30" /></a></p>
			      </div>
					<div class="clear"></div>
			  </div>
			</div>
			<!--header end-->
			<div id="middle">
				<div class="indent">
					<div class="columns1">
					  <div class="column1">
							<div class="border">
								<div class="btall">
									<div class="ltall">
										<div class="rtall">
											<div class="tleft">
												<div class="tright">
													<div class="bleft">
														<div class="bright">
															<div class="ind">
																<div class="h_text">
																	<img src="images/workmanbar.jpg" alt="" width="225" height="43" /><br />
															  </div>
																<div class="padding"><strong>Solving Tough Customer Challenges </strong><br />
																	<p class="p1">
																  Our commercial / industrial customers come to us daily to solve their greatest product, process, and key asset performance challenges and achieve a 100% Return on Investment, sometimes in less than a month.  You can too...
																  <br />
															      </p>
																	<a href="clients/solvingchallenges.html"><img src="images/btn_more.png" alt="" width="72" height="22" /></a><br />
															  </div>
															</div>
														</div>
													</div>
												</div>
											</div>
										</div>
									</div>
								</div>
							</div>
					  </div>
						<div class="indent_column">&nbsp;</div>
						<div class="column2">
							<div class="border">
								<div class="btall">
									<div class="ltall">
										<div class="rtall">
											<div class="tleft">
												<div class="tright">
													<div class="bleft">
														<div class="bright">
															<div class="ind">
																<div class="h_text">
																	<img src="images/techbar.jpg" alt="" width="225" height="43" /><br />
															  </div>
																<div class="padding">
																	<strong>Advanced Technologies </strong><br />
																	<p class="p1">
																		Our world-class modeling, prediction and optimization technologies are unparalleled. Understand what causes performance problems, predict and see results on-line and optimize your products and processes. <br /></p>
																	<a href="technology/index.html"><img src="images/btn_more.png" alt="" width="71" height="22" /></a><br />
																</div>
															</div>
														</div>
													</div>
												</div>
											</div>
										</div>
									</div>
								</div>
							</div>
						</div>
						<div class="indent_column">&nbsp;</div>
						<div class="column3">
							<div class="border">
								<div class="btall">
									<div class="ltall">
										<div class="rtall">
											<div class="tleft">
												<div class="tright">
													<div class="bleft">
														<div class="bright">
															<div class="ind">
																<div class="h_text">
																	<img src="images/manbar.jpg" alt="" width="225" height="43" /><br />
															  </div>
															  <div class="padding">
																	<strong>Customer Resources</strong>
														        <p class="p1"><a href="http://www.biocompsystems.com/forums/">Profit / Dakota Forums</a><br />
															      <a href="clients/preferredcustomers.html">Preferred Customer Program<br />
														          </a><a href="http://www.intelli-community.com/store/">																    Order Products and Updates</a><a href="clients/procurement/index.html"><br />
															      </a><a href="http://www.biocompsystems.com/company/newsletter.html">Sign-Up for Our Financial Newsletter</a><br />
													                  <a href="http://www.intellidynamics.net/newsletter-signup.html"> ... or Our Commercial Newsletter</a>                                                                                                                                
														        <p>&nbsp;<br />
<p>&nbsp;<br />
<p>&nbsp;<br />
<div class="slider">&#8226;&nbsp;<a href="http://www.biocompsystems.com/cgi-bin/mojo.cgi?f=s&l=Profit">Financial (Dakota/Profit)</a><br/>
																&#8226;&nbsp;<a href="http://www.biocompsystems.com/cgi-bin/mojo.cgi?f=s&l=BioComp_News">Commercial/Industrial</a></div>
																
															  </div>
														</div>
													</div>
												</div>
											</div>
										</div>
									</div>
								</div>
							</div>
						</div>
						<div class="clear"></div>
					</div>
					<div class="columns2">
						<div class="ver_line">
							<div class="column1">
							  <div class="padding">
									<img src="images/FeaturedTopic.jpg" alt="" width="326" height="29" /><br />
							    <p class="p1">
										<strong class="b_text"><img src="images/intellect3Icon100.jpg" alt="Profit 8" width="100" height="103" class="leftimage" />IntelliDynamics Intellect 3.0 </strong><br />
							      <a href="http://www.intellidynamics.net/products.html">Intellect 3.0</a> is the next generation of real-time performance enhancement and predictive intelligence systems. First destined for process engineering / manufacturing applications, Intellect 3.0 will help producers increase their performance during these challenging economic times. With its history of more than doubling yields and manufacturing capacities, Intellect has proven itself an excellent return on investment in all manufacturing market segments.</p>
								  <div class="more"><a href="http://www.intellidynamics.net/products.html">learn more</a></div>
							    <p>&nbsp;</p>
							    <p>&nbsp;</p>
							    <p>&nbsp;</p>
							      <p>&nbsp;</p>
							  </div>
							</div>
							<div class="column2">
								<div class="padding"><strong>&gt;&gt; Customer Assistance &lt;&lt;</strong><br />								 
                                 <div class="content">
                                   <p>We have a new Customer Assistance site. Check it out...</p>
                                   <p>&nbsp;</p>
                                   <p><a href="http://support.biocompsystems.com">Help Desk &gt;&gt;</a></p>
                                 </div>
                                 <div class="more"></div>
							  </div>
							</div>
							<div class="clear"></div>
						</div>
					</div>
				</div>
			</div>
			<!--footer -->
			<div id="footer">
				<div class="indent">
					춸2013 BioComp Systems, Inc. &bull; <a href="company/privacy.html">Privacy Policy</a></div>
			</div>
			<!--footer end-->
		</div>
	</div>
</body>

<!-- Start of PowerWebTraffic  by PowerObjects  Logging Code  -->
<script type="text/javascript">
 var _powt = 'i%2FwKSwlwD4mT6ko0QJrqyCVDH8QuwLsrN4RNopNJF7NKEPe9%2B%2FLNGNUez8LWkuCU5RRRSRbalbJgeZFRUQ9pJGSqqga4FAlrp%2BtUdYd31oBA16lN%2Fmb%2FBQ5zWYFzB%2BvBMQRWIsETnYaS%2BNG4tNAJQ13oRs2Ui5f21HNt1D5fW2A%3D';
(function () {
var s = document.createElement('script');
s.type = 'text/javascript'; 
s.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'cloud.crm.powerobjects.net/powerwebtraffic/powt.js.aspx';
var hs = document.getElementsByTagName('script')[0];
hs.parentNode.insertBefore(s, hs); 
})(); 
</script>
<!-- End of PowerWebTraffic by PowerObjects Logging Code -->

</html>++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://www.ingber.com/>====================
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=iso-8859-1">
  <title>Lester Ingber's Archive</title>
  <link rel="icon" href="http://ingber.com/favicon.ico">
  <meta name="reply-to" content="lester@ingber.com">
  <meta name="name" content="Lester Ingber">
  <meta name="author" content="Lester Ingber">
  <meta name="description"
  content=" ASA: Adaptive Simulated Annealing, Optimization, Importance Sampling, Nonlinear Systems, Stochastic Systems. COMBAT: Statistical Mechanical Analyses of Combat and Simulations. KARATE: The Art and Science of Karate, Applications to Learning. MARKETS: Statistical Mechanics of Financial Markets, Bond Futures, Options, Risk, Portfolios, Trading. NEOCORTEX: Statistical Mechanics of Neocortical Interactions, Applications to Memory, EEG, Intelligent Systems. NUCLEAR: Nucleons, Nuclear Matter, Riemannian Interactions. PATH-INTEGRAL: Path Integrals in Stochastic Systems with Nonlinear Diffusion. ">
  <meta name="keywords"
  content=" finance, neocortex, optimization, statistical, nonlinear, karate, combat, risk, trading, nuclear, physics, stochastic, intelligence, options">
  <meta name="verify-v1" content="GkAsSZI/iiUM70+hSxbauv1QHqGASF5aRzrnYv+ZSPk=">
  <meta name="google-translate-customization"
  content="ab7088b64db0ed01-c814cc1ad83ab21a-gc3cb079a361956ab-e">
  <link href="http://url.ingber.com/feeds" rel="alternate"
  type="application/rss+xml" title="Subscribe Lester.Ingber.com">
  <link href="http://google.com/+LesterIngber?rel=author">
  <link href="http://google.com/+Ingber?rel=publisher">
  <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-23383940-1']);
  _gaq.push(['_setDomainName', '.ingber.com']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
  </script>
  <style type="text/css">

body {
     font-size:100%;
         }
 h1 {
      font-size:160%;
      color:#000;
          background-color:#ccc;
 } 
 
 h2 {
     font-size:130%;
     color:#000;
         background-color:#ccc;
 } 
 
 h3 {
     font-size:110%;
     color:#000;
         background-color:#ccc;
         }
  h4 {
     font-size:100%;
     color:#000;
         background-color:transparent;
         }         
 .wrapper {
        text-align: left;
        margin: 0px auto;
        padding: 0px;
        width: 100%;
}
  </style>
</head>

<body text="#000066" bgcolor="#FFFFFF" link="#660000" vlink="#993300"
alink="#003300" background="http://www.ingber.com/background.jpg">
<h1><a name="To-Top-of-Archive">Lester Ingber's Archive</a></h1>
<br>


<div id="google_translate_element">
</div>
<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({pageLanguage: 'en', layout: google.translate.TranslateElement.InlineLayout.SIMPLE, gaTrack: true, gaId: 'UA-23383940-1'}, 'google_translate_element');
}</script>
<script type="text/javascript"
src="http://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit">
</script>
<a href="http://url.ingber.com/feeds" type="application/rss+xml"><img
src="http://www.ingber.com/feeds.png" alt="[RSS]" width="40"
border="0">Subscribe Lester.Ingber.com</a> 

<h2>Lester Ingber, Ph.D. (Theoretical Physics)</h2>

<h2><a href="http://www.ingber.com/"><img
src="http://www.ingber.com/LesterHead.jpg" alt="[Lester Ingber]" width="200"
border="0"> http://www.ingber.com/ <img
src="http://www.ingber.com/ingber_com_qr.png" alt="[QR]" width="100"
border="0"></a> <br>
<a href="http://alumni.caltech.edu/~ingber/"><img
src="http://www.ingber.com/caltech.jpg" alt="[Caltech Alumni]" width="200"
border="0"> http://alumni.caltech.edu/~ingber/</a> (mirror) </h2>
<dl>
  <dt><b>E-mail</b> queries may be addressed to </dt>
    <dd><a href="mailto:lester@ingber.com">lester@ingber.com</a> </dd>
    <dd><a
      href="mailto:ingber@alumni.caltech.edu">ingber@alumni.caltech.edu</a>
    </dd>
</dl>

<p>This archive is a sampling of my 100+ publications. Limited help assisting
people with queries on my codes and papers is available only by electronic mail
correspondence. Sorry, but given that this site has 1000's hits/day, I cannot
mail out hardcopies of code or papers. </p>

<p><a href="http://www.ingber.com/"><img src="http://www.ingber.com/LIR.jpg"
alt="[LIR]" width="50" border="0"> <b>Lester Ingber Research (LIR)</b></a>
develops and consults on projects in several areas of expertise documented in
the ingber.com InterNet archive. Terms of use, downloading policies, and
consulting/contracting are discussed in <br>
<a href="http://www.ingber.com/ingber_terms.html">ingber_terms.html</a>. <br>
Donations to support LIR research, and pre-arranged payments for services, can
be made online at <br>
<a href="http://www.ingber.com/private/lir-pay.html">LIR-Payment</a>. </p>

<p><b>SSL</b> access, accepting my Certificate, is through <br>
<a href="https://www.ingber.com/">https://www.ingber.com</a> </p>

<p><b>Search</b> ingber.com at <a
href="http://www.ingber.com/ingber_search.html">ingber_search.html</a>. </p>
<hr>

<p>Categories of research files in this / directory are </p>
<ul>
  <li><h3><a href="#AUTHOR">AUTHOR</a></h3>
    Information on Lester Ingber </li>
  <li><h3><a href="#KARATE">KARATE</a></h3>
    The Art and Science of Karate, Applications to Learning </li>
  <li><h3><a href="#ASA">ASA</a></h3>
    Adaptive Simulated Annealing, Optimization, Importance Sampling, Nonlinear
    Systems, Stochastic Systems: <a href="#ASA-CODE">ASA-CODE</a>, <a
    href="#ASA-REPRINTS">ASA-REPRINTS</a> </li>
  <li><h3><a href="#COMBAT">COMBAT</a></h3>
    Statistical Mechanical Analyses of Combat and Simulations </li>
  <li><h3><a href="#MARKETS">MARKETS</a></h3>
    Statistical Mechanical Analyses of Financial Markets, Bond Futures,
    Options, Risk, Portfolios, Trading </li>
  <li><h3><a href="#NUCLEAR">NUCLEAR</a></h3>
    Nucleons, Nuclear Matter, Riemannian Interactions </li>
  <li><h3><a href="#PATH-INTEGRAL">PATH-INTEGRAL</a></h3>
    Path Integrals in Stochastic Systems with Nonlinear Diffusion </li>
  <li><h3><a href="#NEOCORTEX">NEOCORTEX</a></h3>
    Statistical Mechanics of Neocortical Interactions, Applications to Memory,
    EEG, Intelligent Systems </li>
</ul>
<hr>

<p><b>Additional Files and Information</b></p>

<p>See <a
href="http://www.ingber.com/utils_file_formats.txt">utils_file_formats.txt</a>
for some links to information on gzip, PostScript, PDF, tar, and shar
utilities. See <a
href="http://www.ingber.com/utils_code.html">utils_code.html</a> for additional
references to files and instructions for retrieval of utilities for DOS, MAC,
UNIX, and VMS systems, e.g., ghostscript, gzip, pgp (containing my PGP PUBLIC
KEY BLOCK), unshar, uudecode, unix2dos, zip, etc. Note that gzip will expand
.gz, .Z, and .zip files (not .zip archives). </p>
<dl>
  <dt><b><a href="http://www.ingber.com/netlinks.html">netlinks.html</a>
  </b></dt>
    <dd>This file contains some links to other interesting sites. 
      <p></p>
    </dd>
  <dt><b><a href="http://www.ingber.com/private/">private/</a> </b></dt>
    <dd>private/ is a "blind" directory which cannot be viewed using `ls` or
      `dir`, although files can be downloaded via WWW if you know their names. 
      <p></p>
    </dd>
  <dt><b><a href="http://www.ingber.com/louise/">louise/</a> </b></dt>
    <dd>This directory belongs to my spouse and her business Creek House
      Patisserie. 
      <p></p>
      I helped her develop her ballet book and videos in <a
      href="http://www.ingber.com/louise/louise_ballet.html">louise/louise_ballet.html</a> 
      <p></p>
    </dd>
  <dt><b><a
  href="http://alumni.caltech.edu/~ingber/">http://alumni.caltech.edu/~ingber/</a>
  </b></dt>
    <dd>This is my mirror homepage containing just their own html files, </dd>
</dl>

<p>This is a UNIX system, and all file names are case sensitive. Files that end
in .gz (gzip), .zip (zip), or .exe (DOS) are in binary format. Therefore,
before retrieving these files, be sure your browser is set to handle binary
transfer. </p>

<p>Note that files that contain a .ps are PostScript files (after expanding as
appropriate, which can result in an expansion by a factor of 10 for some
files); these will require a PostScript viewer or printer. Portable Document
Format files have a .pdf suffix and can be viewed with Adobe acroread or some
PostScript viewers. </p>
<dl>
  <dt><b>Unmanaged VPS</b></dt>
    <dd>ingber.com is run on an unmanaged linux virtual private server (VPS)
      machine, serving both IPv4 and IPv6 <img
      src="http://www.ingber.com/ipv6.jpg" alt="[IPv6]" width="50">. Lester
      Ingber is responsible for all software maintenance. This machine is
      monitored externally, and in case of a crash, full service should be
      restored quickly, i.e., less than an hour. </dd>
</dl>
<dl>
  <dt>Some information on our home for sale is in </dt>
    <dd><a href="http://www.ingber.com/ashland4sale/">ashland4sale</a> <br>
      and some information on life in Ashland Oregon is in </dd>
    <dd><a href="http://www.ingber.com/ashland.html">ashland.html</a> 
      <p></p>
    </dd>
</dl>
<dl>
  <dt>[<a href="http://www.ingber.com/">To-Top-of-Archive</a>] </dt>
    <dd></dd>
</dl>
<hr>

<h2><a name="AUTHOR">AUTHOR</a></h2>

<table>
  <tbody>
    <tr>
      <td><img src="http://www.ingber.com/sigmapisigma.jpg" align="left"
        alt="[Sigma Pi Sigma]" width="85" border="5"> </td>
      <td><img src="http://www.ingber.com/sigmaxi.jpg" align="left"
        alt="[Sigma Xi]" width="85" border="5"> </td>
    </tr>
  </tbody>
</table>
<dl>
  <dt><a href="http://www.ingber.com/ingber_CV.pdf">ingber_CV.pdf</a> or <a
  href="http://www.ingber.com/ingber_CV.ps.gz">ingber_CV.ps.gz</a> or <a
  href="http://www.ingber.com/ingber_CV.txt">ingber_CV.txt</a> </dt>
    <dd>Lester Ingber's Curriculum Vitae </dd>
  <dt><a
  href="http://www.ingber.com/ingber_summary_1.pdf">ingber_summary_1.pdf</a> or
  <a href="http://www.ingber.com/ingber_summary_1.txt">ingber_summary_1.txt</a>
  </dt>
    <dd>1-page summary of Lester Ingber's Curriculum Vitae 
      <p></p>
    </dd>
  <dt>Projects and interests are described in </dt>
    <dd><a
      href="http://www.ingber.com/ingber_projects_brief.pdf">ingber_projects_brief.pdf</a> 
      <p></p>
    </dd>
  <dt>Additional information related to some projects is in </dt>
    <dd><a
      href="http://www.ingber.com/ingber_projects.html">ingber_projects.html</a> 
      <p></p>
    </dd>
  <dt>A one-minute video introduction can be downloaded in mpg, avi or pptx
  format: </dt>
    <dd><a
      href="http://lester-x.ingber.com/ingber_projects">ingber_projects</a> 
      <p></p>
    </dd>
  <dt>In this archive I have listed only papers accessible on the internet. All
  my references are given in bibtex and refer format for over 100 papers and
  books in </dt>
    <dd><a href="http://www.ingber.com/ingber.bib.html">ingber.bib.html</a>
      (html-bibtex format) </dd>
    <dd><a href="http://www.ingber.com/ingber.ref.html">ingber.ref.html</a>
      (html-refer format) </dd>
    <dd><a href="http://www.ingber.com/ingber.bib">ingber.bib</a> (bibtex
      format) </dd>
    <dd><a href="http://www.ingber.com/ingber.end">ingber.end</a> (endnote
      format) </dd>
    <dd><a href="http://www.ingber.com/ingber.ref">ingber.ref</a> (refer
      format) </dd>
    <dd><a href="http://www.ingber.com/ingber.ref">ingber.ref</a> (refer
      format) </dd>
    <dd><a href="http://www.ingber.com/ingber.ris">ingber.ris</a> (refman
      format) </dd>
    <dd><a href="http://www.ingber.com/ingber.xml">ingber.xml</a> (xml format) 
      <p></p>
    </dd>
  <dt>LIR terms of use and downloading policies are discussed in </dt>
    <dd><a href="http://www.ingber.com/ingber_terms.html">ingber_terms.html</a> 
      <p></p>
    </dd>
  <dt>Some reasonable conditions for collaboration are discussed in </dt>
    <dd>
      <a href="http://www.ingber.com/lir_computational_physics_group.html">lir_computational_physics_group.html</a> 
      <p></p>
    </dd>

  <dt>I am Principal Investigator, for the National Science Foundation resource
  The Extreme Science and Engineering Discovery Environment,
  "Electroencephalographic field influence on calcium momentum waves" project.
  </dt>
    <dd><a href="http://www.xsede.org/"><img
      src="http://www.ingber.com/xsede.png" alt="[XSEDE]" width="150"
      border="0">http://www.xsede.org</a> </dd>
    </dd>
  <dt>You can view my Profile on LinkedIn </dt>
    <dd><a href="http://www.linkedin.com/in/ingber"><img
      src="http://www.ingber.com/linkedin.gif" alt="[LinkedIn]" width="100"
      border="0">http://www.linkedin.com/in/ingber</a> </dd>
  <dt>You can view my
      <a href="http://google.com/+Ingber?rel=publisher"></a>Google+ Profile at </dt>
    <dd><a href="http://google.com/+LesterIngber/about"><img
      src="http://www.ingber.com/googleplus.jpg" alt="[Google+]" width="100"
      border="0">http://google.com/+LesterIngber/about</a> </dd>
    <dd>This profile also contains links to some of my other networking sites. 
      <p></p>
    </dd>
  <dt>You can view my Facebook Profile at </dt>
    <dd><a href="http://www.facebook.com/lester.ingber/about"><img
      src="http://www.ingber.com/facebook.jpg" alt="[Facebook]" width="50"
      border="0">http://www.facebook.com/lester.ingber/about</a> </dd>
    <dd>This profile also contains links to some of my other networking sites. 
      <p></p>
    </dd>
  <dt><img src="http://www.ingber.com/bths.jpg" alt="[BTHS Alumni]" width="90"
  border="0"> I'm an alumnus of Brooklyn Technical High School (BTHS) class of
  1958. </dt>
    <dd><p></p>
    </dd>
  <dt><img src="http://www.ingber.com/caltech.jpg" alt="[Caltech Alumni]"
  width="200" border="0"> I'm an alumnus of California Institute of Technology
  (Caltech) class of 1962 (Physics). </dt>
    <dd><p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/ingber82_LegendsOfCaltech.pdf">ingber82_LegendsOfCaltech.pdf</a>
  reports another "incident" </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T The OXY cornerstone </dd>
    <dd>%B Legends of Caltech </dd>
    <dd>%E W.A. Dodge, Jr. </dd>
    <dd>%E R.B. Moulton </dd>
    <dd>%E H.W. Sigworth </dd>
    <dd>%E A.C. Smith, Jr. </dd>
    <dd>%I Alumni Association, California Institute of Technology </dd>
    <dd>%C Pasadena, CA </dd>
    <dd>%D 1982 </dd>
    <dd>%P 28 </dd>
    <dd>%O ISBN ISBN 0-215-12345-X. URL
      http://www.ingber.com/ingber82_LegendsOfCaltech.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/ingber12_EandS.pdf">ingber12_EandS.pdf</a>
  classmate reports Les(ter) in Chem Lab</dt>
    <dd>%A Anonymous </dd>
    <dd>%T Endnotes </dd>
    <dd>%J (Caltech) Engineering and Science </dd>
    <dd>%V 75 </dd>
    <dd>%D 2012 </dd>
    <dd>%P 44 </dd>
    <dd>%O URL http://www.ingber.com/ingber12_EandS.pdf 
      <p></p>
    </dd>
  <dt><img src="http://www.ingber.com/ucsd.jpg" alt="[UCSD Alumni]" width="70"
  border="0"> I'm an alumnus of UC San Diego (UCSD) Graduate School class of
  1966 (Theoretical Nuclear Physics). </dt>
    <dd><p></p>
    </dd>
  <dt><img src="http://www.ingber.com/cal.jpg" alt="[UCB Alumni]" width="70"
  border="0"> I'm an alumnus of UC Berkeley (UCB), National Science Foundation
  (NSF) Physics Postdoctoral Fellow 1966-1968 (transferred second year to UC
  Los Angeles (UCLA)). </dt>
    <dd><p></p>
    </dd>
  <dt>[<a href="http://www.ingber.com/">To-Top-of-Archive</a>] </dt>
    <dd></dd>
</dl>

<h2><a name="KARATE">KARATE</a></h2>
<br>
<img src="http://www.ingber.com/LesterIngberKarate.jpg" alt="[Lester Ingber]"
width="200" border="0"> <b>Lester Ingber, 8th Dan</b> <br>

<dl>
  <dt><img src="http://www.ingber.com/jka.gif" alt="[JKA]" width="225"
  border="0"> I'm an alumnus of Japan Karate Association (JKA)/All American
  Karate Federation (AAKF) Instructor/Sensei School class of 1968 (the first
  JKA-AAKF class). </dt>
    <dd><p><img src="http://www.ingber.com/BlackBelt.jpg"
      alt="[Lester Ingber Black Belt]" width="100" border="0"> </p>
    </dd>
  <dt><a href="http://www.ingber.com/karate.html">karate.html</a> </dt>
    <dd>This file is a collection of notes and edited replies to some postings
      and e-mail on karate. 
      <p></p>
    </dd>
    <dd><a href="http://www.ingber.com/karate.txt">karate.txt</a> is an
      ASCII-formatted version of this file. 
      <p></p>
    </dd>
    <dt><a
  href="http://www.ingber.com/karate72_encinitas.jpg"><img
      src="http://www.ingber.com/karate72_encinitas3.jpg"
      alt="[Lester Ingber 1972]" width="100"
      border="0">karate72_encinitas.jpg</a>
  </dt>
    <dd>Lester Ingber photo</dd>
    <dd>Encinitas, CA</dd>
    <dd>30 Mar 1972</dd>
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/karate72_learning.pdf">karate72_learning.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Editorial: Learning to learn </dd>
    <dd>%J Explore </dd>
    <dd>%V 7 </dd>
    <dd>%P 5-8 </dd>
    <dd>%D 1972 </dd>
    <dd>%O URL http://www.ingber.com/karate72_learning.pdf 
      <p></p>
    </dd>
    <dd>Link from smni72_learning.pdf to karate72_learning.pdf. 
      <p>This set of courses includes my course using physics and attentional
      approaches to karate which was taught in my dogos, as well as in this
      UCSD course, since 1968. My 1976 book just below includes my 1968
      "Physics of Karate Techniques" Instructor's thesis for my JKA/AAKF
      Instructor's degree, as well as my techniques developed to promote the
      learning of attentional skills in parallel with this physics approach to
      the learning of traditional physical skills. </p>
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/karate76_book.html">karate76_book.html</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T The Karate Instructor's Handbook </dd>
    <dd>%I Physical Studies Institute-Institute for the Study of Attention </dd>
    <dd>%C Solana Beach, CA </dd>
    <dd>%D 1976 </dd>
    <dd>%O OCR-scanned with figures. URL
      http://www.ingber.com/karate76_book.html 
      <p><a
      href="http://www.ingber.com/karate76_book.txt">http://www.ingber.com/karate76_book.txt</a>
      is an ASCII-formatted version of this file without .jpg figures. </p>
      <p><a
      href="http://www.ingber.com/karate76_cover.gif">karate76_cover.gif</a> is
      a .gif rendering of the cover. </p>
      <p><a
      href="http://www.ingber.com/karate76_cover_back.gif">karate76_cover_back.gif</a>
      is a .gif rendering of the back cover. </p>
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/karate81_book.txt">karate81_book.txt</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Karate: Kinematics and Dynamics </dd>
    <dd>%I Unique </dd>
    <dd>%C Hollywood, CA </dd>
    <dd>%D 1981 </dd>
    <dd>%O ISBN 0-86568-025-6. OCR-scanned text-only version of this book. URL
      http://www.ingber.com/karate81_book.txt 
      <p><a
      href="http://www.ingber.com/karate81_cover.gif">karate81_cover.gif</a> is
      a .gif rendering of the cover. </p>
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/karate81_attention.pdf">karate81_attention.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Attention, physics and teaching </dd>
    <dd>%J Journal Social Biological Structures </dd>
    <dd>%V 4 </dd>
    <dd>%P 225-235 </dd>
    <dd>%D 1981 </dd>
    <dd>%O URL http://www.ingber.com/smni81_attention.pdf </dd>
    <dd>(This file may only be shared for research.) 
      <p></p>
    </dd>
    <dd>This paper describes a methodology and implementation of teaching a
      full curriculum of academics, fine arts and physical disciplines, also
      tested as described in <a
      href="http://www.ingber.com/smni72_learning.pdf">smni72_learning.pdf</a>,
      based in part on a similar methodology I developed for teaching karate,
      in turn based on my brain research. 
      <p></p>
    </dd>
    <dd>Links from smni81_attention.pdf to karate81_attention.pdf. 
      <p></p>
    </dd>
  <dt>1982 Training Videos</dt>
  <dd>Below are VHS converted tapes of a karate camp with five 1st Dan Black
  Belt students in the Summer of 1982, emphasizing sparring exercises which
  were eventually published in Elements of Advanced Karate, <a
  href="http://www.ingber.com/karate85_book.html">karate85_book.html</a> below.
  To make these files reasonable size they were converted to 320x240. (The
  files still are quite large at about 200MB each.) The sound is quite a bit
  out of sync with the video a lot of the time due to the device used to
  capture the VHS recording. The sequence of files below follows the order of
  training sessions, each about 1.5 hours, in this special camp. </dd>
  <dd>As in any person-person combat, important intense events can take place
  within what often superficially appears to a non-interacting observer as a
  flow of somewhat ordinary movement. It takes some patience to view several
  hours of actual classes to see and appreciate crucial moments in these
  exercises that reflect the gradual influence of advanced training. </dd>
  <dd>These videos may be viewed as full training sessions from my Drive archive </dd>
    <dd><a
      href="https://docs.google.com/file/d/1yGIjd6Om7TvAtw2dxkclWqErq-6ta33CrHh890lvtgqis3O7oO-k-Jk/edit?usp=sharing">karate82_exam</a>
      <br>
    </dd>
    <dd><a
      href="https://docs.google.com/file/d/1w2CA0CWzDEunU96fti0OdfbXGxmR5n2FJdRzoNhcCMoj17G1oj_Hyqw/edit?usp=sharing">karate82_Earth</a>
      <br>
    </dd>
    <dd><a
      href="https://docs.google.com/file/d/1qiE6lzzgn3MtGkJrHbSztiaQ_mBYnenCP6Z3dGfkfQ6MI7xrUit6qsA/edit?usp=sharing">karate82_Wind</a>
      <br>
    </dd>
    <dd><a
      href="https://docs.google.com/file/d/1eAqT1Lk7uwNu7NZsKYHvpqSM9IVp2aosaCN_ShGV8XjLI2K6SrmsGrc/edit?usp=sharing">karate82_Fire</a>
      <br>
    </dd>
    <dd><a
      href="https://docs.google.com/file/d/1ddfTAl6Wjxdkz6C6LZ8kCreoZnfSRzb4A9W4u__0LNxBE3-MRV4up_4/edit?usp=sharing">karate82_Water</a>
      <br>
    </dd>
    <dd><a
      href="https://docs.google.com/file/d/1ABFlXSEUTRkqhSta-Z-MW3cNz34WPpym-Agd5RkH3puFnVeSzi7Z-3s/edit?usp=sharing">karate82_Void</a>
      <p></p>
    </dd>

  <dd>These videos can be downloaded from my Sites archive, where they have been
  conveniently cut into playable parts less than 50 MBytes each: <br>
  <dd>
    <dd><a
      href="http://lester-x.ingber.com/karate82_exam">karate82_exam.mpg</a> <br>
    </dd>
    <dd><a
      href="http://lester-x.ingber.com/karate82_earth">karate82_Earth.mpg</a>
      <br>
    </dd>
    <dd><a
      href="http://lester-x.ingber.com/karate82_wind">karate82_Wind.mpg</a> <br>
    </dd>
    <dd><a
      href="http://lester-x.ingber.com/karate82_fire">karate82_Fire.mpg</a> <br>
    </dd>
    <dd><a
      href="http://lester-x.ingber.com/karate82_water">karate82_Water.mpg</a>
      <br>
    </dd>
    <dd><a
      href="http://lester-x.ingber.com/karate82_void">karate82_Void.mpg</a>
    <p></p>
    </dd>

  <dd>These videos also can be viewed on my youtube account, where they have
  been cut into playable parts less than 14 minutes each. </dd>
  <dd><a
  href="http://www.youtube.com/user/ingber#p/p">http://www.youtube.com/user/ingber#p/p</a></dd>
  <dd>
  <p></p>
  </dd>
  <dt><a href="http://www.ingber.com/karate85_book.html">karate85_book.html</a> 
    <dd>%A L. Ingber </dd>
    <dd>%T Elements of Advanced Karate </dd>
    <dd>%I Ohara </dd>
    <dd>%C Burbank, CA </dd>
    <dd>%D 1985 </dd>
    <dd>%O ISBN 0-89750-127-6. OCR scanned with photos. URL
      http://www.ingber.com/karate85_book.html 
      <p></p>
    </dd>
    <dd>Note the video files above that recorded development of these exercises
      in 1982. 
      <p><a
      href="http://www.ingber.com/karate85_book.txt">http://www.ingber.com/karate85_book.txt</a>
      is an ASCII-formatted version of this file without .jpg photos. </p>
      <p><a
      href="http://www.ingber.com/karate85_cover.pdf">karate85_cover.pdf</a> or
      <a
      href="http://www.ingber.com/karate85_cover.ps.gz">karate85_cover.ps.gz</a>
      is a PDF or PostScript rendering of the photo on the cover; </p>
      <p><a
      href="http://www.ingber.com/karate85_cover.gif">karate85_cover.gif</a> is
      a .gif rendering of karate85_cover.ps. </p>
      <p><a
      href="http://www.ingber.com/karate85_cover.txt">karate85_cover.txt</a> is
      a coarse ASCII rendering of karate85_cover.ps. </p>
      <p><a
      href="http://www.ingber.com/karate85_photo.jpg">karate85_photo.jpg</a> is
      a jpeg (.jpg) rendering of a photo taken at the same time as
      karate85_cover.ps. </p>
      <p></p>
    </dd>
    <dt><a
    href="http://www.ingber.com/karate00_keri_no_kata.html">karate00_keri_no_kata.html</a>
    </dt>
      <dd>%A L. Ingber </dd>
      <dd>%T Keri No Kata </dd>
      <dd>%J Shotokan Research Society International (SRSI) </dd>
      <dd>%V 1 </dd>
      <dd>%N 4 </dd>
      <dd>%D 2000 </dd>
      <dd>%O URL http://www.ingber.com/karate00_keri_no_kata.html 
        <p></p>
      </dd>
      <dd>Keri No Kata is a kicking form, containing many different kicking and
        other leg techniques in a context of full sparring interactions. 
        <p></p>
      </dd>
      <dd>When accessed via WWW, karate00_keri_no_kata.html loads 1 MByte of
        photos. 
        <p></p>
      </dd>
    <dt>[<a href="http://www.ingber.com/">To-Top-of-Archive</a>] </dt>
      <dd></dd>
  </dl>

<h2><a name="ASA">ASA</a></h2>
<br>
<img src="http://www.ingber.com/asa.jpg" alt="[ASA]" width="100" border="0"> 

<h4><a name="ASA-CODE">ASA-CODE</a></h4>
<dl>
  <dt>Adaptive Simulated Annealing (ASA) </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Adaptive Simulated Annealing (ASA) </dd>
    <dd>%R Global optimization C-code </dd>
    <dd>%I Caltech Alumni Association </dd>
    <dd>%C Pasadena, CA </dd>
    <dd>%D 1993 </dd>
    <dd>%O URL http://www.ingber.com/#ASA-CODE 
      <p></p>
    </dd>
  <dt>Mirrors of the ASA code are at </dt>
    <dd><a href="http://alumni.caltech.edu/~ingber/"><img
      src="http://www.ingber.com/caltech.jpg"
      alt="[alumni.caltech.edu/~ingber]" width="125" height="37" border="0">
      http://alumni.caltech.edu/~ingber</a> </dd>
    <dd><a href="http://asa-caltech.sourceforge.net/"><img
      src="http://www.ingber.com/sflogo.jpg" alt="[SourceForge.net]"
      width="125" height="37" border="0">
      http://asa-caltech.sourceforge.net</a> </dd>
    <dd><a href="https://code.google.com/p/adaptive-simulated-annealing"><img
      src="http://www.ingber.com/googleproject.jpg" alt="[code.google.com]"
      width="125" height="37" border="0">
      https://code.google.com/p/adaptive-simulated-annealing</a> (Closed 14 Jan 2014) 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/ASA-README.html">ASA-README.html</a> </dt>
    <dd>HTML version of the ASA-README file included with the current ASA-30.6
      code. Cross-references are local to this file, so you may view it under a
      local browser. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/ASA-README.pdf">ASA-README.pdf</a> or <a
  href="http://www.ingber.com/ASA-README.ps.gz">ASA-README.ps.gz</a> </dt>
    <dd>PDF ASA-README.pdf and gzip-compressed PostScript ASA-README.ps files
      included with the current ASA-30.6 code. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/ASA-README.txt">ASA-README.txt</a> </dt>
    <dd>ASCII ASA-README file included with the current ASA-30.6 code. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/ASA.tar.gz">ASA.tar.gz</a> </dt>
    <dd>gzip tarfile of the current ASA-30.6 code. All files expand into
      directory ASA. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/ASA.zip">ASA.zip</a> </dt>
    <dd>zip file of the current ASA-30.6 code. All files are processed for DOS
      format. All files expand into directory ASA. 
      <p></p>
    </dd>
    <dd></dd>
</dl>
<dl>
  <dt><a href="http://www.ingber.com/asa_new.txt">asa_new.txt</a> </dt>
    <dd>List of major recent changes in ASA. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/asa_papers.html">asa_papers.html</a> </dt>
    <dd>Addendum to the NOTES file in the ASA code, listing some patents and
      papers that have used ASA or its precursor VFSR. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/asa_retrieve.txt">asa_retrieve.txt</a>
  </dt>
    <dd>This contains instructions for retrieval of the ASA code. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/asa_contrib.txt">asa_contrib.txt</a> </dt>
    <dd>This is an addendum to the NOTES file in the ASA code, giving some code
      contributed by users that may be useful to others. I have included my
      first Very Fast Simulated Re-annealing (VFSR) code prepared in 1987,
      RATFOR vfsr.r and vfsr_com.r code (unsupported), subsequently compiled
      into FORTRAN to run on a Lawrence Livermore supercomputer. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/asa_examples.txt">asa_examples.txt</a>
  </dt>
    <dd>This is an addendum to the NOTES file in the ASA code, giving some
      examples of use of the ASA code. 
      <p></p>
    </dd>
  <dt>[<a href="http://www.ingber.com/">To-Top-of-Archive</a>] </dt>
    <dd></dd>
</dl>

<h4><a name="ASA-REPRINTS">ASA-REPRINTS</a></h4>
<dl>
  <dt><a href="http://www.ingber.com/asa89_vfsr.pdf">asa89_vfsr.pdf</a> or <a
  href="http://www.ingber.com/asa89_vfsr.ps.gz">asa89_vfsr.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Very fast simulated re-annealing </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 12 </dd>
    <dd>%N 8 </dd>
    <dd>%P 967-973 </dd>
    <dd>%D 1989 </dd>
    <dd>%O URL http://www.ingber.com/asa89_vfsr.pdf 
      <p></p>
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/asa92_mnn.pdf">asa92_mnn.pdf</a> or <a
  href="http://www.ingber.com/asa92_mnn.ps.gz">asa92_mnn.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Generic mesoscopic neural networks based on statistical mechanics of
      neocortical interactions </dd>
    <dd>%J Physical Review A </dd>
    <dd>%V 45 </dd>
    <dd>%N 4 </dd>
    <dd>%P R2183-R2186 </dd>
    <dd>%D 1992 </dd>
    <dd>%O URL http://www.ingber.com/smni92_mnn.pdf 
      <p></p>
    </dd>
    <dd>Links from smni92_mnn.pdf and smni92_mnn.ps.gz to asa92_mnn.pdf and
      asa92_mnn.ps.gz. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/asa92_saga.pdf">asa92_saga.pdf</a> or <a
  href="http://www.ingber.com/asa92_saga.ps.gz">asa92_saga.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A B. Rosen </dd>
    <dd>%T Genetic algorithms and very fast simulated reannealing: A comparison
    </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 16 </dd>
    <dd>%N 11 </dd>
    <dd>%P 87-100 </dd>
    <dd>%D 1992 </dd>
    <dd>%O URL http://www.ingber.com/asa92_saga.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/asa93_sapvt.pdf">asa93_sapvt.pdf</a> or <a
  href="http://www.ingber.com/asa93_sapvt.ps.gz">asa93_sapvt.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Simulated annealing: Practice versus theory </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 18 </dd>
    <dd>%N 11 </dd>
    <dd>%D 1993 </dd>
    <dd>%P 29-57 </dd>
    <dd>%O URL http://www.ingber.com/asa93_sapvt.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/asa96_lessons.pdf">asa96_lessons.pdf</a>
  or <a
  href="http://www.ingber.com/asa96_lessons.ps.gz">asa96_lessons.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Adaptive simulated annealing (ASA): Lessons learned </dd>
    <dd>%J Control and Cybernetics </dd>
    <dd>%V 25 </dd>
    <dd>%N 1 </dd>
    <dd>%P 33-54 </dd>
    <dd>%D 1996 </dd>
    <dd>%O URL http://www.ingber.com/asa96_lessons.pdf 
      <p></p>
    </dd>
    <dd>Invited paper to a special issue of Control and Cybernetics on
      "Simulated Annealing Applied to Combinatorial Optimization." The listing
      of other contributors to this special issue is in <a
      href="http://www.ingber.com/asa96_vidal_nahorski.txt">asa96_vidal_nahorski.txt</a> 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/asa01_lecture.html">asa01_lecture.html</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Adaptive Simulated Annealing (ASA) and Path-Integral (PATHINT)
      Algorithms: Generic Tools for Complex Systems </dd>
    <dd>%R ASA-PATHINT Lecture Plates </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%D 2001 </dd>
    <dd>%O Invited talk U Calgary, Canada, April 2001. URL
      http://www.ingber.com/asa01_lecture.pdf 
      <p></p>
    </dd>
    <dd><a href="http://www.ingber.com/asa01_lecture.pdf">asa01_lecture.pdf</a>
      or <a
      href="http://www.ingber.com/asa01_lecture.ps.gz">asa01_lecture.ps.gz</a>
      offers an alternative PDF or PostScript format. 
      <p></p>
    </dd>
    <dd>Links from asa01_lecture.html, asa01_lecture.pdf and
      asa01_lecture.ps.gz, to path01_lecture.html, path01_lecture.pdf and
      path01_lecture.ps.gz. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/asa03_reinforce.pdf">asa03_reinforce.pdf</a> </dt>
    <dd>%A A.F. Atiya </dd>
    <dd>%A A.G. Parlos </dd>
    <dd>%A L. Ingber </dd>
    <dd>%T A reinforcement learning method based on adaptive simulated
      annealing </dd>
    <dd>%B Proceedings International Midwest Symposium on Circuits and Systems
      (MWCAS), December 2003 </dd>
    <dd>%I IEEE CAS </dd>
    <dd>%C Cairo, Egypt </dd>
    <dd>%D 2003 </dd>
    <dd>%O URL http://www.ingber.com/asa03_reinforce.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/asa06_ism.pdf">asa06_ism.pdf</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Ideas by statistical mechanics (ISM) </dd>
    <dd>%R Report 2006:ISM </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%D 2006 </dd>
    <dd>%O URL http://www.ingber.com/smni06_ism.pdf 
      <p></p>
    </dd>
    <dd>ISM integrates previous projects to model evolution and propagation of
      ideas/patterns throughout populations subjected to endogenous and
      exogenous interactions. A short version appears as "AI and Ideas by
      Statistical Mechanics (ISM)" in the Encyclopedia of Artificial
      Intelligence, pp. 58-64 (2008), and details in this paper appear in
      "Ideas by Statistical Mechanics (ISM)", Journal of Integrated Systems
      Design and Process Science, Vol. 11, No. 3, pp. 31-54 (2007), Special
      Issue: Biologically Inspired Computing. 
      <p></p>
    </dd>
    <dd>Links from smni06_ism.pdf to asa06_ism.pdf and markets06_ism.pdf. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/asa11_options.pdf">asa11_options.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Adaptive Simulated Annealing </dd>
    <dd>%B Stochastic global optimization and its applications with fuzzy
      adaptive simulated annealing </dd>
    <dd>%E H.A. Oliveira, Jr. </dd>
    <dd>%E A. Petraglia </dd>
    <dd>%E L. Ingber </dd>
    <dd>%E M.A.S. Machado </dd>
    <dd>%E M.R. Petraglia </dd>
    <dd>%I Springer </dd>
    <dd>%C New York </dd>
    <dd>%D 2012 </dd>
    <dd>%P 33-61 </dd>
    <dd>%O Invited Paper. http://www.ingber.com/asa11_options.pdf 
      <p></p>
    </dd>
  <dt>[<a href="http://www.ingber.com/">To-Top-of-Archive</a>] </dt>
    <dd></dd>
</dl>

<h2><a name="COMBAT">COMBAT</a></h2>
<dl>
  <dt><a href="http://www.ingber.com/combat85_smart.pdf">combat85_smart.pdf</a>
  or <a
  href="http://www.ingber.com/combat85_smart.ps.gz">combat85_smart.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics algorithm for response to targets (SMART) </dd>
    <dd>%B Workshop on Uncertainty and Probability in Artificial Intelligence:
      UC Los Angeles, 14-16 August 1985 </dd>
    <dd>%I American Association for Artificial Intelligence </dd>
    <dd>%C Menlo Park, CA </dd>
    <dd>%D 1985 </dd>
    <dd>%P 258-264 </dd>
    <dd>%O URL http://www.ingber.com/combat85_smart.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/combat86_approach.pdf">combat86_approach.pdf</a>
  or <a
  href="http://www.ingber.com/combat86_approach.ps.gz">combat86_approach.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Nonlinear nonequilibrium statistical mechanics approach to C3
      systems </dd>
    <dd>%B 9th MIT/ONR Workshop on C3 Systems: Naval Postgraduate School,
      Monterey, CA, 2-5 June 1986 </dd>
    <dd>%P 237-244 </dd>
    <dd>%D 1986 </dd>
    <dd>%I MIT </dd>
    <dd>%C Cambridge, MA </dd>
    <dd>%O URL http://www.ingber.com/combat86_approach.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/combat91_data.pdf">combat91_data.pdf</a>
  or <a
  href="http://www.ingber.com/combat91_data.ps.gz">combat91_data.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A H. Fujio </dd>
    <dd>%A M.F. Wehner </dd>
    <dd>%T Mathematical comparison of combat computer models to exercise data
    </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 15 </dd>
    <dd>%N 1 </dd>
    <dd>%P 65-90 </dd>
    <dd>%D 1991 </dd>
    <dd>%O URL http://www.ingber.com/combat91_data.pdf 
      <p></p>
    </dd>
    <dd>Links from combat91_data.pdf and combat91_data.ps.gz to path91_data.pdf
      and path91_data.ps.gz. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/combat91_human.pdf">combat91_human.pdf</a>
  or <a
  href="http://www.ingber.com/combat91_human.ps.gz">combat91_human.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A D.D. Sworder </dd>
    <dd>%T Statistical mechanics of combat with human factors </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 15 </dd>
    <dd>%N 11 </dd>
    <dd>%D 1991 </dd>
    <dd>%P 99-127 </dd>
    <dd>%O URL http://www.ingber.com/combat91_human.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/combat93_c3sci.pdf">combat93_c3sci.pdf</a>
  or <a
  href="http://www.ingber.com/combat93_c3sci.ps.gz">combat93_c3sci.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of combat and extensions </dd>
    <dd>%B Toward a Science of Command, Control, and Communications </dd>
    <dd>%E C. Jones </dd>
    <dd>%I American Institute of Aeronautics and Astronautics </dd>
    <dd>%C Washington, D.C. </dd>
    <dd>%D 1993 </dd>
    <dd>%P 117-149 </dd>
    <dd>%O ISBN 1-56347-068-3. URL http://www.ingber.com/combat93_c3sci.pdf 
      <p></p>
    </dd>
    <dd>This summarizes a series of papers started in 1985, which led to the
      baselining of JANUS(T) simulation to National Training Center (NTC) data. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/combat97_cmi.pdf">combat97_cmi.pdf</a> or
  <a href="http://www.ingber.com/combat97_cmi.ps.gz">combat97_cmi.ps.gz</a>
  </dt>
    <dd>%A M. Bowman </dd>
    <dd>%A L. Ingber </dd>
    <dd>%T Canonical momenta of nonlinear combat </dd>
    <dd>%B Proceedings of the 1997 Simulation Multi-Conference, 6-10 April
      1997, Atlanta, GA </dd>
    <dd>%I Society for Computer Simulation </dd>
    <dd>%C San Diego, CA </dd>
    <dd>%D 1997 </dd>
    <dd>%O URL http://www.ingber.com/combat97_cmi.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/combat01_lecture.html">combat01_lecture.html</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical Mechanics of Combat (SMC): Mathematical Comparison of
      Computer Models to Exercise Data </dd>
    <dd>%R SMC Lecture Plates </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%D 2001 </dd>
    <dd>%O URL http://www.ingber.com/combat01_lecture.pdf 
      <p></p>
    </dd>
    <dd><a
      href="http://www.ingber.com/combat01_lecture.pdf">combat01_lecture.pdf</a>
      or <a
      href="http://www.ingber.com/combat01_lecture.ps.gz">combat01_lecture.ps.gz</a>
      offers an alternative PDF or PostScript format. 
      <p></p>
    </dd>
  <dt>[<a href="http://www.ingber.com/">To-Top-of-Archive</a>] </dt>
    <dd></dd>
</dl>

<h2><a name="MARKETS">MARKETS</a></h2>
<a name="markets84-statmech"></a> 

<p>Upon a reasonable request, I may make some of my MARKETS/TRD codes available for collaboration.  </p>

<dl>
  <dt><a
  href="http://www.ingber.com/markets84_statmech.pdf">markets84_statmech.pdf</a>
  or <a
  href="http://www.ingber.com/markets84_statmech.ps.gz">markets84_statmech.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of nonlinear nonequilibrium financial markets
    </dd>
    <dd>%J Mathematical Modelling </dd>
    <dd>%V 5 </dd>
    <dd>%N 6 </dd>
    <dd>%P 343-361 </dd>
    <dd>%D 1984 </dd>
    <dd>%O URL http://www.ingber.com/markets84_statmech.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets90_interest.pdf">markets90_interest.pdf</a>
  or <a
  href="http://www.ingber.com/markets90_interest.ps.gz">markets90_interest.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanical aids to calculating term structure models
    </dd>
    <dd>%J Physical Review A </dd>
    <dd>%V 42 </dd>
    <dd>%N 12 </dd>
    <dd>%D 1990 </dd>
    <dd>%P 7057-7064 </dd>
    <dd>%O URL http://www.ingber.com/markets90_interest.pdf 
      <p></p>
    </dd>
    <dd>markets90_interest was the first paper on finance published in Physical
      Review. 
      <p><a name="markets91-interest"></a> </p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets91_interest.pdf">markets91_interest.pdf</a>
  or <a
  href="http://www.ingber.com/markets91_interest.ps.gz">markets91_interest.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A M.F. Wehner </dd>
    <dd>%A G.M. Jabbour </dd>
    <dd>%A T.M. Barnhill </dd>
    <dd>%T Application of statistical mechanics methodology to term-structure
      bond-pricing models </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 15 </dd>
    <dd>%N 11 </dd>
    <dd>%D 1991 </dd>
    <dd>%P 77-98 </dd>
    <dd>%O URL http://www.ingber.com/markets91_interest.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets96_brief.pdf">markets96_brief.pdf</a> or
  <a
  href="http://www.ingber.com/markets96_brief.ps.gz">markets96_brief.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Trading markets with canonical momenta and adaptive simulated
      annealing </dd>
    <dd>%R Report 1996:TMCMASA </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%C McLean, VA </dd>
    <dd>%D 1996 </dd>
    <dd>%O URL http://www.ingber.com/markets96_brief.pdf 
      <p></p>
    </dd>
    <dd>This paper gives relatively non-technical descriptions of ASA and
      canonical momenta, and their applications to markets and EEG. The paper
      was solicited by AI in Finance prior to cessation of their publication. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets96_momenta.pdf">markets96_momenta.pdf</a>
  or <a
  href="http://www.ingber.com/markets96_momenta.ps.gz">markets96_momenta.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Canonical momenta indicators of financial markets and neocortical
      EEG </dd>
    <dd>%B Progress in Neural Information Processing </dd>
    <dd>%E S.-I. Amari, L. Xu, I. King, and K.-S. Leung </dd>
    <dd>%I Springer </dd>
    <dd>%C New York </dd>
    <dd>%P 777-784 </dd>
    <dd>%D 1996 </dd>
    <dd>%O Invited paper to the 1996 International Conference on Neural
      Information Processing (ICONIP'96), Hong Kong, 24-27 September 1996. ISBN
      981 3083-05-0. URL http://www.ingber.com/markets96_momenta.pdf 
      <p></p>
    </dd>
    <dd>Tables of data supporting this paper are given in <a
      href="http://www.ingber.com/markets96_momenta_tbl.txt.gz">markets96_momenta_tbl.txt.gz</a> 
      <p></p>
    </dd>
    <dd><a
      href="http://www.ingber.com/markets96_lag_cmi.c">markets96_lag_cmi.c</a>
      contains C-code for the Lagrangian cost function described in
      /markets96_momenta.ps.gz to be fit to data. Also included is code for the
      CMI derived from this Lagrangian. 
      <p></p>
    </dd>
    <dd>This paper advances the work described in markets96_trading.ps.gz. A
      brief introduction to canonical momenta is included in <a
      href="http://www.ingber.com/ingber_projects.html">ingber_projects.html</a> 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets96_trading.pdf">markets96_trading.pdf</a>
  or <a
  href="http://www.ingber.com/markets96_trading.ps.gz">markets96_trading.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of nonlinear nonequilibrium financial markets:
      Applications to optimized trading </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 23 </dd>
    <dd>%N 7 </dd>
    <dd>%P 101-121 </dd>
    <dd>%D 1996 </dd>
    <dd>%O URL http://www.ingber.com/markets96_trading.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets98_smfm_appl.pdf">markets98_smfm_appl.pdf</a>
  or <a
  href="http://www.ingber.com/markets98_smfm_appl.ps.gz">markets98_smfm_appl.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Some Applications of Statistical Mechanics of Financial Markets </dd>
    <dd>%R LIR-98-1-SASMFM </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%C Chicago, IL </dd>
    <dd>%D 1998 </dd>
    <dd>%O URL http://www.ingber.com/markets98_smfm_appl.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets98_lecture.pdf">markets98_lecture.pdf</a>
  or <a
  href="http://www.ingber.com/markets98_lecture.ps.gz">markets98_lecture.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of financial markets (SMFM) </dd>
    <dd>%R SMFM Lecture Plates </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%C Chicago, IL </dd>
    <dd>%D 1998 </dd>
    <dd>%O Update invited talk to U of Chicago Financial Mathematics Seminar,
      20 Nov 1998. URL http://www.ingber.com/markets98_lecture.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets99_spread.pdf">markets99_spread.pdf</a> or
  <a
  href="http://www.ingber.com/markets99_spread.ps.gz">markets99_spread.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T A simple options training model </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 30 </dd>
    <dd>%P 167-182 </dd>
    <dd>%D 1999 </dd>
    <dd>%O URL http://www.ingber.com/markets99_spread.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/markets99_vol.pdf">markets99_vol.pdf</a>
  or <a
  href="http://www.ingber.com/markets99_vol.ps.gz">markets99_vol.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A J.K. Wilson </dd>
    <dd>%T Volatility of volatility of financial markets </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 29 </dd>
    <dd>%P 39-57 </dd>
    <dd>%D 1999 </dd>
    <dd>%O URL http://www.ingber.com/markets99_vol.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/markets00_exp.pdf">markets00_exp.pdf</a>
  or <a
  href="http://www.ingber.com/markets00_exp.ps.gz">markets00_exp.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A J.K. Wilson </dd>
    <dd>%T Statistical mechanics of financial markets: Exponential
      modifications to Black-Scholes </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 31 </dd>
    <dd>%N 8/9 </dd>
    <dd>%P 167-192 </dd>
    <dd>%D 2000 </dd>
    <dd>%O URL http://www.ingber.com/markets00_exp.pdf 
      <p></p>
    </dd>
    <dd>Links from markets00_exp.pdf and markets00_exp.ps.gz to path00_exp.pdf
      and path00_exp.ps.gz. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets00_highres.pdf">markets00_highres.pdf</a>
  or <a
  href="http://www.ingber.com/markets00_highres.ps.gz">markets00_highres.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T High-resolution path-integral development of financial options </dd>
    <dd>%J Physica A </dd>
    <dd>%V 283 </dd>
    <dd>%N 3-4 </dd>
    <dd>%P 529-558 </dd>
    <dd>%D 2000 </dd>
    <dd>%O URL http://www.ingber.com/markets00_highres.pdf 
      <p></p>
    </dd>
    <dd>Links from markets00_highres.pdf and markets00_highres.ps.gz to
      path00_highres.pdf and path00_highres.ps.gz. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets01_optim_trading.pdf">markets01_optim_trading.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A R.P. Mondescu </dd>
    <dd>%T Optimization of Trading Physics Models of Markets </dd>
    <dd>%D 2001 </dd>
    <dd>%V 12 </dd>
    <dd>%N 4 </dd>
    <dd>%P 776-790 </dd>
    <dd>%J IEEE Trans. Neural Networks </dd>
    <dd>%O Invited paper for special issue on Neural Networks in Financial
      Engineering. URL http://www.ingber.com/markets01_optim_trading.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets01_pathtree.pdf">markets01_pathtree.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A C. Chen </dd>
    <dd>%A R.P. Mondescu </dd>
    <dd>%A D. Muzzall </dd>
    <dd>%A M. Renedo </dd>
    <dd>%T Probability tree algorithm for general diffusion processes </dd>
    <dd>%J Physical Review E </dd>
    <dd>%V 64 </dd>
    <dd>%N 5 </dd>
    <dd>%P 056702-056707 </dd>
    <dd>%D 2001 </dd>
    <dd>%O URL http://www.ingber.com/path01_pathtree.pdf 
      <p></p>
    </dd>
    <dd>My PATHTREE algorithm, like my PATHINT code, accurately propagates
      quite generally nonlinear multiplicative-noise probability distributions
      defined as a path integral, but PATHTREE is very fast and very easy to
      implement. 
      <p></p>
    </dd>
    <dd>Link from path01_pathtree.pdf to markets01_pathtree.pdf. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets01_lecture.html">markets01_lecture.html</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical Mechanics of Financial Markets (SMFM): Applications to
      Trading Indicators and Options </dd>
    <dd>%R SMFM Lecture Plates </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%D 2001 </dd>
    <dd>%O Invited talk U Calgary, Canada, April 2001. Invited talk U Florida,
      Gainesville, April 2002. Invited talk Tulane U, New Orleans, January
      2003. URL http://www.ingber.com/markets01_lecture.pdf 
      <p></p>
    </dd>
    <dd><a
      href="http://www.ingber.com/markets01_lecture.pdf">markets01_lecture.pdf</a>
      or <a
      href="http://www.ingber.com/markets01_lecture.ps.gz">markets01_lecture.ps.gz</a>
      offers an alternative PDF or PostScript format. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets02_portfolio.pdf">markets02_portfolio.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of portfolios of options </dd>
    <dd>%D 2002 </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%C Chicago, IL </dd>
    <dd>%O URL http://www.ingber.com/markets02_portfolio.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/markets03_automated.pdf">markets03_automated.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A R.P. Mondescu </dd>
    <dd>%B Intelligent Internet-Based Information Processing Systems </dd>
    <dd>%T Automated internet trading based on optimized physics models of
      markets </dd>
    <dd>%E R.J. Howlett </dd>
    <dd>%E N.S. Ichalkaranje </dd>
    <dd>%E L.C. Jain </dd>
    <dd>%E G. Tonfoni </dd>
    <dd>%I World Scientific </dd>
    <dd>%C Singapore </dd>
    <dd>%D 2003 </dd>
    <dd>%P 305-356 </dd>
    <dd>%O Invited paper. URL http://www.ingber.com/markets03_automated.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/markets05_trd.pdf">markets05_trd.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Trading in Risk Dimensions (TRD) </dd>
    <dd>%R Report 2005:TRD </dd>
    <dd>%D 2005 </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%C Ashland, OR </dd>
    <dd>%O URL http://www.ingber.com/markets05_trd.pdf 
      <p></p>
    </dd>
    <dd>A shorter updated paper with this title appears in the Handbook of
      Trading: Strategies for Navigating and Profiting from Currency, Bond, and
      Stock Markets published by McGraw-Hill (2010). Only the Table of Contents
      and the Abstract appear in http://www.ingber.com/markets05_trd.pdf, as
      the publisher has requested all previous markets_trd_report.pdf papers be
      removed from the internet. (This report is not online and is occasionally
      updated. Contact me for more information.) 
      <p>A pdf of the TRD section of the ppt presentation is in </p>
    </dd>
    <dd><a
      href="http://www.ingber.com/markets10_trd_present.pdf">markets10_trd_present.pdf</a> 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/markets06_ism.pdf">markets06_ism.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Ideas by statistical mechanics (ISM) </dd>
    <dd>%R Report 2006:ISM </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%D 2006 </dd>
    <dd>%O URL http://www.ingber.com/smni06_ism.pdf 
      <p></p>
    </dd>
    <dd>ISM integrates previous projects to model evolution and propagation of
      ideas/patterns throughout populations subjected to endogenous and
      exogenous interactions. A short version appears as "AI and Ideas by
      Statistical Mechanics (ISM)" in the Encyclopedia of Artificial
      Intelligence, pp. 58-64 (2008), and in "Ideas by Statistical Mechanics
      (ISM)", Journal of Integrated Systems Design and Process Science, Vol.
      11, No. 3, pp. 31-54 (2007), Special Issue: Biologically Inspired
      Computing. 
      <p></p>
    </dd>
    <dd>Links from smni06_ism.pdf to asa06_ism.pdf and markets06_ism.pdf. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/markets07_rops.pdf">markets07_rops.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Real Options for Project Schedules (ROPS) </dd>
    <dd>%R Report 2007:ROPS </dd>
    <dd>%D 2007 </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%O URL http://www.ingber.com/markets07_rops.pdf 
      <p></p>
    </dd>
    <dd>An updated invited paper is published in SGI Reflections International
      Journal of Science, Technology &amp; Management (2010). URL
      http://sgi.ac.in/colleges/newsletters/1146080820111637301.pdf 
      <p></p>
    </dd>
  <dt>[<a href="http://www.ingber.com/">To-Top-of-Archive</a>] </dt>
    <dd></dd>
</dl>

<h2><a name="NUCLEAR">NUCLEAR</a></h2>
<dl>
  <dt><a
  href="http://www.ingber.com/nuclear65_nonadiabatic.pdf">nuclear65_nonadiabatic.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Non-adiabatic corrections to the method of stationary states </dd>
    <dd>%J Phys. Rev. A </dd>
    <dd>%V 139 </dd>
    <dd>%P 35-39 </dd>
    <dd>%D 1965 </dd>
    <dd>%O URL http://www.ingber.com/nuclear65_nonadiabatic.pdf </dd>
    <dd>(This file may only be shared for research.) 
      <p>This was my first publication in a physics journal. </p>
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/nuclear68_forces.pdf">nuclear68_forces.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Nuclear Forces </dd>
    <dd>%J Physical Review </dd>
    <dd>%V 174 </dd>
    <dd>%P 1250-1263 </dd>
    <dd>%D 1968 </dd>
    <dd>%O URL http://www.ingber.com/nuclear68_forces.pdf </dd>
    <dd>(This file may only be shared for research.) 
      <p>The 1968 paper nuclear68_forces.pdf above is based on my 1966 Ph.D.
      thesis. </p>
      <p>Three papers from 1983-1986 calculated a contribution to the binding
      energy of nuclear matter induced by nonlinearities of realistic
      momentum-dependent nucleon-nucleon interactions, enhancing calculations I
      published in the 1960's based on my 1966 Ph.D. thesis (e.g., the 1968
      paper nuclear68_forces.pdf above). Those calculations used such
      interactions to detail properties of nucleon-nucleon scattering, deuteron
      bound state, nuclear matter, and neutron stars. </p>
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/nuclear83_riemann.pdf">nuclear83_riemann.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Riemannian corrections to velocity-dependent nuclear forces </dd>
    <dd>%J Physical Review C </dd>
    <dd>%V 28 </dd>
    <dd>%P 2536-2539 </dd>
    <dd>%D 1983 </dd>
    <dd>%O URL http://www.ingber.com/nuclear83_riemann.pdf </dd>
    <dd>(This file may only be shared for research.) 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/nuclear84_riemann.pdf">nuclear84_riemann.pdf</a>
  or <a
  href="http://www.ingber.com/nuclear84_riemann.ps.gz">nuclear84_riemann.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Path-integral Riemannian contributions to nuclear Schrodinger
      equation </dd>
    <dd>%J Physical Review D </dd>
    <dd>%V 29 </dd>
    <dd>%P 1171-1174 </dd>
    <dd>%D 1984 </dd>
    <dd>%O URL http://www.ingber.com/nuclear84_riemann.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/nuclear86_riemann.pdf">nuclear86_riemann.pdf</a>
  or <a
  href="http://www.ingber.com/nuclear86_riemann.ps.gz">nuclear86_riemann.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Riemannian contributions to short-ranged velocity-dependent
      nucleon-nucleon interactions </dd>
    <dd>%J Physical Review D </dd>
    <dd>%V 33 </dd>
    <dd>%P 3781-3784 </dd>
    <dd>%D 1986 </dd>
    <dd>%O URL http://www.ingber.com/nuclear86_riemann.pdf 
      <p></p>
    </dd>
  <dt>[<a href="http://www.ingber.com/">To-Top-of-Archive</a>] </dt>
    <dd></dd>
</dl>

<h2><a name="PATH-INTEGRAL">PATH-INTEGRAL</a></h2>

<p>Upon a reasonable request, I may make some of my PATHINT and PATHTREE codes available for collaboration.  </p>

<p>Papers <a href="#markets84-statmech">markets84_statmech.pdf</a>, <a
href="#markets91-interest">markets91_interest.pdf</a> and <a
href="#smni91-eeg">smni91_eeg.pdf</a> contain an Appendix giving a compact
derivation of the path-integral Lagrangian representation equivalent to the
Langevin rate-equation and Fokker-Planck/Schroedinger-type representations for
multivariate systems with nonlinear drifts and diffusions. </p>

<dl>
  <dt><a
  href="http://www.ingber.com/path86_GinsburgLandau.pdf">path86_GinsburgLandau.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Noise-induced extrema in time-dependent Ginsburg-Landau systems </dd>
    <dd>%J Math. Modelling </dd>
    <dd>%V 7 </dd>
    <dd>%P 525-528 </dd>
    <dd>%D 1986 </dd>
    <dd>%O URL http://www.ingber.com/path86_GinsburgLandau.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/path91_data.pdf">path91_data.pdf</a> or <a
  href="http://www.ingber.com/path91_data.ps.gz">path91_data.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A H. Fujio </dd>
    <dd>%A M.F. Wehner </dd>
    <dd>%T Mathematical comparison of combat computer models to exercise data
    </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 15 </dd>
    <dd>%N 1 </dd>
    <dd>%P 65-90 </dd>
    <dd>%D 1991 </dd>
    <dd>%O URL http://www.ingber.com/combat91_data.pdf 
      <p></p>
    </dd>
    <dd>Links from combat91_data.pdf and combat91_data.ps.gz to path91_data.pdf
      and path91_data.ps.gz. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/path94_stm.pdf">path94_stm.pdf</a> or <a
  href="http://www.ingber.com/path94_stm.ps.gz">path94_stm.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: Path-integral
      evolution of short-term memory </dd>
    <dd>%J Physical Review E </dd>
    <dd>%V 49 </dd>
    <dd>%N 5B </dd>
    <dd>%D 1994 </dd>
    <dd>%P 4652-4664 </dd>
    <dd>%O URL http://www.ingber.com/smni94_stm.pdf 
      <p></p>
    </dd>
    <dd>Higher resolution calculations performed with a supercomputer are
      presented in <a href="#path95-stm">path95_stm.pdf</a> (Links from
      smni95_stm.pdf and smni95_stm.ps.gz to path95_stm.pdf and
      path95_stm.ps.gz.) 
      <p></p>
    </dd>
    <dd>Links from smni94_stm.pdf and smni94_stm.ps.gz to path94_stm.pdf and
      path94_stm.ps.gz. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/path95_nonl.pdf">path95_nonl.pdf</a> or <a
  href="http://www.ingber.com/path95_nonl.ps.gz">path95_nonl.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Path-integral evolution of multivariate systems with moderate noise
    </dd>
    <dd>%J Physical Review E </dd>
    <dd>%V 51 </dd>
    <dd>%N 2 </dd>
    <dd>%P 1616-1619 </dd>
    <dd>%D 1995 </dd>
    <dd>%O URL http://www.ingber.com/path95_nonl.pdf 
      <p><a name="path95-stm"></a> </p>
    </dd>
  <dt><a href="http://www.ingber.com/path95_stm.pdf">path95_stm.pdf</a> or <a
  href="http://www.ingber.com/path95_stm.ps.gz">path95_stm.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A P.L. Nunez </dd>
    <dd>%T Statistical mechanics of neocortical interactions: High resolution
      path-integral calculation of short-term memory </dd>
    <dd>%J Physical Review E </dd>
    <dd>%V 51 </dd>
    <dd>%N 5 </dd>
    <dd>%P 5074-5083 </dd>
    <dd>%D 1995 </dd>
    <dd>%O URL http://www.ingber.com/smni95_stm.pdf 
      <p></p>
    </dd>
    <dd>Links from smni95_stm.pdf and smni95_stm.ps.gz to path95_stm.pdf and
      path95_stm.ps.gz. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/path96_duffing.pdf">path96_duffing.pdf</a>
  or <a
  href="http://www.ingber.com/path96_duffing.ps.gz">path96_duffing.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A R. Srinivasan </dd>
    <dd>%A P.L. Nunez </dd>
    <dd>%T Path-integral evolution of chaos embedded in noise: Duffing
      neocortical analog </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 23 </dd>
    <dd>%N 3 </dd>
    <dd>%P 43-53 </dd>
    <dd>%D 1996 </dd>
    <dd>%O URL http://www.ingber.com/path96_duffing.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/path98_datamining.pdf">path98_datamining.pdf</a>
  or <a
  href="http://www.ingber.com/path98_datamining.ps.gz">path98_datamining.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Data mining and knowledge discovery via statistical mechanics in
      nonlinear stochastic systems </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 27 </dd>
    <dd>%N 3 </dd>
    <dd>%P 9-31 </dd>
    <dd>%D 1998 </dd>
    <dd>%O URL http://www.ingber.com/path98_datamining.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/path00_exp.pdf">path00_exp.pdf</a> or <a
  href="http://www.ingber.com/path00_exp.ps.gz">path00_exp.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A J.K. Wilson </dd>
    <dd>%T Statistical mechanics of financial markets: Exponential
      modifications to Black-Scholes </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 31 </dd>
    <dd>%N 8/9 </dd>
    <dd>%P 167-192 </dd>
    <dd>%D 2000 </dd>
    <dd>%O URL http://www.ingber.com/markets00_exp.pdf 
      <p></p>
    </dd>
    <dd>Links from markets00_exp.pdf and markets00_exp.ps.gz to path00_exp.pdf
      and path00_exp.ps.gz. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/path00_highres.pdf">path00_highres.pdf</a>
  or <a
  href="http://www.ingber.com/path00_highres.ps.gz">path00_highres.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T High-resolution path-integral development of financial options </dd>
    <dd>%J Physica A </dd>
    <dd>%V 283 </dd>
    <dd>%N 3-4 </dd>
    <dd>%P 529-558 </dd>
    <dd>%D 2000 </dd>
    <dd>%O URL http://www.ingber.com/markets00_highres.pdf 
      <p></p>
    </dd>
    <dd>Links from markets00_highres.pdf and markets00_highres.ps.gz to
      path00_highres.pdf and path00_highres.ps.gz. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/path01_pathtree.pdf">path01_pathtree.pdf</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A C. Chen </dd>
    <dd>%A R.P. Mondescu </dd>
    <dd>%A D. Muzzall </dd>
    <dd>%A M. Renedo </dd>
    <dd>%T Probability tree algorithm for general diffusion processes </dd>
    <dd>%J Physical Review E </dd>
    <dd>%V 64 </dd>
    <dd>%N 5 </dd>
    <dd>%P 056702-056707 </dd>
    <dd>%D 2001 </dd>
    <dd>%O URL http://www.ingber.com/path01_pathtree.pdf 
      <p></p>
    </dd>
    <dd>My PATHTREE algorithm, like my PATHINT code, accurately propagates
      quite generally nonlinear multiplicative-noise probability distributions
      defined as a path integral, but PATHTREE is very fast and very easy to
      implement. 
      <p></p>
    </dd>
    <dd>Link from path01_pathtree.pdf to markets01_pathtree.pdf. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/path01_lecture.html">path01_lecture.html</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Adaptive Simulated Annealing (ASA) and Path-Integral (PATHINT)
      Algorithms: Generic Tools for Complex Systems </dd>
    <dd>%R ASA-PATHINT Lecture Plates </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%D 2001 </dd>
    <dd>%O Invited talk U Calgary, Canada, April 2001. URL
      http://www.ingber.com/asa01_lecture.pdf 
      <p></p>
    </dd>
    <dd><a
      href="http://www.ingber.com/path01_lecture.pdf">path01_lecture.pdf</a> or
      <a
      href="http://www.ingber.com/path01_lecture.ps.gz">path01_lecture.ps.gz</a>,
      offers an alternative PDF or PostScript format. 
      <p></p>
    </dd>
    <dd>Links from asa01_lecture.html, asa01_lecture.pdf and
      asa01_lecture.ps.gz, to path01_lecture.html, path01_lecture.pdf and
      path01_lecture.ps.gz. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/path10_multiple_scales.pdf">path10_multiple_scales.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A P.L. Nunez </dd>
    <dd>%T Neocortical Dynamics at Multiple Scales: EEG Standing Waves,
      Statistical Mechanics, and Physical Analogs </dd>
    <dd>%J Mathematical Biosciences </dd>
    <dd>%D 2010 </dd>
    <dd>%O URL http://www.ingber.com/path10_multiple_scales.pdf 
      <p></p>
    </dd>
    <dd>This paper includes PATHINT evolution of probability distributions of
      columnar activity with explicit oscillatory firings, and integration of
      such mesoscopic processes with global brain EEG activity. 
      <p></p>
    </dd>
    <dd>Link from smni10_multiple_scales.pdf to path10_multiple_scales.pdf. 
      <p></p>
    </dd>
  <dt>[<a href="http://www.ingber.com/">To-Top-of-Archive</a>] </dt>
    <dd></dd>
</dl>

<h2><a name="NEOCORTEX">NEOCORTEX</a></h2>
<br>
<img src="http://www.ingber.com/scales.jpg" alt="[SMNI]" width="100"
border="0"> <br>
<img src="http://www.ingber.com/stm.jpg" alt="[STM]" width="100" border="0"> 

<p>Upon a reasonable request, I may make some of my SMNI codes available for collaboration.  </p>

<dl>
  <dt><a
  href="http://www.ingber.com/smni72_learning.pdf">smni72_learning.pdf</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Editorial: Learning to learn </dd>
    <dd>%J Explore </dd>
    <dd>%V 7 </dd>
    <dd>%P 5-8 </dd>
    <dd>%D 1972 </dd>
    <dd>%O URL http://www.ingber.com/smni72_learning.pdf 
      <p></p>
    </dd>
    <dd>Link from smni72_learning.pdf to karate72_learning.pdf. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni81_attention.pdf">smni81_attention.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Attention, physics and teaching </dd>
    <dd>%J Journal Social Biological Structures </dd>
    <dd>%V 4 </dd>
    <dd>%P 225-235 </dd>
    <dd>%D 1981 </dd>
    <dd>%O URL http://www.ingber.com/smni81_attention.pdf </dd>
    <dd>(This file may only be shared for research.) 
      <p></p>
    </dd>
    <dd>This paper describes a methodology and implementation of teaching a
      full curriculum of academics, fine arts and physical disciplines, also
      tested as described in <a
      href="http://www.ingber.com/smni72_learning.pdf">smni72_learning.pdf</a>,
      based in part on a similar methodology I developed for teaching karate,
      in turn based on my brain research. 
      <p></p>
    </dd>
    <dd>Links from smni81_attention.pdf to karate81_attention.pdf. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni81_unified.pdf">smni81_unified.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Towards a unified brain theory </dd>
    <dd>%J Journal Social Biological Structures </dd>
    <dd>%V 4 </dd>
    <dd>%P 211-224 </dd>
    <dd>%D 1981 </dd>
    <dd>%O URL http://www.ingber.com/smni81_unified.pdf </dd>
    <dd>(This file may only be shared for research.) 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni82_reader.pdf">smni82_reader.pdf</a>
   <dd>Lester Ingber</dd>
   <dd>Prediction of neural implants</dd>
   <dd>The Reader</dd>
   <dd>Del Mar, CA</dd>
   <dd>8 Jan 1982</dd>
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni82_basic.pdf">smni82_basic.pdf</a> or
  <a href="http://www.ingber.com/smni82_basic.ps.gz">smni82_basic.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions. I. Basic
      formulation </dd>
    <dd>%J Physica D </dd>
    <dd>%V 5 </dd>
    <dd>%P 83-107 </dd>
    <dd>%D 1982 </dd>
    <dd>%O URL http://www.ingber.com/smni82_basic.pdf 
      <p></p>
    </dd>
    <dd>smni82_basic is a detailed mathematical presentation of smni81_unified. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni83_dynamics.pdf">smni83_dynamics.pdf</a> or
  <a
  href="http://www.ingber.com/smni83_dynamics.ps.gz">smni83_dynamics.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions. Dynamics of
      synaptic modification </dd>
    <dd>%J Physical Review A </dd>
    <dd>%V 28 </dd>
    <dd>%P 395-416 </dd>
    <dd>%D 1983 </dd>
    <dd>%O URL http://www.ingber.com/smni83_dynamics.pdf 
      <p></p>
    </dd>
    <dd>smni83_dynamics was the first paper on the brain published in Physical
      Review. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni84_stm.pdf">smni84_stm.pdf</a> or <a
  href="http://www.ingber.com/smni84_stm.ps.gz">smni84_stm.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions. Derivation of
      short-term-memory capacity </dd>
    <dd>%J Physical Review A </dd>
    <dd>%V 29 </dd>
    <dd>%P 3346-3358 </dd>
    <dd>%D 1984 </dd>
    <dd>%O URL http://www.ingber.com/smni84_stm.pdf 
      <p></p>
    </dd>
    <dd>The Appendix contains more details on calculations in the 1982 and 1983
      SMNI papers. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni85_eeg.pdf">smni85_eeg.pdf</a> or <a
  href="http://www.ingber.com/smni85_eeg.ps.gz">smni85_eeg.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions. EEG dispersion
      relations </dd>
    <dd>%J IEEE Transactions Biomedical Engineering </dd>
    <dd>%V 32 </dd>
    <dd>%P 91-94 </dd>
    <dd>%D 1985 </dd>
    <dd>%O URL http://www.ingber.com/smni95_eeg.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni85_stm.pdf">smni85_stm.pdf</a> or <a
  href="http://www.ingber.com/smni85_stm.ps.gz">smni85_stm.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: Stability and
      duration of the 7+-2 rule of short-term-memory capacity </dd>
    <dd>%J Physical Review A </dd>
    <dd>%V 31 </dd>
    <dd>%P 1183-1186 </dd>
    <dd>%D 1985 </dd>
    <dd>%O URL http://www.ingber.com/smni85_stm.pdf 
      <p><a name="smni91-eeg"></a> </p>
    </dd>
  <dt><a href="http://www.ingber.com/smni91_eeg.pdf">smni91_eeg.pdf</a> or <a
  href="http://www.ingber.com/smni91_eeg.ps.gz">smni91_eeg.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: A scaling
      paradigm applied to electroencephalography </dd>
    <dd>%J Physical Review A </dd>
    <dd>%N 6 </dd>
    <dd>%V 44 </dd>
    <dd>%P 4017-4060 </dd>
    <dd>%D 1991 </dd>
    <dd>%O URL http://www.ingber.com/smni91_eeg.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni92_mnn.pdf">smni92_mnn.pdf</a> or <a
  href="http://www.ingber.com/smni92_mnn.ps.gz">smni92_mnn.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Generic mesoscopic neural networks based on statistical mechanics of
      neocortical interactions </dd>
    <dd>%J Physical Review A </dd>
    <dd>%V 45 </dd>
    <dd>%N 4 </dd>
    <dd>%P R2183-R2186 </dd>
    <dd>%D 1992 </dd>
    <dd>%O URL http://www.ingber.com/smni92_mnn.pdf 
      <p></p>
    </dd>
    <dd>Links from smni92_mnn.pdf and smni92_mnn.ps.gz to asa92_mnn.pdf and
      asa92_mnn.ps.gz. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni94_stm.pdf">smni94_stm.pdf</a> or <a
  href="http://www.ingber.com/smni94_stm.ps.gz">smni94_stm.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: Path-integral
      evolution of short-term memory </dd>
    <dd>%J Physical Review E </dd>
    <dd>%V 49 </dd>
    <dd>%N 5B </dd>
    <dd>%D 1994 </dd>
    <dd>%P 4652-4664 </dd>
    <dd>%O URL http://www.ingber.com/smni94_stm.pdf 
      <p></p>
    </dd>
    <dd>Higher resolution calculations performed with a supercomputer are in <a
      href="#smni95-stm">smni95_stm.pdf</a>. (Links from smni95_stm.pdf and
      smni95_stm.ps.gz to path95_stm.pdf and path95_stm.ps.gz.) 
      <p></p>
    </dd>
    <dd>Links from smni94_stm.pdf and smni94_stm.ps.gz to path94_stm.pdf and
      path94_stm.ps.gz. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni95_images.pdf">smni95_images.pdf</a>
  or <a
  href="http://www.ingber.com/smni95_images.ps.gz">smni95_images.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Multiple scales of brain-mind interaction </dd>
    <dd>%J Behavioral and Brain Sciences </dd>
    <dd>%V 18 </dd>
    <dd>%N 2 </dd>
    <dd>%P 360-362 </dd>
    <dd>%D 1995 </dd>
    <dd>%O URL http://www.ingber.com/smni95_images.pdf 
      <p></p>
    </dd>
    <dd>Invited commentary on Images of Mind, by M.I. Posner and M.E. Raichle. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni95_scales.pdf">smni95_scales.pdf</a>
  or <a
  href="http://www.ingber.com/smni95_scales.ps.gz">smni95_scales.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of multiple scales of neocortical interactions
    </dd>
    <dd>%B Neocortical Dynamics and Human EEG Rhythms </dd>
    <dd>%E P.L. Nunez </dd>
    <dd>%I Oxford University Press </dd>
    <dd>%C New York, NY </dd>
    <dd>%P 628-681 </dd>
    <dd>%D 1995 </dd>
    <dd>%O ISBN 0-19-505728-7. URL http://www.ingber.com/smni95_scales.pdf 
      <p><a name="smni95-stm"></a> </p>
    </dd>
  <dt><a href="http://www.ingber.com/smni95_stm.pdf">smni95_stm.pdf</a> or <a
  href="http://www.ingber.com/smni95_stm.ps.gz">smni95_stm.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A P.L. Nunez </dd>
    <dd>%T Statistical mechanics of neocortical interactions: High resolution
      path-integral calculation of short-term memory </dd>
    <dd>%J Physical Review E </dd>
    <dd>%V 51 </dd>
    <dd>%N 5 </dd>
    <dd>%P 5074-5083 </dd>
    <dd>%D 1995 </dd>
    <dd>%O URL http://www.ingber.com/smni95_stm.pdf 
      <p></p>
    </dd>
    <dd>Links from smni95_stm.pdf and smni95_stm.ps.gz to path95_stm.pdf and
      path95_stm.ps.gz. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni95_stm40hz.pdf">smni95_stm40hz.pdf</a>
  or <a
  href="http://www.ingber.com/smni95_stm40hz.ps.gz">smni95_stm40hz.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: Constraints on 40
      Hz models of short-term memory </dd>
    <dd>%J Physical Review E </dd>
    <dd>%V 52 </dd>
    <dd>%N 4 </dd>
    <dd>%D 1995 </dd>
    <dd>%P 4561-4563 </dd>
    <dd>%O URL http://www.ingber.com/smni95_stm40hz.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni96_eeg.pdf">smni96_eeg.pdf</a> or <a
  href="http://www.ingber.com/smni96_eeg.ps.gz">smni96_eeg.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: Multiple scales
      of EEG </dd>
    <dd>%B Frontier Science in EEG: Continuous Waveform Analysis
      (Electroencephalography Clinical Neurophysiology Suppl. 45) </dd>
    <dd>%E R.M. Dasheiff and D.J. Vincent </dd>
    <dd>%I Elsevier </dd>
    <dd>%C Amsterdam </dd>
    <dd>%D 1996 </dd>
    <dd>%P 79-112 </dd>
    <dd>%O URL http://www.ingber.com/smni96_eeg.pdf 
      <p></p>
    </dd>
    <dd>Invited talk to the Frontier Science in EEG Symposium, New Orleans, 9
      Oct 1993. The listing of other contributors to this supplement is in <a
      href="http://www.ingber.com/smni96_dasheiff_vincent.txt">smni96_dasheiff_vincent.txt</a> 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni96_nonlinear.pdf">smni96_nonlinear.pdf</a> or
  <a
  href="http://www.ingber.com/smni96_nonlinear.ps.gz">smni96_nonlinear.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Nonlinear nonequilibrium nonquantum nonchaotic statistical mechanics
      of neocortical interactions </dd>
    <dd>%J Behavioral and Brain Sciences </dd>
    <dd>%V 19 </dd>
    <dd>%N 2 </dd>
    <dd>%P 300-301 </dd>
    <dd>%D 1996 </dd>
    <dd>%O URL http://www.ingber.com/smni96_nonlinear.pdf 
      <p></p>
    </dd>
    <dd>Invited commentary on Dynamics of the brain at global and microscopic
      scales: Neural networks and the EEG, by J.J. Wright and D.T.J. Liley. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni97_cmi.pdf">smni97_cmi.pdf</a> or <a
  href="http://www.ingber.com/smni97_cmi.ps.gz">smni97_cmi.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: Canonical momenta
      indicators of electroencephalography </dd>
    <dd>%J Physical Review E </dd>
    <dd>%V 55 </dd>
    <dd>%N 4 </dd>
    <dd>%P 4578-4593 </dd>
    <dd>%D 1997 </dd>
    <dd>%O URL http://www.ingber.com/smni97_cmi.pdf 
      <p></p>
    </dd>
    <dd>Raw EEG data used for this study can be freely downloaded, as described
      in <a
      href="http://www.ingber.com/smni97_eeg_data.html">smni97_eeg_data.html</a> 
      <p></p>
    </dd>
    <dd>Additional results (tables of ASA-fitted parameters and 60 files
      containing 240 PostScript graphs) are contained in <a
      href="http://www.ingber.com/smni97_eeg_cmi.tar.gz">smni97_eeg_cmi.tar.gz</a>
      . 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni97_lecture.pdf">smni97_lecture.pdf</a>
  or <a
  href="http://www.ingber.com/smni97_lecture.ps.gz">smni97_lecture.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions (SMNI) </dd>
    <dd>%R SMNI Lecture Plates </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%C Chicago, IL </dd>
    <dd>%D 1997 </dd>
    <dd>%O URL http://www.ingber.com/smni97_lecture.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni98_cmi_test.pdf">smni98_cmi_test.pdf</a> or
  <a
  href="http://www.ingber.com/smni98_cmi_test.ps.gz">smni98_cmi_test.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: Training and
      testing canonical momenta indicators of EEG </dd>
    <dd>%J Mathematical Computer Modelling </dd>
    <dd>%V 27 </dd>
    <dd>%N 3 </dd>
    <dd>%P 33-64 </dd>
    <dd>%D 1998 </dd>
    <dd>%O URL http://www.ingber.com/smni98_cmi_test.pdf 
      <p></p>
    </dd>
    <dd>Raw EEG data used for this study can be freely downloaded, as described
      in <a
      href="http://www.ingber.com/smni97_eeg_data.html">smni97_eeg_data.html</a> 
      <p></p>
    </dd>
    <dd>Additional results (tables of ASA-fitted parameters and 60 files
      containing 240 PostScript graphs) are contained in <a
      href="http://www.ingber.com/smni97_eeg_cmi.tar.gz">smni97_eeg_cmi.tar.gz</a> 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni99_g_factor.pdf">smni99_g_factor.pdf</a> or
  <a
  href="http://www.ingber.com/smni99_g_factor.ps.gz">smni99_g_factor.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: Reaction time
      correlates of the g factor </dd>
    <dd>%J Psycholoquy </dd>
    <dd>%V 10 </dd>
    <dd>%N 068 </dd>
    <dd>%D 1999 </dd>
    <dd>%O URL http://www.ingber.com/smni99_g_factor.pdf 
      <p></p>
    </dd>
    <dd>Invited commentary on The g Factor: The Science of Mental Ability by
      Arthur Jensen. 
      <p></p>
    </dd>
    <dd>A link to this and other commentaries in html format, and the abstract
      to the precis by Jensen, followed by ASCII-formatted versions of my
      commentary and his reply, are in <a
      href="http://www.ingber.com/smni00_jensen.txt">smni00_jensen.txt</a> 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni00_eeg_rt.pdf">smni00_eeg_rt.pdf</a>
  or <a
  href="http://www.ingber.com/smni00_eeg_rt.ps.gz">smni00_eeg_rt.ps.gz</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: EEG correlates of
      reaction times </dd>
    <dd>%B Proceedings World Congress on Medical Physics and Biomedical
      Engineering, July 23-28, 2000 </dd>
    <dd>%I World Congress on Medical Physics and Biomedical Engineering </dd>
    <dd>%C Chicago, IL </dd>
    <dd>%D 2000 </dd>
    <dd>%O URL http://www.ingber.com/smni00_eeg_rt.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni00_eeg_stm.pdf">smni00_eeg_stm.pdf</a>
  or <a
  href="http://www.ingber.com/smni00_eeg_stm.ps.gz">smni00_eeg_stm.ps.gz</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: EEG
      eigenfunctions of short-term memory </dd>
    <dd>%J Behavioral and Brain Sciences </dd>
    <dd>%V 23 </dd>
    <dd>%N 3 </dd>
    <dd>%P 403-405 </dd>
    <dd>%D 2000 </dd>
    <dd>%O URL http://www.ingber.com/smni00_eeg_stm.pdf 
      <p></p>
    </dd>
    <dd>Invited commentary on Toward a Quantitative Description of Large-Scale
      Neocortical Dynamic Function and EEG, by P.L. Nunez. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni01_lecture.html">smni01_lecture.html</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical Mechanics of Neocortical Interactions (SMNI): Multiple
      Scales of Short-Term Memory and EEG Phenomena </dd>
    <dd>%R SMNI Lecture Plates </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%D 2001 </dd>
    <dd>%O Invited talk U Calgary, Canada, April 2001. URL
      http://www.ingber.com/smni01_lecture.pdf 
      <p></p>
    </dd>
    <dd><a
      href="http://www.ingber.com/smni01_lecture.pdf">smni01_lecture.pdf</a> or
      <a
      href="http://www.ingber.com/smni01_lecture.ps.gz">smni01_lecture.ps.gz</a>
      offers an alternative PDF or PostScript format. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni06_ism.pdf">smni06_ism.pdf</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Ideas by statistical mechanics (ISM) </dd>
    <dd>%R Report 2006:ISM </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%D 2006 </dd>
    <dd>%O URL http://www.ingber.com/smni06_ism.pdf 
      <p></p>
    </dd>
    <dd>ISM integrates previous projects to model evolution and propagation of
      ideas/patterns throughout populations subjected to endogenous and
      exogenous interactions. A short version appears as "AI and Ideas by
      Statistical Mechanics (ISM)" in the Encyclopedia of Artificial
      Intelligence, pp. 58-64 (2008), and details in this paper appear in
      "Ideas by Statistical Mechanics (ISM),", Journal of Integrated Systems
      Design and Process Science, Vol. 11, No. 3, pp. 31-54 (2007), Special
      Issue: Biologically Inspired Computing. 
      <p></p>
    </dd>
    <dd>Links from smni06_ism.pdf to asa06_ism.pdf and markets06_ism.pdf. 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni06_ppi.pdf">smni06_ppi.pdf</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: Portfolio of
      physiological indicators </dd>
    <dd>%R Report 2006:PPI </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%D 2006 </dd>
    <dd>%O URL http://www.ingber.com/smni06_ppi.pdf 
      <p></p>
    </dd>
    <dd>A modified paper is published in The Open Cybernetics Systemics
      Journal, vol. 3, pp. 13-26 (2009). 
      <p></p>
    </dd>
    <dd>PPI is used in the context of developing experimental data for testing
      theories of neocortical interactions in an invited paper referenced
      below, "Statistical mechanics of neocortical interactions (SMNI): Testing
      theories with multiple imaging data" in smni08_tt.pdf below. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni07_timedelays.pdf">smni07_timedelays.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: Time delays </dd>
    <dd>%R Report 2007:TD </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%D 2007 </dd>
    <dd>%O URL http://www.ingber.com/smni07_timedelays.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni08_tt.pdf">smni08_tt.pdf</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions (SMNI): Testing
      theories with multiple imaging data </dd>
    <dd>%J NeuroQuantology Journal </dd>
    <dd>%V 6 </dd>
    <dd>%N 2 </dd>
    <dd>%D 2008 </dd>
    <dd>%P 97-104 </dd>
    <dd>%O Invited paper. URL http://www.ingber.com/smni08_tt.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni09_columnar_eeg.pdf">smni09_columnar_eeg.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: Columnar EEG </dd>
    <dd>%R Report 2009:CEEG </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%D 2009 </dd>
    <dd>%O URL http://www.ingber.com/smni09_columnar_eeg.pdf 
      <p></p>
    </dd>
    <dd>This early report considered oscillations in quasi-linearized
      Euler-Lagrange equations. The subsequent study in <a
      href="http://www.ingber.com/smni09_nonlin_column_eeg.pdf">smni09_nonlin_column_eeg.pdf</a>
      considers the full nonlinear system and gives more background and
      implications of the calculations. The study after that in <a
      href="http://www.ingber.com/smni10_multiple_scales.pdf">smni10_multiple_scales.pdf</a>
      includes definitive calculations using PATHINT to evolve multivariate
      probability distributions of firing states. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni09_nonlin_column_eeg.pdf">smni09_nonlin_column_eeg.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Statistical mechanics of neocortical interactions: Nonlinear
      columnar electroencephalography </dd>
    <dd>%J NeuroQuantology Journal </dd>
    <dd>%V 7 </dd>
    <dd>%N 4 </dd>
    <dd>%P 500-529 </dd>
    <dd>%D 2009 </dd>
    <dd>%O URL http://www.ingber.com/smni09_nonlin_column_eeg.pdf 
      <p>The next study in <a
      href="http://www.ingber.com/smni10_multiple_scales.pdf">smni10_multiple_scales.pdf</a>
      includes definitive calculations using PATHINT to evolve multivariate
      probability distributions of firing states. </p>
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni10_multiple_scales.pdf">smni10_multiple_scales.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A P.L. Nunez </dd>
    <dd>%T Neocortical Dynamics at Multiple Scales: EEG Standing Waves,
      Statistical Mechanics, and Physical Analogs </dd>
    <dd>%J Mathematical Biosciences </dd>
    <dd>%D 2010 </dd>
    <dd>%O URL http://www.ingber.com/smni10_multiple_scales.pdf 
      <p></p>
    </dd>
    <dd>This paper includes PATHINT evolution of probability distributions of
      columnar activity with explicit oscillatory firings, and integration of
      such mesoscopic processes with global brain EEG activity. 
      <p></p>
    </dd>
    <dd>Link from smni10_multiple_scales.pdf to path10_multiple_scales.pdf. 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni11_cog_comp.pdf">smni11_cog_comp.pdf</a>
    <dd>%A L. Ingber </dd>
    <dd>%T Computational algorithms derived from multiple scales of neocortical
      processing </dd>
    <dd>%B Pointing at Boundaries: Integrating Computation and Cognition on
      Biological Grounds </dd>
    <dd>%E A. Pereira, Jr. </dd>
    <dd>%E E. Massad </dd>
    <dd>%E N. Bobbitt </dd>
    <dd>%I Springer </dd>
    <dd>%C New York </dd>
    <dd>%D 2011 </dd>
    <dd>%O Invited Paper. URL http://www.ingber.com/smni11_cog_comp.pdf and http://dx.doi.org/10.1007/s12559-011-9105-4 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni11_stm_scales.pdf">smni11_stm_scales.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Columnar EEG magnetic influences on molecular development of
      short-term memory </dd>
    <dd>%B Short-Term Memory: New Research </dd>
    <dd>%E G. Kalivas </dd>
    <dd>%E S.F. Petralia </dd>
    <dd>%D 2012 </dd>
    <dd>%P 37-72 </dd>
    <dd>%I Nova </dd>
    <dd>%C Hauppauge, NY </dd>
    <dd>%O Invited Paper. URL http://www.ingber.com/smni11_stm_scales.pdf </dd>
    <dd>(This file may only be shared for research.) 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni12_vectpot.pdf">smni12_vectpot.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Influence of macrocolumnar EEG on Ca waves </dd>
    <dd>%J Current Progress Journal </dd>
    <dd>%D 2012 </dd>
    <dd>%V 1 </dd>
    <dd>%N 1 </dd>
    <dd>%P 4-8 </dd>
    <dd>%D 2012 </dd>
    <dd>%O URL http://www.ingber.com/smni12_vectpot.pdf 
      <p></p>
    </dd>
  <dt><a
  href="http://www.ingber.com/smni13_eeg_ca_lect.pptx">smni13_eeg_ca_lect.pptx</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%R Report 2013:LFIC </dd>
    <dd>%T Electroencephalographic (EEG) influence on Ca<sup>2+</sup> waves</dd>
    <dd>%D 2013 </dd>
    <dd>%I Lester Ingber Research </dd>
    <dd>%C Ashland, OR </dd>
    <dd>%O 2nd World Neuroscience Online Conference 18 June 2013. URL
      http://www.ingber.com/smni13_eeg_ca_lect.pptx 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni14_eeg_ca.pdf">smni14_eeg_ca.pdf</a>
  </dt>
    <dd>%A L. Ingber </dd>
    <dd>%A M. Pappalepore</dd>
    <dd>%A R.R. Stesiak</dd>
    <dd>%J Journal of Theoretical Biology </dd>
    <dd>%T Electroencephalographic field influence on calcium momentum
    waves </dd>
    <dd>%V 343
    <dd>%P 138-153
    <dd>%D 2014 </dd>
    <dd>%O URL http://www.ingber.com/smni14_eeg_ca.pdf and http://dx.doi.org/10.1016/j.jtbi.2013.11.002
      <p></p>
    </dd>
    <dd>Graphs and Supplemental Analysis for this project are in
  <a href="http://www.ingber.com/smni14_eeg_ca_supp.pdf">smni14_eeg_ca_supp.pdf</a>
      <p></p>
    </dd>
    <dd>A short 5-minute audio-slide presentation accompanies this paper, which can be viewed or downloaded (if your browser will not show the slides) from
  <a href="http://www.ingber.com/smni14_eeg_ca_audio-slides.mp4">smni14_eeg_ca_audio-slides.mp4</a>
      <br>or from my youtube channel
      <a href="http://url.ingber.com/eeg_ca">http://url.ingber.com/eeg_ca</a>
      <br>or slides only
      <a href="smni14_eeg_ca_slides.pdf">smni14_eeg_ca_slides.pdf</a>
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni14_ismai.pdf">smni14_ismai.pdf</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Ideas by multiple scales of statistical mechanics (ISM) </dd>
    <dd>%J International Journal of Robotics Applications and Technologies (IJRAT) </dd>
    <dd>%D 2014 </dd>
    <dd>%O Invited paper based on 2008 contribution to Encyclopedia of Artificial Intelligence. URL http://www.ingber.com/smni14_ismai.pdf 
      <p></p>
    </dd>
  <dt><a href="http://www.ingber.com/smni14_conscious_scales.pdf">smni14_conscious_scales.pdf</a> </dt>
    <dd>%A L. Ingber </dd>
    <dd>%T Influences on consciousness from multiple scales of neocortical interactions </dd>
    <dd>%B Non-chemical distant cellular interactions: experimental evidences, theoretical modeling, and physiological significance </dd>
    <dd>%E M. Cifra </dd>
    <dd>%E V. Salari </dd>
    <dd>%E D. Fels </dd>
    <dd>%E F. Scholkmann </dd>
    <dd>%D 2014 </dd>
    <dd>%O Invited Paper. URL http://www.ingber.com/smni14_conscious_scales.pdf </dd>
      <p></p>
    </dd>
    <dd>Abstract only.  Full paper is due 1 Sep 2014.
  <dt>[<a href="http://www.ingber.com/">To-Top-of-Archive</a>] </dt>
    <dd></dd>
</dl>
<hr>
<hr>

<p></p>
<address>
  Lester Ingber &lt;lester@ingber.com&gt; 
</address>
Copyright &copy; 1958-2014 Lester Ingber. All Rights Reserved. 

<p>$Id: index.html,v 11.726 2014/02/24 16:51:54 ingber Exp ingber $ </p>
</body>
</html>
++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://www.andcorporation.com/>====================
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<title>AND Corporation</title>

<script language="JavaScript" type="text/JavaScript" src="page.js"></script>

</head>

<frameset onload="initSite();" cols="*,1024,*" rows="30" frameborder="no" border="0" framespacing="0">
   <frame src="left.html" name="left"  >
   <frame src="main.html" name="main"  >
   <frame src="left.html" name="right" >
</frameset>

</html>++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://attrasoft.com/>====================
<html>
<head>
<script language="javascript" type="text/javascript" src="javascript/menu_items_sub.js"></script>
<script language="JavaScript" src="javascript/mm_menu.js"></script>
<link rel="stylesheet" href="css/main.css" type="text/css" />
</head>
<body topmargin="0" leftmargin="0" bgcolor="#ffffff" onLoad="MM_preloadImages('images/topbar_company_on.gif','images/topbar_company_off.gif','images/topbar_products_on.gif','images/topbar_products_off.gif','images/topbar_news_off.gif','images/topbar_news_on.gif','images/topbar_customers_off.gif','images/topbar_customers_on.gif','images/topbar_contact_off.gif','images/topbar_contact_on.gif');">
<script language="JavaScript1.2">mmLoadMenus();</script>
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr>
<td width="50%"></td>
<td background="images/gutter_left.jpg" width="51"><img src="images/gutter_left.jpg"/></td>
<td width="798">

 <table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr>
	<td align="left"><a href="default.asp"><img src="images/topbar_logo.gif" border="0"/></a></td>
	<td valign="top">
		<table border="0" cellpadding="0" cellspacing="0">
			<tr>
				<td><img src="images/topbar_top.gif" border="0"/></td>
			</tr>
		<tr>
			<td align="right" style="cursor:hand;">
            <a href="company_company.asp" onMouseOut="MM_swapImgRestore();MM_startTimeout();" onMouseOver="MM_swapImage('company','','images/topbar_company_on.gif');MM_showMenu(window.mm_menu_0827160239_0,0,23,null,'company')">
            <img border="0" src="images/topbar_company_off.gif" name="company" id="company"></a>
            <img border="0" src="images/topbar_navbreak.gif">


            <a href="products_products.asp" onMouseOut="MM_swapImgRestore();MM_startTimeout();" onMouseOver="MM_swapImage('products','','images/topbar_products_on.gif');MM_showMenu(window.mm_menu_0827160240_0,0,23,null,'products')">
            <img border="0" src="images/topbar_products_off.gif" name="products" id="products"></a>
            <img border="0" src="images/topbar_navbreak.gif">


            <a href="customers.asp" onMouseOut="MM_swapImgRestore();MM_startTimeout();" onMouseOver="MM_swapImage('customers','','images/topbar_customers_on.gif')">
            <img border="0" src="images/topbar_customers_off.gif" id="customers" name="customers"></a>
            <img border="0" src="images/topbar_navbreak.gif">

            <a href="contact.asp" onMouseOut="MM_swapImgRestore();MM_startTimeout();" onMouseOver="MM_swapImage('contact','','images/topbar_contact_on.gif')">

            <img border="0" src="images/topbar_contact_off.gif" id="contact" name="contact"></a>&nbsp;&nbsp;&nbsp;</td>
		</tr>
			<tr>
				<td></td>
			</tr>
		</table>
	</td>
</tr>
</table>

 <table border="0" cellpadding="0" cellspacing="0" width="100%">
 <tr>
	<td background="images/mainstage_intro.jpg" border="0" width="798" height="246" align="right" valign="middle" class="orangeHit"></td>
 </tr>
 </table>







 <table border="0" cellpadding="12" cellspacing="0" width="100%">
 <tr>
	<td width="305" align="center" valign="top" background="images/mainstage_intro3.jpg" style="background-repeat:no-repeat;">
	<table border="0" cellpadding="15" cellspacing="0" width="285" bgcolor="#E3E8E8">

		<tr>
			<td class="bodyText">

             <span class="head1"><h2>About Attrasoft </h2></span>

              <p></p>

              Attrasoft Core Technology:
               <p></p>

<a href="http://www.attraseek.com/">1. Find an image in a billion images in 3 seconds.</a>
          <p></p>

<a href="http://attrasoft.com/videofinderlite/">2. Find a video in 1 million videos in 3 seconds.</a>
       <p></p>
       <a href="http://attrasoft.com/service_imagetagger.asp">3. Tag images automatically using software.</a>
      <p></p>
  <a href="http://attrasoft.com/imagefinderlite2010/">Download image demo, ImageFinderLite.</a>
  </p>
              <p></p>
            <p>Attrasoft, Inc. is an Image-Recognition Search Engine company with market proven core technology.    </p>


             <p></p>
            <span class="head1"><h2>Market Pain</h2></span>
             <p>

<ul>
  <li>95% of Digital Universe is not searchable via keywords (IDC white paper).</li>
  <li>Number of Net images will double in three years and redouble in another three years, thus exacerbating the problem.</li>
  <li>Manual tagging is labor intensive, time consuming, and unscalable.</li>
</ul>
  <p></p>
Other problems with manual image tagging:

<ul>
  <li>Yields millions of results.</li>
  <li>Difficult to search via keyword.</li>
  <li>All possible Internet locations not provided.</li>
  <li>Inaccurately tagged, or mislabeled (human error versus sometimes intentionally).</li>

</ul>

 <p></p>
            <span class="head1"><h2>Service</h2></span>
             <p>

  <a href="http://attrasoft.com/service_imagetagger.asp">Attrasoft specializes in image tagging using computer software.   </a>
 <p></p>
  We use Attrasoft software, <a href="http://attrasoft.com/products_imagetagger.asp">ImageTagger</a>, to tag random images automatically.

             <p></p>
            <span class="head1"><h2>Product</h2></span>
             <p>

<a href="http://attraseek.com/">Mini-AttraSeek</a>  is our flagship product:<p></p>
<ul>

  <li>It provides <b>automated image matching</b> within your website or database <b>without manual tagging</b>.</li>
  <li>It is an off-the-shelf product.</li>
  <li>It can be deployed in your website or hosted/managed by Attrasoft for your website.</li>
</ul>

 <p></p>
It eliminates the descriptive image word tagging process, which is used in the image identification process today.
</p>

 <p></p>
<span class="head1"><h2>Applications </h2></span>
<p></p>
<ul>
  <li>To search untagged images, which are 95% of all images;</li>
   <p></p>
   <li>To check newly uploaded images, which have no tags;</li>
   <p></p>
   <li>To aid commerce sites where people perform web search for product purchase and search management;</li>
   <p></p>
   <li>To search within your website or database for images where image tagging is difficult or requires expertise;</li>
   <p></p>
   <li>To increase sales for companies selling over the web where image is critical to the sale, examples includes art, photos, products with visual similarities such as wheel rims, real estate, dinner plates, etc.</li>
   <p></p>
   <li>To create cell phone apps that require image recognition.</li>
   <p></p>
   <li>To reduce manual labor in reviewing surveillance video.</li>
</ul>

<p></p>
<span class="head1"><h2>Benefits</h2></span>
         <p>
<ul>
  <li>Reduces your manual image compliance search personnel by 90%.</li>
     <p></p>

     <li>Eliminates the manual image tagging process by 90% or more.</li>
       <p></p>
    <li>Reduces user search time from 5 minutes to 5 - 20 seconds , which can increase sales for companies selling over the web where image is critical to the sale (i.e., art, wheel rims, real estate, home decor items, etc.).</li>
       <p></p>

    <li>Increase searchable images by a factor of 20 (From 5% searchable images to 100% searchable images).</li>
       <p></p>

  <li>Provides for a wide range of applications for any web-based image location need.</li>
      <p></p>
  <li>Gives confidence scores to aid user discernment of exact and similar matches.</li>
       <p></p>
  <li>Reduces the time required by the user to do the same image search with word descriptions by 50 % or more.</li>
        <p></p>


  <li>Uses patent-pending algorithms to compare images to images without text descriptions. </li>
  <p></p>
  <li>Solves image tagging problems such as inaccurate tag, mislabeled tag, lacking proper details, and other human errors </li>
</ul>


         </p>

			<center><img src="images/cats.gif" border="0"/></center>
           

</td>
		</tr>


	</table>
	</td>
	<td background="images/mainstage_intro2.jpg" style="color:#095CAB;" style="background-repeat: no-repeat;" align="left" valign="top" width="473" class="bodyText"><br>

    <p></p>
    <span class="head1"><h2>Testimonials</h2></span>
        <p>
         &quot;We love Attrasoft.&quot; Peter A. Andrell III, CTO, TNS Media Intelligence.
         </p>

         <p>
TNS Media Intelligence is the leading provider of strategic advertising intelligence to advertisers, advertising agencies, and media properties.
TNS has currently deployed customized Attrasoft ImageFinder software into
their US magazine monitoring system and US newspaper monitoring system.
</p>
<p>
Attrasoft technology matches newly captured images with an image database and
is in large-scale deployment with emphasis on speed, accuracy, and scalability. It has been deployed in TNS several years.

        </p>


    <span class="head1"><h2>Dramatic Difference</h2></span><br>
	<img src="images/who.jpg" border="0" style="float:right;margin-left:10px;margin-bottom:20px;margin-top:20px;margin-right:10px;"/>

     Image search with an image as the search criteria is a revolutionary addition to today뇹 image search with words.
    <p></p>
    One of our clients has increased their market share from 85% to 90% in their industry because they are matching images against images directly, instead of matching images with keywords.

   <p></p>

    <p></p>
  <span class="head1"><h2>Demos</h2></span>
  <p>The demos below allow anyone to test the ease-and-use of the technology by accurately locating ANY image among a large database of images in seconds.</p>

  <p>
     <font  color="red">

  NOTE:  This is a demo that only
has 1.5 million images.  It is not meant
to replicate a Net Search.  Use of random
images may not get a match. Please test with our 40,000 test images. </font>
</p>
    <p></p>
    <a href="demo_googlelogo1.asp">One Quick Test with Google Logos.</a>
     <p></p>
    <a href="demo_googlelogo2.asp">Many Quick Tests with Google Logos.</a>
    <p></p>
    <a href="demo_wheelrim1.asp">Many Quick Tests with Product Images.</a>
    <p></p>
     <a href="demo_art2.asp">Many Quick Tests with Art images.</a>
     <p></p>
    <a href="demo_label1.asp">Many Quick Tests with Product Images.</a>
    <p></p>
      <a href="demo_ads1.asp">Many Quick Tests with multiple scanned advertisement images. </a>
     <p></p>

    <a href="demo_newspaper.asp">Many Quick Tests with multiple scanned document images.</a>
  <p>
  <a href="http://www.attraseek.com/">Do your own test with 40,000 test images online.</a>
  </p>

  <p>
  <a href="http://attrasoft.com/imagefinderlite2010/">Local testing with your own test images.</a>
  </p>

  <p>
  <a href="http://attrasoft.com/products_videofinder.asp">Video search demo.</a>
  </p>

            <p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

<span class="head1"><h2>What is New?</h2></span>
   <p ></p>


<ul>
  <p></p>
  <li><a href="http://attrasoft.com/products_imagefinder_surveillance.asp">IFSurveillance 2013 (Feb 2013 ) </a></li>
  <p></p>
<li><a href="http://attrasoft.com/products_imagefinderSeg.asp">ImageFinderSeg 2013 (Jan 2012) </a></li>

 <p></p>
<li><a href="http://attrasoft.com/transapplet80/">TransApplet</a></li>
  <p></p>
<li><a href="http://attrasoft.com/products_imagefinder.asp">ImageFinder</a></li>
  <p></p>
  <li><a href="http://attrasoft.com/products_flashfinderlite.asp">FlashFinderLite</a></li>
  <p></p>
  <li><a href="http://attrasoft.com/products_videofinderlite.asp">VideoFinderLite</a></li>
  <p></p>
  <li><a href="http://attrasoft.com/products_imagefinderLite.asp">ImageFinderLite</a></li>

  <p></p>
  <li><a href="http://attrasoft.com/products_imagetagger.asp">Image Tagging Service</a></li>
</ul>


            <p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

      <span class="head1"><h2>Features</h2></span>
    Mini-AttraSeek:
    <p></p>
     <ul>
  <li>Is scalable to billions of images.</li>
  <li>Has constant search time regardless of the number of images in the searched database.</li>
  <li>Is 100% accurate if the image is in the database.</li>
  <li><a href="demo_googlelogo1.asp">Can accommodate small variations.</a></li>
  <li>Can accommodate larger variations with multiple image signatures.</li>
</ul>
<p></p>
        <span class="head1"><h2>Guarantee</h2></span>
        <p>
         We Guarantee to match every image in your database against your database with 100% accuracy within seconds or we will waive all fees.
        </p>


        <span class="head1"><h2>Price</h2></span>
        <p></p>
        Mini-AttraSeek price is 80% of the cost of a &quot;Google Mini&quot; (with documents being replaced by Images).
<p></p>
Mini-AttraSeek starts with a subscription for 2-years/50,000 images, all the way to a subscription for 1-year/1-billion images or more. Customization available.  <p></p>
Please contact Gina Porter at gina@attrasoft.com for a product whitepaper.


     <p></p>
     <span class="head1"><h2>Contact</h2></span>
        <p></p>
        <p>
        <A href="mailto:gina@attrasoft.com">gina@attrasoft.com</A> <br><br>
        </p>



	</td>
 </tr>
 </table>

</td>
<td background="images/gutter_right.jpg" width="51"><img src="images/gutter_right.jpg"/></td>
<td width="50%"></td>
</tr>

 <tr>
<td></td>
<td colspan="3" width="900" background="images/gutter_bottom.jpg" style="background-repeat: no-repeat;" align="center" valign="top"><br>
<table border="0" cellpadding="0" cellspacing="0" ID="Table1">
         <tr>
            <td width="100%" align="center" style="color:black;" class="bodyText"><br><span style="font-family: Arial, Helvetica, sans-serif"><a href="default.asp" class="bodyText">Home</a>
            | <a href="http://attrasoft.com/company_company.asp" class="bodyText">Company</a>
            | <a href="http://attrasoft.com/products_products.asp" class="bodyText">Services</a>
              | <a href="http://attrasoft.com/products_products.asp" class="bodyText">Products</a>
            | <a href="customers.asp" class="bodyText">Customers</a>
            | <a href="contact.asp" class="bodyText">Contact Us</a></span><span style="line-height:5px;"><br><br></span>copyright  2007 - 2013 Attrasoft, Inc. All Rights Reserved.
            </td>
         </tr>
       </table><br>
</td>
<td></td>
</tr>
</table>


</body>
</html>++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://www.hav.com/>====================
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//hav//DTD XHTML 1.0 Transitional with havindex Extension//EN" "http://www.hav.com/include/hav-xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<link rel="SHORTCUT ICON" href="http://www.hav.com/favicon.ico"/>
<link rel="P3Pv1" href="http://www.hav.com/w3c/p3p.xml"/>
<link rel="stylesheet" type="text/css" href="http://www.hav.com/include/hav.css"/>
<link rel="stylesheet" type="text/css" href="http://www.hav.com/include/hav_print.css"  media="print"/>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="meta" href="http://www.hav.com/labels.rdf" type="application/rdf+xml" title="ICRA labels" />
<meta http-equiv="pics-label" content='(pics-1.1 "http://www.icra.org/ratingsv02.html" comment "basic online form" l gen true for "http://www.hav.com/" r (nz 1 vz 1 lz 1 oz 1 ca 1 cb 1) "http://www.rsac.org/ratingsv01.html" l gen true for "http://www.hav.com/" r (n 0 s 0 v 0 l 0))' />

<!--
#
# This file and the page in which it appears is
# Copyright (c) 2008 by hav.Software and Horace A. "Kicker" Vallas, Jr. - All rights reserved
#
# No right is granted to copy or distribute this file or any of its contents except 
# as may occur in normal browsing of the page in which it appears.
#
-->	
<havindex id="havfp"></havindex>
<title>hav.Software - Neural Nets to Internet demos and software for neural networks, chat, visitor tracking, counters  etc. in Java,  c++, javascript, Tcl, NeoWebscript and others</title>

<meta http-equiv="description" content="hav.Software, Software Development, Outsourcing, Neural Nets, visitor tracking, counters, Java Chat Server, Java Chat Applets, Java Chat Bot, index search engine, Java and Database Demos, Bonsai Stye, Pictures, Icons, Horace Vallas, Kicker Vallas, Pensacola, Florida, classical guitar videos" />

<meta http-equiv="keywords" content="Java Chat Server, Java Chat Applet, Java Chat Bot, Call Center Chat, Customer Service Chat, index, search engine, servlets, GD, GIF Generation, Dynamic graphics, Credit Card Validation, CC, Neural Nets, Neural Net, Backpropagation, havBpNet++, havBpNetJ, havBpNet, Kohonen Feature map, som, self organizing, havFmNet++, havFmNet, AI, counter, visitor tracking, access timing, JavaScript, Java demos, Java Script, C++, hav.Software, NeoWebScript, Tcl, CGI, HTMLScript, Horace Vallas,  Kicker Vallas, Tennis, Icons, Bonsai Style, Bonsai Pictures, Web Demos, nph, Apache, bonsai,  timezones, Database, Postgres95, PostgreSQL, Software Engineering, hav, graphic generation, java, applets, Pensacola, Florida, C++ Libraries, Class, outsource, outsourcing, outsourcing intranet, extranet, virtual private network, vpn, Loyola University, New Orleans, St. Martins, UH, Louisiana" />

<meta name="robots" content="index,follow" />

<meta name="distribution" content="global"/>
<meta name="resource-type" content="document"/>
<meta name="MSSmartTagsPreventParsing" content="true"/>
<link rel="stylesheet" type="text/css" href="/include/scripts/hav-menu.css" />
<script language="JavaScript" type="text/javascript" src="/include/scripts/basicPageUtils.js"></script>
<script language="JavaScript" type="text/javascript" src="/include/scripts/imagePopup.js"></script>
<script type="text/javascript" src="/include/scripts/ddlevelsmenu.js"></script>
</head>
<body onload="javascript:clearFrames('/index.htm');" >
<a name="TopOfPage"></a>

<div id="pageContainer" class="pageRelDiv">
<div id="headerContent" class="headerDiv"><img src="/images/hav-logo.png" width="802" height="169" border="0" hspace="0" vspace="0" alt="hav.Software" /></div>
<div id="pageContent" class="contentDiv"><img src="/images/space.gif" width="600" height="1" border="0" hspace="0" vspace="0" alt="hav.Software" /><br clear="all" />

<!-- main page content here-->

<center><div class="lightYellowCell" style="font-size: 9pt; border: #003366 solid medium; padding: 8px; margin: 5px auto; width: 80%;">
<center><span class="red12bold"><i>Just Reworking the Site...</i></span></center><br/>
<div align="left"><strong>Well, the last time I overhauled the hav.com website was back in 1998</strong> - I had to verify that, in a fun way, using the <a href="http://web.archive.org/web/*/http://www.hav.com/" target="_blank">Internet WayBack Machine</a>. <i>Soooo...</i>, it's probably about due for a rework.
<br/><br/>
The website has always been - and remains - a mish-mash of both business and personal pages. Heck, if you snarf around the site, you may even still find a couple of old Mosaic pages ;-) ... During the rework, <span style="color: #cc0000;"><strong>some things may go away</strong></span>, temporarily or maybe forever. But, if you miss something in particular, <a href="mailto:hav@hav.com">let me know</a> and I'll see about putting it back up.
<br/><br/>
I've linked a couple of visitor favorite <i>fun pages</i> at the bottom of the left sidebar for now.
<br/><br/>
<center>Anyway - enjoy the new site and watch out for falling rocks...</center>
<br/><i>Kicker</i></div>
</div></center>
<br/>


<!--close main page content table-->
<!--
#
# This file and the page in which it appears is
# Copyright (c) 2000 by hav.Software and Horace A. Vallas, Jr.
# All rights reserved
#
# No right is granted to copy or distribute this file or any of its
# contents except as may occur in normal browsing of the page 
# in which it appears.
#
-->


<br/><br/>
<table width="98%" border="0" cellpadding="0" cellspacing="0" align="center">
<tr><td align="center"  class="pageRelFooterWhiteCell"><strong>[ <a class="tiny" href="http://www.hav.com/" target="_top">HOME</a> ]</strong><br/>Copyright &copy; 1994-2014 by hav.Software and Horace "Kicker" Vallas. All Rights Reserved.
</td></tr>
</table>
<br/>
<!-- === copyright stuff for the page bottom === -->
<havhide>
<table width="98%" border="0" cellpadding="5" cellspacing="0" align="center">
<tr><td class="pageRelFooterWhiteCell">

<!--
<table border="0" width="200" align="right">
<tr><td>&nbsp;<br/>&nbsp;</td></tr>
<tr><td style="background-color: #ffffff;" align="right">
<span style="font-size: 9pt;"><strong>Visitors on hav.com</strong></span>
<br/><span style="font-size: 8pt;"><i>(20 seconds per division)</i></span>
<br/>
 <applet codebase="http://www.hav.com/java/scroll/" 
			code="CounterScroll.class" 
			id="CounterScroll" 
			width="200" height="50">
 <param name="dataSource" value="http://www.hav.com/java/scroll/nph-getcount.htm?CVIS" />
 <param name="bufSize" value="52" />
 <param name="chartHeight" value="25" />
 <param name="xStep" value="5" />
 <param name="ySpace" value="2" />
 <param name="speed" value="10" />	
 </applet> 
</td></tr>
</table>
<br/><br/>
-->

<a href="http://www.hav.com/" target="_top"><img src="/images/icona32.gif" width="32" height="32" hspace="3" align="left" border="0" alt="hav.Software"/></a>
<strong><i>havBpNet:J</i></strong>, <strong><i>havFmNet:J</i></strong>, <strong><i>havBpNet++</i></strong>, <strong><i>havFmNet++</i></strong>, <strong><i>havBpETT</i></strong>,  <strong><i>havCNet</i></strong>, <strong><i>WebSnarfer</i></strong>, <strong><i>havIndex</i></strong>  and <strong><i>havChat</i></strong> are all trademarks of <strong><i>hav.Software</i></strong>.
<br/><br/>
Java and all Java-based marks are trademarks or registered trademarks of Sun Microsystems, Inc. in the U.S. and other countries.
<br/><br/>
There may be other trademarks or tradenames listed in this document to refer to the entities claiming the marks and names or products.  <strong><i>hav.Software</i></strong> disclaims any proprietary interest in any trademark, tradename or products other than its own.
<br/><br/>
<div align="right" class="black07normal"><br/>6,276,052 / 29,178,545<br/>Page Modified Mon Sep 19 16:28:39 CDT 2011</div>
</td></tr>
</table>
</havhide>
<!-- === close main content div === -->
</div>
<!-- === left sidebar div === -->
<div id="leftSidebar" class="sidebarDiv">
<strong>Open source,<br/>
Commercial and<br/>
Custom Software<br/>
Development, Outsource<br/>
and Consultation for ...<br/><br/>
&nbsp;&nbsp;-&nbsp;Project Management<br/>
&nbsp;&nbsp;-&nbsp;Scientific Applications<br/>
&nbsp;&nbsp;-&nbsp;Internet Applications<br/>
&nbsp;&nbsp;-&nbsp;Website Hosting &amp; Colo<br/>
&nbsp;&nbsp;-&nbsp;E-Commerce<br/>
&nbsp;&nbsp;-&nbsp;Database Design<br/>
&nbsp;&nbsp;-&nbsp;Quality Deployment<br/>
</strong>
<br/>

<!--<div class="chatdiv">
<table border="0" cellpadding="3" cellspacing="0"><tr><td><script language="JavaScript" type="text/javascript" src="http://www.hav.com/include/chat/chatPage_test.js"></script><table align="center" width="150" border="0" cellpadding="2" cellspacing="0"><tr><td>
<applet codebase="http://www.hav.com/chat3/pushListener/"
	archive="havChatPushListener30.jar"
	code="havChatPushListener.havChatPushListener.class"
	width="146" height="53"
>
<param name="MAYSCRIPT" value="MAYSCRIPT"/>
<param name="cabbase" value="http://www.hav.com/chat3/pushListener/"/>
<param name="cabinets" value="havChatPushListener30.cab"/>
<param name="host" value="www.hav.com"/>
<param name="chatServerPort" value="5639"/>
<param name="handshakeTimeout" value="45"/>
<param name="quietTimeout" value="10"/>
<param name="roomIdRequested" value="invite"/>
<param name="appletBgColor"    value="#90908e"/>

<param name="buttonBgColor"    value="#ffffff"/>
<param name="buttonFgColor"    value="#336699"/>
<param name="showTheApplet" value="false"/>
<param name="showIfAdmin" value="true"/>
<param name="ChatImageURL" value="../images/havchat-livesupport-online-a.jpg"/>
<param name="chatURL" value="http://www.hav.com/include/chat/chatIMCPopupFrameset_test.htm?auto=1&amp;roomID=invite"/>
<param name="chatURLTarget" value="havChatImWinFrameset"/>
<param name="imRequestChatFunction" value="imRequestChatFunction"/>
<param name="mailImageURL" value="../images/havchat-livesupport-offline.jpg"/>
<param name="mailURL" value="mailto:techSupport@hav.com"/>
<param name="infoBgColor"    value="#ffffff"/>
<param name="infoFgColor"    value="#333333"/>
<param name="cardBgColor" value="#336699"/>
<param name="cardFgColor" value="#ffffff"/>
<param name="cardButtonBgColor" value="#ffffff"/>
<param name="cardButtonFgColor" value="#336699"/>
<param name="userName" value="x_.216.62.136"/>
</applet></td></tr></table>
</td></tr></table>
</div>-->

<br/><br/>
<center><strong><i>... Just for Fun ...</i></strong></center>
<img src="/images/funStuffMap2.jpg" border="0" width="163" height="332" alt="Just some favorite fun stuff based on hav.com visitor history" ismap="ismap" usemap="#funStuffMap2" />

<br/><br/>
<table border="0" cellpadding="0" cellspacing="0" width="85%"><tr><td style="font-size: 9pt;"><center>Copyright &copy; 1994-2014<br/>by hav.Software and<br/>Horace "Kicker" Vallas.<br/>All Rights Reserved.</center></td></tr></table>

<br/>

</div>

<!-- === close left sidebar div === -->

<havhide>

<map name="funStuffMap2" id="funStuffMap1">
<area shape="rect" alt="TennisPensacola.com - the official online hub for tennis in Pensacola, Fl." coords="8,6,157,116" href="http://www.tennispensacola.com/" target="_blank" title="TennisPensacola.com - the official online hub for tennis in Pensacola, Fl." />
<area shape="rect" alt="Kick-kick the  Australian Shepherd - Welcome the newest Vallas pack member" coords="8,257,76,328" href="http://www.hav.com/kick-kick.htm" title="Kick-kick the  Australian Shepherd - Welcome the newest Vallas pack member" />
<area shape="rect" alt="In memory of Bear Dudley Vallas, a Long Time and Faithful Companion" coords="87,257,156,329" href="http://www.hav.com/bear/" title="In memory of Bear Dudley Vallas, a Long Time and Faithful Companion" />
<area shape="rect" alt="ndex of Kicker's Classical Guitar Videos" coords="6,154,80,255" href="http://www.hav.com/junk/" title="ndex of Kicker's Classical Guitar Videos" />
<area shape="rect" alt="Bonsai Information, sites and icons etc." coords="85,154,160,255" href="http://www.hav.com/trees.htm" title="Bonsai Information, sites and icons etc." />
<area shape="rect" alt="Horace's Horror - just why is it so hard to test software?" coords="6,120,160,153" href="http://www.hav.com/horror.html" title="Horace's Horror - just why is it so hard to test software?" />
<area shape="default" nohref="nohref" alt="" />
</map>
<!--<map name="funStuffMap1" id="funStuffMap1">
<area shape="rect" alt="TennisPensacola.com - the official online hub for tennis in Pensacola, Fl." coords="8,6,157,116" href="http://www.tennispensacola.com/" target="_blank" title="TennisPensacola.com - the official online hub for tennis in Pensacola, Fl." />
<area shape="rect" alt="Kick-kick the  Australian Shepherd - Welcome the newest Vallas pack member" coords="8,125,76,191" href="http://www.hav.com/kick-kick.htm" title="Kick-kick the  Australian Shepherd - Welcome the newest Vallas pack member" />
<area shape="rect" alt="In memory of Bear Dudley Vallas, a Long Time and Faithful Companion" coords="87,125,156,191" href="http://www.hav.com/bear/" title="In memory of Bear Dudley Vallas, a Long Time and Faithful Companion" />
<area shape="rect" alt="ndex of Kicker's Classical Guitar Videos" coords="6,198,80,296" href="http://www.hav.com/junk/" title="ndex of Kicker's Classical Guitar Videos" />
<area shape="rect" alt="Bonsai Information, sites and icons etc." coords="85,198,154,295" href="http://www.hav.com/trees.htm" title="Bonsai Information, sites and icons etc." />
<area shape="rect" alt="Horace's Horror - just why is it so hard to test software?" coords="6,298,160,330" href="http://www.hav.com/horror.html" title="Horace's Horror - just why is it so hard to test software?" />
<area shape="default" nohref="nohref" alt="" />
</map>
-->

<div id="navMenuContainer" class="navDiv">

<ul id="havtopmenu"  class="havNavMenu"><li><a href="/index.htm"><img src="/images/hav-home.gif" width="24" height="18" border="0" alt="hav.com Home" vspace="0" hspace="0"/></a></li>
	
<li><a href="/products/index.htm" rel="havsubmenu1">Products</a></li>
<!--<li><a href="/index.htm" rel="havsubmenu2">Services</a></li>-->
<li><a href="/demos/index.htm" rel="havsubmenu3">Demos</a></li>
<li><a href="/about/index.htm" rel="havsubmenu4">About</a></li>
</ul>

<script type="text/javascript">
ddlevelsmenu.setup("havtopmenu", "topbar");
</script>

</div>

<!--Home Drop Down Menu HTML-->



<!--Proucts Drop Down Menu HTML-->

<ul id="havsubmenu1" class="havsubmenustyle">
<li><a href="/products/havChat/index.htm">havChat</a></li>
<li><a href="/products/havBpNet/havbpnet.htm">havBpNet++</a></li>
<li><a href="/products/havBpNet/havbpnetj.htm">havBpNet:J</a></li>
<li><a href="/products/havFmNet/havfmnet.htm">havFmNet++</a></li>
<li><a href="/products/havFmNet/havfmnetJ.htm">havFmNet:J</a></li>
<li><a href="/products/havBpNet/demo.htm">havETT</a></li>
</ul>


<!--Services Drop Down Menu HTML

<ul id="havsubmenu2" class="havsubmenustyle">
<li><a href="/index.htm">Development</a></li>
<li><a href="/index.htm">Quality Deployment</a></li>
</ul>
-->


<!--Demos Drop Down Menu HTML-->

<ul id="havsubmenu3" class="havsubmenustyle">
<li><a href="/java/">Java</a></li>
<li><a href="/index.htm">Javascript</a></li>
<li><a href="/index.htm">JSP/Servlets</a>
	<ul>
	<li><a href="http://www.hav.com/jsp/tclTag/">Tcl&nbsp;Tag&nbsp;set</a></li>
	<li><a href="http://www.hav.com/jsp/upsQuoteDemo/">Live&nbsp;UPS&nbsp;Quote&nbsp;Demo</a></li>
	<li><a href="http://www.hav.com/jspexamples/dbTestTomcat.jsp">BerkeleyDB&nbsp;Access&nbsp;Demo</a></li>
	<li><a href="http://www.hav.com/jspexamples/snoopJSPstyle.jsp">JSP&nbsp;Request&nbsp;Snoop</a></li>
	</ul>
</li>
<li><a href="/index.htm">NeoWebScript</a>
	<ul>
	<li><a href="/neoscript/gd/">GD&nbsp;Graphics</a>
		<ul>
		<li><a href="/neoscript/gd/default.htm">Simple&nbsp;Strip&nbsp;Chart</a></li>
		<li><a href="/neoscript/gd/varichart.htm">Averaged&nbsp;Strip&nbsp;Chart</a></li>
		<li><a href="/neoscript/gd/gauge.htm">Gauge</a></li>
		<li><a href="http://www.hav.com/customicons.htm">Customizable&nbsp;Icons</a></li>
		<li><a href="/neoscript/gd/fonts.htm">GD&nbsp;Fonts</a></li>
		<li><a href="/neoscript/gd/blur.htm">Weak&nbsp;Blur</a></li>
		</ul>
	</li>
	<li><a href="http://www.hav.com/neoscript/imagemap/">Homegrown&nbsp;Imagemap</a></li>
	<li><a href="http://www.hav.com/guestbook/read.htm">Guestbook</a></li>
	<li><a href="http://www.hav.com/neoscript/timezone/tznonets.htm">Static&nbsp;World&nbsp;clock</a></li>
	</ul>
</li>
</ul>


<!--About Drop Down Menu HTML-->

<ul id="havsubmenu4" class="havsubmenustyle">
<li><a href="/about/index.htm">Who is <i>hav.Software</i></a></li>
<li><a href="/about/services.htm">Services &amp; Clients</a></li>
<li><a href="/about/contact.htm">Contact info</a></li>
<li><a href="/about/validation.htm">Compliance &amp; Validation</a></li>
</ul>

<br/><br/>&nbsp;
</havhide>
</div>



</body>
</html>++++++++++++++++++++<Over>++++++++++++++++++++
====================<http://www.jurikres.com/catalog/ms_bcel.htm>====================
<!DOCTYPE HTML PUBLIC "-//SQ//DTD HTML 2.0 HoTMetaL + extensions//EN"> 
 
<HTML>
 
<HEAD>
<TITLE>Braincel Neural Network Add-In</TITLE>
<LINK HREF="../about/mainserv.cfm#webmaster"> 
</HEAD>
 
<BODY BACKGROUND="../gifs/blueback.gif" BGCOLOR="000060" TEXT="FFFFFF"
 LINK="00FFFF" VLINK="C0C0C0" ALINK="00FFFF">
<P><A NAME="top"></A></P>
<P><A HREF="../DIRECTRY.MAP"><IMG ALIGN="RIGHT" SRC="../gifs/directry.gif"
ISMAP="ISMAP" BORDER="0" WIDTH="204" HEIGHT="99"></A></P>
<P ALIGN="LEFT"><A HREF="../index.htm"><IMG ALIGN="LEFT"
SRC="../gifs/homebutn.gif" ALT="Home Page" WIDTH="47" HEIGHT="40"
BORDER="0"></A><IMG SRC="../gifs/sml_logo.gif" ALT="JRC Logo" WIDTH="238"
HEIGHT="36" BORDER="0" ALIGN="TOP"></P>
<BR>
<BR>
<BR>
<P ALIGN="CENTER"><IMG ALIGN="MIDDLE" SRC="../gifs/colorbr3.gif" ALT="colorbar"
WIDTH="757" HEIGHT="2"></P>
<CENTER>
<P><FONT FACE="Arial" SIZE="+3">BrainCel</FONT></P>
</CENTER>
<CENTER>
<FONT FACE="Arial">Neural Net Module for Microsoft Excel</FONT> 
</CENTER>
<P ALIGN="CENTER"><IMG SRC="../gifs/colorbr3.gif" ALT="colorbar" WIDTH="757"
HEIGHT="2"></P>
<BR>
<TABLE WIDTH="250" ALIGN="LEFT">
<TR>
<TD><TABLE WIDTH="170" BGCOLOR="#000000" ALIGN="LEFT">
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153" ALIGN="LEFT"><FONT FACE="Arial" COLOR="#00FF00"
SIZE="-1">AmiBroker</FONT></TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">eSignal</FONT> 
</TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">Extreme
Charts</FONT></TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"><IMG SRC="../gifs/checkico.gif" BORDER="0"
WIDTH="16" HEIGHT="16"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">Generic DLL</FONT>
</TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">Investor
R/T</FONT> </TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">Market
Delta</FONT> </TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">MATLAB</FONT> 
</TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">MetaStock</FONT> 
</TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"><IMG SRC="../gifs/checkico.gif" BORDER="0"
WIDTH="16" HEIGHT="16"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">Microsoft
Excel</FONT></TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">MultiCharts</FONT>
</TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">NeoTicker</FONT> 
</TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">NeuroShell</FONT> 
</TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">NinjaTrader</FONT>
</TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">Tradecision</FONT>
</TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">Trade
Navigator</FONT> </TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00" SIZE="-1">Trading
Solutions</FONT></TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"><IMG SRC="../gifs/checkico.gif" BORDER="0"
WIDTH="16" HEIGHT="16"></TD>
<TD WIDTH="153"><FONT FACE="Arial" COLOR="#00FF00"
SIZE="-1">TradeStation</FONT></TD>
</TR>
<TR>
<TD WIDTH="39" ALIGN="CENTER"></TD>
<TD WIDTH="153"></TD>
</TR>
</TABLE>
</TD>
</TR>
</TABLE>
<BR>
<P><IMG ALIGN="BASELINE" SRC="../gifs/blueball.gif" WIDTH="16"
HEIGHT="16"><FONT FACE="Bookman Old Style" SIZE="+2"> Create leading
indicators</FONT></P>
<P><IMG ALIGN="BASELINE" SRC="../gifs/blueball.gif" WIDTH="16"
HEIGHT="16"><FONT FACE="Bookman Old Style" SIZE="+2"> Create complex concurrent
indicators</FONT></P>
<P><IMG ALIGN="BASELINE" SRC="../gifs/blueball.gif" WIDTH="16"
HEIGHT="16"><FONT FACE="Bookman Old Style" SIZE="+2"> Apply to stocks, futures,
... almost anything</FONT></P>
<BR>
<BR>
<TABLE WIDTH="60%">
<TR>
<TD><P><FONT FACE="Bookman Old Style">Want to create a formula that responds to
data a certain way, but have no idea how? Here's a way to create them without
performing any calculations yourself. <BR>
<BR>
Neural networks can be &quot;trained&quot; to analyze your data and
automatically manipulate hidden formulas to approximate your desired
responses.</FONT></P>
</TD>
</TR>
</TABLE>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<TABLE WIDTH="40%" BORDER="0" ALIGN="CENTER" BGCOLOR="#BCB449">
<TR>
<TD ALIGN="CENTER"><FONT FACE="Arial" SIZE="+1" COLOR="#000000">Why Neural
Networks?</FONT></TD>
</TR>
</TABLE>
<BR>
<BR>
<P><FONT FACE="Bookman Old Style">Neural networks are mathematical functions
that you &quot;train&quot; to covert input data to whatever you specify as
output data. For example, you may want to convert various technical indicators
into:</FONT></P>
<UL>
<LI><FONT FACE="Bookman Old Style"><A
HREF="../faq/trading.htm#modeling">leading</A> indicators.</FONT> </LI>
<LI><FONT FACE="Bookman Old Style">an assessment of whether or not a market
reversal is imminent.</FONT></LI>
<LI><FONT FACE="Bookman Old Style">a forecast of the MACD indicator a few bars
into the future.</FONT></LI>
</UL>
<P><FONT FACE="Bookman Old Style"> You may not know the math to perform such
calculations, but a Neural Net may be able to figure it out! Afterwards, you
can develop trading decisions based on the neural net's forecasts. </FONT></P>
<P><FONT FACE="Bookman Old Style"> The power of Braincel is well established.
In each of the following articles in Future's Magazine, Braincel was used to
build a leading indicator...</FONT></P>
<UL>
<LI><FONT FACE="Bookman Old Style"><I>Futures</I> Aug '94<IMG
SRC="../gifs/invisbar.gif" BORDER="0" WIDTH="30" HEIGHT="2">Training Neural
Nets for Intermarket Analysis</FONT></LI>
<LI><FONT FACE="Bookman Old Style"><I>Futures</I> May '96<IMG
SRC="../gifs/invisbar.gif" BORDER="0" WIDTH="30" HEIGHT="2">How to Predict
Tomorrow's Indicators Today</FONT> </LI>
<LI><FONT FACE="Bookman Old Style"><I>Futures</I> Dec '96<IMG
SRC="../gifs/invisbar.gif" BORDER="0" WIDTH="30" HEIGHT="2">Nets and
Intermarket Divergence</FONT></LI>
</UL>
<BR>
<BR>
<BR>
<BR>
<TABLE WIDTH="40%" BORDER="0" ALIGN="CENTER" BGCOLOR="#BCB449">
<TR>
<TD ALIGN="CENTER"><FONT FACE="Arial" SIZE="+1" COLOR="#000000">Braincel in
Microsoft Excel</FONT></TD>
</TR>
</TABLE>
<BR>
<BR>
<IMG ALIGN="RIGHT" SRC="../gifs/bcelchrt.gif" WIDTH="468" HEIGHT="293"> 
<P><FONT FACE="Bookman Old Style">BRAINCEL is an Excel add-in that enhances
your forecasts with the power of neural networks. Amazingly, no knowledge of
neural net math or statistics is required to use it. You supply both input data
and desired output (target) data in rows and columns on a spreadsheet. Braincel
will study this data and after training, you supply it with new input data and
Braincel will give you answers that will astound you! It's almost as simple to
use as Excel's standard regression tool, but more powerful!</FONT></P>
<P><FONT FACE="Bookman Old Style">To create a neural net, you simply set up
your data on an Excel spreadsheet, select a few options, and the neural net
will automatically train and test itself. The neural net models you produce
will work in ...</FONT></P>
<UL>
<LI><FONT FACE="Bookman Old Style"> Microsoft Excel</FONT></LI>
<LI><FONT FACE="Bookman Old Style">TradeStation 2000<I>i</I>, 6, 7, 8</FONT> 
</LI>
<LI><FONT FACE="Bookman Old Style">your own custom programming in C and
VB.</FONT></LI>
</UL>
<P><FONT FACE="Bookman Old Style">Novices enjoy the guidance from standard
dialog boxes and programmers love the power in using Excel's VBA to automate
the more complex tasks that Braincel can perform. </FONT></P>
<P><FONT FACE="Bookman Old Style">Braincel's advanced features include . .
.</FONT></P>
<UL>
<LI><FONT FACE="Bookman Old Style">Jurik's own algorithms. It does not use the
old Back-Propagation formula</FONT> </LI>
<LI><FONT FACE="Bookman Old Style">Built-in Excel charts show training
progress</FONT></LI>
<LI><FONT FACE="Bookman Old Style">Tables for total control over learning
parameters</FONT></LI>
<LI><FONT FACE="Bookman Old Style">Automatic search for the best net
architecture</FONT></LI>
<LI><FONT FACE="Bookman Old Style">Automatic search for the best selection of
inputs</FONT></LI>
</UL>
<BR>
<BR>
<BR>
<BR>
<TABLE WIDTH="40%" BORDER="0" ALIGN="CENTER" BGCOLOR="#BCB449">
<TR>
<TD ALIGN="CENTER"><FONT FACE="Arial" SIZE="+1" COLOR="#000000">Braincel in
TradeStation</FONT></TD>
</TR>
</TABLE>
<BR>
<BR>
<P><FONT FACE="Bookman Old Style">Here's how to create a neural net for use in
TradeStation 2000<I>i</I>, 6, 7 or 8...</FONT></P>
<OL>
<LI><FONT FACE="Bookman Old Style">Plot your favorite technical indicators on
your TradeStation screen.</FONT></LI>
<LI><FONT FACE="Bookman Old Style">Export the chart data to a .CSV file and
open the file with Microsoft Excel.</FONT></LI>
<LI><FONT FACE="Bookman Old Style">Create and train a neural net on the
spreadsheet data and save it.</FONT></LI>
<LI><FONT FACE="Bookman Old Style">Write code in Easy Language to apply the
neural net to market data. You can access your neural net within INDICATORS,
SYSTEMS, and USER FUNCTIONS. (Example Easy Language code is provided.) </FONT> 
</LI>
<LI><FONT FACE="Bookman Old Style">Run your new studies on real-time or
end-of-day data!</FONT> </LI>
</OL>
<BR>
<P><FONT FACE="Bookman Old Style">For a working example of how you can access
from TradeStation Easy Language, a neural net built by Braincel, look for
<A HREF="../freebies/mainfree.htm#omega5">&quot;Braincel - Neural Net
Example&quot;</A> in our &quot;Free Stuff&quot; section.</FONT></P>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<TABLE WIDTH="40%" BORDER="0" ALIGN="CENTER" BGCOLOR="#BCB449">
<TR>
<TD ALIGN="CENTER"><FONT FACE="Arial" SIZE="+1" COLOR="#000000">Software
Requirements</FONT></TD>
</TR>
</TABLE>
<BR>
<BR>
<P><FONT FACE="Bookman Old Style">To run Braincel and create neural nets, you
need one of the following:</FONT></P>
<TABLE WIDTH="700" BGCOLOR="#006C6C" ALIGN="CENTER">
<TR>
<TD WIDTH="290"><FONT FACE="Bookman Old Style"><A
HREF="../compatible/excel.htm">Microsoft EXCEL</A> 97, 2000, 2002</FONT></TD>
<TD WIDTH="294"><FONT FACE="Bookman Old Style">running on Windows 2000, XP,
Vista</FONT></TD>
</TR>
<TR>
<TD WIDTH="290"><FONT FACE="Bookman Old Style"><A
HREF="../compatible/excel.htm">Microsoft EXCEL</A> 2003</FONT></TD>
<TD WIDTH="294"><FONT FACE="Bookman Old Style">running on Windows 2000,
XP</FONT></TD>
</TR>
</TABLE>
<P><FONT FACE="Bookman Old Style">You can run your newly created neural nets on
market data in <A HREF="../compatible/excel.htm">Microsoft EXCEL</A> and
<A HREF="../compatible/tradestation.htm">TradeStation</A></FONT></P>
<BR>
<P><FONT FACE="Bookman Old Style">LATEST VERSION:
<IMG SRC="../gifs/invisbar.gif" BORDER="0" WIDTH="30" HEIGHT="2">Braincel
3.7<IMG SRC="../gifs/invisbar.gif" BORDER="0" WIDTH="30" HEIGHT="2">released
JUNE 2005 </FONT> </P>
<P><FONT FACE="Bookman Old Style">To determine if you have the latest version,
proceed as follows:</FONT></P>
<OL>
<LI><FONT FACE="Bookman Old Style">Open the folder on your hard drive
containing the installed Braincel files</FONT> </LI>
<LI><FONT FACE="Bookman Old Style">Right click on the file
&quot;BRAINCEL.XLL&quot; and select PROPERTIES</FONT> </LI>
<LI><FONT FACE="Bookman Old Style">For version 3.7, the &quot;Created&quot;
date should read 27-MAY-2005.</FONT> </LI>
<LI><FONT FACE="Bookman Old Style">To order the latest version, go to our
<A
 HREF="http://va.eftsecure.net/eftcart/products.asp?M_id=101000127780">SHOPPING
CART</A> and select the &quot;Upgrade/Replace&quot; department.</FONT></LI>
</OL>
<BR>
<BR>
<BR>
<BR>
<P><A NAME="questions"></A></P>
<TABLE WIDTH="40%" BORDER="0" ALIGN="CENTER" BGCOLOR="#BCB449">
<TR>
<TD ALIGN="CENTER"><FONT FACE="Arial" SIZE="+1" COLOR="#000000">Additional
Information</FONT></TD>
</TR>
</TABLE>
<BR>
<BR>
<TABLE WIDTH="100%">
<TR>
<TD WIDTH="24%"><IMG SRC="../gifs/portrait3.jpg" BORDER="0" WIDTH="165"
HEIGHT="180"></TD>
<TD WIDTH="76%"><P><FONT FACE="Bookman Old Style">Download our
<A HREF="../faq/reports.htm#top">TECHNICAL REPORTS.</A>
<IMG SRC="../gifs/graybar.gif" ALIGN="BOTTOM" WIDTH="30" HEIGHT="2"> Lots of
charts and comparisons.</FONT></P>
<P><FONT FACE="Bookman Old Style">Read our
<A HREF="../quotes/feedback_2.htm#braincel">CUSTOMER's LETTERS.</A>
<IMG SRC="../gifs/graybar.gif" ALIGN="BOTTOM" WIDTH="30" HEIGHT="2"> See what
others have to say.</FONT></P>
<P><FONT FACE="Bookman Old Style">Read our <A HREF="../faq/faq_ama.htm#top">
FREQUENTLY ASKED QUESTIONS.</A>
<IMG SRC="../gifs/graybar.gif" ALIGN="BOTTOM" WIDTH="30" HEIGHT="2"> For
answers to technical issues.</FONT></P>
<P><A HREF="../sales/mainsale.cfm#top"><IMG SRC="../gifs/how2order.gif"
ALT="How to order" BORDER="0" WIDTH="165" HEIGHT="34" ALIGN="ABSMIDDLE"></A>
<IMG SRC="../gifs/graybar.gif" ALIGN="BOTTOM" WIDTH="30" HEIGHT="2"><FONT
FACE="Bookman Old Style"> Prices, order form, specific questions to ask.
</FONT></P>
</TD>
</TR>
</TABLE>
<BR>
<P><FONT FACE="Bookman Old Style"><IMG ALIGN="BOTTOM"
SRC="../gifs/exclama2.gif" WIDTH="18" HEIGHT="20">SPECIAL REPORT:
BACK-PERCOLATION</FONT></P>
<P><FONT FACE="Bookman Old Style">Braincel uses a newer algorithm called
Back-Percolation instead of the rather old formula called Back-Propagation.
Back-Percolation was designed by Mark Jurik and this report explains his
philosophy of the successful algorithm. For a real visual treat, the report
displays parameter learning trajectories of amazing symmetry and beauty. --- Go
to <A HREF="../faq/reports.htm#top">TECHNICAL REPORTS</A> page.</FONT></P>
<BR>
<BR>
<BR>
<CENTER>
<A HREF="../catalog/ms_bcel.htm#top"><IMG ALIGN="BOTTOM"
SRC="../gifs/pagetop.gif" BORDER="0" WIDTH="36" HEIGHT="36"></A> 
</CENTER>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<P><A HREF="../legal/copywrit.htm#copyright">[ Copyright ]</A><A
HREF="../about/mainserv.cfm#webmaster"> [ WebMaster ]</A> </P>
</BODY>
</HTML>
 
++++++++++++++++++++<Over>++++++++++++++++++++
